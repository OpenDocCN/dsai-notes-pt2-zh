- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: æœªåˆ†ç±»'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:40:31'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable
    Metrics and Diverse Prompt Templates'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2408.13006](https://ar5iv.labs.arxiv.org/html/2408.13006)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Hui Wei^(*â€ 1,2) Shenghua He^(*2), Tian XiaÂ², Andy WongÂ², Jingyang Lin^(â€ 2,3),
    Mei HanÂ²
  prefs: []
  type: TYPE_NORMAL
- en: Â¹ University of Massachusetts Amherst, MA, United States
  prefs: []
  type: TYPE_NORMAL
- en: Â² PAII Inc., CA, United States
  prefs: []
  type: TYPE_NORMAL
- en: Â³ University of Rochester, NY, United States
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Alignment approaches such as RLHF and DPO are actively investigated to align
    large language models (LLMs) with human preferences. Commercial large language
    models (LLMs) like GPT-4 have been recently employed to evaluate and compare different
    LLM alignment approaches. These models act as surrogates for human evaluators
    due to their promising abilities to approximate human preferences with remarkably
    faster feedback and lower costs. This methodology is referred to as *LLM-as-a-judge*.
    However, concerns regarding its reliability have emerged, attributed to LLM judgesâ€™
    biases and inconsistent decision-making. Previous research has sought to develop
    robust evaluation frameworks for assessing the reliability of LLM judges and their
    alignment with human preferences. However, the employed evaluation metrics often
    lack adequate explainability and fail to address the internal inconsistency of
    LLMs. Additionally, existing studies inadequately explore the impact of various
    prompt templates when applying LLM-as-a-judge methods, which leads to potentially
    inconsistent comparisons between different alignment algorithms. In this work,
    we systematically evaluate LLM judges on *alignment tasks* (e.g. summarization)
    by defining evaluation metrics with improved theoretical interpretability and
    disentangling reliability metrics with LLM internal inconsistency. We develop
    a framework to evaluate, compare, and visualize the reliability and alignment
    of LLM judges to provide informative observations that help choose LLM judges
    for alignment tasks. In the experiments, we examine the effect of diverse prompt
    templates on LLM-judge reliability and also demonstrate our developed framework
    by evaluating and comparing various LLM judges on two common alignment datasets.
    Our results indicate *a significant impact of prompt templates on LLM judge performance*,
    as well as *a mediocre alignment level between the tested LLM judges and human
    evaluators*. The code of the developed framework is available at [https://github.com/shenghh2015/llm-judge-eval](https://github.com/shenghh2015/llm-judge-eval).
  prefs: []
  type: TYPE_NORMAL
- en: '^*^*footnotetext: Equal contribution; corresponding authors:^â€ ^â€ footnotetext:
    ðŸ–‚Â [huiwei@cs.umass.edu](mailto:huiwei@cs.umass.edu),Â Â ðŸ–‚Â [shenghh2015@gmail.com](mailto:shenghh2015@gmail.com)^â€ ^â€ footnotetext:
    Work was done when H. WeiÂ &Â J. Lin were interns at PAII Inc.'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Alignment techniques, such as RLHFÂ [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4)], DPOÂ [[5](#bib.bib5), [6](#bib.bib6)], and N-best sampling (or
    rejection sampling)Â [[7](#bib.bib7), [8](#bib.bib8)] have been actively investigated
    to align large language models (LLMs) with human preferences. These studies typically
    use human-based pairwise evaluations as the gold standard for method evaluation
    and comparison. During the evaluation procedure, the human judge is presented
    with a question and two associated responses generated by different LLMs and is
    tasked with evaluating which response is preferred based on general criteria,
    such as helpfulness, honesty, and harmlessness [[9](#bib.bib9)]. A win-rate metric
    is subsequently calculated based on these judgment results and utilized to assess
    which LLM more effectively aligns with human preferences. Despite its high effectiveness,
    human-based evaluation is notably slow and costly [[10](#bib.bib10)], rendering
    it generally impractical for rapid assessments and advancements in alignment methodologies.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, commercial LLMs, such as GPT-4 [[11](#bib.bib11)] and GPT-3.5-turbo
    [[12](#bib.bib12)], have been widely used as the surrogates for human evaluators,
    referred to as LLM-as-a-judge, to perform pairwise evaluation on numerous LLM
    alignment tasks, such as summarizationÂ [[5](#bib.bib5), [13](#bib.bib13), [14](#bib.bib14),
    [15](#bib.bib15)] as well as single- or multi-turn conversationsÂ [[5](#bib.bib5),
    [13](#bib.bib13), [15](#bib.bib15), [16](#bib.bib16), [17](#bib.bib17), [18](#bib.bib18)].
    Since these commercial models have already been extensively trained with advanced
    alignment techniques [[11](#bib.bib11), [4](#bib.bib4)], they are promisingly
    capable of approximating human preferences [[5](#bib.bib5), [19](#bib.bib19)].
  prefs: []
  type: TYPE_NORMAL
- en: While it is plausible to utilize these models as surrogates for human judges,
    biases and inconsistencies are frequently observed in their judgment results,
    despite the application of various bias-mitigation techniques[[20](#bib.bib20),
    [5](#bib.bib5)]. This necessitates a systematic investigation of LLM judge reliability
    and alignment with human preferences in the context of LLM alignment tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Previous studies have evaluated LLM-as-a-judge methods on various language
    generation tasksÂ [[21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24),
    [25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29),
    [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32), [19](#bib.bib19), [33](#bib.bib33),
    [34](#bib.bib34), [35](#bib.bib35)]. However, these studies encounter three main
    limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lacking clear theoretical interpretability for bias definitions (e.g. position
    bias and length bias).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Not considering internal inconsistencies (i.e., system noise) by assuming LLM
    judges make deterministic decisions across identical experiments.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Concentrating on evaluating various LLMs, while the effects of prompt templates
    have been insufficiently examined.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In this study, we aim to address these limitations and advance the systematic
    evaluation of LLM judges on LLM alignment tasks. The contributions of our study
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We enhance the theoretical explainability of current evaluation metrics to assess
    LLM-judge alignment with human preferences and their reliability, including accuracy,
    position bias, and length bias by defining them within a unified evaluation framework.
    Then, we provide practical ways to compute these metrics. In addition, we explicitly
    define and measure the LLM internal self-inconsistency as *flipping noise*, and
    mitigate its impact on position bias and length bias. *To the best of our knowledge,
    this is the first study to address this issue.*
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We develop a framework to evaluate, compare, and visualize the alignment and
    reliability of LLM judges, with a general and flexible design, allowing for application
    across a wide range of LLMs and user-defined prompt templates. We utilize *a wide
    range of up-to-date prompt templates with diverse formats* to investigate their
    impact on LLM judge performance. We also demonstrate our proposed framework through
    experiments to evaluate and compare various LLM judges comprehensively and consistently.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our results indicate *a significant impact of prompt templates on LLM judge
    performance*, as well as *a mediocre alignment level between the tested LLM judges
    and human evaluators*. This underscores the need for a thorough and careful comparison
    of various LLMs and prompt templates before employing the LLM-as-a-judge methodology.
    Additionally, it highlights the importance of human evaluation for achieving more
    precise comparisons between different LLM alignment systems, provided that time
    and cost constraints are manageable. Finally, we present the ranking results of
    all the LLM judges for both datasets to facilitate the selection of the most appropriate
    judge.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Background and Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we define the pairwise evaluation task conducted by both human
    and LLM judges, and examine self-inconsistencies and biases inherent in LLM judges.
    Additionally, we review relevant literature on position bias and length bias.
  prefs: []
  type: TYPE_NORMAL
- en: Human-based Pairwise Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given a set of $N$, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: LLM-based Pairwise Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLM-judges are subjected to the same evaluation procedures as human judges.
    However, compared with humans, LLMs are more sensitive to instructions (i.e. prompt
    templates) [[36](#bib.bib36), [30](#bib.bib30)]. Thus, in this study, we define
    an LLM-judge as *the combination of a specific LLM and a particular prompt template*.
  prefs: []
  type: TYPE_NORMAL
- en: LLM-Judge Self-Inconsistency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Previous studies have observed that LLM judges [[25](#bib.bib25), [36](#bib.bib36)]
    may produce inconsistent judgments even when presented with identical prompts.
    This is caused by non-greedy decoding strategies leveraged by LLMs, such as top-p
    and top-k, which generate non-deterministic outputs. The non-deterministic level
    is controlled by the parameter *temperature*. In this work, we refer to these
    inconsistencies as self-inconsistency or system noise in LLM judges and model
    and quantify them using the concept of *flipping noise*.
  prefs: []
  type: TYPE_NORMAL
- en: LLM-Judge Bias
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Position bias and length bias are two predominant biases frequently observed
    in LLM judges utilizing commercial LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Positioin bias refers to LLM-judgeâ€™s systematic preference for a specific response
    position (the first or the second in the pairwise evaluation task). Wang et. alÂ [[21](#bib.bib21)]
    and Lee et. alÂ [[37](#bib.bib37)] observed the position bias when using GPT-4Â [[11](#bib.bib11)]
    and PaLM 2Â [[38](#bib.bib38)] as the judge for the pairwise comparison between
    candidate LLMs. They measured the position bias by the ratio of inconsistent decisions
    made by LLM judges after swapping response positions. Differently, studies from
    Liusie et. alÂ [[39](#bib.bib39)] and Zheng et. al Â [[19](#bib.bib19)] quantified
    the position bias as the disparity of selection probabilities after reversing
    the response order.
  prefs: []
  type: TYPE_NORMAL
- en: Length bias refers to LLM judgeâ€™s systematic preference for longer responses
    even when their qualities are similar to shorter versions. Saito et al.Â [[22](#bib.bib22)]
    observed a discrepancy between LLMs and human preferences regarding response length.
    They employed accuracy parityâ€”related to human preferences for longer responses
    and shorter responsesâ€”to measure relative length bias.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to the aforementioned studies, our work examines the impact of LLM
    judge self-inconsistency on the evaluation of both position bias and length bias,
    and provides methodologies for disentangling these biases from flipping noise.
    We also offer a theoretical analysis and validation of our defined metrics to
    enhance their interpretability. Additionally, we investigate the relationship
    between these biases and accuracy, revealing significant insights. Finally, our
    study includes an extensive evaluation of position and length bias across a diverse
    set of LLM judges with various prompt templates.
  prefs: []
  type: TYPE_NORMAL
- en: Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we introduce our evaluation metrics and framework for assessing
    LLM-judge biases and alignment with human preferences. We begin by introducing
    notations. Then we present our proposed evaluation metrics with enhanced theoretical
    interpretability of accuracy, flipping noise, position bias, and length bias,
    as well as provide practical methods to compute them. Subsequently, we describe
    our developed framework for the systematic evaluation of LLM-as-a-judge methods.
  prefs: []
  type: TYPE_NORMAL
- en: Notations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let $\mathcal{D}\!=\!\{h_{n}|n\!=\!1\!\ldots\!N\}$ for brevity when the context
    is clear.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Accuracy Accuracy measures the alignment level of LLM judges with human preferences.
    Formally, we denote $\theta_{l}$ is the human preference defined in the last section.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the literature Â [[19](#bib.bib19), [21](#bib.bib21)], there are two versions
    of the accuracy metric: $\text{Acc}_{\text{both}}$ as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{Acc}_{\text{both}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\text{Acc}_{\text{random}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $y_{\text{random}}$ with the probability of 0.5.
  prefs: []
  type: TYPE_NORMAL
- en: Flipping Noise As mentioned in the background section, LLM outputs are generally
    non-deterministic, which can lead to inconsistent judgments even when the LLM
    judge is presented with the identical data case $h=(x,y_{c},y_{r})$.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we model the LLM judgeâ€™s decision as a *binary* variable that
    indicates if the human-preferred response $y_{c}$ is also selected by the LLM
    judge. When an inconsistent decision occurs for the same data case, we refer to
    it as â€œflippingâ€ the decision to the opposite value. This behavior is quantified
    using the concept of *flipping noise*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, for a data case $(x,y_{c},y_{r})$ as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle Z$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $q$.
  prefs: []
  type: TYPE_NORMAL
- en: Position Bias (PB) As a reminder, we define accuracy based on two sets of responses
    with reversed orders, namely $(y_{c},y_{r})$. To assess accuracy, we require the
    LLM judge to be evaluated in both orders. Here, we employ the same setting to
    define position bias.
  prefs: []
  type: TYPE_NORMAL
- en: First of all, we define $p\left[X=1|(y_{c},y_{r})\right]$ as the probability
    that the LLM-judgeâ€™s result aligns with the human selection when the order is
    reversed. It is important to note these two probabilities are essentially *accuracy*
    metrics for the two response positions.
  prefs: []
  type: TYPE_NORMAL
- en: We first consider a special case where the LLM judge makes a *fully consistent
    decision* (i.e. $q=0$.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, if the LLM-judge *exhibits position bias favoring the first position
    over the second*, it will select $y_{c}$. The same rationale applies when the
    second position is preferred.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on these intuitions, we define position bias as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{PB}=p\left[X=1&#124;(y_{c},y_{r})\right]-p\left[X=1&#124;(y_{r},y_{c})\right]$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where the absolute value $\left|\text{PB}\right|$ measures the degree of position
    bias, with positive and negative values indicating preferences for the first and
    second positions, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we address the general case in which the LLM-judge *makes non-deterministic
    decisions and exhibits position bias*. Here, only noisy observation $Z$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p\left[X=1&#124;(y_{c},y_{r})\right]$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle p\left[X=1&#124;(y_{r},y_{c})\right]$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle q_{cr}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle q_{rc}$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $q_{cr}$ are the probabilities that the LLM judgeâ€™s decision is flipped
    for both response orders.
  prefs: []
  type: TYPE_NORMAL
- en: In AppendixÂ IV, we derive the above relationships, validate the position bias
    measurement based on de-noised accuracies, and provide a practical method for
    their computation.
  prefs: []
  type: TYPE_NORMAL
- en: Length Bias (LB) Previous studies have indicated that human evaluators exhibit
    the length bias when assessing responses [[19](#bib.bib19), [22](#bib.bib22)].
    If LLM judges are employed as surrogates for human judges, it is expected they
    have the same length bias in general. Thus, this study aims to measure the *relative*
    length bias of LLM-judges compared with human evaluators, rather than their absolute
    length bias. For brevity, we use â€œlength biasâ€ to refer to the *relative* length
    bias in the paper.
  prefs: []
  type: TYPE_NORMAL
- en: For each data case $(x,\!y_{c},\!y_{r})$ as the probability that the LLM-judgeâ€™s
    result align when the length relationship is reversed. Moreover, these two probabilities
    are defined within the same accuracy framework, analogous to the definition of
    position bias.
  prefs: []
  type: TYPE_NORMAL
- en: Following the same rationale as in the position bias section, we define length
    bias as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\displaystyle\text{LB}\!=p\left[X\!=\!1&#124;\Delta l\!></math> |  |'
  prefs: []
  type: TYPE_TB
- en: where $|\text{LB}|$ measures how significantly the LLM judge exhibits different
    length bias compared to human judges and the sign of LB indicates it biases more
    towards longer response or shorter responses than human judges, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'In cases where *flipping noise cannot be neglected*, analogous to the approach
    for position bias, we first compute accuracies from noisy observations $Z$ as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id=$$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle p\left[X=1&#124;\Delta l\leq 0\right]$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $$\displaystyle q_{\Delta l></math> |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle q_{\Delta l\leq 0}$ |  |'
  prefs: []
  type: TYPE_TB
- en: where <math id=$$, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: In Appendix IV, we derive the above relationships, validate the length bias
    measurement based on de-noised accuracies, and provide a practical method for
    their computation.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this study, we introduce an evaluation framework that integrates our proposed
    methods for computing metrics, including accuracy ($\text{Acc}_{\text{both}}$),
    position bias and length bias. Furthermore, a set of visualization tools is developed
    to facilitate the analysis and comparison of the reliability of various LLM judges
    and their alignment with human preferences.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline of the framework, as depicted in Fig.Â [1](#Sx3.F1 "Figure 1 â€£
    Evaluation Framework â€£ Methods â€£ Systematic Evaluation of LLM-as-a-Judge in LLM
    Alignment Tasks: Explainable Metrics and Diverse Prompt Templates"), is structured
    into four modular components: 1) *Data Sampler*, 2) *LLM Judges*, 3) *Metrics
    Computation*, and 4) *Metrics Visualization*. The functionality of each component
    is detailed as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/69dfa92b7f6bca69bd82e23f3d045d4a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: LLM-as-a-Judge Evaluation Framework'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data Sampler: When dealing with a large human preference dataset and a limited
    budget for using commercial LLM models, it becomes necessary to sample a manageable-size
    subset from the full dataset for LLM judge evaluation. Our framework employs a
    *stratified sampling strategy* to ensure that the subset maintains the same proportion
    of different conditions (e.g. length difference distribution) as the original
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM Judges: As defined in the Background section, an LLM judge refers to the
    combination of a particular LLM and a specific prompt template. Given an LLM judge,
    this module is responsible for generating textual judging decisions for each sampled
    data case and subsequently converting them into a binary outcome for metrics computation.
    This module allows the flexible creation of varied LLM judges by configuring different
    LLMs and prompt templates for evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Metrics Computation: This module computes alignment and reliability evaluation
    metrics (i.e. accuracy, position bias, and length bias) using the judging results
    from the LLM Judge module and the human preference labels provided by the dataset,
    based on the computational methods described in the Method section.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Metrics Visualization: This module visualizes both the *individual* computed
    metrics and their *inter-relationships*, providing comprehensive insights for
    comparing LLM judges and aiding in the selection of the most suitable LLM judge
    for specific LLM-alignment tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we introduce the datasets and LLM judges used in our experiments,
    as well as how to leverage them to compute the aforementioned evaluation metrics.
    The corresponding results and findings will be presented in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Data Selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We demonstrate our evaluation framework using two datasets that are commonly
    used to evaluate LLM alignment algorithms: TL;DR summarization dataset [[40](#bib.bib40),
    [41](#bib.bib41)] and HH-RLHF-Helpfulness dataset [[3](#bib.bib3)]. Both datasets
    contain a prompt (a post for the summarization dataset and a conversation history
    between humans and LLM assistants for HH-RLHF dataset) with two responses generated
    by distinct LLMs for each sample. Also, human preference labels are available
    to indicate which response is more aligned with human preference. Both datasets
    have already been partitioned into train and test sets by the authors in the original
    studies.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our experiment, it is highly time-consuming and expensive to evaluate LLM
    judges on all the data cases of both datasets (143,356 for summarization and 124,243
    for HH-RLHF-Helpfulness), so we randomly sample a subset from each dataset to
    perform all the evaluation experiments. Compared with the summarization dataset,
    the HH-RLHF-Helpfulness dataset has a much smaller test set (6,240 vs. 70,228),
    thus, we select a subset from the TL;DR summarization *test* set following the
    previous study [[5](#bib.bib5)] and a subset from the *entire* HH-RLHF-Helpfulness
    dataset. Moreover, multiple data cases may share the same prompt (post or conversation
    history) with distinct response pairs. To make our collected datasets as diverse
    as possible, only one pair is kept for this prompt and others are removed. After
    this step, each unique prompt corresponds to only one unique answer pair. Then
    we randomly sample the prompts and their associated responses five times without
    replacement, resulting in five non-overlapping splits. Since measuring length
    bias requires dividing all the data cases into two conditions: whether longer
    responses are preferred by humans or not, we leverage the stratified sampling
    to preserve the same ratio of these two conditions as in the entire dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, both datasets used in our experiments contain 200 distinct samples
    for each split, which results in 1000 samples in total. The summarization and
    HH-RLHF-Helpfulness datasets have a stratified ratio (# of humans prefer longer
    responses: # of humans prefer shorter responses) of 115:85 and 111:89 respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: LLM Judges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our LLM judges integrate a range of up-to-date and varied commercial large language
    models and prompt templates. Particularly, we assess GPT-4o, GPT-4o-mini and GPT-3.5-turbo
    with 8 templates on the summarization dataset and 10 templates on the HH-RLHF-Helpfulness
    dataset. Thus, there are $3\!\times\!8\!=\!24$ LLM judges for the HH-RLHF-Helpfulness
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4o is the most advanced model which has the latest checkpoint on 05/13/2024,
    GPT-4o-mini is the most cost-efficient model, while GPT-3.5-turbo is from the
    last OpenAI model generation and serves as the baseline in our experiments. Our
    preliminary studies suggest that GPT-4o exhibits comparable performance to GPT-4
    in judging decision-making, but at a cost that is 4 to 6 times lower. Due to limited
    budget, we select GPT-4o for evaluation over GPT-4 from the list of *commercial*
    LLMs, despite GPT-4 being the most widely-used model in LLM alignment studies
    before the release of GPT-4o.
  prefs: []
  type: TYPE_NORMAL
- en: 'All the considered templates were actually used in the pairwise comparison
    tasks to evaluate different LLM alignment algorithms by papers of the last (2023)
    and this year (2024) and we make sure they all have *dissimilar* prompt formats.
    Furthermore, since our evaluation datasets have no â€œtiedâ€ labels from human annotations,
    which indicate two responses are equally preferred, we remove sentences from the
    prompt templates which allow LLM judges to select â€œtiedâ€ labels. Please refer
    to Table [1](#Sx4.T1 "Table 1 â€£ LLM Judges â€£ Experiments â€£ Systematic Evaluation
    of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt
    Templates") and [2](#Sx4.T2 "Table 2 â€£ LLM Judges â€£ Experiments â€£ Systematic Evaluation
    of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt
    Templates") for the complete list of all the prompt templates used in this study
    and the corresponding papers from which they are derived. Example templates for
    each dataset are provided in Appendix I.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Template Name | Paper Link |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Publication Time &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| guo | [Guo et al.](https://arxiv.org/pdf/2402.04792) [[42](#bib.bib42)] |
    02/2024 |'
  prefs: []
  type: TYPE_TB
- en: '| scheurer | [Scheurer et al.](https://arxiv.org/pdf/2303.16755) [[43](#bib.bib43)]
    | 02/2024 |'
  prefs: []
  type: TYPE_TB
- en: '| liusie | [Liusie et al.](https://arxiv.org/pdf/2307.07889) [[39](#bib.bib39)]
    | 02/2024 |'
  prefs: []
  type: TYPE_TB
- en: '| wang | [Wang et al.](https://arxiv.org/pdf/2401.06080) [[13](#bib.bib13)]
    | 01/2024 |'
  prefs: []
  type: TYPE_TB
- en: '| zheng | [Zheng et al.](https://arxiv.org/pdf/2306.05685)[[19](#bib.bib19)]
    | 12/2023 |'
  prefs: []
  type: TYPE_TB
- en: '| wu | [Wu et al.](https://arxiv.org/pdf/2307.03025) [[44](#bib.bib44)] | 11/2023
    |'
  prefs: []
  type: TYPE_TB
- en: '| chen | [Chen et al.](https://arxiv.org/pdf/2304.00723) [[45](#bib.bib45)]
    | 09/2023 |'
  prefs: []
  type: TYPE_TB
- en: '| rafailov | [Rafailov et al.](https://arxiv.org/pdf/2305.18290) [[5](#bib.bib5)]
    | 07/2023 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Prompt templates used for the TL;DR summarization dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Template Name | Paper Link |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Publication Time &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| cheng | [Cheng et al.](https://arxiv.org/pdf/2311.08045) [[45](#bib.bib45)]
    | 06/2024 |'
  prefs: []
  type: TYPE_TB
- en: '| zeng | [Zeng et al.](https://arxiv.org/pdf/2312.07401) [[46](#bib.bib46)]
    | 04/2024 |'
  prefs: []
  type: TYPE_TB
- en: '| shen | [Shen et al.](https://arxiv.org/pdf/2403.07708v2) [[47](#bib.bib47)]
    | 02/2024 |'
  prefs: []
  type: TYPE_TB
- en: '| guo | [Guo et al.](https://arxiv.org/pdf/2402.04792) [[42](#bib.bib42)] |
    02/2024 |'
  prefs: []
  type: TYPE_TB
- en: '| zheng | [Zheng et al.](https://arxiv.org/pdf/2306.05685)[[19](#bib.bib19)]
    | 12/2023 |'
  prefs: []
  type: TYPE_TB
- en: '| mehta | [Mehta et al.](https://arxiv.org/pdf/2312.00267)[[48](#bib.bib48)]
    | 12/2023 |'
  prefs: []
  type: TYPE_TB
- en: '| wu | [Wu et al.](https://arxiv.org/pdf/2307.03025) [[44](#bib.bib44)] | 11/2023
    |'
  prefs: []
  type: TYPE_TB
- en: '| bai | [Bai et al.](https://arxiv.org/pdf/2306.04181) [[49](#bib.bib49)] |
    11/2023 |'
  prefs: []
  type: TYPE_TB
- en: '| rafailov | [Rafailov et al.](https://arxiv.org/pdf/2305.18290) [[5](#bib.bib5)]
    | 07/2023 |'
  prefs: []
  type: TYPE_TB
- en: '| xu | [Xu et al.](https://arxiv.org/pdf/2305.18201) [[50](#bib.bib50)] | 05/2023
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Prompt templates used for the HH-RLHF-Helpfulness dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Temperature Parameter Selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Temperature parameter determines how deterministic LLM outputs are, which might
    affect the performance of LLM-judges. However, few previous studies that use LLMs
    as judges explicitly explain how and why they choose the temperature in their
    experiments. In this study, we assess the impact of the temperature parameter
    on the self-consistency (i.e. 1-flipping probability $q$) and accuracies of the
    large language models, which helps to select the temperature before evaluating
    LLM-judge performance using other metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'In detail, we investigate five temperature settings: $0.0$ across all the samples.'
  prefs: []
  type: TYPE_NORMAL
- en: Through preliminary experiments, we found the impact of different temperatures
    is the same to the *same* LLM with *different* prompt templates, so in the large-scale
    experiments, only the prompt templates from DPO paper [[5](#bib.bib5)] are utilized
    for both datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics Computation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we introduce how to compute the flipping probability $q$ of
    flipping noise and other evaluation metrics in our experiments.
  prefs: []
  type: TYPE_NORMAL
- en: To compute the flipping probability, same as selecting the temperature parameter,
    we let LLM judges select their preferred response from each sample repeatedly
    for $K=5$ are computed as well to calculate de-noised length bias.
  prefs: []
  type: TYPE_NORMAL
- en: To compute accuracy, position bias, and length bias, we compute each metric
    on all the splits ($S=5$). In the result, we report the mean and standard deviation
    of LLM judge performances across these five splits.
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we present the results and findings regarding temperature,
    accuracy, position bias, and length bias in our experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Temperature
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [3](#Sx5.T3 "Table 3 â€£ Temperature â€£ Results â€£ Systematic Evaluation
    of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt
    Templates") contains the results of self-consistent rate (SCR) and accuracy with
    various temperatures. The self-consistent rate, given by $1-q$ as defined in Eq.
    [1](#Sx3.E1 "In Evaluation Metrics â€£ Methods â€£ Systematic Evaluation of LLM-as-a-Judge
    in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates"), measures
    the probability that the LLMâ€™s judgments are consistent across identical inputs.
    Since different LLMs show the same trend on both datasets, we only include GPT-4o
    here for the demonstration. Results regarding other LLMs are included in Appendix
    III.'
  prefs: []
  type: TYPE_NORMAL
- en: From the table, we observe that *higher temperatures result in lower self-consistency
    for both positions, while accuracy is not significantly affected by temperatures*.
    Specifically, even when the temperature is set to 0.0, complete self-consistency
    (i.e. SCR=1.0) remains unachievable. Furthermore, *self-consistency varies with
    different positions*, thereby necessitating the separate measurement of flipping
    probabilities related to flipping noise associated with each position.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we aim to demonstrate the generalizability of our evaluation framework
    by employing a value that is not a special case, such as 0.0\. Thus, *we select
    0.1 as the temperature in all of our experiments*, which has the highest level
    of self-consistency compared with higher temperatures.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | TL;DR Summarization | HH-RLHF-Helpfulness |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Temperature |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SCR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($y_{c}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SCR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($y_{r}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Acc &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($\text{Acc}_{\text{both}}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SCR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($y_{c}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SCR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($y_{r}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Acc &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($\text{Acc}_{\text{both}}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0.0 | 0.977 | 0.971 | 0.665 (0.003) | 0.974 | 0.967 | 0.573 (0.005) |'
  prefs: []
  type: TYPE_TB
- en: '| 0.1 | 0.973 | 0.967 | 0.666 (0.004) | 0.966 | 0.957 | 0.575 (0.005) |'
  prefs: []
  type: TYPE_TB
- en: '| 0.3 | 0.963 | 0.956 | 0.668 (0.003) | 0.950 | 0.944 | 0.574 (0.005) |'
  prefs: []
  type: TYPE_TB
- en: '| 0.5 | 0.953 | 0.949 | 0.663 (0.003) | 0.942 | 0.926 | 0.579 (0.009) |'
  prefs: []
  type: TYPE_TB
- en: '| 0.7 | 0.946 | 0.927 | 0.657 (0.000) | 0.934 | 0.914 | 0.577 (0.006) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Self-consistent rate (SCR) and accuracy (Acc) of tested temperatures
    for the TL;DR summarization and HH-RLHF-Helpfulness datasets. Results are demonstrated
    using GPT-4o and prompt templates from the DPO paperÂ [[5](#bib.bib5)].'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figure [2](#Sx5.F2 "Figure 2 â€£ Accuracy â€£ Results â€£ Systematic Evaluation of
    LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt
    Templates") shows accuracies ($\text{Acc}_{\text{both}}$) of LLM judges on both
    datasets, where identical colors represent the same prompt template within the
    same dataset (the same coloring rule is applied to all the result figures except
    for FigureÂ [5](#Sx5.F5 "Figure 5 â€£ Position Bias â€£ Results â€£ Systematic Evaluation
    of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt
    Templates")). As we can see, different LLM judges have distinct accuracy, which
    means they have varied alignment levels with human preferences. Also, it demonstrates
    *the performance of an LLM judge is highly sensitive to prompt templates.*'
  prefs: []
  type: TYPE_NORMAL
- en: Notably, several LLM judges have very low accuracies ($\text{Acc}_{\text{both}}\!<\!0.2$).
    Thus, it is significantly important to carefully evaluate and compare different
    LLM judges before actually using them to evaluate LLM alignment algorithms. Moreover,
    we find that all the accuracies on both datasets are below 0.7, which shows the
    mediocre alignment level and demonstrates that human evaluation is necessary to
    precisely compare different LLM alignment systems.
  prefs: []
  type: TYPE_NORMAL
- en: Compared with GPT-3.5-turbo, both GPT-4o and GPT-4o-mini have higher accuracies
    no matter which prompt template is used. It demonstrates that the superior internal
    capacities of recent LLMs, compared to older versions, are independent of the
    prompt templates used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [3](#Sx5.F3 "Figure 3 â€£ Accuracy â€£ Results â€£ Systematic Evaluation of
    LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt
    Templates") shows accuracy ($\text{Acc}_{\text{random}}$ is used to demonstrate
    the relationship between accuracy and position bias as well as length bias in
    the following sections.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/84259cf9a970ec0c0e18aa21d58de61e.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) TL;DR summarization
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/448071267d8a1e1a28c1bdd9ccbd66d4.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) HH-RLHF-Helpfulness
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Accuracy ($\text{Acc}_{\text{both}}$) for TL;DR the summarization
    and HH-RLHF-Helpfulness datasets. Please refer to TableÂ [1](#Sx4.T1 "Table 1 â€£
    LLM Judges â€£ Experiments â€£ Systematic Evaluation of LLM-as-a-Judge in LLM Alignment
    Tasks: Explainable Metrics and Diverse Prompt Templates") and TableÂ [2](#Sx4.T2
    "Table 2 â€£ LLM Judges â€£ Experiments â€£ Systematic Evaluation of LLM-as-a-Judge
    in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates") for
    details on the prompt templates used in all the result figures throughout the
    Results section. *The results suggest the high sensitivity of LLM-judge accuracy
    to prompt templates and mediocre level of alignment to human judges.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/159f965677eda1b507b14ae92508ecb8.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) TL;DR summarization
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/faee4d4d8d2b42798d2c479902a988fc.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) HH-RLHF-Helpfulness
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Accuracy ($\text{Acc}_{\text{random}}$*.'
  prefs: []
  type: TYPE_NORMAL
- en: Position Bias
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Position biases of all the LLM judges are shown in Figure [4](#Sx5.F4 "Figure
    4 â€£ Position Bias â€£ Results â€£ Systematic Evaluation of LLM-as-a-Judge in LLM Alignment
    Tasks: Explainable Metrics and Diverse Prompt Templates"), where positive values
    mean judges prefer the first position while negative values mean judges prefer
    the second position.'
  prefs: []
  type: TYPE_NORMAL
- en: We observe that varying prompt templates can cause the same large language model
    to exhibit preferential biases towards different positions. Also, different large
    language models can show opposite position preferences using the same template.
    Thus, *the position bias/preference depends on both the LLMs themselves and the
    prompt templates*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we illustrate the relationship between accuracy and the absolute
    value of position bias in Figure [5](#Sx5.F5 "Figure 5 â€£ Position Bias â€£ Results
    â€£ Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable
    Metrics and Diverse Prompt Templates"). Here, absolute position bias reflects
    the bias level without specifying the preferred position. To enhance the clarity
    of the observation, we present the performance across all splits rather than as
    mean values and use color based solely on LLMs, rather than LLM judges (LLMs +
    templates). *Our evaluation results reveal a significant negative correlation
    between accuracy and the level of position bias.* The underlying reasons for this
    correlation need further investigation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9ff0081568c518d89352903541eac9d2.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) TL;DR summarization
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/83c165506f8699a05fff354c1f3cc5b0.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) HH-RLHF-Helpfulness
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Position bias for the TL;DR summarization and HH-RLHF-Helpfulness
    datasets. *Our results suggest that the position bias/preference depends on both
    the LLMs themselves and the prompt templates.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/55673ff01ce1d94c683c4ae41b70dfe4.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) TL;DR summarization
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dd62c3d603f8f140aa72786ba08b32d0.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) HH-RLHF-Helpfulness
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Absolute position bias vs. accuracy for TL;DR summarization and HH-RLHF-Helpfulness
    dataset. *Our results reveal a significant negative correlation between accuracy
    and the level of position bias.*'
  prefs: []
  type: TYPE_NORMAL
- en: Length Bias
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figure [6](#Sx5.F6 "Figure 6 â€£ Length Bias â€£ Results â€£ Systematic Evaluation
    of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt
    Templates") displays the (relative) length bias of all the judges across both
    datasets. Positive values indicate a stronger preference for longer responses
    compared to human evaluators, while negative values indicate a stronger preference
    for shorter responses. The figure shows that *all the tested LLM judges have stronger
    preferences for longer responses compared to human judges*, which is consistent
    with previous studies [[19](#bib.bib19), [22](#bib.bib22)]. Furthermore, compared
    to the summarization task, LLM judges exhibit a greater degree of length bias
    on the multi-turn conversation task (HH-RLHF-Helpfulness dataset).'
  prefs: []
  type: TYPE_NORMAL
- en: Generally, longer responses tend to provide more detailed and comprehensive
    answers, which are more favored by humans compared to shorter ones [[51](#bib.bib51),
    [52](#bib.bib52)]. We suspect that the length bias results from the over-alignment
    of commercial models with human preferences.
  prefs: []
  type: TYPE_NORMAL
- en: Different from position bias, length bias does not have a negative correlation
    with accuracy (please refer to Appendix III for their relationship).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/da50dbf4d1ec59bd36691aa5e8d56d92.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) TL;DR summarization
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cb0214127f81c9eb75505be709b134e0.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) HH-RLHF-Helpfulness
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Length bias for TL;DR summarization and HH-RLHF-Helpfulness dataset.
    *The results suggest that all the tested LLM judges have stronger preferences
    for longer responses compared to human judges, which might result from the over-alignment
    of the commercial models with human preferences.*'
  prefs: []
  type: TYPE_NORMAL
- en: Rankings of Prompt Templates and LLM Judges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To facilitate selecting appropriate LLM judges for each LLM-alignment dataset
    (i.e. TL;DR summarization and HH-RLHF-Helpfulness), we rank all the LLM judges
    (LLM + template) for each dataset, as well as all the prompt templates for each
    LLM used in our study (i.e. GPT-3.5-turbo, GPT-4o and GPT-4-mini) separately.
    We display top five templates or LLM judges and report their $\text{Acc}_{\text{both}}$,
    position bias and length bias. Please see Table [4](#Sx5.T4 "Table 4 â€£ Rankings
    of Prompt Templates and LLM Judges â€£ Results â€£ Systematic Evaluation of LLM-as-a-Judge
    in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates") for
    the ranking results of all the LLM judges for the TL;DR summarization dataset
    and Table [5](#Sx5.T5 "Table 5 â€£ Rankings of Prompt Templates and LLM Judges â€£
    Results â€£ Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable
    Metrics and Diverse Prompt Templates") for the HH-RLHF-Helpfulness dataset. The
    ranking results related to separate LLMs are provided in Appendix III.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, *the rankings are based on $\text{Acc}_{\text{both}}$*, which
    is because:'
  prefs: []
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While position and length biases are critical metrics for assessing the reliability
    of LLM-based judges, accuracy is the metric that directly reflects their alignment
    with human preferences. Accuracy can be viewed as a measure of the reliability
    of the â€œwin rateâ€ derived from LLM-judge evaluation results in practice.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the primary study, our findings indicate that $\text{Acc}_{\text{both}}$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| TL;DR Summarization (All LLMs) |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Template / LLM | $\text{Acc}_{\text{both}}$ | Position Bias | Length Bias
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| rafailov / gpt-4o | 0.667 (0.011) | 0.737 (0.014) | 0.022 (0.015) | 0.197
    (0.031) |'
  prefs: []
  type: TYPE_TB
- en: '| chen / gpt-4o | 0.658 (0.028) | 0.734 (0.029) | -0.081 (0.023) | 0.117 (0.055)
    |'
  prefs: []
  type: TYPE_TB
- en: '| guo / gpt-4o | 0.655 (0.011) | 0.733 (0.024) | -0.140 (0.014) | 0.193 (0.038)
    |'
  prefs: []
  type: TYPE_TB
- en: '| liusie / gpt-4o | 0.632 (0.023) | 0.724 (0.019) | -0.154 (0.041) | 0.084
    (0.056) |'
  prefs: []
  type: TYPE_TB
- en: '| rafailov / gpt-4o-mini | 0.631 (0.014) | 0.701 (0.023) | -0.060 (0.027) |
    0.162 (0.038) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Rankings of LLM judges (LLM+prompt template) on the TL;DR summarization
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| HH-RLHF-Helpfulness (All LLMs) |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Template / LLM | $\text{Acc}_{\text{both}}$ | Position Bias | Length Bias
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| guo / gpt-4o | 0.618 (0.040) | 0.694 (0.030) | -0.005 (0.013) | 0.135 (0.075)
    |'
  prefs: []
  type: TYPE_TB
- en: '| xu / gpt-4o | 0.610 (0.025) | 0.702 (0.019) | 0.086 (0.010) | 0.029 (0.057)
    |'
  prefs: []
  type: TYPE_TB
- en: '| bai / gpt-4o | 0.603 (0.027) | 0.697 (0.014) | 0.034 (0.017) | 0.255 (0.067)
    |'
  prefs: []
  type: TYPE_TB
- en: '| guo / gpt-4o-mini | 0.602 (0.036) | 0.681 (0.030) | -0.028 (0.026) | 0.294
    (0.059) |'
  prefs: []
  type: TYPE_TB
- en: '| rafailov / gpt-4o-mini | 0.594 (0.014) | 0.657 (0.019) | 0.047 (0.020) |
    0.463 (0.039) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Rankings of LLM judges (LLM+prompt template) on HH-RLHF-Helpfulness
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Limitations and Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we discuss the limitations in this study and outline the directions
    for future research.
  prefs: []
  type: TYPE_NORMAL
- en: First, our current studies focus on commercial LLMs (e.g., GPT-3.5, GPT-4o,
    and GPT-4o-mini) rather than open-source LLMs. This is due to the fact that commercial
    LLMs remain the predominant choice in LLM-as-a-judge methods used in LLM alignment
    studies, making their reliability evaluation more urgent compared to open-source
    LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Second, our evaluation studies concentrate on LLM-as-a-judge methods, although
    open-source reward models (RMs) also hold the potential to serve as judges on
    LLM alignment tasks [[53](#bib.bib53)]. Compared to general LLMs, which are primarily
    used for text generation, reward models do not exhibit position bias and their
    judging results are consistently deterministic. Nevertheless, the accuracy and
    length bias metrics and evaluation framework we have introduced are still applicable
    for assessing â€œRM-as-a-judgeâ€ methods.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, while our evaluation results confirm the presence of position and length
    biases, which are commonly observed in LLM alignment studies, the accuracy of
    the metrics has not been thoroughly investigated. Comprehensive validation of
    the defined evaluation metrics would require extensive human-based assessments,
    which are not available in this study.
  prefs: []
  type: TYPE_NORMAL
- en: In the future, we plan to expand our evaluation study to include powerful open-source
    LLM models, such as Llama 3.1Â [[54](#bib.bib54)], and open-source reward models,
    such as Nemotron-4-340B-RewardÂ [[53](#bib.bib53)], across a broader range of datasets,
    including RewardBenchÂ [[55](#bib.bib55)]. Additionally, we will advance the evaluation
    of the introduced reliability metrics once human assessment resources become available.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this study, we introduced a set of reliability metrics, including accuracy,
    position bias, and length bias, with improved theoretical interpretability. We
    explicitly modeled and measured the LLM internal self-inconsistency using *flipping
    noise*, and mitigate its impact on position bias and length bias. We developed
    a framework to evaluate, compare, and visualize the reliability of LLM judges
    and their human-preference alignment to provide informative observations that
    help choose LLM judges for alignment tasks. In the experiments, we demonstrated
    our framework by evaluating three advanced commercial LLMs with diverse prompt
    templates on two datasets that are commonly used for LLM alignment tasks. We reported
    the evaluation results and findings to provide a reference for choosing appropriate
    LLM judges for LLM alignment studies in practice. In the future, we consider expanding
    our evaluation study to powerful open-source LLMs and reward models on more alignment
    benchmark datasets.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg
    Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Long Ouyang, Jeffrey Wu, XuÂ Jiang, Diogo Almeida, Carroll Wainwright, Pamela
    Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, etÂ al. Training
    language models to follow instructions with human feedback. Advances in neural
    information processing systems, 35:27730â€“27744, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
    Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, etÂ al. Training a helpful
    and harmless assistant with reinforcement learning from human feedback. arXiv
    preprint arXiv:2204.05862, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    etÂ al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Rafael Rafailov, Archit Sharma, Eric Mitchell, ChristopherÂ D Manning, Stefano
    Ermon, and Chelsea Finn. Direct preference optimization: Your language model is
    secretly a reward model. Advances in Neural Information Processing Systems, 36,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Weizhe Yuan, RichardÂ Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar,
    Jing Xu, and Jason Weston. Self-rewarding language models. arXiv preprint arXiv:2401.10020,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Afra Amini, Tim Vieira, and Ryan Cotterell. Variational best-of-n alignment.
    arXiv preprint arXiv:2407.06057, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] PierÂ Giuseppe Sessa, Robert Dadashi, LÃ©onard Hussenot, Johan Ferret, Nino
    Vieillard, Alexandre RamÃ©, Bobak Shariari, Sarah Perrin, Abe Friesen, Geoffrey
    Cideron, etÂ al. Bond: Aligning llms with best-of-n distillation. arXiv preprint
    arXiv:2407.14622, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan,
    Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, etÂ al. A general language
    assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Aparna Elangovan, Ling Liu, Lei Xu, Sravan Bodapati, and Dan Roth. Considers-the-human
    evaluation framework: Rethinking human evaluation for generative large language
    models. arXiv preprint arXiv:2405.18638, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,
    FlorenciaÂ Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal
    Anadkat, etÂ al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] TomÂ B Brown. Language models are few-shot learners. arXiv preprint ArXiv:2005.14165,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Binghai Wang, Rui Zheng, LuÂ Chen, Yan Liu, Shihan Dou, Caishuang Huang,
    Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, etÂ al. Secrets of rlhf in large language
    models part ii: Reward modeling. arXiv preprint arXiv:2401.06080, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Shengyi Huang, Michael Noukhovitch, Arian Hosseini, Kashif Rasul, Weixun
    Wang, and Lewis Tunstall. The n+ implementation details of rlhf with ppo: A case
    study on tl; dr summarization. arXiv preprint arXiv:2403.17031, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Han Zhong, Guhao Feng, Wei Xiong, LiÂ Zhao, DiÂ He, Jiang Bian, and Liwei
    Wang. Dpo meets ppo: Reinforced token optimization for rlhf. arXiv preprint arXiv:2404.18922,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang,
    Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, etÂ al. Secrets of rlhf in large language
    models part i: Ppo. arXiv preprint arXiv:2307.04964, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Baolin Peng, Linfeng Song, YeÂ Tian, Lifeng Jin, Haitao Mi, and Dong Yu.
    Stabilizing rlhf through advantage model and selective rehearsal. arXiv preprint
    arXiv:2309.10202, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] YuÂ Zhu, Chuxiong Sun, Wenfei Yang, Wenqiang Wei, BoÂ Tang, Tianzhu Zhang,
    Zhiyu Li, Shifeng Zhang, Feiyu Xiong, Jie Hu, etÂ al. Proxy-rlhf: Decoupling generation
    and alignment in large language model with proxy. arXiv preprint arXiv:2403.04283,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu,
    Yonghao Zhuang, ZiÂ Lin, Zhuohan Li, Dacheng Li, Eric Xing, etÂ al. Judging llm-as-a-judge
    with mt-bench and chatbot arena. Advances in Neural Information Processing Systems,
    36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. From $r$: Your
    language model is secretly a q-function. arXiv preprint arXiv:2404.12358, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo
    Cao, QiÂ Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators.
    arXiv preprint arXiv:2305.17926, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Keita Saito, Akifumi Wachi, Koki Wataoka, and Youhei Akimoto. Verbosity
    bias in preference labeling by large language models. arXiv preprint arXiv:2310.10076,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Yann Dubois, BalÃ¡zs Galambosi, Percy Liang, and TatsunoriÂ B Hashimoto.
    Length-controlled alpacaeval: A simple way to debias automatic evaluators. arXiv
    preprint arXiv:2404.04475, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Arjun Panickssery, SamuelÂ R Bowman, and Shi Feng. Llm evaluators recognize
    and favor their own generations. arXiv preprint arXiv:2404.13076, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Lin Shi, Weicheng Ma, and Soroush Vosoughi. Judging the judges: A systematic
    investigation of position bias in pairwise comparative assessments by llms. arXiv
    preprint arXiv:2406.07791, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] AmanÂ Singh Thakur, Kartik Choudhary, VenkatÂ Srinik Ramayapally, Sankaran
    Vaidyanathan, and Dieuwke Hupkes. Judging the judges: Evaluating alignment and
    vulnerabilities in llms-as-judges. arXiv preprint arXiv:2406.12624, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Cheng-Han Chiang and Hung-yi Lee. Can large language models be an alternative
    to human evaluations? arXiv preprint arXiv:2305.01937, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang
    Zhu. G-eval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint
    arXiv:2303.16634, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu
    Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. Is chatgpt a good nlg evaluator? a preliminary
    study. arXiv preprint arXiv:2303.04048, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Lianghui Zhu, Xinggang Wang, and Xinlong Wang. Judgelm: Fine-tuned large
    language models are scalable judges, october 2023. URL http://arxiv. org/abs/2310.17631.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao
    Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, etÂ al. Pandalm: An automatic
    evaluation benchmark for llm instruction tuning optimization. arXiv preprint arXiv:2306.05087,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei
    Liu. Generative judge for evaluating alignment. arXiv preprint arXiv:2310.05470,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua
    Zhu, JosephÂ E Gonzalez, and Ion Stoica. From crowdsourced data to high-quality
    benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, AnastasiosÂ Nikolas Angelopoulos,
    Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, JosephÂ E Gonzalez,
    etÂ al. Chatbot arena: An open platform for evaluating llms by human preference.
    arXiv preprint arXiv:2403.04132, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Yann Dubois, ChenÂ Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani,
    Jimmy Ba, Carlos Guestrin, PercyÂ S Liang, and TatsunoriÂ B Hashimoto. Alpacafarm:
    A simulation framework for methods that learn from human feedback. Advances in
    Neural Information Processing Systems, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Rickard Stureborg, Dimitris Alikaniotis, and Yoshi Suhara. Large language
    models are inconsistent and biased evaluators. arXiv preprint arXiv:2405.01724,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard,
    Colton Bishop, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement
    learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Rohan Anil, AndrewÂ M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin,
    Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen,
    etÂ al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Adian Liusie, Potsawee Manakul, and MarkÂ JF Gales. Zero-shot nlg evaluation
    through pairware comparisons with llms. arXiv preprint arXiv:2307.07889, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Michael VÃ¶lske, Martin Potthast, Shahbaz Syed, and Benno Stein. Tl; dr:
    Mining reddit to learn automatic summarization. In Proceedings of the Workshop
    on New Frontiers in Summarization, pages 59â€“63, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea
    Voss, Alec Radford, Dario Amodei, and PaulÂ F Christiano. Learning to summarize
    with human feedback. Advances in Neural Information Processing Systems, 33:3008â€“3021,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe
    Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, etÂ al. Direct
    language model alignment from online ai feedback. arXiv preprint arXiv:2402.04792,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] JÃ©rÃ©my Scheurer, JonÂ Ander Campos, Tomasz Korbak, JunÂ Shern Chan, Angelica
    Chen, Kyunghyun Cho, and Ethan Perez. Training language models with language feedback
    at scale. arXiv preprint arXiv:2303.16755, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Minghao Wu and AlhamÂ Fikri Aji. Style over substance: Evaluation biases
    for large language models. arXiv preprint arXiv:2307.03025, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Pengyu Cheng, Yifan Yang, Jian Li, Yong Dai, and Nan Du. Adversarial preference
    optimization. arXiv preprint arXiv:2311.08045, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Dun Zeng, Yong Dai, Pengyu Cheng, Longyue Wang, Tianhao Hu, Wanshun Chen,
    Nan Du, and Zenglin Xu. On diversified preferences of large language model alignment,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Wei Shen, Xiaoying Zhang, Yuanshun Yao, Rui Zheng, Hongyi Guo, and Yang
    Liu. Improving reinforcement learning from human feedback using contrastive rewards.
    arXiv preprint arXiv:2403.07708, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Viraj Mehta, Vikramjeet Das, Ojash Neopane, Yijia Dai, Ilija Bogunovic,
    Jeff Schneider, and Willie Neiswanger. Sample efficient reinforcement learning
    from human feedback via active exploration. 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan
    Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, etÂ al. Benchmarking foundation models
    with language-model-as-an-examiner. Advances in Neural Information Processing
    Systems, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Fangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol Choi. A critical evaluation
    of evaluations for long-form question answering. arXiv preprint arXiv:2305.18201,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Kerry Hart and Anita Sarma. Perceptions of answer quality in an online
    technical question and answer forum. In Proceedings of the 7th International Workshop
    on Cooperative and Human Aspects of Software Engineering, pages 103â€“106, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] FÂ Maxwell Harper, Daphne Raban, Sheizaf Rafaeli, and JosephÂ A Konstan.
    Predictors of answer quality in online q&a sites. In Proceedings of the SIGCHI
    Conference on human factors in computing systems, pages 865â€“874, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Zhilin Wang, YiÂ Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel
    Egert, JimmyÂ J. Zhang, MakeshÂ Narsimhan Sreedhar, and Oleksii Kuchaiev. Helpsteer2:
    Open-source dataset for training top-performing reward models, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad
    Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, etÂ al.
    The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJÂ Miranda, BillÂ Yuchen
    Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, etÂ al. Rewardbench:
    Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] YiÂ Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and Ruifeng Xu. Exploring
    the use of large language models for reference-free text quality evaluation: An
    empirical study. arXiv preprint arXiv:2304.00723, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Dun Zeng, YongÂ Dai andPengyu Cheng, Tianhao Hu, Wanshun Chen, Nan Du,
    and Zenglin Xu. On diversified preferences of large language model alignment.
    CoRR, abs/2312.07401, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I. Prompt Templates in This Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Examples of Prompt Templates
  prefs: []
  type: TYPE_NORMAL
- en: (TL;DR Summarization Dataset)
  prefs: []
  type: TYPE_NORMAL
- en: Template from Rafailov et al.Â [[5](#bib.bib5)]
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following summaries does a better job of summarizing the most important
    points in the given forum post, without including unimportant or irrelevant details?
    A good summary is both precise and concise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Post: <post>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary A: <summary A>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary B: <summary B>'
  prefs: []
  type: TYPE_NORMAL
- en: 'FIRST provide a one-sentence comparison of the two summaries, explaining which
    you prefer and why. SECOND, on a new line, state only "A" or "B" to indicate your
    choice. Your response should use the format:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparison: <one-sentence comparison and explanation>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Preferred: <"A" or "B">'
  prefs: []
  type: TYPE_NORMAL
- en: Template from Wang et al.Â [[13](#bib.bib13)]
  prefs: []
  type: TYPE_NORMAL
- en: As a neutral observer, your task is to assess the responses provided by two
    TL;DR summarizations according to the same SUBREDDIT prompt shown below. Begin
    by comparing the two responses and provide a brief explanation. Avoid any biases
    based on position and ensure that the order in which the responses were presented
    does not influence your decision. Do not let the length of the responses influence
    your evaluation. Do not favor certain names of the assistants. Strive to be as
    objective as possible. You need to choose only one of the two answers and respond
    by either A or B.
  prefs: []
  type: TYPE_NORMAL
- en: '{prompt}'
  prefs: []
  type: TYPE_NORMAL
- en: A. {answer_a}
  prefs: []
  type: TYPE_NORMAL
- en: B. {answer_b}
  prefs: []
  type: TYPE_NORMAL
- en: Which one is better? A or B?
  prefs: []
  type: TYPE_NORMAL
- en: Examples of Prompt Templates
  prefs: []
  type: TYPE_NORMAL
- en: (HH-RLHF-Helpfulness Dataset)
  prefs: []
  type: TYPE_NORMAL
- en: Template from Rafailov et al.Â [[5](#bib.bib5)]
  prefs: []
  type: TYPE_NORMAL
- en: For the following query to a chatbot, which response is more helpful?
  prefs: []
  type: TYPE_NORMAL
- en: 'Query: {the user query}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Response A:'
  prefs: []
  type: TYPE_NORMAL
- en: '{either the test method or baseline}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Response B:'
  prefs: []
  type: TYPE_NORMAL
- en: '{the other response}'
  prefs: []
  type: TYPE_NORMAL
- en: 'FIRST provide a one-sentence comparison of the two responses and explain which
    you feel is more helpful. SECOND, on a new line, state only "A" or "B" to indicate
    which response is more helpful. Your response should use the format:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparison: <one-sentence comparison and explanation>'
  prefs: []
  type: TYPE_NORMAL
- en: 'More helpful: <"A" or "B">'
  prefs: []
  type: TYPE_NORMAL
- en: Template from Shen et al.Â [[47](#bib.bib47)]
  prefs: []
  type: TYPE_NORMAL
- en: Please act as an impartial judge and evaluate the quality of the responses provided
    by two AI assistants to the user question displayed below. You should choose the
    assistant that follows the userâ€™s instructions better and provides more tailored
    responses to the userâ€™s questions.
  prefs: []
  type: TYPE_NORMAL
- en: A helpful response should directly address the human questions without going
    off-topic. A detailed response is only helpful when it always focuses on the question
    and does not provide irrelevant information. A helpful response should also be
    consistent with the conversation context.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if the human is going to close the conversation, then a good response
    should tend to close the conversation, too, rather than continuing to provide
    more information. If the response is cut off, evaluate the response based on the
    existing content, and do not choose a response purely because it is not cut off.
    Begin your evaluation by comparing the two responses and provide a short explanation.
    Avoid any positional biases and ensure that the order in which the responses were
    presented does not influence your decision. Do not allow the length of the responses
    to influence your evaluation. Do not favor specific names of the assistants.
  prefs: []
  type: TYPE_NORMAL
- en: 'Be as objective as possible. After providing your explanation, output your
    final verdict by strictly following this format: [[A]] if assistant A is better,
    [[B]] if assistant B is better. Please make sure the last word is your choice.'
  prefs: []
  type: TYPE_NORMAL
- en: --User Question--
  prefs: []
  type: TYPE_NORMAL
- en: '{prompt}'
  prefs: []
  type: TYPE_NORMAL
- en: --The Start of Assistant Aâ€™s Answer--
  prefs: []
  type: TYPE_NORMAL
- en: '{response_1}'
  prefs: []
  type: TYPE_NORMAL
- en: --The End of Assistant Aâ€™s Answer--
  prefs: []
  type: TYPE_NORMAL
- en: --The Start of Assistant Bâ€™s Answer--
  prefs: []
  type: TYPE_NORMAL
- en: '{response_2}'
  prefs: []
  type: TYPE_NORMAL
- en: --The End of Assistant Bâ€™s Answer--
  prefs: []
  type: TYPE_NORMAL
- en: II. Human Preference Data Used in This Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Example from the TL;DR Summarization dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Post: "SUBREDDIT: r/relationship_advice'
  prefs: []
  type: TYPE_NORMAL
- en: 'TITLE: [17/m] in a sticky situation with her [17/f], my Asian parents, and
    the school administration'
  prefs: []
  type: TYPE_NORMAL
- en: 'POST: Over two years ago my girlfriend and I started dating in secret. We were
    in secret because my parents are (racist?) in the way that they only want me to
    date people from an Asian background like me, and she is white. Eventually, because
    our school is super small and rumors spread like crazy, the staff found out maybe
    about a year ago. We went and made sure they knew not to go to our parents, and
    they all agreed. Fast forward to now and the principal and guidance counselor
    have called my parents and spilled the entire story to them. They apparently even
    had to use generic words like "girlfriend and her mom" instead of saying names
    to get around privacy rules. After talking it over with some of our close friends,
    no one has any insight or heard of anything that could cause them to do this,
    and itâ€™s very uncharacteristic of them. My parents have told me that the school
    administration has said things such as, "Sheâ€™s in a lower social class, he can
    do better," "Sheâ€™s bringing his grades down" (I have a 4.0 GPA), etc. While my
    parents have also said things such as "Sheâ€™s white trash," and "Sheâ€™s a gold digger,"
    I know for sure that those are both incorrect. Heck, my parents havenâ€™t even spoken
    to her for more than 3 minutes. Now that the jig is up, Iâ€™m not sure what to do.
    Do I secretly talk to her and risk losing everything except food and school supplies
    for the next year if I get caught? Do I make arrangements to get back together
    in a year? Or Do I just give up and accept it as over? All of this stress is really
    getting to me, and I have no idea what the best choice is. Thanks for reading
    all of that, and giving me any feedback!'
  prefs: []
  type: TYPE_NORMAL
- en: 'TL;DR: â€™ Chosen Summary by Human Evaluators:'
  prefs: []
  type: TYPE_NORMAL
- en: '"[17/m] in a sticky situation with her [17/f], my Asian parents, and the school
    administration" Rejected Summary by Human Evaluators: "My parents found out my
    girlfriend and I are dating and the school administration has told them she is
    â€™white trashâ€™ and â€™a gold diggerâ€™"'
  prefs: []
  type: TYPE_NORMAL
- en: Example from the HH-RLHF-Helpfulness dataset
  prefs: []
  type: TYPE_NORMAL
- en: Single-turn Conversation Example
  prefs: []
  type: TYPE_NORMAL
- en: 'Conversation: "Human: What are the rules to basketball?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assistant: " Chosen Response by Human Evaluators: "Well, itâ€™s played by two
    teams of five people each. Each team tries to get the ball through the other teamâ€™s
    hoop. The team that scores more points than their opponent is the winner." Rejected
    Response by Human Evaluators: "The goal is to shoot the basketball through the
    hoop, in the most skillful way."'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-turn Conversation Example
  prefs: []
  type: TYPE_NORMAL
- en: 'Conversation: "Human: Have you heard of Santana?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assistant: Yes! "Evil Ways", and "Smooth", right?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Human: I like "Smooth". Have you heard of it?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assistant: Yes, itâ€™s a great song! I like how the vocal is nice and slow at
    first, then faster at the end. Itâ€™s one of my favorite Santana songs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Human: Someone else sang the song. Santana played guitar accompanying the song.
    Do you know the name of the singer?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assistant: Oh yeah, I know who sang "Smooth". Heâ€™s a great singer. His name
    is Chris Standring.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Human: Both of your responses were inaccurate. The singer who sang "Smooth"
    is Rob Thomas.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assistant: " Chosen Response by Human Evaluators: "Oh, Iâ€™m sorry. I didnâ€™t
    recognize Rob Thomas. "Smooth" is a great song. Did you know Rob Thomas was a
    cast member of the band One Tree Hill?" Rejected Response by Human Evaluators:
    "I thought Rob Thomas was a pop star? I thought he played guitar and drums."'
  prefs: []
  type: TYPE_NORMAL
- en: III. Additional Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Temperature Results
  prefs: []
  type: TYPE_NORMAL
- en: '|  | TL;DR Summarization | HH-RLHF-Helpfulness |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Temperature |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SCR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($y_{c}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SCR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($y_{r}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Acc &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($\text{Acc}_{\text{both}}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SCR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($y_{c}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SCR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($y_{r}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Acc &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($\text{Acc}_{\text{both}}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0.0 | 0.976 | 0.972 | 0.659 (0.002) | 0.973 | 0.973 | 0.585 (0.002) |'
  prefs: []
  type: TYPE_TB
- en: '| 0.1 | 0.972 | 0.968 | 0.660 (0.003) | 0.965 | 0.966 | 0.585 (0.003) |'
  prefs: []
  type: TYPE_TB
- en: '| 0.3 | 0.964 | 0.963 | 0.661 (0.006) | 0.947 | 0.944 | 0.586 (0.003) |'
  prefs: []
  type: TYPE_TB
- en: '| 0.5 | 0.954 | 0.951 | 0.655 (0.003) | 0.942 | 0.926 | 0.579 (0.009) |'
  prefs: []
  type: TYPE_TB
- en: '| 0.7 | 0.939 | 0.941 | 0.650 (0.004) | 0.924 | 0.916 | 0.578 (0.008) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Self-consistent rate (SCR) and accuracy (Acc) related to the tested
    temperatures for TL;DR summarization and HH-RLHF-Helpfulness datasets. Results
    are demonstrated using GPT-4o and the prompt template *chen* [[56](#bib.bib56)]
    for the summarization dataset and the template *zeng* [[57](#bib.bib57)] for the
    HH-RLHF-Helpfulness dataset, respectively. The conclusions are the same as those
    using prompt templates from the templates *rafailov* [[5](#bib.bib5)] for both
    datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | TL;DR Summarization | HH-RLHF-Helpfulness |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Temperature |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SCR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($y_{c}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SCR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($y_{r}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Acc &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($\text{Acc}_{\text{both}}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SCR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($y_{c}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SCR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($y_{r}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Acc &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($\text{Acc}_{\text{both}}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0.0 | 0.989 | 0.991 | 0.631 (0.001) | 0.987 | 0.990 | 0.589 (0.003) |'
  prefs: []
  type: TYPE_TB
- en: '| 0.1 | 0.986 | 0.985 | 0.630 (0.001) | 0.983 | 0.988 | 0.591 (0.003) |'
  prefs: []
  type: TYPE_TB
- en: '| 0.3 | 0.974 | 0.982 | 0.627 (0.003) | 0.970 | 0.968 | 0.593 (0.003) |'
  prefs: []
  type: TYPE_TB
- en: '| 0.5 | 0.972 | 0.978 | 0.629 (0.004) | 0.965 | 0.967 | 0.587 (0.003) |'
  prefs: []
  type: TYPE_TB
- en: '| 0.7 | 0.961 | 0.973 | 0.622 (0.003) | 0.960 | 0.957 | 0.585 (0.006) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Self-consistent rate (SCR) and accuracy (Acc) related to the tested
    temperatures for TL;DR summarization and HH-RLHF-Helpfulness datasets. Results
    are demonstrated using GPT-4o-mini and prompt templates *rafailov* [[5](#bib.bib5)]
    for both datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | TL;DR Summarization | HH-RLHF-Helpfulness |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Temperature |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SCR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($y_{c}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SCR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($y_{r}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Acc &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($\text{Acc}_{\text{both}}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SCR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($y_{c}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SCR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($y_{r}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Acc &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($\text{Acc}_{\text{both}}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0.0 | 0.948 | 0.936 | 0.554 (0.004) | 0.970 | 0.951 | 0.371 (0.003) |'
  prefs: []
  type: TYPE_TB
- en: '| 0.1 | 0.925 | 0.907 | 0.548 (0.008) | 0.964 | 0.948 | 0.369 (0.002) |'
  prefs: []
  type: TYPE_TB
- en: '| 0.3 | 0.876 | 0.856 | 0.538 (0.003) | 0.941 | 0.906 | 0.373 (0.006) |'
  prefs: []
  type: TYPE_TB
- en: '| 0.5 | 0.824 | 0.807 | 0.516 (0.008) | 0.925 | 0.889 | 0.375 (0.010) |'
  prefs: []
  type: TYPE_TB
- en: '| 0.7 | 0.780 | 0.772 | 0.498 (0.006) | 0.901 | 0.853 | 0.382 (0.008) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Self-consistent rate (SCR) and accuracy (Acc) related to the tested
    temperatures for the TL;DR summarization and HH-RLHF-Helpfulness datasets. Results
    are demonstrated using GPT-3.5-turbo and prompt templates *rafailov* [[5](#bib.bib5)]
    for both datasets. *GPT-3.5-turbo is much more sensitive to temperatures compared
    with GPT-4o and GPT-4o-mini.*'
  prefs: []
  type: TYPE_NORMAL
- en: Length Bias and Accuracy Relationship
  prefs: []
  type: TYPE_NORMAL
- en: 'Please refer to Figure [7](#Sx8.F7 "Figure 7 â€£ III. Additional Results â€£ Appendix
    â€£ Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable
    Metrics and Diverse Prompt Templates") for the relationship between length bias
    and accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d7e731719449c3a55109a4d611134427.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) TL;DR summarization
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1654d7638ce9373532ac1e5f7c614f9b.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) HH-RLHF-Helpfulness
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: Length bias vs. accuracy for the TL;DR summarization and HH-RLHF-Helpfulness
    datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: Rankings of Prompt Templates and LLM Judges
  prefs: []
  type: TYPE_NORMAL
- en: 'Please see the ranking results of prompt templates for separate LLMs (i.e.
    GPT-3.5-turbo, GPT-4o, GPT-4o-mini) in Table [9](#Sx8.T9 "Table 9 â€£ III. Additional
    Results â€£ Appendix â€£ Systematic Evaluation of LLM-as-a-Judge in LLM Alignment
    Tasks: Explainable Metrics and Diverse Prompt Templates") - [11](#Sx8.T11 "Table
    11 â€£ III. Additional Results â€£ Appendix â€£ Systematic Evaluation of LLM-as-a-Judge
    in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates") for
    TL;DR Summarization and Table [12](#Sx8.T12 "Table 12 â€£ III. Additional Results
    â€£ Appendix â€£ Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable
    Metrics and Diverse Prompt Templates") - [14](#Sx8.T14 "Table 14 â€£ III. Additional
    Results â€£ Appendix â€£ Systematic Evaluation of LLM-as-a-Judge in LLM Alignment
    Tasks: Explainable Metrics and Diverse Prompt Templates") for HH-RLHF-Helpfulness
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| TL;DR Summarization (GPT-3.5-turbo) |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Template | $\text{Acc}_{\text{both}}$ | Position Bias | Length Bias |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| guo | 0.566 (0.020) | 0.675 (0.019) | -0.047 (0.017) | 0.174 (0.039) |'
  prefs: []
  type: TYPE_TB
- en: '| rafailov | 0.547 (0.022) | 0.668 (0.040) | 0.049 (0.018) | 0.152 (0.045)
    |'
  prefs: []
  type: TYPE_TB
- en: '| chen | 0.516 (0.028) | 0.652 (0.030) | -0.291 (0.020) | 0.085 (0.050) |'
  prefs: []
  type: TYPE_TB
- en: '| liusie | 0.496 (0.044) | 0.654 (0.038) | 0.204 (0.032) | 0.237 (0.078) |'
  prefs: []
  type: TYPE_TB
- en: '| wang | 0.464 (0.023) | 0.640 (0.027) | 0.240 (0.039) | 0.089 (0.094) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Rankings of prompt templates for GPT-3.5-turbo on the TL;DR summarization
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| TL;DR Summarization (GPT-4o) |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Template | $\text{Acc}_{\text{both}}$ | Position Bias | Length Bias |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| rafailov | 0.667 (0.011) | 0.737 (0.014) | 0.022 (0.015) | 0.197 (0.031)
    |'
  prefs: []
  type: TYPE_TB
- en: '| chen | 0.658 (0.028) | 0.734 (0.029) | -0.081 (0.023) | 0.117 (0.055) |'
  prefs: []
  type: TYPE_TB
- en: '| guo | 0.655 (0.011) | 0.733 (0.024) | -0.140 (0.014) | 0.193 (0.038) |'
  prefs: []
  type: TYPE_TB
- en: '| liusie | 0.632 (0.023) | 0.724 (0.019) | -0.154 (0.041) | 0.084 (0.056) |'
  prefs: []
  type: TYPE_TB
- en: '| wang | 0.601 (0.015) | 0.695 (0.016) | 0.108 (0.022) | 0.137 (0.066) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: Rankings of prompt templates for GPT-4o on the TL;DR summarization
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| TL;DR Summarization (GPT-4o-mini) |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Template | $\text{Acc}_{\text{both}}$ | Position Bias | Length Bias |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| rafailov | 0.631 (0.014) | 0.701 (0.023) | -0.060 (0.027) | 0.162 (0.038)
    |'
  prefs: []
  type: TYPE_TB
- en: '| guo | 0.619 (0.032) | 0.715 (0.010) | 0.090 (0.036) | 0.257 (0.068) |'
  prefs: []
  type: TYPE_TB
- en: '| chen | 0.615 (0.021) | 0.692 (0.031) | 0.010 (0.014) | 0.104 (0.049) |'
  prefs: []
  type: TYPE_TB
- en: '| liusie | 0.563 (0.018) | 0.684 (0.026) | -0.122 (0.030) | 0.169 (0.061) |'
  prefs: []
  type: TYPE_TB
- en: '| zheng | 0.516 (0.015) | 0.667 (0.020) | 0.280 (0.030) | 0.544 (0.086) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 11: Rankings of prompt templates for GPT-4o-mini on the TL;DR summarization
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| HH-RLHF-Helpfulness (GPT-3.5-turbo) |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Template | $\text{Acc}_{\text{both}}$ | Position Bias | Length Bias |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| zeng | 0.536 (0.012) | 0.654 (0.023) | 0.013 (0.036) | 0.531 (0.044) |'
  prefs: []
  type: TYPE_TB
- en: '| guo | 0.506 (0.025) | 0.651 (0.016) | 0.029 (0.060) | 0.280 (0.062) |'
  prefs: []
  type: TYPE_TB
- en: '| bai | 0.458 (0.022) | 0.659 (0.032) | 0.317 (0.033) | 0.342 (0.043) |'
  prefs: []
  type: TYPE_TB
- en: '| zheng | 0.423 (0.018) | 0.594 (0.009) | 0.368 (0.035) | 0.581 (0.051) |'
  prefs: []
  type: TYPE_TB
- en: '| xu | 0.386 (0.027) | 0.622 (0.030) | 0.488 (0.037) | 0.309 (0.050) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 12: Rankings of prompt templates for GPT-3.5-turbo on the HH-RLHF-Helpfulness
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| HH-RLHF-Helpfulness (GPT-4o) |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Template | $\text{Acc}_{\text{both}}$ | Position Bias | Length Bias |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| guo | 0.618 (0.040) | 0.694 (0.030) | -0.005 (0.013) | 0.135 (0.075) |'
  prefs: []
  type: TYPE_TB
- en: '| xu | 0.610 (0.025) | 0.702 (0.019) | 0.086 (0.010) | 0.029 (0.057) |'
  prefs: []
  type: TYPE_TB
- en: '| bai | 0.603 (0.027) | 0.697 (0.014) | 0.034 (0.017) | 0.255 (0.067) |'
  prefs: []
  type: TYPE_TB
- en: '| cheng | 0.589 (0.029) | 0.664 (0.029) | 0.049 (0.020) | 0.364 (0.082) |'
  prefs: []
  type: TYPE_TB
- en: '| zeng | 0.580 (0.034) | 0.674 (0.027) | 0.139 (0.023) | 0.402 (0.090) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 13: Rankings of prompt templates for GPT-4o on the HH-RLHF-Helpfulness
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| HH-RLHF-Helpfulness (GPT-4o-mini) |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Template | $\text{Acc}_{\text{both}}$ | Position Bias | Length Bias |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| guo | 0.602 (0.036) | 0.681 (0.030) | -0.028 (0.026) | 0.294 (0.059) |'
  prefs: []
  type: TYPE_TB
- en: '| rafailov | 0.594 (0.014) | 0.657 (0.019) | 0.047 (0.020) | 0.463 (0.039)
    |'
  prefs: []
  type: TYPE_TB
- en: '| zeng | 0.587 (0.031) | 0.650 (0.029) | 0.032 (0.022) | 0.494 (0.061) |'
  prefs: []
  type: TYPE_TB
- en: '| xu | 0.580 (0.018) | 0.681 (0.017) | 0.036 (0.015) | 0.272 (0.065) |'
  prefs: []
  type: TYPE_TB
- en: '| bai | 0.576 (0.033) | 0.665 (0.022) | -0.086 (0.010) | 0.397 (0.061) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 14: Rankings of prompt templates for GPT-4o-mini on the HH-RLHF-Helpfulness
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: IV. Derivations, Proofs, and Computational Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Position Bias (PB)
  prefs: []
  type: TYPE_NORMAL
- en: '1) Proof: Position bias definition is intrinsically length bias-mitigated'
  prefs: []
  type: TYPE_NORMAL
- en: In this proof, we demonstrate that the impact of length bias has been effectively
    mitigated from the measurement of position bias using the definition in the main
    paper.
  prefs: []
  type: TYPE_NORMAL
- en: 'To prove this, we analyze two separate conditions: (1) the LLM judge prefers
    the *first* position, (2) the LLM judge prefers the *second* position. In each
    case, we first establish that the de-noising process reduces the four possible
    outcome combinations in Table [15](#Sx8.T15 "Table 15 â€£ IV. Derivations, Proofs,
    and Computational Methods â€£ Appendix â€£ Systematic Evaluation of LLM-as-a-Judge
    in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates") into
    three as shown in Table [16](#Sx8.T16 "Table 16 â€£ IV. Derivations, Proofs, and
    Computational Methods â€£ Appendix â€£ Systematic Evaluation of LLM-as-a-Judge in
    LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates"). Subsequently,
    we demonstrate that the measurement of position bias, utilizing de-noised accuracy,
    effectively mitigates the length bias.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the purpose of this proof, we assume that *(noisy) outcomes are influenced
    by four factors: response quality, position bias, length bias, and flipping noise.*
    This assumption will be relaxed at the end of the proof. Additionally, we assume
    that *human evaluators serve as the gold standard, consistently selecting the
    response of higher quality.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before formally prove the claim, we remind readers that the position bias is
    defined based on the setting where the LLM judge decides on two reversed response
    orders for each data case: $h=(x,y_{c},y_{r})$) is chosen or not by the LLM judge,
    respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '| $y$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $y_{c}$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| âœ“ | âœ— | âœ— | âœ“ |'
  prefs: []
  type: TYPE_TB
- en: '| âœ— | âœ“ | âœ“ | âœ— |'
  prefs: []
  type: TYPE_TB
- en: '| âœ“ | âœ— | âœ“ | âœ— |'
  prefs: []
  type: TYPE_TB
- en: '| âœ— | âœ“ | âœ— | âœ“ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 15: All possible outcomes from LLM judge decisions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we consider the case that the LLM judge demonstrates the position bias
    that prefers the *first* position. Consequently, we can examine the likely causes
    for each outcome $y,y^{\prime}=(y_{c},y_{r},y_{r},y_{c})$:'
  prefs: []
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(âœ“âœ—âœ—âœ“): The LLM judge has selected the same response as human evaluators on
    both positions, either by emphasizing the response quality or due to the length
    bias (e.g. $y_{c}$ and the LLM judge prefers longer responses than human evaluators
    regardless of the response quality).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(âœ—âœ“âœ“âœ—): The LLM-judge is primarily influenced by the length bias since it selects
    the response with lower quality $y_{r}$ for both response postions.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(âœ“âœ—âœ“âœ—): The LLM judge is predominantly influenced by positional bias, as length
    bias alone would only result in the LLM selecting a consistent response (either
    $y_{c}$, not both) across different orders.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(âœ—âœ“âœ—âœ“): The primary cause of the observed outcome is likely the flipping noise,
    given our assumption that the LLM judge favors the *first* position. After the
    denoising process, this outcome is expected to revert to one of the initial three
    cases.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also observe that the first three cases could arise from flipping noise.
    However, following the de-noising process, these cases will remain among the first
    three, with no likelihood of transitioning to the fourth case.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Therefore, if the LLM judge exhibits the position bias towards the first position,
    the outcomes of the LLM-judge decisions *with no flipping noise* on $h$ are shown
    in TableÂ [16(a)](#Sx8.T16.st1 "In Table 16 â€£ IV. Derivations, Proofs, and Computational
    Methods â€£ Appendix â€£ Systematic Evaluation of LLM-as-a-Judge in LLM Alignment
    Tasks: Explainable Metrics and Diverse Prompt Templates"). Thus, the PB of the
    LLM judge is computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{PB}_{\text{first}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\!=\!\lim_{N\rightarrow\infty}\!\frac{1}{N}\sum_{n=1}^{N}\!\mathds{1}\!\left(\mbox{\char
    51}\mbox{\char 55}\mbox{\char 51}\mbox{\char 55}\right)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: This corresponds to the proportion of the third case $(\mbox{\char 51}\mbox{\char
    55}\mbox{\char 51}\mbox{\char 55})$ is length-bias mitigated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, if the observed outcomes are influenced by factors beyond response
    quality, positional bias, length bias, and flipping noise, these factors can be
    categorized into two types: position-dependent and position-independent. Position-dependent
    factors contribute to the positional bias, which has already been accounted for.
    Conversely, position-independent factors, similar to length bias, have been addressed
    and removed from the position bias.'
  prefs: []
  type: TYPE_NORMAL
- en: Second, we consider the case that the LLM judge demonstrates the position bias
    that prefers the *second* position. In this context, we can employ the same analytical
    approach as in the first case to investigate the underlying reasons for each outcome
    and to derive the positional bias accordingly as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{PB}_{\text{second}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\!=-\!\lim_{N\rightarrow\infty}\!\frac{1}{N}\sum_{n=1}^{N}\!\mathds{1}\!\left(\mbox{\char
    55}\mbox{\char 51}\mbox{\char 55}\mbox{\char 51}\right)$ |  |'
  prefs: []
  type: TYPE_TB
- en: In contrast to the first case, when the LLM judge prefers the second position,
    the third case is represented as $(\mbox{\char 55}\mbox{\char 51}\mbox{\char 55}\mbox{\char
    51})$ is listed first in the definition.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |'
  prefs: []
  type: TYPE_TB
- en: '| $y$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $y_{c}$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| âœ“ | âœ— | âœ— | âœ“ |'
  prefs: []
  type: TYPE_TB
- en: '| âœ— | âœ“ | âœ“ | âœ— |'
  prefs: []
  type: TYPE_TB
- en: '| âœ“ | âœ— | âœ“ | âœ— |'
  prefs: []
  type: TYPE_TB
- en: (a) Prefer first position
  prefs: []
  type: TYPE_NORMAL
- en: '| $y$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $y_{c}$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| âœ“ | âœ— | âœ— | âœ“ |'
  prefs: []
  type: TYPE_TB
- en: '| âœ— | âœ“ | âœ“ | âœ— |'
  prefs: []
  type: TYPE_TB
- en: '| âœ— | âœ“ | âœ— | âœ“ |'
  prefs: []
  type: TYPE_TB
- en: (b) Prefer second position
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 16: De-noised outcomes of the LLM judgeâ€™s decision in cases where the
    LLM judge favors the (a) first and (b) second responses, respectively. Here, âœ“
    and âœ— indicate whether a response ($y_{c}$) is chosen by the LLM judge or not.'
  prefs: []
  type: TYPE_NORMAL
- en: 2) Derivations of de-noised position bias
  prefs: []
  type: TYPE_NORMAL
- en: The derivations related to the de-noising process of PB are provided as follows.
    As a reminder, $Z$. Specifically,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle q_{cr}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle q_{rc}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'In this study, we assume *the flipping probability does not depend on the value
    of $X$ is derived as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p\left[Z=1&#124;(y_{c},y_{r})\right]$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=(1\!-\!q_{cr})\!\cdot\!p\left[X\!=\!1&#124;(y_{c},y_{r})\right]\!$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}~{}~{}~{}~{}+\!q_{cr}\!\cdot\!(1\!-p\left[X\!=\!1&#124;(y_{c},y_{r})\right])$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=(1-2\cdot q_{cr})\cdot p\left[X=1&#124;(y_{c},y_{r})\right]+q_{cr}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Therefore,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'Accordingly, the relationship between $p\left[X=1|(y_{r},y_{c})\right]$ is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 3) Position bias computation procedure
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a dataset $\mathcal{D}\!=\!\{h_{n}|n\!=\!1\!\ldots\!N\}$, a practical
    method for computing the PB related to an LLM judge is described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 1: Accuracy (based on $Z$) Computation*'
  prefs: []
  type: TYPE_NORMAL
- en: Since LLM judge evaluation results consistently contain flipping noise, even
    with the temperature parameter set to 0.0, we first calculate the accuracy for
    both response positions $(y_{c},y_{r})$.
  prefs: []
  type: TYPE_NORMAL
- en: In order to achieve this, we employ the LLM judge to generate judging result
    on each data in $\mathcal{D}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Broadly, we denote the set of judging results as $\mathcal{J}\!=\!\{s_{n}|n=1\ldots
    N\}$. Then the accuracy for each position can be computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\hat{p}\left[Z\!=\!1&#124;(y_{c},y_{r})\right]\!$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\hat{p}\left[Z\!=\!1&#124;(y_{r},y_{c})\right]\!$ |  |'
  prefs: []
  type: TYPE_TB
- en: '*Step 2: Flipping Probability Estimation*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Repeat the identical judging experiments in the *Step 1* for extra $K\!-\!1$
    are then computed by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\hat{q}_{cr}\!$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\hat{q}_{rc}\!$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $k^{(n)}_{cr}\!=\!\sum^{K}_{k=1}\mathds{1}\!(y^{(n)}_{k}\!=\!y^{(n)}_{c})$,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 3: De-noising Process*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The position bias is computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Length Bias (LB)
  prefs: []
  type: TYPE_NORMAL
- en: '1) Proof: Length bias measurement is entangled with position bias'
  prefs: []
  type: TYPE_NORMAL
- en: Here we demonstrate the entanglement between length bias (LB) and position bias
    (PB) in LB measurements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume the LLM judge exhibits position bias, namely $\text{PB}=p\left[X\!=\!1|(y_{c},y_{r})\right]-p\left[X\!=\!1|(y_{r},y_{c})\right]\!\neq\!0$
    in all the data cases. Mathematically, they can be formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{LB}_{cr}\!=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\text{LB}_{rc}\!=$ |  |'
  prefs: []
  type: TYPE_TB
- en: Due to the position bias, $$p\left[X\!=\!1|\Delta l\!></math> by mitigating
    the effect of PB.
  prefs: []
  type: TYPE_NORMAL
- en: 2) Accuracy defintion selection
  prefs: []
  type: TYPE_NORMAL
- en: Previous workÂ [[19](#bib.bib19), [21](#bib.bib21)] suggests both $\text{Acc}_{\text{both}}$
    in terms of mitigating the influence of position bias for length bias measurement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Without loss of generality, we assume *the LLM judge has the position bias
    favoring the first response*. The possible outcomes of $y^{\prime}$ after the
    de-noising process can be thus found in TableÂ [16(a)](#Sx8.T16.st1 "In Table 16
    â€£ IV. Derivations, Proofs, and Computational Methods â€£ Appendix â€£ Systematic Evaluation
    of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt
    Templates").'
  prefs: []
  type: TYPE_NORMAL
- en: 'When $\text{Acc}_{\text{both}}$ in Table [16(a)](#Sx8.T16.st1 "In Table 16
    â€£ IV. Derivations, Proofs, and Computational Methods â€£ Appendix â€£ Systematic Evaluation
    of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt
    Templates"). As discussed previously in the proof section of position bias , this
    case is not affected by the position bias. Consequently, employing this measure
    for accuracy helps mitigate the influence of positional bias in the assessment
    of length bias.'
  prefs: []
  type: TYPE_NORMAL
- en: When $\text{Acc}_{\text{random}}$ with a 50% probability, giving the third case
    a 50% chance of contributing to the correct selection for accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: As previously discussed, the third case is primarily attributed to position
    bias and thus cannot fully mitigate the influence of positional bias, unlike $\text{Acc}_{\text{both}}$
    in our length bias computation procedures.
  prefs: []
  type: TYPE_NORMAL
- en: 3) Length bias computation procedure
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a dataset $\mathcal{D}\!=\!\{h_{n}|n\!=\!1\!\ldots\!N\}$, a practical
    method for computing LB related to an LLM judge is described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 1: Accuracy (based on $Z$) Estimation*'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we use the same way as for computing position bias to generate the judging
    result set $\mathcal{J}$ can then be computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id=$$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\hat{p}\left[Z\!=\!1&#124;\Delta l\!\leq\!0\right]\!$ |  |'
  prefs: []
  type: TYPE_TB
- en: '*Step 2: Flipping Probability Estimation*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Analogous to the PB computation procedure, we repeat the identical judging
    experiments for extra $K\!-\!1$ is then computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\displaystyle\hat{q}_{\Delta></math> |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\hat{q}_{\Delta\leq 0}\!$ |  |'
  prefs: []
  type: TYPE_TB
- en: where <math id=$$, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 3: De-noising Process*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The length bias is computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
