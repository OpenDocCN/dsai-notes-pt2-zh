- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: æœªåˆ†ç±»'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:44:47'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt-Guided Generation of Structured Chest X-Ray Report Using a Pre-trained
    LLM â€ â€ thanks: ğŸ–‚Corresponding authors.'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2404.11209](https://ar5iv.labs.arxiv.org/html/2404.11209)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 1^(st) Hongzhao Li Network and Data Center
  prefs: []
  type: TYPE_NORMAL
- en: Northwest University Xiâ€™an, China
  prefs: []
  type: TYPE_NORMAL
- en: lihongzhao@stumail.nwu.edu.cn â€ƒâ€ƒ 2^(nd) Hongyu Wang School of Computer Science
    and Technology
  prefs: []
  type: TYPE_NORMAL
- en: Xiâ€™an University of Posts and Telecommunications Xiâ€™an, China
  prefs: []
  type: TYPE_NORMAL
- en: hywang@xupt.edu.cn â€ƒâ€ƒ 3^(rd) Xia Sun^(ğŸ–‚) School of Information Science and Technology
  prefs: []
  type: TYPE_NORMAL
- en: Northwest University Xiâ€™an, China
  prefs: []
  type: TYPE_NORMAL
- en: raindy@nwu.edu.cn â€ƒâ€ƒ 4^(th) Hua He School of Foreign Languages
  prefs: []
  type: TYPE_NORMAL
- en: Northwest University Xiâ€™an, China
  prefs: []
  type: TYPE_NORMAL
- en: hehua@nwu.edu.cn â€ƒâ€ƒ 5^(th) Jun Feng^(ğŸ–‚) School of Information Science and Technology
  prefs: []
  type: TYPE_NORMAL
- en: Northwest University Xiâ€™an, China
  prefs: []
  type: TYPE_NORMAL
- en: fengjun@nwu.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Medical report generation automates radiology descriptions from images, easing
    the burden on physicians and minimizing errors. However, current methods lack
    structured outputs and physician interactivity for clear, clinically relevant
    reports. Our method introduces a prompt-guided approach to generate structured
    chest X-ray reports using a pre-trained large language model (LLM). First, we
    identify anatomical regions in chest X-rays to generate focused sentences that
    center on key visual elements, thereby establishing a structured report foundation
    with anatomy-based sentences. We also convert the detected anatomy into textual
    prompts conveying anatomical comprehension to the LLM. Additionally, the clinical
    context prompts guide the LLM to emphasize interactivity and clinical requirements.
    By integrating anatomy-focused sentences and anatomy/clinical prompts, the pre-trained
    LLM can generate structured chest X-ray reports tailored to prompted anatomical
    regions and clinical contexts. We evaluate using language generation and clinical
    effectiveness metrics, demonstrating strong performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: prompt, large language model (LLM), interpretability, interactivity
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Medical report generation aims to automatically create textual descriptions
    from radiological images like chest X-rays, a time-consuming and error-prone task
    even for experienced radiologists, while demand exceeds current medical capabilities
    [[1](#bib.bib1)]. Automatically generating reports could reduce physician workload
    and diagnostic errors. This field integrates computer vision and natural language
    processing areas, making it an important area of multimodal research [[2](#bib.bib2)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Previous work on medical report generation has primarily employed encoder-decoder
    models [[3](#bib.bib3)], using CNN visual encoders to extract image features and
    Transformer [[4](#bib.bib4)] text decoders to convert those features into textual
    output. Research has focused on the end-to-end goal of generating reports from
    radiological images. However, some key issues have been overlooked. Structural
    Deficiency: Unstructured radiology reports reduce clarity and make it challenging
    for physicians to identify critical information [[1](#bib.bib1)]. However, current
    end-to-end methods are trained on datasets containing unstandardized reports from
    diverse sources, hindering the generation of uniformly structured outputs. This
    poses a barrier to optimal patient care, and yet there is limited current work
    addressing this lack of structure. Lack of Interpretability and Interactivity:
    Existing methods often insufficiently explain reasoning through local attention
    visualizations rather than clearly elucidating decisions. Furthermore, doctors
    cannot effectively intervene in the generation process to adjust content based
    on patient context. This opacity and rigidity restrict practical application [[5](#bib.bib5)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/55736e8b3526911fbb0added25a1985c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The high-level representation of our architecture. Each sentence
    can be explicitly associated with a specific region.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5393dab478b0688cb7b112e048dd5cde.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: In the architecture overview, the process includes identifying and
    extracting anatomical region features, generating region descriptions, and ultimately
    integrating anatomical prompts with clinical context prompts to produce a structured
    report.'
  prefs: []
  type: TYPE_NORMAL
- en: In response to the aforementioned issues, we propose an structured report generation
    model (Fig.Â [1](#S1.F1 "Figure 1 â€£ I Introduction â€£ Prompt-Guided Generation of
    Structured Chest X-Ray Report Using a Pre-trained LLM ğŸ–‚Corresponding authors."))
    using a pre-trained large language model (LLM) guided by anatomical regions and
    clinical contextual prompts to achieve high interpretability and interactivity.
    First, we detect anatomical regions in chest X-rays to generate region-focused
    descriptions, establishing an anatomy foundation for structured report. We additionally
    translate anatomical information into textual prompts, enabling subsequent modules
    comprehension of anatomy-guided data for more accurate descriptions. Second, our
    model incorporates clinical contextual information, including the patientâ€™s medical
    history and the reason for examination, etc., typically provided by the physician.
    This interactivity allows physicians to actively participate in report generation
    by providing relevant information. Lastly, we utilize a large language model to
    integrate anatomical region descriptions, anatomical prompts, and clincial contextual
    prompts into a single anatomically based structured by coordinating and consolidating
    these data sources.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, our work contributes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We introduced an anatomy-guided structured report generation framework by identifying
    anatomical regions to construct anatomy-centric sentences, establishing the basis
    for structured reports with lucid expressions. We also integrated patient contextual
    information for comprehensive clinical understanding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By inputting anatomy-focused sentences and anatomy/clinica prompts into a large
    language model, we produced structured, interpretable reports bearing anatomical
    and clinical relevance. Additionally, our architecture empowers physicians to
    provide clinical context, enabling intervention and adjustment in report generation
    to address variable clinical needs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We evaluated our method on the MIMIC-CXR dataset [[6](#bib.bib6)] using both
    natural language generation and clinical effectiveness metrics. We performed well
    on both sets of metrics and conducted detailed visual experiments to demonstrate
    and support the structured nature of our generated reports, as well as the interpretability
    and interactivity of the method.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: II Related work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Medical report generation. Recent frameworks build on medical image captioning
    to incorporate expert or external knowledge for richer contextual understanding
    in generated radiology reports [[7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9)].
    Recent approaches like the MET [[9](#bib.bib9)], Kiut [[9](#bib.bib9)], and KBMA
    [[8](#bib.bib8)] incorporate specialized knowledge through fusion of expert tokens,
    multimodal injection, and trainable knowledge bases to enhance contextual understanding
    in radiology report generation. The work [[10](#bib.bib10)] uses anatomical regions
    for the first time to generate reports. To further increase interpretability,
    some efforts introduce memory networks [[3](#bib.bib3), [11](#bib.bib11), [12](#bib.bib12)]
    to store knowledge and learn implicit image-text links. However, unlike these
    algorithmically synthesized components imitating expertise, we use actual clinical
    documents to provide contextual knowledge. Furthermore, we guide the generation
    process using anatomical regions, enhancing interpretability by specifying clear
    sentence-to-region mappings within the report.
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Model. Recent advances in Transformer [[4](#bib.bib4)] and computing
    have enabled very large language models (LLMs) like ChatGPT [[13](#bib.bib13)]
    with enhanced performance on text generation and translation. Architectures like
    CLIP [[14](#bib.bib14)] achieve multimodal understanding via image-text pretraining.
    Additionally, recent work guides LLMs via prompts rather than explicit training.
    For example, VisualChatGPT [[15](#bib.bib15)] connects ChatGPT with vision models
    for image-inclusive conversations, exemplifying this more flexible prompt-based
    approach to unleashing LLM potential. The idea of guiding LLMs through prompts
    has inspired our research. Guiding an LLM through anatomical region prompts results
    in structured, interpretable reports. Furthermore, clinical context prompts facilitate
    physician interactivity, enhancing the practical utility of the generated reports.
  prefs: []
  type: TYPE_NORMAL
- en: III Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We propose a structured report generation framework, guided by anatomy and clinical
    prompts, to simulate radiologist workflow. First, we identify anatomical regions
    in chest X-rays and extract per-region features [(Section 3.1)](#S3.SS1 "III-A
    Anatomy Region Detection and Feature Extraction â€£ III Methodology â€£ Prompt-Guided
    Generation of Structured Chest X-Ray Report Using a Pre-trained LLM ğŸ–‚Corresponding
    authors."). Then, a sentence generator produces region descriptions, forming the
    basis for the structured report [(Section 3.2)](#S3.SS2 "III-B Sentence Generator
    â€£ III Methodology â€£ Prompt-Guided Generation of Structured Chest X-Ray Report
    Using a Pre-trained LLM ğŸ–‚Corresponding authors."). Concurrently, we generate anatomical
    prompts indicating sentence presence and abnormalities per region [(Section 3.3)](#S3.SS3
    "III-C Anatomy Prompts Generation â€£ III Methodology â€£ Prompt-Guided Generation
    of Structured Chest X-Ray Report Using a Pre-trained LLM ğŸ–‚Corresponding authors.").
    Finally, we integrate region descriptions, anatomical prompts, and clinical context
    from doctors into prompts for a large language model, it generates the final structured
    report [(Section 3.4)](#S3.SS4 "III-D Structured report generation â€£ III Methodology
    â€£ Prompt-Guided Generation of Structured Chest X-Ray Report Using a Pre-trained
    LLM ğŸ–‚Corresponding authors."). We show the proposed architecture in (Fig.Â [2](#S1.F2
    "Figure 2 â€£ I Introduction â€£ Prompt-Guided Generation of Structured Chest X-Ray
    Report Using a Pre-trained LLM ğŸ–‚Corresponding authors.")) and describe these steps
    in more detail below.
  prefs: []
  type: TYPE_NORMAL
- en: III-A Anatomy Region Detection and Feature Extraction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We employed Faster R-CNN [[16](#bib.bib16)] with a ResNet-50 backbone [[17](#bib.bib17)]
    for anatomy detection and feature extraction. Faster R-CNN generates region proposals,
    then extracts features via RoI pooling and classifies anatomy regions, optimized
    using the standard Faster R-CNN loss. Next, for each detected region, we pooled
    the features and transformed them into a 1024-dimensional representation as the
    image feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $V=\operatorname{Faster\ R-CNN}\left(Img\right).$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: Finally, it outputs 29 anatomy regions along with visual features $V\in\mathbb{R}^{29\times
    1024}$ capturing morphological and pathological information per region.
  prefs: []
  type: TYPE_NORMAL
- en: III-B Sentence Generator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To generate region sentences, we employed a Transformer Decoder model similar
    to [[18](#bib.bib18)], which attends to context from preceding tokens. We integrated
    regional visual features into the attention computation, allowing the model to
    jointly consider previous tokens along with anatomy visuals during text generation.
    We trained the model by minimizing cross-entropy loss to align the generated text
    with expected reports. Ultimately, this facilitates learning language expressions
    for anatomical regions, forming the basis of our structured report.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Y=\operatorname{SentenceGen}\left(V\right).$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: Specifically, $Y\in\mathbb{R}^{29\times l}$ represents the sentence length.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6eccb0de49d057bb67d444c74ea0569b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: An example is presented to illustrate the inputs for the LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Comparison results on MIMIC-CXR show that the numbers in bold represent
    the best results per column.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Year | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 | METEOR | ROUGE-L | F1
    | Precision | Recall |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| R2GEN [[3](#bib.bib3)] | 2020 | 0.353 | 0.218 | 0.145 | 0.103 | 0.142 | 0.270
    | 0.276 | 0.333 | 0.273 |'
  prefs: []
  type: TYPE_TB
- en: '| R2GENCMN [[11](#bib.bib11)] | 2022 | 0.353 | 0.218 | 0.148 | 0.106 | 0.149
    | 0.284 | 0.278 | 0.334 | 0.275 |'
  prefs: []
  type: TYPE_TB
- en: '| CMM+RL [[12](#bib.bib12)] | 2022 | 0.381 | 0.232 | 0.155 | 0.109 | 0.151
    | 0.287 | 0.292 | 0.342 | 0.294 |'
  prefs: []
  type: TYPE_TB
- en: '| CvT [[19](#bib.bib19)] | 2023 | 0.392 | 0.245 | 0.169 | 0.124 | 0.153 | 0.285
    | 0.384 | 0.359 | 0.412 |'
  prefs: []
  type: TYPE_TB
- en: '| MET [[9](#bib.bib9)] | 2023 | 0.386 | 0.250 | 0.169 | 0.124 | 0.152 | 0.291
    | 0.311 | 0.364 | 0.309 |'
  prefs: []
  type: TYPE_TB
- en: '| KiUT [[7](#bib.bib7)] | 2023 | 0.393 | 0.243 | 0.159 | 0.113 | 0.160 | 0.285
    | 0.321 | 0.371 | 0.318 |'
  prefs: []
  type: TYPE_TB
- en: '| KBMA [[8](#bib.bib8)] | 2023 | 0.386 | 0.237 | 0.157 | 0.111 | - | 0.274
    | 0.352 | 0.420 | 0.339 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | 2023 | 0.395 | 0.260 | 0.178 | 0.131 | 0.161 | 0.261 | 0.441 | 0.469
    | 0.470 |'
  prefs: []
  type: TYPE_TB
- en: III-C Anatomy Prompts Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Anatomy Prompts Generation module has three components: sentence detection,
    abnormal detection, and a prompts converter. The first two are binary classifiers
    indicating whether the region should generate sentence and if abnormalities are
    present. For example, sentence detection may include the heart region as crucial,
    while abnormal detection flags heart abnormalities. We optimized with binary cross-entropy
    loss. Then, the prompts converter translates the classifications into clear prompts
    for language models, such as â€The aortic arch is definitely abnormalâ€.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $P_{1,2}=Prompt\ Generation\left(V\right),$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $P_{1}$ are the anatomical location and abnormality prompts. Ultimately,
    this enables effective anatomical prompt incorporation to integrate anatomy into
    decision-making, supporting structured report generation.
  prefs: []
  type: TYPE_NORMAL
- en: III-D Structured report generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Large language models (LLMs) possess robust medical knowledge and reasoning
    abilities. Appropriate prompts allow them to produce precise medical reports.
    When anatomical descriptions and prompts are organized, we integrate them with
    clinical context $P_{3}$ (e.g. â€Generate a structured report based on the anatomical
    and clinical detailsâ€). This enables the LLM to generate appropriately structured
    outputs.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{Y}=\text{LLM}(C,Y,P_{1},P_{2},P_{3})$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: In the end, the LLM synthesizes the sentence descriptions $Y$ (Fig.Â [3](#S3.F3
    "Figure 3 â€£ III-B Sentence Generator â€£ III Methodology â€£ Prompt-Guided Generation
    of Structured Chest X-Ray Report Using a Pre-trained LLM ğŸ–‚Corresponding authors."))
    .
  prefs: []
  type: TYPE_NORMAL
- en: IV Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: IV-A Datasets and Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The recently released MIMIC-CXR [[6](#bib.bib6)] is currently the largest public
    dataset containing many chest radiograph images and reports. In total, this dataset
    has 377,110 images and 227,835 reports for 64,588 patients. For experimental and
    fair comparisons, we followed the previous methodology [[3](#bib.bib3), [7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9), [11](#bib.bib11), [12](#bib.bib12)] used the official
    MIMIC-CXR divisions: 222,758 samples for training, 1,808 for validation, and 3,269
    for testing. Furthermore, we utilized labels generated by Chest ImaGenome [[20](#bib.bib20)],
    where the tags succinctly represent the 29 chest anatomical regions in the image,
    aligning with the sentences describing each region in the report.'
  prefs: []
  type: TYPE_NORMAL
- en: We evaluated radiology report generation using standard natural language generation
    (NLG) metrics and a clinical efficiency (CE) metric. The NLG metrics were BLEU
    [[21](#bib.bib21)], METEOR [[22](#bib.bib22)], and ROUGE [[23](#bib.bib23)] scores,
    which are standard metrics used to assess the fluency of generated natural language.
    As NLG metrics insufficiently measure clinical correctness, the CE metric utilized
    14 common disease type labels to compute F1, precision and recall versus basic
    facts and reports, thereby quantitatively measuring clinical correctness.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Six prominent regions are identified as follows: right lung (RL),
    left lung (LL), spine (SP), mediastinum (MED), cardiac silhouette (CS), and abdomen
    (AB).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Region | RL | LL | SP | MED | CS | AB |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| IoU | 0.891 | 0.900 | 0.943 | 0.827 | 0.782 | 0.896 |'
  prefs: []
  type: TYPE_TB
- en: '| METEOR | 0.121 | 0.113 | 0.173 | 0.123 | 0.101 | 0.109 |'
  prefs: []
  type: TYPE_TB
- en: IV-B Implementation Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For anatomy detection, we did not use the Faster R-CNN [[16](#bib.bib16)] features
    directly to avoid over-coupling. Instead, we pooled and transformed the region
    features, ensuring detection performance. We extracted 29 regions and 1024-dimensional
    visual features as inputs for generation. The classifiers use three FC layers
    (1024-512-128-1) with ReLU activations for non-linearity. The sentence generator
    has three 8-headed attention layers with 512 units each. We trained all modules
    on one NVIDIA 3090 GPU over three stages: first, train anatomy detection; then
    add and train the two classifiers; finally, add and train the sentence generator.
    Importantly, each new module trained concurrently with the previously trained
    ones to maintain performance. The integration module uses fixed GPT-4 without
    separate training. All modules used mixed precision, AdamW, learning rate decay,
    and early stopping.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: Results from sentence detection and abnormality detection.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Module | Regions | F1 | P | R |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Sentence Detection | All | 0.701 | 0.571 | 0.906 |'
  prefs: []
  type: TYPE_TB
- en: '| Abnormal | 0.965 | 1.000 | 0.932 |'
  prefs: []
  type: TYPE_TB
- en: '| Normal | 0.489 | 0.341 | 0.869 |'
  prefs: []
  type: TYPE_TB
- en: '| Abnormal Detection | All | 0.557 | 0.394 | 0.951 |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/3c13a7faf33ec758232a2a324a1595af.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: We represented the detected anatomical regions, the corresponding
    generated sentences, and the semantically matched reference sentences using the
    same color, while also highlighting the clinical context.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-C Quantitative results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Comparison with state-of-the-art. We conducted a comprehensive comparison of
    our model with recent methods, as referenced in [[3](#bib.bib3), [7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9), [11](#bib.bib11), [12](#bib.bib12)], using NLG
    and CE metrics (Tab.Â [I](#S3.T1 "TABLE I â€£ III-B Sentence Generator â€£ III Methodology
    â€£ Prompt-Guided Generation of Structured Chest X-Ray Report Using a Pre-trained
    LLM ğŸ–‚Corresponding authors.")). These methods encompass a diverse array of techniques,
    including the utilization of knowledge graphs, large language models, reinforcement
    learning, and various network architectures. In terms of NLG metrics, our model
    emerges as the top performer overall, displaying remarkable language quality and
    fluency. Notably, we observe a minor decrease only in ROUGE-L, which doesnâ€™t compromise
    the modelâ€™s excellence in report generation. Importantly, our model outperforms
    others significantly in clinical correctness. Our reports not only exhibit superior
    language quality but also align more closely with real diagnoses. This outstanding
    performance is evident across all CE metrics, underscoring the exceptional medical
    accuracy and reliability of our approach.
  prefs: []
  type: TYPE_NORMAL
- en: Analysis of Key Regions. In Tab.Â [II](#S4.T2 "TABLE II â€£ IV-A Datasets and Metrics
    â€£ IV Experiments â€£ Prompt-Guided Generation of Structured Chest X-Ray Report Using
    a Pre-trained LLM ğŸ–‚Corresponding authors."), we present the IoU and METEOR scores
    for six key regions, where IoU gauges localization accuracy, and METEOR assesses
    semantic report consistency. Across most regions, high IoU values indicate precise
    identification. And SP and MED exhibit higher METEOR scores, implying more consistent
    reports. However, RL and LL show a certain degree of negative correlation, indicating
    that after imperfect detection, subsequent modules can still accurately convey
    anatomical semantics. Therefore, minor deviations in detection will not have a
    decisive impact on the final report.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: The displayed results depict the outcomes of the ablation experiments.
    NLG scores represent the average of all metrics in NLG. In this context, SDet
    corresponds to sentence detection, ADet corresponds to anomaly detection, loss
    is represented by $L$.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | SDet | ADet | $P_{3}$ | NLG | CE |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $L_{1}$ | F1 | P | R |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| (a) |  |  |  |  |  | 0.183 | 0.304 | 0.355 | 0.301 |'
  prefs: []
  type: TYPE_TB
- en: '| (b) | $\checkmark$ |  |  | 0.233 | 0.404 | 0.432 | 0.432 |'
  prefs: []
  type: TYPE_TB
- en: '| (c) | $\checkmark$ |  |  | 0.229 | 0.405 | 0.421 | 0.450 |'
  prefs: []
  type: TYPE_TB
- en: '| (d) | $\checkmark$ |  | 0.232 | 0.434 | 0.450 | 0.459 |'
  prefs: []
  type: TYPE_TB
- en: '| (e) | $\checkmark$ | 0.233 | 0.408 | 0.436 | 0.437 |'
  prefs: []
  type: TYPE_TB
- en: '| (f) | $\checkmark$ | 0.231 | 0.441 | 0.469 | 0.470 |'
  prefs: []
  type: TYPE_TB
- en: Evaluation of Sentence and Anomaly Detection. Tab.Â [III](#S4.T3 "TABLE III â€£
    IV-B Implementation Details â€£ IV Experiments â€£ Prompt-Guided Generation of Structured
    Chest X-Ray Report Using a Pre-trained LLM ğŸ–‚Corresponding authors.") shows results
    for the sentence and anomaly detection modules. The sentence detection module
    focuses on whether the region generates a sentence, while the abnormal detection
    module focuses on whether the region is abnormal. For sentence detection, our
    model demonstrates a commendable overall high recall, indicating a propensity
    to generate a larger number of sentences. Nevertheless, the precision of normal
    sentences is relatively low, resulting in the inclusion of more additional normal
    sentences in the generated reports. This expanded sentence generation contributes
    to increased textual detail, and consequently, our modelâ€™s ROUGE-L score is marginally
    underrepresented. However, itâ€™s crucial to highlight that the decision to include
    normal sentences is highly subjective in a clinical context and depends on the
    physicianâ€™s judgment. Some physicians may choose not to mention normal areas.
    Therefore, the reduction in normal accuracy is comprehensible. In contrast, the
    accuracy in detecting abnormalities is consistently at 1.0, given that abnormal
    regions are inherently included in the reference reports. In terms of abnormal
    detection, we found a higher recall rate but a lower precision rate. A higher
    recall rate can effectively identify most true abnormalities. In a clinical setting,
    emphasizing achieving high sensitivity and ensuring timely detection is crucial,
    highlighting the importance of high recall in practical applications.
  prefs: []
  type: TYPE_NORMAL
- en: Ablation Analysis. Tab.Â [IV](#S4.T4 "TABLE IV â€£ IV-C Quantitative results â€£
    IV Experiments â€£ Prompt-Guided Generation of Structured Chest X-Ray Report Using
    a Pre-trained LLM ğŸ–‚Corresponding authors.") presents ablation studies focusing
    on loss functions and prompts. We employ a unique custom prompt $C$ in all models
    except for model (a), enabling the LLM to eliminate redundant sentences and make
    more effective use of the generated information. In the case of model (b), the
    inclusion of sentence and anomaly detection losses proves to be instrumental in
    significantly enhancing performance, guiding the modelâ€™s attention towards abnormalities
    even in the absence of explicit prompts. Upon scrutinizing prompt analyses, distinct
    effects emerge. Anatomical and contextual prompts (Models c-e) exert a positive
    influence on clinical accuracy, underscoring their pivotal role. However, itâ€™s
    noteworthy that they marginally decrease NLG metrics. This decrease can be attributed
    to the generated reports containing more anatomical details than reference report.
    Importantly, this doesnâ€™t imply a reduction in practicality; instead, it suggests
    that the report is more structured, comprehensive, and consistent with medical
    standards.
  prefs: []
  type: TYPE_NORMAL
- en: IV-D Qualitative results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Fig.[4](#S4.F4 "Figure 4 â€£ IV-B Implementation Details â€£ IV Experiments â€£
    Prompt-Guided Generation of Structured Chest X-Ray Report Using a Pre-trained
    LLM ğŸ–‚Corresponding authors."), we visually illustrate the generation process for
    a specific example. Our visualization also enables demonstrating the interpretability,
    interactivity, and structural nature of the report. First, we observed Model (a),
    which represents sentences generated before prompts inclusion. The colors of anatomical
    regions boxes correspond to generated sentences colors, strengthening interpretability
    as physicians can associate sentences with anatomical regions. While Model (a)
    laid the initial foundation for an anatomy-oriented structured report, the sentences
    lacked specific anatomical information and clinical context while redundant. To
    address this, location, abnormality, context and customized prompts were incorporated,
    whereby the LLM successfully generated more accurate, detailed and structured
    reports. The final report focuses on anatomical areas, providing region-specific
    findings regarding pulmonary edema severity across lung areas and noting bilateral
    pleural effusion. Additionally, cardiac enlargement and aortic calcification are
    described. This structured report boosts understanding and comparison by supplying
    region-specific anatomical and examination details alongside abnormality presence/extent
    descriptions. Moreover, physician-furnished contextual information like indications
    is included, relating reported cough, shortness of breath to suggested cardiopulmonary
    issues. This supplementary context enables fuller clinical condition comprehension
    while providing diagnosis/treatment clues, demonstrating prompts can align model
    focus with expectations.
  prefs: []
  type: TYPE_NORMAL
- en: V Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We propose an structured report generation model using a pre-trained large language
    model LLM guided by anatomical regions and clinical contextual prompts to achieve
    high interpretability and interactivity. First, we introduced anatomical structure
    detection to establish structured, anatomy-centric visual descriptions, a novel
    contribution. Second, through interactive textual prompts and a large language
    model, we enabled physician guidance catered to variable clinical contexts, also
    lacking in prior work. Our emphasis on report structure and process interpretability
    and interactivity, coupled with strong performance on relevant metrics, contributes
    to the solution of pervasive limitations in this domain.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This research was supported by the National Natural Science Foundation of China
    (NSFC Grant No. 62073260, 62001380); Key & Dprojects in Shaanxi Province (No.
    2023-YBSF-493).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] D.Â Ganeshan, P.-A.Â T. Duong, L.Â Probyn, L.Â Lenchik, T.Â A. McArthur, M.Â Retrouvey,
    E.Â H. Ghobadi, S.Â L. Desouches, D.Â Pastel, and I.Â R. Francis, â€œStructured reporting
    in radiology,â€ *Academic radiology*, vol.Â 25, no.Â 1, pp. 66â€“73, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] D.Â W. Otter, J.Â R. Medina, and J.Â K. Kalita, â€œA survey of the usages of
    deep learning for natural language processing,â€ *IEEE transactions on neural networks
    and learning systems*, vol.Â 32, no.Â 2, pp. 604â€“624, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Z.Â Chen, Y.Â Song, T.-H. Chang, and X.Â Wan, â€œGenerating radiology reports
    via memory-driven transformer,â€ *arXiv preprint arXiv:2010.16056*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] A.Â Vaswani, N.Â Shazeer, N.Â Parmar, J.Â Uszkoreit, L.Â Jones, A.Â N. Gomez,
    Å.Â Kaiser, and I.Â Polosukhin, â€œAttention is all you need,â€ *Advances in neural
    information processing systems*, vol.Â 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] T.Â Miller, â€œExplanation in artificial intelligence: Insights from the social
    sciences,â€ *Artificial intelligence*, vol. 267, pp. 1â€“38, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] A.Â E. Johnson, T.Â J. Pollard, N.Â R. Greenbaum, M.Â P. Lungren, C.-y. Deng,
    Y.Â Peng, Z.Â Lu, R.Â G. Mark, S.Â J. Berkowitz, and S.Â Horng, â€œMimic-cxr-jpg, a large
    publicly available database of labeled chest radiographs,â€ *arXiv preprint arXiv:1901.07042*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Z.Â Huang, X.Â Zhang, and S.Â Zhang, â€œKiut: Knowledge-injected u-transformer
    for radiology report generation,â€ in *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*, 2023, pp. 19â€‰809â€“19â€‰818.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] S.Â Yang, X.Â Wu, S.Â Ge, Z.Â Zheng, S.Â K. Zhou, and L.Â Xiao, â€œRadiology report
    generation with a learned knowledge base and multi-modal alignment,â€ *Medical
    Image Analysis*, vol.Â 86, p. 102798, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Z.Â Wang, L.Â Liu, L.Â Wang, and L.Â Zhou, â€œMetransformer: Radiology report
    generation by transformer with multiple learnable expert tokens,â€ in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2023,
    pp. 11â€‰558â€“11â€‰567.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] T.Â Tanida, P.Â MÃ¼ller, G.Â Kaissis, and D.Â Rueckert, â€œInteractive and explainable
    region-guided radiology report generation,â€ in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2023, pp. 7433â€“7442.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Z.Â Chen, Y.Â Shen, Y.Â Song, and X.Â Wan, â€œCross-modal memory networks for
    radiology report generation,â€ *arXiv preprint arXiv:2204.13258*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] H.Â Qin and Y.Â Song, â€œReinforced cross-modal alignment for radiology report
    generation,â€ in *Findings of the Association for Computational Linguistics: ACL
    2022*, 2022, pp. 448â€“458.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] OpenAI, â€œGpt-4 technical report,â€ 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] A.Â Radford, J.Â W. Kim, C.Â Hallacy, A.Â Ramesh, G.Â Goh, S.Â Agarwal, G.Â Sastry,
    A.Â Askell, P.Â Mishkin, J.Â Clark *etÂ al.*, â€œLearning transferable visual models
    from natural language supervision,â€ in *International conference on machine learning*.Â Â Â PMLR,
    2021, pp. 8748â€“8763.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] C.Â Wu, S.Â Yin, W.Â Qi, X.Â Wang, Z.Â Tang, and N.Â Duan, â€œVisual chatgpt:
    Talking, drawing and editing with visual foundation models,â€ *arXiv preprint arXiv:2303.04671*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] S.Â Ren, K.Â He, R.Â Girshick, and J.Â Sun, â€œFaster r-cnn: Towards real-time
    object detection with region proposal networks,â€ *Advances in neural information
    processing systems*, vol.Â 28, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] K.Â He, X.Â Zhang, S.Â Ren, and J.Â Sun, â€œDeep residual learning for image
    recognition,â€ in *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, 2016, pp. 770â€“778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] A.Â Radford, J.Â Wu, R.Â Child, D.Â Luan, D.Â Amodei, I.Â Sutskever *etÂ al.*,
    â€œLanguage models are unsupervised multitask learners,â€ *OpenAI blog*, vol.Â 1,
    no.Â 8, p.Â 9, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] A.Â Nicolson, J.Â Dowling, and B.Â Koopman, â€œImproving chest x-ray report
    generation by leveraging warm starting,â€ *Artificial Intelligence in Medicine*,
    vol. 144, p. 102633, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] J.Â T. Wu, N.Â N. Agu, I.Â Lourentzou, A.Â Sharma, J.Â A. Paguio, J.Â S. Yao,
    E.Â C. Dee, W.Â Mitchell, S.Â Kashyap, A.Â Giovannini *etÂ al.*, â€œChest imagenome dataset
    for clinical reasoning,â€ *arXiv preprint arXiv:2108.00316*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] K.Â Papineni, S.Â Roukos, T.Â Ward, and W.-J. Zhu, â€œBleu: a method for automatic
    evaluation of machine translation,â€ in *Proceedings of the 40th annual meeting
    of the Association for Computational Linguistics*, 2002, pp. 311â€“318.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] S.Â Banerjee and A.Â Lavie, â€œMeteor: An automatic metric for mt evaluation
    with improved correlation with human judgments,â€ in *Proceedings of the acl workshop
    on intrinsic and extrinsic evaluation measures for machine translation and/or
    summarization*, 2005, pp. 65â€“72.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] C.-Y. Lin, â€œRouge: A package for automatic evaluation of summaries,â€ in
    *Text summarization branches out*, 2004, pp. 74â€“81.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
