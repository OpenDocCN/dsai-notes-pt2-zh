- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†ç±»ï¼šæœªåˆ†ç±»
- en: 'date: 2024-09-08 18:36:36'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¥æœŸï¼š2024-09-08 18:36:36
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Mallows-DPOï¼šé€šè¿‡åå¥½ç¦»æ•£åŒ–å¾®è°ƒæ‚¨çš„LLM
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2405.14953](https://ar5iv.labs.arxiv.org/html/2405.14953)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2405.14953](https://ar5iv.labs.arxiv.org/html/2405.14953)
- en: Haoxian Chen^âˆ— ,Â  Hanyang Zhao^âˆ— ,Â  Henry Lam ,Â  David D. Yao Â andÂ  Wenpin Tang
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Haoxian Chen^âˆ— ï¼ŒHanyang Zhao^âˆ— ï¼ŒHenry Lam ï¼ŒDavid D. Yao å’Œ Wenpin Tang
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ‘˜è¦ã€‚
- en: Direct Preference Optimization (DPO) has recently emerged as a popular approach
    to improve reinforcement learning with human feedback (RLHF), leading to better
    techniques to fine-tune large language models (LLM). A weakness of DPO, however,
    lies in its lack of capability to characterize the diversity of human preferences.
    Inspired by Mallowsâ€™ theory of preference ranking, we develop in this paper a
    new approach, the Mallows-DPO. A distinct feature of this approach is a dispersion
    index, which reflects the dispersion of human preference to prompts. We show that
    existing DPO models can be reduced to special cases of this dispersion index,
    thus unified with Mallows-DPO. More importantly, we demonstrate (empirically)
    how to use this dispersion index to enhance the performance of DPO in a broad
    array of benchmark tasks, from synthetic bandit selection to controllable generations
    and dialogues, while maintaining great generalization capabilities.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æœ€è¿‘ä½œä¸ºä¸€ç§æµè¡Œçš„æ–¹æ³•å‡ºç°ï¼Œç”¨äºæ”¹è¿›å…·æœ‰äººå·¥åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ï¼Œä»è€Œæ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¾®è°ƒæŠ€æœ¯ã€‚ç„¶è€Œï¼ŒDPOçš„ä¸€ä¸ªå¼±ç‚¹åœ¨äºå…¶ç¼ºä¹æè¿°äººç±»åå¥½å¤šæ ·æ€§çš„èƒ½åŠ›ã€‚å—Mallowsåå¥½æ’åç†è®ºçš„å¯å‘ï¼Œæˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå³Mallows-DPOã€‚è¯¥æ–¹æ³•çš„ä¸€ä¸ªæ˜¾è‘—ç‰¹å¾æ˜¯ç¦»æ•£æŒ‡æ•°ï¼Œå®ƒåæ˜ äº†å¯¹æç¤ºçš„äººç±»åå¥½çš„ç¦»æ•£ç¨‹åº¦ã€‚æˆ‘ä»¬å±•ç¤ºäº†ç°æœ‰çš„DPOæ¨¡å‹å¯ä»¥ç®€åŒ–ä¸ºè¿™ç§ç¦»æ•£æŒ‡æ•°çš„ç‰¹æ®Šæƒ…å†µï¼Œä»è€Œä¸Mallows-DPOç»Ÿä¸€ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬é€šè¿‡ï¼ˆç»éªŒï¼‰æ¼”ç¤ºäº†å¦‚ä½•åˆ©ç”¨è¿™ç§ç¦»æ•£æŒ‡æ•°æ¥å¢å¼ºDPOåœ¨ä»åˆæˆèµŒåšé€‰æ‹©åˆ°å¯æ§ç”Ÿæˆå’Œå¯¹è¯ç­‰å¹¿æ³›åŸºå‡†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒåŒæ—¶ä¿æŒå‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚
- en: '^âˆ—Equal Contribution. Emails: {hc3136,hz2684,khl2114,ddy1,wt2319}@columbia.edu,
    Department of Industrial Engineering and Operations Research, Columbia University.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ^âˆ—å¹³ç­‰è´¡çŒ®ã€‚ç”µå­é‚®ä»¶ï¼š{hc3136,hz2684,khl2114,ddy1,wt2319}@columbia.eduï¼Œå“¥ä¼¦æ¯”äºšå¤§å­¦å·¥ä¸šå·¥ç¨‹ä¸è¿ç­¹å­¦ç³»ã€‚
- en: 'Key words: Dispersion, direct preference optimization, fine-tuning, large language
    models (LLMs), Mallows models, reinforcement learning with human feedback (RLHF).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å…³é”®è¯ï¼šç¦»æ•£åº¦ã€ç›´æ¥åå¥½ä¼˜åŒ–ã€å¾®è°ƒã€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€Mallowsæ¨¡å‹ã€å…·æœ‰äººå·¥åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ã€‚
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. å¼•è¨€
- en: Reinforcement Learning with Human Feeback (RLHF, [[28](#bib.bib28), [33](#bib.bib33),
    [44](#bib.bib44)]) has made significant contributions to the success of modern
    Large Language Models (LLMs) such as ChatGPT and GPT4 [[1](#bib.bib1)]. More recently,
    Direct Preference Optimization (DPO) [[30](#bib.bib30)] solves essentially the
    same problem as RLHF, but bypasses the training of the reward model, and thus
    leading to faster speed and better resource efficiency. More importantly, DPO
    also achieves comparable or superior performance against RLHF in downstream tasks
    such as fine-tuning LLMs in Llama3, Zephyr [[37](#bib.bib37)], Neural Chat, BTLM-DPO
    [[18](#bib.bib18)], etc. DPOâ€™s success has attracted much research attention,
    leading to variants beyond pairwise ranking [[32](#bib.bib32)], and unified perspectives
    on loss parameterization [[2](#bib.bib2), [15](#bib.bib15), [36](#bib.bib36),
    [40](#bib.bib40)], etc.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: äººå·¥åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼Œ[[28](#bib.bib28), [33](#bib.bib33), [44](#bib.bib44)]ï¼‰å¯¹ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ChatGPTå’ŒGPT4çš„æˆåŠŸåšå‡ºäº†é‡è¦è´¡çŒ®[[1](#bib.bib1)]ã€‚æœ€è¿‘ï¼Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰[[30](#bib.bib30)]
    å®è´¨ä¸Šè§£å†³äº†ä¸RLHFç›¸åŒçš„é—®é¢˜ï¼Œä½†ç»•è¿‡äº†å¥–åŠ±æ¨¡å‹çš„è®­ç»ƒï¼Œå› æ­¤é€Ÿåº¦æ›´å¿«ï¼Œèµ„æºæ•ˆç‡æ›´é«˜ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒDPOåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ä¹Ÿå®ç°äº†ä¸RLHFç›¸å½“æˆ–æ›´ä¼˜çš„è¡¨ç°ï¼Œå¦‚åœ¨Llama3ã€Zephyr
    [[37](#bib.bib37)]ã€Neural Chatã€BTLM-DPO [[18](#bib.bib18)]ç­‰ä¸­å¯¹LLMsè¿›è¡Œå¾®è°ƒã€‚DPOçš„æˆåŠŸå¸å¼•äº†å¤§é‡ç ”ç©¶å…³æ³¨ï¼Œå¯¼è‡´äº†è¶…è¶Šæˆå¯¹æ’åçš„å˜ä½“[[32](#bib.bib32)]ï¼Œä»¥åŠå¯¹æŸå¤±å‚æ•°åŒ–çš„ç»Ÿä¸€è§‚ç‚¹[[2](#bib.bib2),
    [15](#bib.bib15), [36](#bib.bib36), [40](#bib.bib40)]ç­‰ã€‚
- en: 'Notwithstanding the successes achieved by RLHF and DPO, both are limited by
    the restrictive assumption that the underlying preference follows the Bradley-Terry
    (BT) model [[5](#bib.bib5)]. In particular, the degree of possible agreement or
    disagreement in response to different prompts is not accounted for in the objective
    function. For instance, people are more likely to agree on â€œ$1+1=$.â€ as opposed
    to â€œWhat is the best city to live in the U.S.? // New York.â€ In the context of
    language models, this concerns the issue of dispersion of the next-token prediction,
    which was also observed in [[9](#bib.bib9), [16](#bib.bib16)]. See Section [5.1](#S5.SS1
    "5.1\. Evidence of preference dispersion â€£ 5\. Experiments â€£ Mallows-DPO: Fine-Tune
    Your LLM with Preference Dispersions") for dispersion analysis of IMDB and Anthropic
    HH datasets.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 'å°½ç®¡RLHFå’ŒDPOéƒ½å–å¾—äº†ä¸€å®šçš„æˆåŠŸï¼Œä½†ä¸¤è€…éƒ½å—åˆ°é™åˆ¶æ€§å‡è®¾çš„åˆ¶çº¦ï¼Œå³å‡è®¾åŸºç¡€åå¥½éµå¾ªBradley-Terry (BT)æ¨¡å‹[[5](#bib.bib5)]ã€‚ç‰¹åˆ«æ˜¯ï¼Œç›®æ ‡å‡½æ•°ä¸­æ²¡æœ‰è€ƒè™‘å¯¹ä¸åŒæç¤ºçš„å¯èƒ½ä¸€è‡´æ€§æˆ–ä¸ä¸€è‡´æ€§çš„ç¨‹åº¦ã€‚ä¾‹å¦‚ï¼Œäººä»¬æ›´å®¹æ˜“åœ¨â€œ$1+1=$â€è¿™ä¸€é—®é¢˜ä¸Šè¾¾æˆä¸€è‡´ï¼Œè€Œä¸æ˜¯åœ¨â€œç¾å›½æœ€å¥½çš„åŸå¸‚æ˜¯å“ªé‡Œï¼Ÿ
    // çº½çº¦ã€‚â€è¿™ä¸ªé—®é¢˜ä¸Šã€‚åœ¨è¯­è¨€æ¨¡å‹çš„èƒŒæ™¯ä¸‹ï¼Œè¿™æ¶‰åŠåˆ°ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹çš„åˆ†æ•£é—®é¢˜ï¼Œè¿™ä¹Ÿåœ¨[[9](#bib.bib9), [16](#bib.bib16)]ä¸­è¢«è§‚å¯Ÿåˆ°ã€‚æœ‰å…³IMDBå’ŒAnthropic
    HHæ•°æ®é›†çš„åˆ†æ•£åˆ†æï¼Œè¯·å‚è§[5.1](#S5.SS1 "5.1\. Evidence of preference dispersion â€£ 5\. Experiments
    â€£ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")ã€‚'
- en: 'The purpose of this paper is to formalize the idea of prompt dispersion in
    the design of DPO. We adapt Mallowsâ€™ preference ranking theory [[12](#bib.bib12),
    [24](#bib.bib24)], a family of ranking models that provide a natural carrier for
    prompt dispersion, and propose the following decomposition/factorization of the
    (latent) reward function:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡çš„ç›®çš„æ˜¯åœ¨DPOè®¾è®¡ä¸­å½¢å¼åŒ–æç¤ºåˆ†æ•£çš„æ€æƒ³ã€‚æˆ‘ä»¬é‡‡ç”¨äº†Mallowsçš„åå¥½æ’åºç†è®º[[12](#bib.bib12), [24](#bib.bib24)]ï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªç„¶é€‚ç”¨äºæç¤ºåˆ†æ•£çš„æ’åºæ¨¡å‹å®¶æ—ï¼Œå¹¶æå‡ºäº†(æ½œåœ¨)å¥–åŠ±å‡½æ•°çš„ä»¥ä¸‹åˆ†è§£/å› å¼åˆ†è§£ï¼š
- en: '|  | $\mbox{reward}(\mbox{prompt, completion})=\mbox{dispersion}(\mbox{prompt})\times\mbox{scaled
    reward}(\mbox{completion}\mid\mbox{prompt}),$ |  |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mbox{reward}(\mbox{prompt, completion})=\mbox{dispersion}(\mbox{prompt})\times\mbox{scaled
    reward}(\mbox{completion}\mid\mbox{prompt}),$ |  |'
- en: where â€œpromptâ€ and â€œcompletionâ€ correspond, respectively, to question and answer.
    This decomposition allows to specify the diverse level of prompt dispersions hidden
    in the DPO, which is translated into a prompt-dependent factor â€“ the dispersion
    index in the preference likelihood. The scaled reward is given by the relative
    rank of the (possible) completions, which further enhances the model interpretability.
    We then leverage the change of variables technique to propose two models, Mallows-$\theta$-DPO.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­â€œpromptâ€å’Œâ€œcompletionâ€åˆ†åˆ«å¯¹åº”é—®é¢˜å’Œç­”æ¡ˆã€‚è¿™ç§åˆ†è§£å…è®¸æŒ‡å®šDPOä¸­éšè—çš„æç¤ºåˆ†æ•£çš„å¤šæ ·åŒ–æ°´å¹³ï¼Œè¿™è½¬åŒ–ä¸ºä¸€ä¸ªæç¤ºç›¸å…³çš„å› å­â€”â€”åå¥½ä¼¼ç„¶ä¸­çš„åˆ†æ•£æŒ‡æ•°ã€‚ç¼©æ”¾å¥–åŠ±ç”±ï¼ˆå¯èƒ½çš„ï¼‰å®Œæˆçš„ç›¸å¯¹æ’åç»™å‡ºï¼Œè¿™è¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬åˆ©ç”¨å˜é‡å˜åŒ–æŠ€æœ¯æå‡ºäº†ä¸¤ä¸ªæ¨¡å‹ï¼Œå³Mallows-$\theta$-DPOã€‚
- en: The main contributions of this paper are as follows. First, we formalize the
    idea of prompt dispersion in DPO, and develop the Mallows-DPO approach to implement
    this idea, so as to improve and generalize DPO. Second, we propose approximations
    to the dispersion index so as to facilitate computation. We also provide various
    analytical results for Mallows-DPO, which lead to various new insights on existing
    DPO models, including a generalized $\Psi$PO model that unifies all DPO models
    (including Mallows-DPO). Finally, our experiments on bandit, IMDB and Anthropic
    HH dataset all show clear advantages of Mallows-DPO over (BT-)DPO.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡çš„ä¸»è¦è´¡çŒ®å¦‚ä¸‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åœ¨DPOä¸­å½¢å¼åŒ–äº†æç¤ºåˆ†æ•£çš„æ€æƒ³ï¼Œå¹¶å¼€å‘äº†Mallows-DPOæ–¹æ³•æ¥å®ç°è¿™ä¸€æ€æƒ³ï¼Œä»è€Œæ”¹è¿›å’Œæ¨å¹¿DPOã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†å¯¹åˆ†æ•£æŒ‡æ•°çš„è¿‘ä¼¼ï¼Œä»¥ä¾¿äºè®¡ç®—ã€‚æˆ‘ä»¬è¿˜æä¾›äº†Mallows-DPOçš„å„ç§åˆ†æç»“æœï¼Œè¿™äº›ç»“æœä¸ºç°æœ‰çš„DPOæ¨¡å‹å¸¦æ¥äº†æ–°çš„è§è§£ï¼ŒåŒ…æ‹¬ä¸€ä¸ªç»Ÿä¸€æ‰€æœ‰DPOæ¨¡å‹ï¼ˆåŒ…æ‹¬Mallows-DPOï¼‰çš„å¹¿ä¹‰$\Psi$POæ¨¡å‹ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨banditã€IMDBå’ŒAnthropic
    HHæ•°æ®é›†ä¸Šçš„å®éªŒå‡æ˜¾ç¤ºå‡ºMallows-DPOç›¸å¯¹äº(BT-)DPOçš„æ˜æ˜¾ä¼˜åŠ¿ã€‚
- en: Other Related Works. Existing literature studying personalization in dialogue
    generation such as [[16](#bib.bib16), [21](#bib.bib21)] have also paid attention
    to the diversity of human preferences (â€œthere are a thousand Hamlets in a thousand
    peopleâ€™s eyesâ€ [[16](#bib.bib16)]); and [[27](#bib.bib27)] proposes a Nash game
    model to incorporate this diversity. There are also recent works that propose
    learning the online preferences [[8](#bib.bib8), [34](#bib.bib34)] or learning
    from AI feedbacks [[4](#bib.bib4), [10](#bib.bib10), [20](#bib.bib20)]. Studies
    to improve the design and capabilities of RLHF include [[14](#bib.bib14), [19](#bib.bib19),
    [38](#bib.bib38), [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43)].
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä»–ç›¸å…³å·¥ä½œã€‚ç°æœ‰æ–‡çŒ®ç ”ç©¶äº†å¯¹è¯ç”Ÿæˆä¸­çš„ä¸ªæ€§åŒ–ï¼Œå¦‚ [[16](#bib.bib16), [21](#bib.bib21)] ä¹Ÿå…³æ³¨äººç±»åå¥½çš„å¤šæ ·æ€§ï¼ˆâ€œåƒäººçœ¼ä¸­æœ‰åƒä¸ªå“ˆå§†é›·ç‰¹â€
    [[16](#bib.bib16)]ï¼‰ï¼›è€Œ [[27](#bib.bib27)] æå‡ºäº†ä¸€ä¸ªçº³ä»€åšå¼ˆæ¨¡å‹æ¥èåˆè¿™ç§å¤šæ ·æ€§ã€‚ä¹Ÿæœ‰æœ€è¿‘çš„ç ”ç©¶æå‡ºäº†å­¦ä¹ åœ¨çº¿åå¥½
    [[8](#bib.bib8), [34](#bib.bib34)] æˆ–è€…ä» AI åé¦ˆä¸­å­¦ä¹  [[4](#bib.bib4), [10](#bib.bib10),
    [20](#bib.bib20)]ã€‚æå‡ RLHF è®¾è®¡å’Œèƒ½åŠ›çš„ç ”ç©¶åŒ…æ‹¬ [[14](#bib.bib14), [19](#bib.bib19), [38](#bib.bib38),
    [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43)]ã€‚
- en: 'The remainder of the paper is organized as follows. Background materials on
    RLHF and DPO are highlighted in Section [2](#S2 "2\. Preliminaries â€£ Mallows-DPO:
    Fine-Tune Your LLM with Preference Dispersions"). Section [3](#S3 "3\. DPO based
    on Mallows Ranking Models â€£ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")
    focuses on the development of Mallows-DPO, followed by more analytical results
    and various perspectives in Section [4](#S4 "4\. Perspectives on Mallows-DPO â€£
    Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions"). Experimental results
    are detailed in Section [5](#S5 "5\. Experiments â€£ Mallows-DPO: Fine-Tune Your
    LLM with Preference Dispersions"), and concluding remarks in Section [6](#S6 "6\.
    Conclusion â€£ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions").'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 'è®ºæ–‡çš„å…¶ä½™éƒ¨åˆ†ç»„ç»‡å¦‚ä¸‹ã€‚ç¬¬[2](#S2 "2\. Preliminaries â€£ Mallows-DPO: Fine-Tune Your LLM with
    Preference Dispersions")èŠ‚é‡ç‚¹ä»‹ç»äº† RLHF å’Œ DPO çš„èƒŒæ™¯ææ–™ã€‚[3](#S3 "3\. DPO based on Mallows
    Ranking Models â€£ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")èŠ‚å…³æ³¨äº
    Mallows-DPO çš„å‘å±•ï¼Œæ¥ä¸‹æ¥çš„[4](#S4 "4\. Perspectives on Mallows-DPO â€£ Mallows-DPO: Fine-Tune
    Your LLM with Preference Dispersions")èŠ‚æä¾›äº†æ›´å¤šçš„åˆ†æç»“æœå’Œå„ç§è§†è§’ã€‚å®éªŒç»“æœè¯¦è§[5](#S5 "5\. Experiments
    â€£ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")èŠ‚ï¼Œç»“è®ºåˆ™åœ¨[6](#S6 "6\.
    Conclusion â€£ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")èŠ‚ä¸­ç»™å‡ºã€‚'
- en: 2\. Preliminaries
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. åŸºç¡€çŸ¥è¯†
- en: 2.1\. Supervised fine-tuning (SFT)
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰
- en: Both RLHF and DPO (reviewed below) start with fine-tuning a pre-trained large
    language model by supervised learning on high-quality data for some downstream
    tasks of interest (e.g. dialogue, summarization, etc.), to acquire a model $\pi^{\mathrm{SFT}}$.
    This step is referred to as the SFT phase. For instance, for training InstructGPT
    [[28](#bib.bib28)], GPT-3 [[6](#bib.bib6)] is first fine-tuned on the given input
    prompt distribution.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF å’Œ DPOï¼ˆä¸‹æ–‡å°†å›é¡¾ï¼‰éƒ½ä»é€šè¿‡ç›‘ç£å­¦ä¹ åœ¨é«˜è´¨é‡æ•°æ®ä¸Šå¾®è°ƒé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹å¼€å§‹ï¼Œä»¥å¤„ç†ä¸€äº›æ„Ÿå…´è¶£çš„ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚å¯¹è¯ã€æ€»ç»“ç­‰ï¼‰ï¼Œä»¥è·å¾—ä¸€ä¸ªæ¨¡å‹
    $\pi^{\mathrm{SFT}}$ã€‚è¿™ä¸ªæ­¥éª¤ç§°ä¸º SFT é˜¶æ®µã€‚ä¾‹å¦‚ï¼Œåœ¨è®­ç»ƒ InstructGPT [[28](#bib.bib28)] æ—¶ï¼Œé¦–å…ˆå¯¹
    GPT-3 [[6](#bib.bib6)] è¿›è¡Œé’ˆå¯¹ç»™å®šè¾“å…¥æç¤ºåˆ†å¸ƒçš„å¾®è°ƒã€‚
- en: 2.2\. RLHF
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. RLHF
- en: '[[28](#bib.bib28), [33](#bib.bib33), [44](#bib.bib44)]. On top of $\pi^{\mathrm{SFT}}$
    to produce pairs of answers (or, â€œcompletionsâ€), $\left\{y_{1},y_{2}\right\}\sim\pi(y\mid
    x)$. The preferences are assumed to be generated by some latent reward model $r^{*}(x,y)$
    in which the prompt $x$ is the action.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[[28](#bib.bib28), [33](#bib.bib33), [44](#bib.bib44)]ã€‚åœ¨ $\pi^{\mathrm{SFT}}$
    çš„åŸºç¡€ä¸Šç”Ÿæˆç­”æ¡ˆå¯¹ï¼ˆæˆ–â€œå®Œæˆâ€ï¼‰ï¼Œ$\left\{y_{1},y_{2}\right\}\sim\pi(y\mid x)$ã€‚å‡è®¾è¿™äº›åå¥½ç”±æŸä¸ªæ½œåœ¨çš„å¥–åŠ±æ¨¡å‹
    $r^{*}(x,y)$ ç”Ÿæˆï¼Œå…¶ä¸­æç¤º $x$ æ˜¯è¡ŒåŠ¨ã€‚'
- en: '(a) Reward model. To capture the underlying human preferences, RLHF assumes
    the Bradley-Terry model [[5](#bib.bib5)] that stipulates the pairwise preference
    distribution:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: (a) å¥–åŠ±æ¨¡å‹ã€‚ä¸ºäº†æ•æ‰æ½œåœ¨çš„äººç±»åå¥½ï¼ŒRLHF å‡è®¾ä½¿ç”¨ Bradley-Terry æ¨¡å‹ [[5](#bib.bib5)]ï¼Œè¯¥æ¨¡å‹è§„å®šäº†æˆå¯¹åå¥½çš„åˆ†å¸ƒï¼š
- en: '|  | $1$2 |  | (1) |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: where $\sigma(s):=\frac{1}{1+e^{-s}}$ is the sigmoid function. Given access
    to a static dataset of comparisons
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\sigma(s):=\frac{1}{1+e^{-s}}$ æ˜¯ sigmoid å‡½æ•°ã€‚ç»™å®šå¯¹é™æ€æ¯”è¾ƒæ•°æ®é›†çš„è®¿é—®
- en: '|  | $\mathcal{D}=\left\{x^{(i)},y_{w}^{(i)},y_{l}^{(i)}\right\}_{i=1,\ldots,N},$
    |  |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{D}=\left\{x^{(i)},y_{w}^{(i)},y_{l}^{(i)}\right\}_{i=1,\ldots,N},$
    |  |'
- en: 'RLHF seeks to approximate the latent reward $r^{*}(x,y)$, and estimate the
    parameters by minimizing the (negative) log-likelihood loss:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF æ—¨åœ¨è¿‘ä¼¼æ½œåœ¨å¥–åŠ± $r^{*}(x,y)$ï¼Œå¹¶é€šè¿‡æœ€å°åŒ–ï¼ˆè´Ÿçš„ï¼‰å¯¹æ•°ä¼¼ç„¶æŸå¤±æ¥ä¼°è®¡å‚æ•°ï¼š
- en: '|  | $1$2 |  | (2) |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: 'Denote by $r_{\psi_{*}}(x,y)$ the solution to the problem in ([2](#S2.E2 "In
    2.2\. RLHF â€£ 2\. Preliminaries â€£ Mallows-DPO: Fine-Tune Your LLM with Preference
    Dispersions")).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è®° $r_{\psi_{*}}(x,y)$ ä¸º ([2](#S2.E2 "åœ¨ 2.2\. RLHF â€£ 2\. åˆæ­¥æ¦‚å¿µ â€£ Mallows-DPOï¼šé€šè¿‡åå¥½åˆ†æ•£æ¥å¾®è°ƒä½ çš„
    LLM")) ä¸­é—®é¢˜çš„è§£ã€‚
- en: '(b) RL. The learned reward function $r_{\psi_{*}}(x,y)$ is then used to provide
    feedback to the language model. More precisely, the following KL-regularized RL
    problem is considered:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: (b) RLã€‚å­¦ä¹ åˆ°çš„å¥–åŠ±å‡½æ•° $r_{\psi_{*}}(x,y)$ éšåç”¨äºå‘è¯­è¨€æ¨¡å‹æä¾›åé¦ˆã€‚æ›´å‡†ç¡®åœ°è¯´ï¼Œè€ƒè™‘ä»¥ä¸‹ KL æ­£åˆ™åŒ– RL é—®é¢˜ï¼š
- en: '|  |  | $\displaystyle\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi(y\mid
    x)}\left[r_{\psi_{*}}(x,y)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right]$ |  | (3) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi(y\mid
    x)}\left[r_{\psi_{*}}(x,y)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right]$ |  | (3) |'
- en: '|  |  | $1$2 |  |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: where $$\beta>. The regularization is important as it prevents deviating
    too far from the SFT model that is trained to conform to the true preference,
    while maintaining the generation diversity to avoid mode-collapsing to a single
    high-reward answer. In terms of optimization, RLHF leverages RL algorithms due
    to the discrete nature of the language and the associated non-differentiable property
    of the objective.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $$\beta>ã€‚æ­£åˆ™åŒ–å¾ˆé‡è¦ï¼Œå› ä¸ºå®ƒé˜²æ­¢åç¦»è®­ç»ƒä»¥ç¬¦åˆçœŸå®åå¥½çš„ SFT æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆçš„å¤šæ ·æ€§ï¼Œä»¥é¿å…æ¨¡å¼å´©æºƒä¸ºå•ä¸€çš„é«˜å¥–åŠ±ç­”æ¡ˆã€‚åœ¨ä¼˜åŒ–æ–¹é¢ï¼ŒRLHF
    åˆ©ç”¨ RL ç®—æ³•ï¼Œå› ä¸ºè¯­è¨€çš„ç¦»æ•£æ€§è´¨å’Œç›®æ ‡å‡½æ•°çš„ä¸å¯å¾®æ€§ã€‚
- en: 'In view of ([3](#S2.E3 "In 2.2\. RLHF â€£ 2\. Preliminaries â€£ Mallows-DPO: Fine-Tune
    Your LLM with Preference Dispersions")), RLHF uses the reward function $r(x,y)=r_{\psi}(x,y)-\beta\left(\log\pi(y\mid
    x)-\log\pi_{\text{ref }}(y\mid x)\right)$, and solves the RL problem by proximal
    policy optimization [[31](#bib.bib31)].'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ® ([3](#S2.E3 "åœ¨ 2.2\. RLHF â€£ 2\. åˆæ­¥æ¦‚å¿µ â€£ Mallows-DPOï¼šé€šè¿‡åå¥½åˆ†æ•£æ¥å¾®è°ƒä½ çš„ LLM"))ï¼ŒRLHF
    ä½¿ç”¨å¥–åŠ±å‡½æ•° $r(x,y)=r_{\psi}(x,y)-\beta\left(\log\pi(y\mid x)-\log\pi_{\text{ref }}(y\mid
    x)\right)$ï¼Œå¹¶é€šè¿‡é‚»è¿‘ç­–ç•¥ä¼˜åŒ–è§£å†³ RL é—®é¢˜ [[31](#bib.bib31)]ã€‚
- en: 2.3\. DPO
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. DPO
- en: '[[30](#bib.bib30)]. One disadvantage of RLHF is that the RL step often requires
    substantial computational effort (e.g., to carry out the proximal policy optimization).
    The idea of DPO is to combine the two steps (a)â€“(b) in RLHF into a single one,
    bypassing the computation in the RL step.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[[30](#bib.bib30)]ã€‚RLHF çš„ä¸€ä¸ªç¼ºç‚¹æ˜¯ RL æ­¥éª¤é€šå¸¸éœ€è¦å¤§é‡çš„è®¡ç®—å·¥ä½œï¼ˆä¾‹å¦‚ï¼Œè¿›è¡Œé‚»è¿‘ç­–ç•¥ä¼˜åŒ–ï¼‰ã€‚DPO çš„æ€æƒ³æ˜¯å°† RLHF
    ä¸­çš„ä¸¤ä¸ªæ­¥éª¤ (a)â€“(b) åˆå¹¶ä¸ºä¸€ä¸ªï¼Œç»•è¿‡ RL æ­¥éª¤ä¸­çš„è®¡ç®—ã€‚'
- en: 'The key idea is that given a reward function $r(x,y)$, the problem in ([3](#S2.E3
    "In 2.2\. RLHF â€£ 2\. Preliminaries â€£ Mallows-DPO: Fine-Tune Your LLM with Preference
    Dispersions")) has a closed-form solution:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å…³é”®æ€æƒ³æ˜¯ï¼Œç»™å®šå¥–åŠ±å‡½æ•° $r(x,y)$ï¼Œåœ¨ ([3](#S2.E3 "åœ¨ 2.2\. RLHF â€£ 2\. åˆæ­¥æ¦‚å¿µ â€£ Mallows-DPOï¼šé€šè¿‡åå¥½åˆ†æ•£æ¥å¾®è°ƒä½ çš„
    LLM")) ä¸­çš„é—®é¢˜æœ‰ä¸€ä¸ªé—­å¼è§£ï¼š
- en: '|  | $\pi_{r}(y\mid x)=\frac{1}{Z(x)}\pi_{\text{ref }}(y\mid x)\exp\left(\frac{1}{\beta}r(x,y)\right),$
    |  | (4) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $\pi_{r}(y\mid x)=\frac{1}{Z(x)}\pi_{\text{ref }}(y\mid x)\exp\left(\frac{1}{\beta}r(x,y)\right),$
    |  | (4) |'
- en: 'where $Z(x)=\sum_{y}\pi_{\text{ref }}(y\mid x)\exp\left(\frac{1}{\beta}r(x,y)\right)$
    is a normalizing constant. Rewrite the above as:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $Z(x)=\sum_{y}\pi_{\text{ref }}(y\mid x)\exp\left(\frac{1}{\beta}r(x,y)\right)$
    æ˜¯ä¸€ä¸ªå½’ä¸€åŒ–å¸¸æ•°ã€‚å°†ä¸Šè¿°å†…å®¹é‡å†™ä¸ºï¼š
- en: '|  | $r(x,y)=\beta\log\frac{\pi_{r}(y\mid x)}{\pi_{\text{ref }}(y\mid x)}+\beta\log
    Z(x).$ |  | (5) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $r(x,y)=\beta\log\frac{\pi_{r}(y\mid x)}{\pi_{\text{ref }}(y\mid x)}+\beta\log
    Z(x).$ |  | (5) |'
- en: 'Through this change of variables, the latent reward $r^{*}(x,y)$. Substituting
    this $r^{*}$ expression into ([1](#S2.E1 "In 2.2\. RLHF â€£ 2\. Preliminaries â€£
    Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")) yields:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿™ç§å˜é‡å˜æ¢ï¼Œæ½œåœ¨å¥–åŠ± $r^{*}(x,y)$ã€‚å°†è¿™ä¸ª $r^{*}$ è¡¨è¾¾å¼ä»£å…¥ ([1](#S2.E1 "åœ¨ 2.2\. RLHF â€£ 2\.
    åˆæ­¥æ¦‚å¿µ â€£ Mallows-DPOï¼šé€šè¿‡åå¥½åˆ†æ•£æ¥å¾®è°ƒä½ çš„ LLM")) å¾—åˆ°ï¼š
- en: '|  | $1$2 |  | (6) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (6) |'
- en: where $Z^{*}(x)$.)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $Z^{*}(x)$ã€‚)
- en: 'The expression in ([6](#S2.E6 "In 2.3\. DPO â€£ 2\. Preliminaries â€£ Mallows-DPO:
    Fine-Tune Your LLM with Preference Dispersions")) motivates the DPO objective:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨è¾¾å¼ä¸­çš„ ([6](#S2.E6 "åœ¨ 2.3\. DPO â€£ 2\. åˆæ­¥æ¦‚å¿µ â€£ Mallows-DPOï¼šé€šè¿‡åå¥½åˆ†æ•£æ¥å¾®è°ƒä½ çš„ LLM")) æ¿€å‘äº†
    DPO ç›®æ ‡ï¼š
- en: '|  | $1$2 |  | (7) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (7) |'
- en: which is a supervised learning problem, requiring much less computation than
    the RLHF.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªç›‘ç£å­¦ä¹ é—®é¢˜ï¼Œç›¸æ¯” RLHF éœ€è¦çš„è®¡ç®—è¦å°‘å¾—å¤šã€‚
- en: 3\. DPO based on Mallows Ranking Models
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. åŸºäº Mallows æ’åºæ¨¡å‹çš„ DPO
- en: 3.1\. Mallows ranking models
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. Mallows æ’åºæ¨¡å‹
- en: 'For $n\geq 1$. Consider the following preference probability:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº $n\geq 1$ã€‚è€ƒè™‘ä»¥ä¸‹åå¥½æ¦‚ç‡ï¼š
- en: '|  | $\mathbb{P}_{\phi,\mu_{0},d}(\mu):=\frac{1}{Z(\phi,d)}\phi^{d\left(\mu,\mu_{0}\right)}\quad\text{
    for }\mu\in\mathfrak{S}_{n},$ |  | (8) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{P}_{\phi,\mu_{0},d}(\mu):=\frac{1}{Z(\phi,d)}\phi^{d\left(\mu,\mu_{0}\right)}\quad\text{
    å¯¹äº }\mu\in\mathfrak{S}_{n},$ |  | (8) |'
- en: 'where $\phi\in(0,1]$ is a discrepancy function that is right invariant: $d(\mu_{1},\mu_{2})=d\left(\mu_{1}\circ\mu_{2}^{-1},id\right)$,
    the distribution ([8](#S3.E8 "In 3.1\. Mallows ranking models â€£ 3\. DPO based
    on Mallows Ranking Models â€£ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions"))
    is concentrated on $\mu_{0}$ items) with pairwise preferences, Mallows [[24](#bib.bib24)]
    considered two specific cases of the discrepancy function in ([8](#S3.E8 "In 3.1\.
    Mallows ranking models â€£ 3\. DPO based on Mallows Ranking Models â€£ Mallows-DPO:
    Fine-Tune Your LLM with Preference Dispersions")):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\phi\in(0,1]$ æ˜¯ä¸€ä¸ªå³ä¸å˜çš„å·®å¼‚å‡½æ•°ï¼š$d(\mu_{1},\mu_{2})=d\left(\mu_{1}\circ\mu_{2}^{-1},id\right)$ï¼Œåˆ†å¸ƒ
    ([8](#S3.E8 "åœ¨ 3.1\. Mallows æ’åæ¨¡å‹ â€£ 3\. åŸºäº Mallows æ’åæ¨¡å‹çš„ DPO â€£ Mallows-DPOï¼šç”¨åå¥½åˆ†æ•£å¾®è°ƒä½ çš„
    LLM")) é›†ä¸­åœ¨ $\mu_{0}$ é¡¹ç›®ä¸Šï¼Œå¹¶å…·æœ‰æˆå¯¹åå¥½ï¼ŒMallows [[24](#bib.bib24)] è€ƒè™‘äº† ([8](#S3.E8 "åœ¨
    3.1\. Mallows æ’åæ¨¡å‹ â€£ 3\. åŸºäº Mallows æ’åæ¨¡å‹çš„ DPO â€£ Mallows-DPOï¼šç”¨åå¥½åˆ†æ•£å¾®è°ƒä½ çš„ LLM")) ä¸­å·®å¼‚å‡½æ•°çš„ä¸¤ä¸ªç‰¹å®šæƒ…å†µï¼š
- en: â€¢
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Mallows-$\theta$ is the Spearmanâ€™s rho,
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Mallows-$\theta$ æ˜¯ Spearman çš„ rhoï¼Œ
- en: â€¢
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Mallows-$\phi$ is the Kendallâ€™s tau,
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Mallows-$\phi$ æ˜¯ Kendall çš„ tauï¼Œ
- en: where $\operatorname{inv}(\mu):=\#\left\{(i,j)\in[n]^{2}:i<j\right.$.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\operatorname{inv}(\mu):=\#\left\{(i,j)\in[n]^{2}:i<j\right.$ã€‚
- en: 'The general form in ([8](#S3.E8 "In 3.1\. Mallows ranking models â€£ 3\. DPO
    based on Mallows Ranking Models â€£ Mallows-DPO: Fine-Tune Your LLM with Preference
    Dispersions")) was suggested by [[12](#bib.bib12)] along with other discrepancy
    functions (e.g. Cayley, Hamming, Ulam distances, etc.) See [[11](#bib.bib11),
    [12](#bib.bib12), [13](#bib.bib13)] for the related group representation approach
    to ranked, or partially ranked data. Note that the Mallows-$\theta$ (see [[26](#bib.bib26),
    [29](#bib.bib29), [35](#bib.bib35)].) In the context of language models, this
    conforms to a possibly infinite number of completions given a prompt, and allows
    interpreting unseen completions conceptually.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€èˆ¬å½¢å¼åœ¨ ([8](#S3.E8 "åœ¨ 3.1\. Mallows æ’åæ¨¡å‹ â€£ 3\. åŸºäº Mallows æ’åæ¨¡å‹çš„ DPO â€£ Mallows-DPOï¼šç”¨åå¥½åˆ†æ•£å¾®è°ƒä½ çš„
    LLM")) ä¸­ç”± [[12](#bib.bib12)] æå‡ºï¼Œå¹¶ä¸”è¿˜æœ‰å…¶ä»–å·®å¼‚å‡½æ•°ï¼ˆä¾‹å¦‚ Cayleyã€Hammingã€Ulam è·ç¦»ç­‰ï¼‰ã€‚æœ‰å…³æ’åæ•°æ®æˆ–éƒ¨åˆ†æ’åæ•°æ®çš„ç›¸å…³ç¾¤ä½“è¡¨ç¤ºæ–¹æ³•ï¼Œè¯·å‚é˜…
    [[11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13)]ã€‚è¯·æ³¨æ„ Mallows-$\theta$ï¼ˆå‚è§
    [[26](#bib.bib26), [29](#bib.bib29), [35](#bib.bib35)]ï¼‰ã€‚åœ¨è¯­è¨€æ¨¡å‹çš„èƒŒæ™¯ä¸‹ï¼Œè¿™ç¬¦åˆç»™å®šæç¤ºçš„å¯èƒ½æ— é™ä¸ªè¡¥å…¨ï¼Œå¹¶å…è®¸ä»æ¦‚å¿µä¸Šè§£é‡Šæœªè§è¿‡çš„è¡¥å…¨ã€‚
- en: 3.2\. Mallows-DPO
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. Mallows-DPO
- en: 'Now we adapt Mallows ranking models highlighted above to the setting of language
    models. First, denote by $\mu(\cdot\mid x)$, such that the preference distribution
    is:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å°†ä¸Šè¿° Mallows æ’åæ¨¡å‹è°ƒæ•´ä¸ºè¯­è¨€æ¨¡å‹çš„è®¾ç½®ã€‚é¦–å…ˆï¼Œè®°ä¸º $\mu(\cdot\mid x)$ï¼Œä½¿å¾—åå¥½åˆ†å¸ƒä¸ºï¼š
- en: '|  | $p^{*}\left(y_{1}\succ y_{2}\mid x\right)=\mathbb{P}\left(\mu(y_{1}\mid
    x)<\mu\left(y_{2}\mid x\right)\right).$ |  | (9) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $p^{*}\left(y_{1}\succ y_{2}\mid x\right)=\mathbb{P}\left(\mu(y_{1}\mid
    x)<\mu\left(y_{2}\mid x\right)\right).$ |  | (9) |'
- en: 'Next, for the preference probability in ([8](#S3.E8 "In 3.1\. Mallows ranking
    models â€£ 3\. DPO based on Mallows Ranking Models â€£ Mallows-DPO: Fine-Tune Your
    LLM with Preference Dispersions")), given an input prompt $x$ may be computationally
    hard. Similar to RLHF in ([3](#S2.E3 "In 2.2\. RLHF â€£ 2\. Preliminaries â€£ Mallows-DPO:
    Fine-Tune Your LLM with Preference Dispersions")), our goal here is:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œç»™å®šè¾“å…¥æç¤º $x$ï¼Œåœ¨ ([8](#S3.E8 "åœ¨ 3.1\. Mallows æ’åæ¨¡å‹ â€£ 3\. åŸºäº Mallows æ’åæ¨¡å‹çš„ DPO
    â€£ Mallows-DPOï¼šç”¨åå¥½åˆ†æ•£å¾®è°ƒä½ çš„ LLM")) ä¸­ï¼Œåå¥½æ¦‚ç‡å¯èƒ½è®¡ç®—èµ·æ¥å¾ˆå›°éš¾ã€‚ç±»ä¼¼äº ([3](#S2.E3 "åœ¨ 2.2\. RLHF â€£
    2\. åˆæ­¥ â€£ Mallows-DPOï¼šç”¨åå¥½åˆ†æ•£å¾®è°ƒä½ çš„ LLM")) ä¸­çš„ RLHFï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œçš„ç›®æ ‡æ˜¯ï¼š
- en: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(y\mid
    x)}\left[-\mu_{0}(y\mid x)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right],$ |  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(y\mid
    x)}\left[-\mu_{0}(y\mid x)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right],$ |  |'
- en: 'where $r^{*}(x,y)$â€”note that a smaller rank is preferred as per ([9](#S3.E9
    "In 3.2\. Mallows-DPO â€£ 3\. DPO based on Mallows Ranking Models â€£ Mallows-DPO:
    Fine-Tune Your LLM with Preference Dispersions"))â€”and hence providing a natural
    candidate for the scaled reward that enhances model interpretation. This yields
    the change of variables similar to ([5](#S2.E5 "In 2.3\. DPO â€£ 2\. Preliminaries
    â€£ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $r^{*}(x,y)$â€”æ³¨æ„ï¼Œè¾ƒå°çš„æ’åæ˜¯é¦–é€‰ï¼ˆå‚è§ ([9](#S3.E9 "åœ¨ 3.2\. Mallows-DPO â€£ 3\. åŸºäº Mallows
    æ’åæ¨¡å‹çš„ DPO â€£ Mallows-DPOï¼šç”¨åå¥½åˆ†æ•£å¾®è°ƒä½ çš„ LLM"))ï¼‰â€”å› æ­¤æä¾›äº†ä¸€ä¸ªè‡ªç„¶çš„å€™é€‰ç¼©æ”¾å¥–åŠ±ï¼Œå¢å¼ºæ¨¡å‹è§£é‡Šã€‚è¿™äº§ç”Ÿäº†ç±»ä¼¼äº ([5](#S2.E5
    "åœ¨ 2.3\. DPO â€£ 2\. åˆæ­¥ â€£ Mallows-DPOï¼šç”¨åå¥½åˆ†æ•£å¾®è°ƒä½ çš„ LLM")) çš„å˜é‡å˜æ¢ï¼š
- en: '|  | $1$2 |  | (10) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (10) |'
- en: which â€œcleverly" avoids estimating $\mu_{0}(\cdot\mid x)$. We then derive the
    two versions of Mallows-DPO.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™â€œå·§å¦™åœ°â€é¿å…äº†ä¼°è®¡ $\mu_{0}(\cdot\mid x)$ã€‚ç„¶åæˆ‘ä»¬æ¨å¯¼äº† Mallows-DPO çš„ä¸¤ä¸ªç‰ˆæœ¬ã€‚
- en: Mallows-$\theta$, we have
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Mallows-$\theta$ï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $1$2 |  | (11) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (11) |'
- en: 'where $\log\phi(x)\in(-\infty,0)$. The change of variables in ([10](#S3.E10
    "In 3.2\. Mallows-DPO â€£ 3\. DPO based on Mallows Ranking Models â€£ Mallows-DPO:
    Fine-Tune Your LLM with Preference Dispersions")) leads to the objective:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\log\phi(x)\in(-\infty,0)$ã€‚ ([10](#S3.E10 "åœ¨ 3.2\. Mallows-DPO â€£ 3\. åŸºäº
    Mallows æ’åºæ¨¡å‹çš„ DPO â€£ Mallows-DPOï¼šç”¨åå¥½ç¦»æ•£åº¦å¾®è°ƒä½ çš„ LLM")) ä¸­çš„å˜é‡å˜æ¢å¯¼è‡´ç›®æ ‡ï¼š
- en: '|  |  | $\displaystyle\mathcal{L}_{\mathrm{MDPO}}\left(\pi;\pi_{\mathrm{ref}}\right):=$
    |  | (12) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathcal{L}_{\mathrm{MDPO}}\left(\pi;\pi_{\mathrm{ref}}\right):=$
    |  | (12) |'
- en: '|  |  | $1$2 |  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: 'In comparison with the objective of the BT-DPO (Bradley-Terry based DPO) in
    ([7](#S2.E7 "In 2.3\. DPO â€£ 2\. Preliminaries â€£ Mallows-DPO: Fine-Tune Your LLM
    with Preference Dispersions")), the objective of Mallows-$\theta$-DPO can be viewed
    as DPO with prompt dispersion. (In contrast, prompt dispersion is not present
    at all in the BT-DPO.)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ ([7](#S2.E7 "åœ¨ 2.3\. DPO â€£ 2\. åŸºç¡€çŸ¥è¯† â€£ Mallows-DPOï¼šç”¨åå¥½ç¦»æ•£åº¦å¾®è°ƒä½ çš„ LLM")) ä¸­çš„ BT-DPOï¼ˆåŸºäº
    Bradley-Terry çš„ DPOï¼‰ç›®æ ‡ç›¸æ¯”ï¼ŒMallows-$\theta$-DPO çš„ç›®æ ‡å¯ä»¥çœ‹ä½œæ˜¯å…·æœ‰æç¤ºç¦»æ•£åº¦çš„ DPOã€‚ï¼ˆç›¸åï¼ŒBT-DPO
    ä¸­å®Œå…¨æ²¡æœ‰æç¤ºç¦»æ•£åº¦ã€‚ï¼‰
- en: 'Mallows-$\phi$ model, it was shown in [[24](#bib.bib24)] (see also [[7](#bib.bib7),
    [25](#bib.bib25)]):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Mallows-$\phi$ æ¨¡å‹ï¼Œå·²åœ¨ [[24](#bib.bib24)] ä¸­å±•ç¤ºï¼ˆå‚è§ [[7](#bib.bib7), [25](#bib.bib25)]ï¼‰ï¼š
- en: '|  | $1$2 |  | (13) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (13) |'
- en: where
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­
- en: '|  | $1$2 |  | (14) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (14) |'
- en: or compactly,
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–ç®€æ´åœ°ï¼Œ
- en: '|  | $1$2 |  | (15) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (15) |'
- en: 'is the link function. Similar to Mallows-$\theta$-DPO:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯é“¾æ¥å‡½æ•°ã€‚ç±»ä¼¼äº Mallows-$\theta$-DPOï¼š
- en: '|  | $1$2 |  | (16) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (16) |'
- en: 'In comparison with the BT-DPO in ([7](#S2.E7 "In 2.3\. DPO â€£ 2\. Preliminaries
    â€£ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")), Mallows-$\phi$.
    Refer to Table [1](#S3.T1 "Table 1 â€£ 3.2\. Mallows-DPO â€£ 3\. DPO based on Mallows
    Ranking Models â€£ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")
    for a summary of the key features of the different models.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ ([7](#S2.E7 "åœ¨ 2.3\. DPO â€£ 2\. åŸºç¡€çŸ¥è¯† â€£ Mallows-DPOï¼šç”¨åå¥½ç¦»æ•£åº¦å¾®è°ƒä½ çš„ LLM")) ä¸­çš„ BT-DPO
    ç›¸æ¯”ï¼ŒMallows-$\phi$ã€‚æœ‰å…³ä¸åŒæ¨¡å‹å…³é”®ç‰¹æ€§çš„æ€»ç»“ï¼Œè¯·å‚è§è¡¨ [1](#S3.T1 "è¡¨ 1 â€£ 3.2\. Mallows-DPO â€£ 3\.
    åŸºäº Mallows æ’åºæ¨¡å‹çš„ DPO â€£ Mallows-DPOï¼šç”¨åå¥½ç¦»æ•£åº¦å¾®è°ƒä½ çš„ LLM")ã€‚
- en: Table 1. Key Features of DPO Models
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 1. DPO æ¨¡å‹çš„å…³é”®ç‰¹æ€§
- en: '| Model Class | Model Name | Dispersion | Link function |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| æ¨¡å‹ç±»åˆ« | æ¨¡å‹åç§° | ç¦»æ•£åº¦ | é“¾æ¥å‡½æ•° |'
- en: '| BT-DPO | DPO [[30](#bib.bib30)] | âœ— | sigmoid $\sigma(\cdot)$ |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| BT-DPO | DPO [[30](#bib.bib30)] | âœ— | sigmoid $\sigma(\cdot)$ |'
- en: '| Mallows-DPO | Mallows-$\theta$ |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Mallows-DPO | Mallows-$\theta$ |'
- en: '| Mallows-$\phi$ in ([15](#S3.E15 "In 3.2\. Mallows-DPO â€£ 3\. DPO based on
    Mallows Ranking Models â€£ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions"))
    |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Mallows-$\phi$ åœ¨ ([15](#S3.E15 "åœ¨ 3.2\. Mallows-DPO â€£ 3\. åŸºäº Mallows æ’åºæ¨¡å‹çš„
    DPO â€£ Mallows-DPOï¼šç”¨åå¥½ç¦»æ•£åº¦å¾®è°ƒä½ çš„ LLM")) |'
- en: 3.3\. Approximate the dispersion index
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. è¿‘ä¼¼ç¦»æ•£åº¦æŒ‡æ•°
- en: As the dispersion index $\phi(x)$ to the empirical output distribution of the
    pre-trained model.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºé¢„è®­ç»ƒæ¨¡å‹çš„ç»éªŒè¾“å‡ºåˆ†å¸ƒçš„ç¦»æ•£åº¦æŒ‡æ•° $\phi(x)$ã€‚
- en: 'Suppose the preference follows Mallows-$\phi$ model. There are two extreme
    cases:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾åå¥½éµå¾ª Mallows-$\phi$ æ¨¡å‹ã€‚æœ‰ä¸¤ç§æç«¯æƒ…å†µï¼š
- en: â€¢
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'When $-\log(\phi(x))\rightarrow\infty$, we have:'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å½“ $-\log(\phi(x))\rightarrow\infty$ æ—¶ï¼Œæˆ‘ä»¬æœ‰ï¼š
- en: '|  | $1$2 |  |'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Thus, the probability distribution of the next token will concentrate on a point
    mass.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä¸‹ä¸€ä¸ªæ ‡è®°çš„æ¦‚ç‡åˆ†å¸ƒå°†é›†ä¸­åœ¨ä¸€ä¸ªç‚¹è´¨é‡ä¸Šã€‚
- en: â€¢
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: When $-\log(\phi(x))\rightarrow 0$ so the next token will be uniformly distributed.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å½“ $-\log(\phi(x))\rightarrow 0$ æ—¶ï¼Œä¸‹ä¸€ä¸ªæ ‡è®°å°†è¢«å‡åŒ€åˆ†å¸ƒã€‚
- en: The above observation motivates us to use Shannonâ€™s entropy, for a discrete
    random variable $X\in\{x_{1},\ldots,x_{n}\}$ when $X$ is uniformly distributed.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°è§‚å¯Ÿä¿ƒä½¿æˆ‘ä»¬ä½¿ç”¨ Shannon ç†µï¼Œå¯¹äºç¦»æ•£éšæœºå˜é‡ $X\in\{x_{1},\ldots,x_{n}\}$ å½“ $X$ å‡åŒ€åˆ†å¸ƒæ—¶ã€‚
- en: 'For a given constant <math id=$$, we propose:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºç»™å®šçš„å¸¸é‡ <math id=$$ï¼Œæˆ‘ä»¬æå‡ºï¼š
- en: '|  | $-\phi^{*}\log\left(\frac{H(\pi(\cdot\mid x))}{\log(n)}\right),$ |  |
    (17) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | $-\phi^{*}\log\left(\frac{H(\pi(\cdot\mid x))}{\log(n)}\right),$ |  |
    (17) |'
- en: as a proxy to $-\log\phi(x)$.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸º $-\log\phi(x)$ çš„ä»£ç†ã€‚
- en: 'Here the hyperparameter $\phi^{*}$:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„è¶…å‚æ•° $\phi^{*}$ï¼š
- en: '|  | $H(\pi(\cdot\mid x))\approx\frac{1}{2}\sum_{i=1}^{N-1}\left[H(X_{i+1}\mid
    X_{i}=x^{w}_{i})+H(X_{i+1}\mid X_{i}=x^{l}_{i})\right],$ |  | (18) |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | $H(\pi(\cdot\mid x))\approx\frac{1}{2}\sum_{i=1}^{N-1}\left[H(X_{i+1}\mid
    X_{i}=x^{w}_{i})+H(X_{i+1}\mid X_{i}=x^{l}_{i})\right],$ |  | (18) |'
- en: which can be directly computed by the logits of the model given the output (preference)
    data. This is also closely related to the predictive entropy [[17](#bib.bib17),
    [23](#bib.bib23)] of the next-token predictions.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯ä»¥ç›´æ¥é€šè¿‡æ¨¡å‹ç»™å‡ºçš„ logits å’Œè¾“å‡ºï¼ˆåå¥½ï¼‰æ•°æ®è®¡ç®—ã€‚è¿™ä¹Ÿä¸ä¸‹ä¸€ token é¢„æµ‹çš„é¢„æµ‹ç†µ [[17](#bib.bib17), [23](#bib.bib23)]
    å¯†åˆ‡ç›¸å…³ã€‚
- en: 3.4\. Unify Mallows-$\theta$ for computation
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. ç»Ÿä¸€è®¡ç®—ä¸­çš„ Mallows-$\theta$
- en: Note that the link function $g_{x}$, with
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„é“¾æ¥å‡½æ•° $g_{x}$ï¼Œå…¶ä¸­
- en: '|  | $1$2 |  | (19) |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (19) |'
- en: For computational purposes, we propose two smooth approximations to $g_{x}$.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å‡ºäºè®¡ç®—ç›®çš„ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§å¹³æ»‘çš„ $g_{x}$ è¿‘ä¼¼ã€‚
- en: (i)
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (i)
- en: 'Sigmoid approximation: Since $g_{x}(1)=\frac{1}{1+\phi(x)}$. See Figure [3](#S3.F3
    "Figure 3 â€£ 3.4\. Unify Mallows-ğœƒ and Mallows-Ï• for computation â€£ 3\. DPO based
    on Mallows Ranking Models â€£ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")
    for an illustration of this approximation. With this approximation, Mallows-$\phi$).
    Thus, Mallows-$\theta$-DPO with sigmoid approximation.'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Sigmoid è¿‘ä¼¼ï¼šç”±äº $g_{x}(1)=\frac{1}{1+\phi(x)}$ã€‚è¯·å‚è§å›¾ [3](#S3.F3 "å›¾ 3 â€£ 3.4\. ç»Ÿä¸€è®¡ç®—ä¸­çš„
    Mallows-ğœƒ å’Œ Mallows-Ï• â€£ 3\. åŸºäº Mallows æ’åæ¨¡å‹çš„ DPO â€£ Mallows-DPOï¼šä½¿ç”¨åå¥½åˆ†æ•£å¾®è°ƒæ‚¨çš„ LLM")
    ä»¥è·å–è¯¥è¿‘ä¼¼çš„è¯´æ˜ã€‚ä½¿ç”¨è¯¥è¿‘ä¼¼ï¼ŒMallows-$\phi$ï¼‰ã€‚å› æ­¤ï¼ŒMallows-$\theta$-DPO ä¸ sigmoid è¿‘ä¼¼ã€‚
- en: (ii)
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (ii)
- en: 'Polynomial fitting: We use a polynomial of form $P(x)=a_{3}x^{3}+a_{1}x+a_{0}$
    being a hyperparameter. We choose $\epsilon$ for $\phi(x)=0.5$). See Figures [3](#S3.F3
    "Figure 3 â€£ 3.4\. Unify Mallows-ğœƒ and Mallows-Ï• for computation â€£ 3\. DPO based
    on Mallows Ranking Models â€£ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")â€“[3](#S3.F3
    "Figure 3 â€£ 3.4\. Unify Mallows-ğœƒ and Mallows-Ï• for computation â€£ 3\. DPO based
    on Mallows Ranking Models â€£ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")
    for an illustration.'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¤šé¡¹å¼æ‹Ÿåˆï¼šæˆ‘ä»¬ä½¿ç”¨å½¢å¼ä¸º $P(x)=a_{3}x^{3}+a_{1}x+a_{0}$ çš„å¤šé¡¹å¼ä½œä¸ºè¶…å‚æ•°ã€‚æˆ‘ä»¬é€‰æ‹© $\epsilon$ ä½¿å¾— $\phi(x)=0.5$ï¼‰ã€‚å‚è§å›¾
    [3](#S3.F3 "å›¾ 3 â€£ 3.4\. ç»Ÿä¸€è®¡ç®—ä¸­çš„ Mallows-ğœƒ å’Œ Mallows-Ï• â€£ 3\. åŸºäº Mallows æ’åæ¨¡å‹çš„ DPO
    â€£ Mallows-DPOï¼šä½¿ç”¨åå¥½åˆ†æ•£å¾®è°ƒæ‚¨çš„ LLM")â€“[3](#S3.F3 "å›¾ 3 â€£ 3.4\. ç»Ÿä¸€è®¡ç®—ä¸­çš„ Mallows-ğœƒ å’Œ Mallows-Ï•
    â€£ 3\. åŸºäº Mallows æ’åæ¨¡å‹çš„ DPO â€£ Mallows-DPOï¼šä½¿ç”¨åå¥½åˆ†æ•£å¾®è°ƒæ‚¨çš„ LLM") ä»¥è·å–è¯´æ˜ã€‚
- en: '![Refer to caption](img/4089e75f667b4937a6f747d63ff862b1.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/4089e75f667b4937a6f747d63ff862b1.png)'
- en: Figure 1. Sigmoid approximation
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 1. Sigmoid è¿‘ä¼¼
- en: '![Refer to caption](img/3c2da4454b7107de2ad5ea0dacb552f3.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/3c2da4454b7107de2ad5ea0dacb552f3.png)'
- en: Figure 2. Poly-fitting on $\pm\epsilon$
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2. åœ¨ $\pm\epsilon$ ä¸Šçš„å¤šé¡¹å¼æ‹Ÿåˆ
- en: '![Refer to caption](img/4ef9060eb0d2d9f488ca4422be0a6a4d.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/4ef9060eb0d2d9f488ca4422be0a6a4d.png)'
- en: Figure 3. Poly-fitting on $\pm 2\log\phi$
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3. åœ¨ $\pm 2\log\phi$ ä¸Šçš„å¤šé¡¹å¼æ‹Ÿåˆ
- en: 4\. Perspectives on Mallows-DPO
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. Mallows-DPOçš„è§†è§’
- en: 'In this section, we provide several alternative perspectives on Mallows-DPO
    in ([12](#S3.E12 "In 3.2\. Mallows-DPO â€£ 3\. DPO based on Mallows Ranking Models
    â€£ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")) and ([16](#S3.E16
    "In 3.2\. Mallows-DPO â€£ 3\. DPO based on Mallows Ranking Models â€£ Mallows-DPO:
    Fine-Tune Your LLM with Preference Dispersions")), with the proofs given in Appendix
    A. We say a DPO is directed by $g(\cdot)$ if the preference distribution can be
    expressed as'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†å…³äº Mallows-DPO çš„å‡ ç§æ›¿ä»£è§†è§’ï¼Œè¯¦è§ ([12](#S3.E12 "åœ¨ 3.2\. Mallows-DPO â€£ 3\.
    åŸºäº Mallows æ’åæ¨¡å‹çš„ DPO â€£ Mallows-DPOï¼šä½¿ç”¨åå¥½åˆ†æ•£å¾®è°ƒæ‚¨çš„ LLM")) å’Œ ([16](#S3.E16 "åœ¨ 3.2\.
    Mallows-DPO â€£ 3\. åŸºäº Mallows æ’åæ¨¡å‹çš„ DPO â€£ Mallows-DPOï¼šä½¿ç”¨åå¥½åˆ†æ•£å¾®è°ƒæ‚¨çš„ LLM"))ï¼Œè¯æ˜è§é™„å½• Aã€‚æˆ‘ä»¬ç§°ä¸€ä¸ª
    DPO ç”± $g(\cdot)$ å¯¼å‘ï¼Œå¦‚æœåå¥½åˆ†å¸ƒå¯ä»¥è¡¨ç¤ºä¸º
- en: '|  | $p^{*}\left(y_{1}\succ y_{2}\mid x\right)=g\left(r^{*}(x,y_{1})-r^{*}\left(x,y_{2}\right)\right),$
    |  |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | $p^{*}\left(y_{1}\succ y_{2}\mid x\right)=g\left(r^{*}(x,y_{1})-r^{*}\left(x,y_{2}\right)\right),$
    |  |'
- en: for some reward function $r^{*}$.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæŸäº›å¥–åŠ±å‡½æ•° $r^{*}$ã€‚
- en: 4.1\. Dispersion weighted objectives
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. ç¦»æ•£åŠ æƒç›®æ ‡
- en: The following result shows that Mallows-$\theta$-DPO can be viewed as a DPO
    with either the reward or the KL-regularizer weighted by the dispersion index.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ç»“æœæ˜¾ç¤ºï¼ŒMallows-$\theta$-DPO å¯ä»¥è¢«è§†ä¸ºä¸€ä¸ª DPOï¼Œå…¶ä¸­å¥–åŠ±æˆ– KL-æ­£åˆ™åŒ–é¡¹ç”±ç¦»æ•£æŒ‡æ•°åŠ æƒã€‚
- en: Theorem 1  (Mallows-$\theta$-DPO as dispersion weighted DPO).
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å®šç† 1ï¼ˆMallows-$\theta$-DPO ä½œä¸ºç¦»æ•£åŠ æƒ DPOï¼‰ã€‚
- en: 'Let $c(x)=-2\log\phi(x)$:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¤ $c(x)=-2\log\phi(x)$ï¼š
- en: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(y\mid
    x)}\left[c(x)^{-1}r^{*}(x,y)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right],$ |  | (20) |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(y\mid
    x)}\left[c(x)^{-1}r^{*}(x,y)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right],$ |  | (20) |'
- en: or
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–
- en: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(y\mid
    x)}\left[r^{*}(x,y)\right]-\beta c(x)\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right].$ |  | (21) |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(y\mid
    x)}\left[r^{*}(x,y)\right]-\beta c(x)\mathrm{KL}\left(\pi(\cdot\mid x)\mid\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right].$ |  | (21) |'
- en: A similar result holds for Mallows-$\phi$-DPO.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºMallows-$\phi$-DPOï¼Œç±»ä¼¼çš„ç»“æœæˆç«‹ã€‚
- en: Theorem 2  (Mallows-$\phi$-DPO as dispersion weighted DPO).
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å®šç†2ï¼ˆMallows-$\phi$-DPOä½œä¸ºåŠ æƒDPOçš„ç¦»æ•£åº¦ï¼‰ã€‚
- en: 'Setting $\phi(x)=e$ in ([15](#S3.E15 "In 3.2\. Mallows-DPO â€£ 3\. DPO based
    on Mallows Ranking Models â€£ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions"))
    yields'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨([15](#S3.E15 "åœ¨3.2\. Mallows-DPO â€£ 3\. åŸºäºMallowsæ’åºæ¨¡å‹çš„DPO â€£ Mallows-DPOï¼šç”¨åå¥½ç¦»æ•£åº¦å¾®è°ƒä½ çš„LLM"))ä¸­è®¾å®š$\phi(x)=e$ï¼Œå¾—åˆ°
- en: '|  | $g(s):=\frac{1-\operatorname{sgn}(s)}{2}+\operatorname{sgn}(s)\left(\frac{&#124;s&#124;+1}{1-e^{&#124;s&#124;+1}}-\frac{&#124;s&#124;}{1-e^{&#124;s&#124;}}\right).$
    |  | (22) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | $g(s):=\frac{1-\operatorname{sgn}(s)}{2}+\operatorname{sgn}(s)\left(\frac{|s|+1}{1-e^{|s|+1}}-\frac{|s|}{1-e^{|s|}}\right).$
    |  | (22) |'
- en: 'Let $c(x)=-2\log\phi(x)$ or the KL-regularizer weighted by $\beta c(x)$:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾$c(x)=-2\log\phi(x)$æˆ–ç”±$\beta c(x)$åŠ æƒçš„KL-æ­£åˆ™åŒ–é¡¹ï¼š
- en: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(y\mid
    x)}\left[c(x)^{-1}r^{*}(x,y)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right],$ |  | (23) |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(y\mid
    x)}\left[c(x)^{-1}r^{*}(x,y)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\mid\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right],$ |  | (23) |'
- en: or
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–
- en: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(y\mid
    x)}\left[r^{*}(x,y)\right]-\beta c(x)\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right].$ |  | (24) |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(y\mid
    x)}\left[r^{*}(x,y)\right]-\beta c(x)\mathrm{KL}\left(\pi(\cdot\mid x)\mid\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right].$ |  | (24) |'
- en: 4.2\. Connection to $\Psi$PO
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. ä¸$\Psi$POçš„è”ç³»
- en: 'The objective of $\Psi$PO [[2](#bib.bib2)] is:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: $\Psi$POçš„ç›®æ ‡[[2](#bib.bib2)]æ˜¯ï¼š
- en: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi(\cdot\mid
    x),\,y^{\prime}\sim\tilde{\pi}(\cdot\mid x)}\left[\Psi\left(p^{*}(y\succ y^{\prime}\mid
    x)\right)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right],$ |  | (25) |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi(\cdot\mid
    x),\,y^{\prime}\sim\tilde{\pi}(\cdot\mid x)}\left[\Psi\left(p^{*}(y\succ y^{\prime}\mid
    x)\right)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\mid\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right],$ |  | (25) |'
- en: where $\Psi:[0,1]\to\mathbb{R}$PO to the Bradley-Terry based DPO.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­$\Psi:[0,1]\to\mathbb{R}$POåº”ç”¨äºåŸºäºBradley-Terryçš„DPOã€‚
- en: 'Roughly speaking, the function $\Psi$. Assume such a function exists, which
    we denote as $\Psi^{M}(\cdot)$ model in ([13](#S3.E13 "In 3.2\. Mallows-DPO â€£
    3\. DPO based on Mallows Ranking Models â€£ Mallows-DPO: Fine-Tune Your LLM with
    Preference Dispersions")), we have'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ç²—ç•¥æ¥è¯´ï¼Œå‡½æ•°$\Psi$ã€‚å‡è®¾å­˜åœ¨è¿™æ ·ä¸€ä¸ªå‡½æ•°ï¼Œæˆ‘ä»¬ç”¨$\Psi^{M}(\cdot)$åœ¨([13](#S3.E13 "åœ¨3.2\. Mallows-DPO
    â€£ 3\. åŸºäºMallowsæ’åºæ¨¡å‹çš„DPO â€£ Mallows-DPOï¼šç”¨åå¥½ç¦»æ•£åº¦å¾®è°ƒä½ çš„LLM"))ä¸­è¡¨ç¤ºï¼Œé‚£ä¹ˆæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\mathbb{E}_{y_{2}\sim\bar{\pi}(\cdot\mid x)}\left[\Psi^{M}\left(p^{*}\left(y_{1}\succ
    y_{2}\mid x\right)\right)\right]$ |  | (26) |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}_{y_{2}\sim\bar{\pi}(\cdot\mid x)}\left[\Psi^{M}\left(p^{*}\left(y_{1}\succ
    y_{2}\mid x\right)\right)\right]$ |  | (26) |'
- en: '|  |  | $\displaystyle\neq r(x,y_{1})-\mathbb{E}_{y_{2}\sim\bar{\pi}(\cdot\mid
    x)}\left[r\left(x,y_{2}\right)\right],$ |  |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\neq r(x,y_{1})-\mathbb{E}_{y_{2}\sim\bar{\pi}(\cdot\mid
    x)}\left[r\left(x,y_{2}\right)\right],$ |  |'
- en: i.e., for any $\Psi^{M}(\cdot)$PO to take into account prompt dispersion.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: å³ï¼Œå¯¹äºä»»ä½•$\Psi^{M}(\cdot)$POéœ€è¦è€ƒè™‘æç¤ºç¦»æ•£åº¦ã€‚
- en: 'Generalized $\Psi$. The generalized $\Psi$PO takes the form:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¿ä¹‰çš„$\Psi$ã€‚å¹¿ä¹‰çš„$\Psi$POå½¢å¼ä¸ºï¼š
- en: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(\cdot\mid
    x),\,y^{\prime}\sim\tilde{\pi}(\cdot\mid x)}\left[\tilde{\Psi}\left(x,p^{*}(y\succ
    y^{\prime}\mid x)\right)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right].$ |  | (27) |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(\cdot\mid
    x),\,y^{\prime}\sim\tilde{\pi}(\cdot\mid x)}\left[\tilde{\Psi}\left(x,p^{*}(y\succ
    y^{\prime}\mid x)\right)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\mid\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right].$ |  | (27) |'
- en: 'A special instance is when $\tilde{\Psi}(x,p)=f(x)\Psi(p)$ is separable:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç‰¹æ®Šçš„å®ä¾‹æ˜¯å½“$\tilde{\Psi}(x,p)=f(x)\Psi(p)$æ˜¯å¯åˆ†ç¦»çš„ï¼š
- en: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(\cdot\mid
    x),\,y^{\prime}\sim\tilde{\pi}(\cdot\mid x)}\left[f(x)\Psi\left(p^{*}(y\succ y^{\prime}\mid
    x)\right)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right].$ |  | (28) |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(\cdot\mid
    x),\,y^{\prime}\sim\tilde{\pi}(\cdot\mid x)}\left[f(x)\Psi\left(p^{*}(y\succ y^{\prime}\mid
    x)\right)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right].$ |  | (28) |'
- en: Theorem 3  (Mallows-DPO as generalized $\Psi$PO).
  id: totrans-149
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å®šç† 3  (Mallows-DPO ä½œä¸ºå¹¿ä¹‰ $\Psi$PO)ã€‚
- en: (i) Mallows-$\theta$ and $f(x)=-\frac{1}{2\log\phi(x)}$.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: (i) Mallows-$\theta$ å’Œ $f(x)=-\frac{1}{2\log\phi(x)}$ã€‚
- en: (ii) Mallows-$\phi$ and $f(x)=-\frac{1}{\log\phi(x)}$.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: (ii) Mallows-$\phi$ å’Œ $f(x)=-\frac{1}{\log\phi(x)}$ã€‚
- en: 5\. Experiments
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. å®éªŒ
- en: In this section, we evaluate the capability of our proposed Mallows-DPO to learn
    the preferences in comparison with DPO. First, we use the preferences dataset
    of IMDB [[22](#bib.bib22)] datasets and Anthropic Helpful and Harmless dialogue
    [[3](#bib.bib3)] dataset to provide evidence that human preferences may be diversed.
    Next, we consider a synthetic bandit problem to demonstrate the effectiveness
    of our proposed Mallows-$\phi$-DPO, even without prompt dispersions. We further
    conduct experiments on tasks such as conditional generation (IMDB) and dialogue
    (Anthropic HH). Our findings show that Mallows-DPO outperforms DPO with an evident
    margin, both for in-distribution performance and out-of-distribution generalization
    capability.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†æˆ‘ä»¬æå‡ºçš„ Mallows-DPO å­¦ä¹ åå¥½çš„èƒ½åŠ›ï¼Œå¹¶ä¸ DPO è¿›è¡Œäº†æ¯”è¾ƒã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨ IMDB [[22](#bib.bib22)]
    æ•°æ®é›†å’Œ Anthropic Helpful å’Œ Harmless å¯¹è¯ [[3](#bib.bib3)] æ•°æ®é›†ï¼Œæä¾›äººç±»åå¥½å¯èƒ½å¤šæ ·åŒ–çš„è¯æ®ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªåˆæˆèµŒåšé—®é¢˜ï¼Œä»¥å±•ç¤ºæˆ‘ä»¬æå‡ºçš„
    Mallows-$\phi$-DPO çš„æœ‰æ•ˆæ€§ï¼Œå³ä½¿æ²¡æœ‰æç¤ºåˆ†æ•£ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åœ¨æ¡ä»¶ç”Ÿæˆï¼ˆIMDBï¼‰å’Œå¯¹è¯ï¼ˆAnthropic HHï¼‰ç­‰ä»»åŠ¡ä¸Šè¿›è¡Œäº†å®éªŒã€‚æˆ‘ä»¬çš„å‘ç°è¡¨æ˜ï¼ŒMallows-DPO
    åœ¨åˆ†å¸ƒå†…è¡¨ç°å’Œåˆ†å¸ƒå¤–æ³›åŒ–èƒ½åŠ›æ–¹é¢æ˜æ˜¾ä¼˜äº DPOã€‚
- en: 5.1\. Evidence of preference dispersion
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. åå¥½åˆ†æ•£çš„è¯æ®
- en: 'A first natural question is: are human preferences dispersed? To verify this
    key motivation for our work, we plot the distribution of the dispersion estimators
    given the SFT model and pairwise preferences. Recall from Section [3](#S3 "3\.
    DPO based on Mallows Ranking Models â€£ Mallows-DPO: Fine-Tune Your LLM with Preference
    Dispersions") that the dispersion estimator is:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¬¬ä¸€ä¸ªè‡ªç„¶çš„é—®é¢˜æ˜¯ï¼šäººç±»åå¥½æ˜¯å¦åˆ†æ•£ï¼Ÿä¸ºäº†éªŒè¯è¿™ä¸€å…³é”®åŠ¨æœºï¼Œæˆ‘ä»¬ç»˜åˆ¶äº†åœ¨ SFT æ¨¡å‹å’Œæˆå¯¹åå¥½ä¸‹çš„åˆ†æ•£ä¼°è®¡é‡çš„åˆ†å¸ƒã€‚å›é¡¾ç¬¬ [3](#S3 "3\.
    DPO based on Mallows Ranking Models â€£ Mallows-DPO: Fine-Tune Your LLM with Preference
    Dispersions) èŠ‚ï¼Œåˆ†æ•£ä¼°è®¡é‡æ˜¯ï¼š'
- en: '|  | $1$2 |  |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: and we take the hyperparameter  such that the empirical mean is equal to 1, so we do
    not need to tune this scaling constant.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é‡‡ç”¨è¶…å‚æ•°ï¼Œä½¿å¾—ç»éªŒå‡å€¼ç­‰äº
    1ï¼Œå› æ­¤æˆ‘ä»¬ä¸éœ€è¦è°ƒæ•´è¿™ä¸ªç¼©æ”¾å¸¸æ•°ã€‚
- en: '![Refer to caption](img/892ecba31376f2f4fe90627f7b4cd48d.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§å›¾ä¾‹](img/892ecba31376f2f4fe90627f7b4cd48d.png)'
- en: (a) IMDB preference dispersion
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: (a) IMDB åå¥½åˆ†æ•£
- en: '![Refer to caption](img/397713bda291ca043c63c48d526dda52.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§å›¾ä¾‹](img/397713bda291ca043c63c48d526dda52.png)'
- en: (b) Anthropic-HH preference dispersion.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Anthropic-HH åå¥½åˆ†æ•£ã€‚
- en: Figure 4. LEFT. Distribution of our dispersion estimator on IMDB. RIGHT. Anthropic-HH
    perference dataset.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4. å·¦ä¾§. æˆ‘ä»¬çš„åˆ†æ•£ä¼°è®¡é‡åœ¨ IMDB ä¸Šçš„åˆ†å¸ƒã€‚å³ä¾§. Anthropic-HH åå¥½æ•°æ®é›†ã€‚
- en: 'We find that for the task of conditional generation such as IMDB, the human
    preferences are not quite diverse: the dispersion estimators are located near
    $1$. However, for tasks such as single dialogue, our plot shows that human preferences
    may be dispersed: the distribution is both skewed and of high variance.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å‘ç°ï¼Œå¯¹äºæ¡ä»¶ç”Ÿæˆä»»åŠ¡å¦‚ IMDBï¼Œäººç±»åå¥½å¹¶ä¸ååˆ†å¤šæ ·ï¼šåˆ†æ•£ä¼°è®¡é‡æ¥è¿‘ $1$ã€‚ç„¶è€Œï¼Œå¯¹äºå•ä¸€å¯¹è¯ç­‰ä»»åŠ¡ï¼Œæˆ‘ä»¬çš„å›¾è¡¨æ˜¾ç¤ºäººç±»åå¥½å¯èƒ½æ˜¯åˆ†æ•£çš„ï¼šåˆ†å¸ƒæ—¢åæ–œåˆå…·æœ‰é«˜æ–¹å·®ã€‚
- en: 5.2\. Mallows-$\phi$-DPO mitigates reward collapse
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. Mallows-$\phi$-DPO ç¼“è§£å¥–åŠ±å´©æºƒ
- en: We study Mallows-DPO in a synthetic bandit experiment where there is no contextual
    information $x$. Second, the limited data availability tests the ability of the
    approaches to produce diversified policies and avoid reward collapse.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨ä¸€ä¸ªæ²¡æœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯ $x$ çš„åˆæˆèµŒåšå®éªŒä¸­ç ”ç©¶äº† Mallows-DPOã€‚å…¶æ¬¡ï¼Œæœ‰é™çš„æ•°æ®å¯ç”¨æ€§æµ‹è¯•äº†è¿™äº›æ–¹æ³•äº§ç”Ÿå¤šæ ·åŒ–ç­–ç•¥å’Œé¿å…å¥–åŠ±å´©æºƒçš„èƒ½åŠ›ã€‚
- en: Concretely, we consider five arms, each associated with a random reward drawn
    from a probability distribution. Preference between any two picked arms is determined
    by the random reward realizations, with larger reward being preferred. In the
    experiment, we collect $16$, and (2) across different epochs. The details are
    provided in Appendix B.1.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬è€ƒè™‘äº”ä¸ªè‡‚ï¼Œæ¯ä¸ªè‡‚éƒ½ä¸ä»æ¦‚ç‡åˆ†å¸ƒä¸­æŠ½å–çš„éšæœºå¥–åŠ±ç›¸å…³ã€‚ä»»ä½•ä¸¤ä¸ªè¢«æŒ‘é€‰çš„è‡‚ä¹‹é—´çš„åå¥½ç”±éšæœºå¥–åŠ±å®ç°å†³å®šï¼Œè¾ƒå¤§çš„å¥–åŠ±æ›´å—æ¬¢è¿ã€‚åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬æ”¶é›†äº†
    $16$ï¼Œä»¥åŠï¼ˆ2ï¼‰è·¨ä¸åŒçš„çºªå…ƒã€‚è¯¦ç»†ä¿¡æ¯è§é™„å½• B.1ã€‚
- en: 'Figure [5](#S5.F5 "Figure 5 â€£ 5.2\. Mallows-Ï•-DPO mitigates reward collapse
    â€£ 5\. Experiments â€£ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")
    displays the efficient frontiers for Mallows-$\phi$ is small. (2) Over all possible
    $\beta$ gets smaller. That is, Mallows-$\phi$-DPO leads to the policies that have
    both high rewards and small KL divergence.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾[5](#S5.F5 "å›¾ 5 â€£ 5.2\. Mallows-Ï•-DPO ç¼“è§£å¥–åŠ±å´©æºƒ â€£ 5\. å®éªŒ â€£ Mallows-DPO: ç”¨åå¥½åˆ†æ•£å¾®è°ƒä½ çš„
    LLM") æ˜¾ç¤ºäº† Mallows-$\phi$ çš„æœ‰æ•ˆå‰æ²¿å¾ˆå°ã€‚ï¼ˆ2ï¼‰åœ¨æ‰€æœ‰å¯èƒ½çš„ $\beta$ ä¸Šå˜å¾—æ›´å°ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒMallows-$\phi$-DPO
    å¯¼è‡´çš„ç­–ç•¥æ—¢å…·æœ‰é«˜å¥–åŠ±åˆå…·æœ‰å°çš„ KL æ•£åº¦ã€‚'
- en: '![Refer to caption](img/ebd4e666fdc6751cb771f3e560f5d24e.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/ebd4e666fdc6751cb771f3e560f5d24e.png)'
- en: (a)
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ï¼ˆaï¼‰
- en: '![Refer to caption](img/e2e0370ae681d81084341acf5b1eec0c.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/e2e0370ae681d81084341acf5b1eec0c.png)'
- en: (b)
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ï¼ˆbï¼‰
- en: 'Figure 5. Efficient frontiers: reward vs KL. LEFT. generated by measuring KL
    and reward for the policy trained with different $\beta$.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5. æœ‰æ•ˆå‰æ²¿ï¼šå¥–åŠ±ä¸ KLã€‚å·¦ä¾§ã€‚é€šè¿‡æµ‹é‡ä¸åŒ $\beta$ è®­ç»ƒçš„ç­–ç•¥çš„ KL å’Œå¥–åŠ±ç”Ÿæˆã€‚
- en: '![Refer to caption](img/68770a2f80f7991cc30bb188271cc9b6.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/68770a2f80f7991cc30bb188271cc9b6.png)'
- en: Figure 6. Training curves of Mallows-$\phi$.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 6. Mallows-$\phi$ çš„è®­ç»ƒæ›²çº¿ã€‚
- en: 5.3\. Mallows-DPO yields better tradeoff between accuracy and regularization
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. Mallows-DPO åœ¨å‡†ç¡®æ€§å’Œæ­£åˆ™åŒ–ä¹‹é—´æä¾›æ›´å¥½çš„æƒè¡¡
- en: We conduct the conditional generation for IMDB dataset. In this task, $x$ with
    positive sentiment. Following the setting in [[30](#bib.bib30)], we first fine-tune
    GPT-2-large on the training split of IMDB datasets until convergence to get the
    SFT model. Next, we use the pairwise preference data from [[39](#bib.bib39)] to
    fine-tune the SFT model by DPO and Mallows-DPO.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯¹ IMDB æ•°æ®é›†è¿›è¡Œæ¡ä»¶ç”Ÿæˆã€‚åœ¨æ­¤ä»»åŠ¡ä¸­ï¼Œ$x$ å…·æœ‰æ­£é¢æƒ…æ„Ÿã€‚æ ¹æ® [[30](#bib.bib30)] çš„è®¾ç½®ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹ IMDB æ•°æ®é›†çš„è®­ç»ƒåˆ†å‰²è¿›è¡Œ
    GPT-2-large å¾®è°ƒï¼Œç›´è‡³æ”¶æ•›ä»¥è·å¾— SFT æ¨¡å‹ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¥è‡ª [[39](#bib.bib39)] çš„æˆå¯¹åå¥½æ•°æ®é€šè¿‡ DPO å’Œ Mallows-DPO
    å¯¹ SFT æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚
- en: '![Refer to caption](img/7e109a6c199521d77942962c0c857b67.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/7e109a6c199521d77942962c0c857b67.png)'
- en: 'Figure 7. Efficient frontiers: accuracy vs KL achieved by Mallows-DPO and BT-DPO.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 7. æœ‰æ•ˆå‰æ²¿ï¼šMallows-DPO å’Œ BT-DPO å®ç°çš„å‡†ç¡®æ€§ä¸ KLã€‚
- en: 'Figure [7](#S5.F7 "Figure 7 â€£ 5.3\. Mallows-DPO yields better tradeoff between
    accuracy and regularization â€£ 5\. Experiments â€£ Mallows-DPO: Fine-Tune Your LLM
    with Preference Dispersions") displays the efficient frontiers (during the training
    process) for BT-DPO, Mallows-$\theta$-DPO outperforms both, achieving the same
    accuracy (evaluated by the reward model) at a smaller KL divergence to the SFT
    model/policy.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾[7](#S5.F7 "å›¾ 7 â€£ 5.3\. Mallows-DPO åœ¨å‡†ç¡®æ€§å’Œæ­£åˆ™åŒ–ä¹‹é—´æä¾›æ›´å¥½çš„æƒè¡¡ â€£ 5\. å®éªŒ â€£ Mallows-DPO:
    ç”¨åå¥½åˆ†æ•£å¾®è°ƒä½ çš„ LLM") æ˜¾ç¤ºäº† BT-DPO çš„æœ‰æ•ˆå‰æ²¿ï¼ˆåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼‰ï¼ŒMallows-$\theta$-DPO ä¼˜äºä¸¤è€…ï¼Œåœ¨è¾ƒå°çš„ KL æ•£åº¦ä¸‹å®ç°ç›¸åŒçš„å‡†ç¡®æ€§ï¼ˆé€šè¿‡å¥–åŠ±æ¨¡å‹è¯„ä¼°ï¼‰ã€‚'
- en: '5.4\. Dispersion matters: Mallows-DPO enhancing both in-distribution and out-of-distribution
    performances'
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4\. åˆ†æ•£æ€§çš„é‡è¦æ€§ï¼šMallows-DPO æå‡äº†åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–çš„è¡¨ç°
- en: 'We compare the performances of Mallows-DPO and BT-DPO in terms of the win rate
    evaluated by GPT4, and generalization capability on the out-of-distribution datasets.
    In the experiment, we choose $\beta$ value leads to a drop both in performance
    and per-input diversity of RLHF and DPO. Results are shown in Figure [8](#S5.F8
    "Figure 8 â€£ 5.4\. Dispersion matters: Mallows-DPO enhancing both in-distribution
    and out-of-distribution performances â€£ 5\. Experiments â€£ Mallows-DPO: Fine-Tune
    Your LLM with Preference Dispersions").'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 'æˆ‘ä»¬æ¯”è¾ƒäº† Mallows-DPO å’Œ BT-DPO åœ¨ GPT4 è¯„ä¼°çš„èƒœç‡ä»¥åŠåœ¨åˆ†å¸ƒå¤–æ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬é€‰æ‹©äº† $\beta$ å€¼ï¼Œè¿™å¯¼è‡´
    RLHF å’Œ DPO çš„è¡¨ç°ä»¥åŠæ¯è¾“å…¥çš„å¤šæ ·æ€§ä¸‹é™ã€‚ç»“æœè§å›¾[8](#S5.F8 "å›¾ 8 â€£ 5.4\. åˆ†æ•£æ€§çš„é‡è¦æ€§ï¼šMallows-DPO æå‡äº†åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–çš„è¡¨ç°
    â€£ 5\. å®éªŒ â€£ Mallows-DPO: ç”¨åå¥½åˆ†æ•£å¾®è°ƒä½ çš„ LLM")ã€‚'
- en: 'In-distribution test. We first fine-tune a pretrained Pythia-2.8B model on
    the training set of Anthropic HH dataset using Mallows-DPO and BT-DPO, and then
    evaluate the responses on a subset of its test split, generated by these fine-tuned
    models. GPT-4 serves as the evaluator, and compares pairs of responses: one from
    the model fine-tuned with Mallows-$\theta$-DPO have an edge over BT-DPO. In particular,
    Mallows-$\phi$ for $\beta=0.1$.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†å¸ƒå†…æµ‹è¯•ã€‚æˆ‘ä»¬é¦–å…ˆåœ¨Anthropic HHæ•°æ®é›†çš„è®­ç»ƒé›†ä¸Šå¯¹é¢„è®­ç»ƒçš„Pythia-2.8Bæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿ç”¨Mallows-DPOå’ŒBT-DPOï¼Œç„¶ååœ¨è¿™äº›å¾®è°ƒæ¨¡å‹ç”Ÿæˆçš„æµ‹è¯•é›†å­é›†ä¸Šè¯„ä¼°å“åº”ã€‚GPT-4ä½œä¸ºè¯„ä¼°è€…ï¼Œæ¯”è¾ƒå“åº”å¯¹ï¼šä¸€ä¸ªæ¥è‡ªMallows-$\theta$-DPOå¾®è°ƒæ¨¡å‹çš„å“åº”ç›¸è¾ƒäºBT-DPOæœ‰ä¼˜åŠ¿ã€‚ç‰¹åˆ«æ˜¯ï¼ŒMallows-$\phi$å¯¹äº$\beta=0.1$ã€‚
- en: '![Refer to caption](img/4dc0d4b95f28003f2f295ed466962f4c.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/4dc0d4b95f28003f2f295ed466962f4c.png)'
- en: Figure 8. Win rates computed by GPT-4 evaluations for responses on both the
    in-distribution dataset (Anthropic HH) and out-of-distribution datasets (H4 Stack
    Exchange and Stanford Human Preferences), generated by the models fine-tuned on
    the Anthropic HH training set.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8. GPT-4è¯„ä¼°è®¡ç®—çš„èƒœç‡ï¼Œé’ˆå¯¹ç”±åœ¨Anthropic HHè®­ç»ƒé›†ä¸Šå¾®è°ƒçš„æ¨¡å‹ç”Ÿæˆçš„åˆ†å¸ƒå†…æ•°æ®é›†ï¼ˆAnthropic HHï¼‰å’Œåˆ†å¸ƒå¤–æ•°æ®é›†ï¼ˆH4
    Stack Exchangeå’Œæ–¯å¦ç¦äººç±»åå¥½ï¼‰çš„å“åº”ã€‚
- en: Out-of-distribution test. We apply the models, fine-tuned on the train split
    of the Antropic HH dataset, to other datasets with different input distributions.
    The H4 Stack Exchange Preferences Dataset, collected from the Stack Overflow,
    and Stanford Human Preferences (SHP), containing questions or instructions in
    various subject areas, are used for evaluation. In these out-of-distribution tasks,
    the advantage of dispersion on generalization becomes apparent, as Mallows-$\theta$,
    the performance of Mallows-DPO is consistently above 55%, with Mallows-$\phi$-DPO
    achieving a notable improvement on the H4 Stack Exchange dataset, reaching a win
    rate with more than 60%.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†å¸ƒå¤–æµ‹è¯•ã€‚æˆ‘ä»¬å°†é’ˆå¯¹Antropic HHæ•°æ®é›†è®­ç»ƒé›†ä¸Šå¾®è°ƒçš„æ¨¡å‹åº”ç”¨äºå…·æœ‰ä¸åŒè¾“å…¥åˆ†å¸ƒçš„å…¶ä»–æ•°æ®é›†ã€‚ä½¿ç”¨H4 Stack Exchangeåå¥½æ•°æ®é›†ï¼ˆæ¥è‡ªStack
    Overflowï¼‰å’Œæ–¯å¦ç¦äººç±»åå¥½ï¼ˆSHPï¼ŒåŒ…å«å„ç§å­¦ç§‘é¢†åŸŸçš„é—®é¢˜æˆ–æŒ‡ä»¤ï¼‰è¿›è¡Œè¯„ä¼°ã€‚åœ¨è¿™äº›åˆ†å¸ƒå¤–ä»»åŠ¡ä¸­ï¼Œæ•£å¸ƒå¯¹æ³›åŒ–çš„ä¼˜åŠ¿å˜å¾—æ˜æ˜¾ï¼ŒMallows-$\theta$ï¼ŒMallows-DPOçš„è¡¨ç°å§‹ç»ˆåœ¨55%ä»¥ä¸Šï¼Œå…¶ä¸­Mallows-$\phi$-DPOåœ¨H4
    Stack Exchangeæ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—æå‡ï¼Œèµ¢ç‡è¶…è¿‡60%ã€‚
- en: 6\. Conclusion
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. ç»“è®º
- en: We have developed in this paper a novel approach, the Mallows-DPO, to fine-tune
    LLM. A distinct feature of this approach is a dispersion index, which naturally
    captures the dispersion of human preference to prompts, and can be systematically
    incorporated into the reward function as a weight factor, thus ushering in a new
    class of dispersion-weighted DPO models. We demonstrate empirically how Mallows-DPO
    achieves improved performance in a broad array of benchmark tasks, including synthetic
    bandit selection, controllable generation, and dialogues.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­å¼€å‘äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå³Mallows-DPOï¼Œç”¨äºå¾®è°ƒLLMã€‚è¿™ç§æ–¹æ³•çš„ä¸€ä¸ªç‹¬ç‰¹ç‰¹ç‚¹æ˜¯æ•£å¸ƒæŒ‡æ•°ï¼Œå®ƒè‡ªç„¶åœ°æ•æ‰äººç±»å¯¹æç¤ºçš„åå¥½æ•£å¸ƒï¼Œå¹¶å¯ä»¥ç³»ç»Ÿåœ°çº³å…¥å¥–åŠ±å‡½æ•°ä½œä¸ºæƒé‡å› å­ï¼Œä»è€Œå¼€åˆ›äº†ä¸€ç±»æ–°çš„æ•£å¸ƒåŠ æƒDPOæ¨¡å‹ã€‚æˆ‘ä»¬é€šè¿‡å®è¯æ¼”ç¤ºäº†Mallows-DPOåœ¨åˆæˆèµŒåšé€‰æ‹©ã€å¯æ§ç”Ÿæˆå’Œå¯¹è¯ç­‰å„ç§åŸºå‡†ä»»åŠ¡ä¸­å¦‚ä½•å®ç°æ€§èƒ½æå‡ã€‚
- en: There are a few issues that we have yet to address in this study, for instance,
    to explore why Mallows-DPO outperforms BT-DPO, how the dispersion index contributes
    to performance improvement, what guidelines to follow to set the $\beta$-divergence
    [[39](#bib.bib39)]). These will be pursued in our future works.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç ”ç©¶ä¸­è¿˜æœ‰ä¸€äº›é—®é¢˜å¾…è§£å†³ï¼Œä¾‹å¦‚ï¼Œæ¢ç´¢ä¸ºä½•Mallows-DPOä¼˜äºBT-DPOï¼Œæ•£å¸ƒæŒ‡æ•°å¦‚ä½•ä¿ƒè¿›æ€§èƒ½æå‡ï¼Œå¦‚ä½•è®¾ç½®$\beta$-æ•£åº¦çš„æŒ‡å—[[39](#bib.bib39)]ã€‚è¿™äº›å°†åœ¨æˆ‘ä»¬æœªæ¥çš„å·¥ä½œä¸­æ·±å…¥æ¢è®¨ã€‚
- en: 'Acknowledgement: Hanyang Zhao and Wenpin Tang are supported by NSF grants DMS-2113779
    and DMS-2206038, and by a start-up grant at Columbia University. The works of
    Haoxian Chen, Hanyang Zhao, Henry Lam and David Yao are part of a Columbia-CityU/HK
    collaborative project that is supported by InnotHK Initiative, The Government
    of the HKSAR and the AIFT Lab. Additionally, Haoxian Chen is supported by the
    Amazon CAIT fellowship. Henry Lam and Wenpin Tang receive support from the Columbia
    Innovation Hub grant.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: è‡´è°¢ï¼šHanyang Zhaoå’ŒWenpin Tangè·å¾—äº†NSFèµ„åŠ©ï¼ˆDMS-2113779å’ŒDMS-2206038ï¼‰ï¼Œä»¥åŠå“¥ä¼¦æ¯”äºšå¤§å­¦çš„å¯åŠ¨èµ„é‡‘ã€‚Haoxian
    Chenã€Hanyang Zhaoã€Henry Lamå’ŒDavid Yaoçš„ç ”ç©¶å±äºå“¥ä¼¦æ¯”äºšå¤§å­¦ä¸åŸå¤§/é¦™æ¸¯çš„åˆä½œé¡¹ç›®ï¼Œè¯¥é¡¹ç›®å¾—åˆ°äº†InnotHK Initiativeã€é¦™æ¸¯ç‰¹åˆ«è¡Œæ”¿åŒºæ”¿åºœå’ŒAIFTå®éªŒå®¤çš„æ”¯æŒã€‚æ­¤å¤–ï¼ŒHaoxian
    Chenè¿˜è·å¾—äº†Amazon CAITå¥–å­¦é‡‘ã€‚Henry Lamå’ŒWenpin Tangåˆ™è·å¾—äº†å“¥ä¼¦æ¯”äºšåˆ›æ–°ä¸­å¿ƒçš„èµ„åŠ©ã€‚
- en: References
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, FlorenciaÂ Leoni
    Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, and Shyamal Anadkat. GPT-4
    technical report. 2023. arXiv:2303.08774.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia
    Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, å’Œ Shyamal Anadkat.
    GPT-4æŠ€æœ¯æŠ¥å‘Šã€‚2023å¹´ã€‚arXiv:2303.08774ã€‚'
- en: '[2] MohammadÂ Gheshlaghi Azar, ZhaohanÂ Daniel Guo, Bilal Piot, Remi Munos, Mark
    Rowland, Michal Valko, and Daniele Calandriello. A general theoretical paradigm
    to understand learning from human preferences. In AISTATS, pages 4447â€“4455, 2024.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark
    Rowland, Michal Valko, å’Œ Daniele Calandriello. ç†è§£ä»äººç±»åå¥½ä¸­å­¦ä¹ çš„é€šç”¨ç†è®ºèŒƒå¼ã€‚å‘è¡¨äºAISTATSï¼Œé¡µé¢4447â€“4455ï¼Œ2024å¹´ã€‚'
- en: '[3] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
    Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, etÂ al. Training a helpful
    and harmless assistant with reinforcement learning from human feedback. 2022.
    arXiv:2204.05862.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
    Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, ç­‰ç­‰ã€‚é€šè¿‡ä»äººç±»åé¦ˆä¸­å­¦ä¹ çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæœ‰ç”¨ä¸”æ— å®³çš„åŠ©æ‰‹ã€‚2022å¹´ã€‚arXiv:2204.05862ã€‚'
- en: '[4] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,
    Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, etÂ al.
    Constitutional AI: Harmlessness from AI feedback. 2022. arXiv:2212.08073.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,
    Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, ç­‰ç­‰ã€‚å®ªæ³•AIï¼šæ¥è‡ªAIåé¦ˆçš„æ— å®³æ€§ã€‚2022å¹´ã€‚arXiv:2212.08073ã€‚'
- en: '[5] RalphÂ Allan Bradley and MiltonÂ E Terry. Rank analysis of incomplete block
    designs: I. the method of paired comparisons. Biometrika, 39(3/4):324â€“345, 1952.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Ralph Allan Bradley å’Œ Milton E Terry. ä¸å®Œå…¨åŒºç»„è®¾è®¡çš„æ’ååˆ†æï¼šI. æˆå¯¹æ¯”è¾ƒçš„æ–¹æ³•ã€‚Biometrika,
    39(3/4):324â€“345, 1952å¹´ã€‚'
- en: '[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, JaredÂ D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, and Amanda
    Askell. Language models are few-shot learners. In Neurips, volumeÂ 33, pages 1877â€“1901,
    2020.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, å’Œ Amanda Askell.
    è¯­è¨€æ¨¡å‹æ˜¯å°‘æ ·æœ¬å­¦ä¹ è€…ã€‚å‘è¡¨äºNeuripsï¼Œç¬¬33å·ï¼Œé¡µé¢1877â€“1901ï¼Œ2020å¹´ã€‚'
- en: '[7] RÃ³bert Busa-Fekete, Eyke HÃ¼llermeier, and BalÃ¡zs SzÃ¶rÃ©nyi. Preference-based
    rank elicitation using statistical models: The case of Mallows. In ICML, pages
    1071â€“1079, 2014.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] RÃ³bert Busa-Fekete, Eyke HÃ¼llermeier, å’Œ BalÃ¡zs SzÃ¶rÃ©nyi. ä½¿ç”¨ç»Ÿè®¡æ¨¡å‹çš„åŸºäºåå¥½çš„æ’åå¼•å‡ºï¼šMallowsçš„æ¡ˆä¾‹ã€‚å‘è¡¨äºICMLï¼Œé¡µé¢1071â€“1079ï¼Œ2014å¹´ã€‚'
- en: '[8] Daniele Calandriello, Daniel Guo, Remi Munos, Mark Rowland, Yunhao Tang,
    BernardoÂ Avila Pires, PierreÂ Harvey Richemond, CharlineÂ Le Lan, Michal Valko,
    Tianqi Liu, etÂ al. Human alignment of large language models through online preference
    optimisation. arXiv preprint arXiv:2403.08635, 2024.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Daniele Calandriello, Daniel Guo, Remi Munos, Mark Rowland, Yunhao Tang,
    Bernardo Avila Pires, Pierre Harvey Richemond, Charline Le Lan, Michal Valko,
    Tianqi Liu, ç­‰ç­‰ã€‚é€šè¿‡åœ¨çº¿åå¥½ä¼˜åŒ–å®ç°å¤§å‹è¯­è¨€æ¨¡å‹çš„äººç±»å¯¹é½ã€‚arXivé¢„å°æœ¬ arXiv:2403.08635ï¼Œ2024å¹´ã€‚'
- en: '[9] DavidÂ M Chan, Yiming Ni, DavidÂ A Ross, Sudheendra Vijayanarasimhan, Austin
    Myers, and John Canny. Distribution aware metrics for conditional natural language
    generation. 2022. arXiv:2209.07518.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] David M Chan, Yiming Ni, David A Ross, Sudheendra Vijayanarasimhan, Austin
    Myers, å’Œ John Canny. å…³æ³¨åˆ†å¸ƒçš„æ¡ä»¶è‡ªç„¶è¯­è¨€ç”Ÿæˆåº¦é‡ã€‚2022å¹´ã€‚arXiv:2209.07518ã€‚'
- en: '[10] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play
    fine-tuning converts weak language models to strong language models. 2024. arXiv:2401.01335.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, å’Œ Quanquan Gu. è‡ªæˆ‘è®­ç»ƒå¾®è°ƒå°†å¼±è¯­è¨€æ¨¡å‹è½¬æ¢ä¸ºå¼ºè¯­è¨€æ¨¡å‹ã€‚2024å¹´ã€‚arXiv:2401.01335ã€‚'
- en: '[11] Douglas Critchlow. Metric methods for analyzing partially ranked data,
    volumeÂ 34. Lecture notes in Statistics, Springer, 1985.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Douglas Critchlow. åˆ†æéƒ¨åˆ†æ’åºæ•°æ®çš„åº¦é‡æ–¹æ³•ï¼Œç¬¬34å·ã€‚ç»Ÿè®¡å­¦è®²ä¹‰ï¼ŒSpringerå‡ºç‰ˆç¤¾ï¼Œ1985å¹´ã€‚'
- en: '[12] Persi Diaconis. Group representations in probability and statistics, volumeÂ 11.
    Lecture Notes-Monograph Series, 1988.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Persi Diaconis. æ¦‚ç‡ä¸ç»Ÿè®¡ä¸­çš„ç¾¤ä½“è¡¨ç¤ºï¼Œç¬¬11å·ã€‚è®²ä¹‰ç¬”è®°-ä¸“è‘—ç³»åˆ—ï¼Œ1988å¹´ã€‚'
- en: '[13] Persi Diaconis. A generalization of spectral analysis with application
    to ranked data. Ann. Stat., pages 949â€“979, 1989.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Persi Diaconis. å…·æœ‰æ’åæ•°æ®åº”ç”¨çš„è°±åˆ†ææ¨å¹¿ã€‚Ann. Stat., é¡µé¢949â€“979ï¼Œ1989å¹´ã€‚'
- en: '[14] Yann Dubois, ChenÂ Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani,
    Jimmy Ba, Carlos Guestrin, PercyÂ S Liang, and TatsunoriÂ B Hashimoto. Alpacafarm:
    A simulation framework for methods that learn from human feedback. In Neurips,
    volumeÂ 36, 2024.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani,
    Jimmy Ba, Carlos Guestrin, Percy S Liang, å’Œ Tatsunori B Hashimoto. Alpacafarmï¼šä¸€ä¸ªå­¦ä¹ äººç±»åé¦ˆçš„æ–¹æ³•æ¨¡æ‹Ÿæ¡†æ¶ã€‚å‘è¡¨äºNeuripsï¼Œç¬¬36å·ï¼Œ2024å¹´ã€‚'
- en: '[15] Kawin Ethayarajh, Winnie Xu, Dan Jurafsky, and Douwe Kiela. Human-aware
    loss functions (halos). Technical report, Contextual AI, 2023. https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Kawin Ethayarajh, Winnie Xu, Dan Jurafsky, å’Œ Douwe Kiela. å…³æ³¨äººç±»çš„æŸå¤±å‡½æ•°ï¼ˆhalosï¼‰ã€‚æŠ€æœ¯æŠ¥å‘Šï¼ŒContextual
    AIï¼Œ2023å¹´ã€‚https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdfã€‚'
- en: '[16] Tingchen Fu, Xueliang Zhao, Chongyang Tao, Ji-Rong Wen, and Rui Yan. There
    are a thousand hamlets in a thousand peopleâ€™s eyes: Enhancing knowledge-grounded
    dialogue with personal memory. 2022. arXiv:2204.02624.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Tingchen Fu, Xueliang Zhao, Chongyang Tao, Ji-Rong Wen, å’Œ Rui Yan. ä¸€åƒä¸ªäººçœ¼ä¸­æœ‰ä¸€åƒä¸ªå“ˆå§†é›·ç‰¹ï¼šé€šè¿‡ä¸ªäººè®°å¿†å¢å¼ºçŸ¥è¯†åŸºç¡€å¯¹è¯ã€‚2022å¹´ã€‚arXiv:2204.02624ã€‚'
- en: '[17] JosÃ©Â Miguel HernÃ¡ndez-Lobato, MatthewÂ W Hoffman, and Zoubin Ghahramani.
    Predictive entropy search for efficient global optimization of black-box functions.
    In NIPS, volumeÂ 27, pages 918â€“â€“926, 2014.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] JosÃ© Miguel HernÃ¡ndez-Lobato, Matthew W Hoffman, å’Œ Zoubin Ghahramani.
    ç”¨äºé«˜æ•ˆå…¨å±€ä¼˜åŒ–é»‘ç®±å‡½æ•°çš„é¢„æµ‹ç†µæœç´¢ã€‚åœ¨ NIPSï¼Œç¬¬ 27 å·ï¼Œé¡µç  918â€“926ï¼Œ2014ã€‚'
- en: '[18] Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew
    Peters, Pradeep Dasigi, Joel Jang, David Wadden, NoahÂ A Smith, and IzÂ Beltagy.
    Camels in a changing climate: Enhancing LM adaptation with Tulu 2. 2023. arXiv:2311.10702.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew
    Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A Smith, å’Œ Iz Beltagy. åœ¨å˜åŒ–çš„æ°”å€™ä¸­éª†é©¼ï¼šé€šè¿‡
    Tulu 2 å¢å¼º LM é€‚åº”æ€§ã€‚2023å¹´ã€‚arXiv:2311.10702ã€‚'
- en: '[19] Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina,
    Eric Hambro, Edward Grefenstette, and Roberta Raileanu. Understanding the effects
    of RLHF on LLM generalisation and diversity. arXiv preprint arXiv:2310.06452,
    2023.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina,
    Eric Hambro, Edward Grefenstette, å’Œ Roberta Raileanu. ç†è§£ RLHF å¯¹ LLM æ³›åŒ–å’Œå¤šæ ·æ€§çš„å½±å“ã€‚arXiv
    é¢„å°æœ¬ arXiv:2310.06452, 2023ã€‚'
- en: '[20] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard,
    Colton Bishop, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement
    learning from human feedback with AI feedback. 2023. arXiv:2309.00267.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard,
    Colton Bishop, Victor Carbune, å’Œ Abhinav Rastogi. Rlaifï¼šé€šè¿‡ AI åé¦ˆæ‰©å±•æ¥è‡ªäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ã€‚2023å¹´ã€‚arXiv:2309.00267ã€‚'
- en: '[21] Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan
    Jurafsky. Deep reinforcement learning for dialogue generation. 2016. arXiv:1606.01541.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, å’Œ Dan
    Jurafsky. ç”¨äºå¯¹è¯ç”Ÿæˆçš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ã€‚2016å¹´ã€‚arXiv:1606.01541ã€‚'
- en: '[22] Andrew Maas, RaymondÂ E Daly, PeterÂ T Pham, Dan Huang, AndrewÂ Y Ng, and
    Christopher Potts. Learning word vectors for sentiment analysis. In ACL, pages
    142â€“150, 2011.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, å’Œ Christopher
    Potts. ä¸ºæƒ…æ„Ÿåˆ†æå­¦ä¹ è¯å‘é‡ã€‚åœ¨ ACLï¼Œé¡µç  142â€“150ï¼Œ2011ã€‚'
- en: '[23] DavidÂ JC MacKay. Information-based objective functions for active data
    selection. Neural computation, 4(4):590â€“604, 1992.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] David JC MacKay. åŸºäºä¿¡æ¯çš„ä¸»åŠ¨æ•°æ®é€‰æ‹©ç›®æ ‡å‡½æ•°ã€‚Neural computation, 4(4):590â€“604, 1992ã€‚'
- en: '[24] ColinÂ L Mallows. Non-null ranking models. I. Biometrika, 44(1/2):114â€“130,
    1957.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Colin L Mallows. éé›¶æ’åæ¨¡å‹ã€‚æˆ‘ã€‚Biometrika, 44(1/2):114â€“130, 1957ã€‚'
- en: '[25] Cheng Mao and Yihong Wu. Learning mixtures of permutations: groups of
    pairwise comparisons and combinatorial method of moments. Ann. Statist., 50(4):2231â€“2255,
    2022.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Cheng Mao å’Œ Yihong Wu. å­¦ä¹ æ’åˆ—çš„æ··åˆï¼šæˆå¯¹æ¯”è¾ƒçš„ç»„å’Œç»„åˆçŸ©æ–¹æ³•ã€‚Ann. Statist., 50(4):2231â€“2255,
    2022ã€‚'
- en: '[26] Marina Meila and LeÂ Bao. An exponential model for infinite rankings. J.
    Mach. Learn. Res., 11:3481â€“3518, 2010.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Marina Meila å’Œ Le Bao. æ— é™æ’åçš„æŒ‡æ•°æ¨¡å‹ã€‚J. Mach. Learn. Res., 11:3481â€“3518, 2010ã€‚'
- en: '[27] RÃ©mi Munos, Michal Valko, Daniele Calandriello, MohammadÂ Gheshlaghi Azar,
    Mark Rowland, ZhaohanÂ Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard,
    and Andrea Michi. Nash learning from human feedback. 2023. arXiv:2312.00886.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] RÃ©mi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar,
    Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard,
    å’Œ Andrea Michi. ä»äººç±»åé¦ˆä¸­å­¦ä¹  Nashã€‚2023å¹´ã€‚arXiv:2312.00886ã€‚'
- en: '[28] Long Ouyang, Jeffrey Wu, XuÂ Jiang, Diogo Almeida, Carroll Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, and Alex Ray. Training
    language models to follow instructions with human feedback. In Neurips, volumeÂ 35,
    pages 27730â€“27744, 2022.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, å’Œ Alex Ray. è®­ç»ƒè¯­è¨€æ¨¡å‹ä»¥éµå¾ªå¸¦æœ‰äººç±»åé¦ˆçš„æŒ‡ä»¤ã€‚åœ¨
    Neuripsï¼Œç¬¬ 35 å·ï¼Œé¡µç  27730â€“27744ï¼Œ2022ã€‚'
- en: '[29] Jim Pitman and Wenpin Tang. Regenerative random permutations of integers.
    Ann. Probab., 47(3):1378â€“1416, 2019.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Jim Pitman å’Œ Wenpin Tang. æ•´æ•°çš„å†ç”Ÿéšæœºæ’åˆ—ã€‚Ann. Probab., 47(3):1378â€“1416, 2019ã€‚'
- en: '[30] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, ChristopherÂ D
    Manning, and Chelsea Finn. Direct preference optimization: Your language model
    is secretly a reward model. In Neurips, volumeÂ 36, 2023.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher
    D Manning, å’Œ Chelsea Finn. ç›´æ¥åå¥½ä¼˜åŒ–ï¼šä½ çš„è¯­è¨€æ¨¡å‹å®é™…ä¸Šæ˜¯ä¸€ä¸ªå¥–åŠ±æ¨¡å‹ã€‚åœ¨Neuripsï¼Œç¬¬36å·ï¼Œ2023ã€‚'
- en: '[31] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg
    Klimov. Proximal policy optimization algorithms. 2017. arXiv:1707.06347.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, å’Œ Oleg Klimov.
    è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ç®—æ³•ã€‚2017ã€‚arXiv:1707.06347ã€‚'
- en: '[32] Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li,
    and Houfeng Wang. Preference ranking optimization for human alignment. In AAAI,
    volumeÂ 38, pages 18990â€“18998, 2024.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li,
    å’Œ Houfeng Wang. ç”¨äºäººç±»å¯¹é½çš„åå¥½æ’åºä¼˜åŒ–ã€‚åœ¨AAAIï¼Œç¬¬38å·ï¼Œç¬¬18990â€“18998é¡µï¼Œ2024ã€‚'
- en: '[33] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea
    Voss, Alec Radford, Dario Amodei, and PaulÂ F Christiano. Learning to summarize
    with human feedback. In Neurips, volumeÂ 33, pages 3008â€“3021, 2020.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea
    Voss, Alec Radford, Dario Amodei, å’Œ Paul F Christiano. å­¦ä¹ æ€»ç»“ä¸äººç±»åé¦ˆã€‚åœ¨Neuripsï¼Œç¬¬33å·ï¼Œç¬¬3008â€“3021é¡µï¼Œ2020ã€‚'
- en: '[34] Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider,
    Tengyang Xie, Stefano Ermon, Chelsea Finn, and Aviral Kumar. Preference fine-tuning
    of llms should leverage suboptimal, on-policy data. arXiv preprint arXiv:2404.14367,
    2024.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider,
    Tengyang Xie, Stefano Ermon, Chelsea Finn, å’Œ Aviral Kumar. LLMçš„åå¥½å¾®è°ƒåº”åˆ©ç”¨æ¬¡ä¼˜çš„ã€åœ¨æ”¿ç­–å†…çš„æ•°æ®ã€‚arXivé¢„å°æœ¬
    arXiv:2404.14367, 2024ã€‚'
- en: '[35] Wenpin Tang. Mallows ranking models: maximum likelihood estimate and regeneration.
    In ICML, pages 6125â€“6134, 2019.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Wenpin Tang. Mallowsæ’åºæ¨¡å‹ï¼šæœ€å¤§ä¼¼ç„¶ä¼°è®¡å’Œå†ç”Ÿã€‚åœ¨ICMLï¼Œç¬¬6125â€“6134é¡µï¼Œ2019ã€‚'
- en: '[36] Yunhao Tang, ZhaohanÂ Daniel Guo, Zeyu Zheng, Daniele Calandriello, RÃ©mi
    Munos, Mark Rowland, PierreÂ Harvey Richemond, Michal Valko, BernardoÂ Ãvila Pires,
    and Bilal Piot. Generalized preference optimization: A unified approach to offline
    alignment. 2024. arXiv:2402.05749.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Yunhao Tang, Zhaohan Daniel Guo, Zeyu Zheng, Daniele Calandriello, RÃ©mi
    Munos, Mark Rowland, Pierre Harvey Richemond, Michal Valko, Bernardo Ãvila Pires,
    å’Œ Bilal Piot. æ³›åŒ–åå¥½ä¼˜åŒ–ï¼šç¦»çº¿å¯¹é½çš„ç»Ÿä¸€æ–¹æ³•ã€‚2024ã€‚arXiv:2402.05749ã€‚'
- en: '[37] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif
    Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, ClÃ©mentine Fourrier,
    and Nathan Habib. Zephyr: Direct distillation of LM alignment. 2023. arXiv:2310.16944.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif
    Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, ClÃ©mentine Fourrier,
    å’Œ Nathan Habib. Zephyrï¼šè¯­è¨€æ¨¡å‹å¯¹é½çš„ç›´æ¥è’¸é¦ã€‚2023ã€‚arXiv:2310.16944ã€‚'
- en: '[38] Binghai Wang, Rui Zheng, LuÂ Chen, Yan Liu, Shihan Dou, Caishuang Huang,
    Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, etÂ al. Secrets of rlhf in large language
    models part II: Reward modeling. 2024. arXiv:2401.06080.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang,
    Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi ç­‰ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ä¸­RLHFçš„ç§˜å¯†ç¬¬äºŒéƒ¨åˆ†ï¼šå¥–åŠ±å»ºæ¨¡ã€‚2024ã€‚arXiv:2401.06080ã€‚'
- en: '[39] Chaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, and Yuxin Chen. Beyond
    reverse KL: Generalizing direct preference optimization with diverse divergence
    constraints. 2023. arXiv:2309.16240.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Chaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, å’Œ Yuxin Chen. è¶…è¶Šåå‘KLï¼šé€šè¿‡å¤šæ ·çš„æ•£åº¦çº¦æŸæ¨å¹¿ç›´æ¥åå¥½ä¼˜åŒ–ã€‚2023ã€‚arXiv:2309.16240ã€‚'
- en: '[40] Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin
    VanÂ Durme, Kenton Murray, and YoungÂ Jin Kim. Contrastive preference optimization:
    Pushing the boundaries of LLM performance in machine translation. 2024. arXiv:2401.08417.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin
    Van Durme, Kenton Murray, å’Œ Young Jin Kim. å¯¹æ¯”åå¥½ä¼˜åŒ–ï¼šæ¨åŠ¨LLMåœ¨æœºå™¨ç¿»è¯‘ä¸­çš„æ€§èƒ½è¾¹ç•Œã€‚2024ã€‚arXiv:2401.08417ã€‚'
- en: '[41] Yuexiang Zhai, Shengbang Tong, Xiao Li, MuÂ Cai, Qing Qu, YongÂ Jae Lee,
    and YiÂ Ma. Investigating the catastrophic forgetting in multimodal large language
    models. 2023. arXiv:2309.10313.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee,
    å’Œ Yi Ma. ç ”ç©¶å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„ç¾éš¾æ€§é—å¿˜ã€‚2023ã€‚arXiv:2309.10313ã€‚'
- en: '[42] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and
    PeterÂ J Liu. Slic-hf: Sequence likelihood calibration with human feedback. 2023.
    arXiv:2305.10425.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, å’Œ
    Peter J Liu. Slic-hfï¼šåŸºäºäººç±»åé¦ˆçš„åºåˆ—ä¼¼ç„¶æ ¡å‡†ã€‚2023ã€‚arXiv:2305.10425ã€‚'
- en: '[43] Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang,
    Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, etÂ al. Secrets of RLHF in large language
    models part I: PPO. 2023. arXiv:2307.04964.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang,
    Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou ç­‰ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ä¸­RLHFçš„ç§˜å¯†ç¬¬ä¸€éƒ¨åˆ†ï¼šPPOã€‚2023ã€‚arXiv:2307.04964ã€‚'
- en: '[44] DanielÂ M Ziegler, Nisan Stiennon, Jeffrey Wu, TomÂ B Brown, Alec Radford,
    Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models
    from human preferences. 2019. arXiv:1909.08593.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford,
    Dario Amodei, Paul Christiano, å’Œ Geoffrey Irving. ä»äººç±»åå¥½ä¸­å¾®è°ƒè¯­è¨€æ¨¡å‹ã€‚2019. arXiv:1909.08593.'
- en: Appendix / Supplemental Material
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é™„å½• / è¡¥å……ææ–™
- en: Appendix A Proofs
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é™„å½• A è¯æ˜
- en: 'Proof of Theorem 1. For ([20](#S4.E20 "In Theorem 1 (Mallows-ğœƒ-DPO as dispersion
    weighted DPO). â€£ 4.1\. Dispersion weighted objectives â€£ 4\. Perspectives on Mallows-DPO
    â€£ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")), the proof follows
    from the derivation of the equivalence between RLHF and DPO, as now the optimal
    policy satisfies'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: å®šç† 1 çš„è¯æ˜ã€‚å¯¹äº ([20](#S4.E20 "åœ¨å®šç† 1 (Mallows-ğœƒ-DPO ä½œä¸ºåŠ æƒ DPO) ä¸­ã€‚ â€£ 4.1\. åŠ æƒç›®æ ‡ â€£
    4\. Mallows-DPO çš„è§‚ç‚¹ â€£ Mallows-DPOï¼šé€šè¿‡åå¥½åˆ†æ•£å¾®è°ƒæ‚¨çš„ LLM"))ï¼Œè¯æ˜éµå¾ª RLHF å’Œ DPO ä¹‹é—´ç­‰ä»·æ€§çš„æ¨å¯¼ï¼Œå› ä¸ºç°åœ¨æœ€ä¼˜ç­–ç•¥æ»¡è¶³
- en: '|  | $c(x)^{-1}r(x,y)=\beta\log\frac{\pi_{r}(y\mid x)}{\pi_{\text{ref }}(y\mid
    x)}+\beta\log Z(x),$ |  |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  | $c(x)^{-1}r(x,y)=\beta\log\frac{\pi_{r}(y\mid x)}{\pi_{\text{ref }}(y\mid
    x)}+\beta\log Z(x),$ |  |'
- en: 'leading to the objective in ([12](#S3.E12 "In 3.2\. Mallows-DPO â€£ 3\. DPO based
    on Mallows Ranking Models â€£ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")).
    The same argument establishes ([21](#S4.E21 "In Theorem 1 (Mallows-ğœƒ-DPO as dispersion
    weighted DPO). â€£ 4.1\. Dispersion weighted objectives â€£ 4\. Perspectives on Mallows-DPO
    â€£ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")). $\blacksquare$'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¼è‡´ç›®æ ‡åœ¨ ([12](#S3.E12 "åœ¨ 3.2\. Mallows-DPO â€£ 3\. åŸºäº Mallows æ’åæ¨¡å‹çš„ DPO â€£ Mallows-DPOï¼šé€šè¿‡åå¥½åˆ†æ•£å¾®è°ƒæ‚¨çš„
    LLM")) ä¸­ã€‚ç›¸åŒçš„è®ºç‚¹å»ºç«‹äº† ([21](#S4.E21 "åœ¨å®šç† 1 (Mallows-ğœƒ-DPO ä½œä¸ºåŠ æƒ DPO) ä¸­ã€‚ â€£ 4.1\. åŠ æƒç›®æ ‡
    â€£ 4\. Mallows-DPO çš„è§‚ç‚¹ â€£ Mallows-DPOï¼šé€šè¿‡åå¥½åˆ†æ•£å¾®è°ƒæ‚¨çš„ LLM"))ã€‚$\blacksquare$
- en: Proof of Theorem 3. (i) With the Bradley-Terry connection as mentioned above,
    we have
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: å®šç† 3 çš„è¯æ˜ã€‚ (i) é€šè¿‡ä¸Šè¿°æåˆ°çš„ Bradley-Terry è¿æ¥ï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\mathbb{E}_{y_{2}\sim\tilde{\pi}}\left[f(x)\Psi\left(p^{*}\left(y_{1}\succ
    y_{2}\mid x\right)\right)\right]$ |  | (29) |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}_{y_{2}\sim\tilde{\pi}}\left[f(x)\Psi\left(p^{*}\left(y_{1}\succ
    y_{2}\mid x\right)\right)\right]$ |  | (29) |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{y_{2}\sim\tilde{\pi}}\left[f(x)\left(r(x,y_{1})-r\left(x,y_{2}\right)\right)\right]$
    |  |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{y_{2}\sim\tilde{\pi}}\left[f(x)\left(r(x,y_{1})-r\left(x,y_{2}\right)\right)\right]$
    |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: which is a weighted reward of DPO, up to an additive constant. It follows that
    the optimal policy of the generalized $\Psi$
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªåŠ æƒå¥–åŠ±çš„ DPOï¼ŒåŠ ä¸Šä¸€ä¸ªé™„åŠ å¸¸æ•°ã€‚å› æ­¤ï¼Œå¹¿ä¹‰çš„ $\Psi$ çš„æœ€ä¼˜ç­–ç•¥
- en: Appendix B Experimental Details
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é™„å½• B å®éªŒç»†èŠ‚
- en: B.1\. Bandit Experiment
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1\. èµŒåšæœºå®éªŒ
- en: 'In the bandit experiment detailed in Section [5.2](#S5.SS2 "5.2\. Mallows-Ï•-DPO
    mitigates reward collapse â€£ 5\. Experiments â€£ Mallows-DPO: Fine-Tune Your LLM
    with Preference Dispersions"), we conduct two sub-experiments to compute the efficient
    frontiers using Mallow-$\phi$ values required to compute the full efficient frontier,
    and for each $\beta$ among the four policies for every $100$ training steps. Given
    that we know the real reward distribution, all these quantities can be computed
    analytically.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬ [5.2](#S5.SS2 "5.2\. Mallows-Ï•-DPO ç¼“è§£å¥–åŠ±å´©æºƒ â€£ 5\. å®éªŒ â€£ Mallows-DPOï¼šé€šè¿‡åå¥½åˆ†æ•£å¾®è°ƒæ‚¨çš„
    LLM") èŠ‚ä¸­è¯¦ç»†æè¿°çš„èµŒåšæœºå®éªŒä¸­ï¼Œæˆ‘ä»¬è¿›è¡Œä¸¤ä¸ªå­å®éªŒä»¥è®¡ç®—ä½¿ç”¨ Mallow-$\phi$ å€¼æ‰€éœ€çš„æœ‰æ•ˆå‰æ²¿ï¼Œå¹¶ä¸”å¯¹æ¯ä¸ª $\beta$ åœ¨æ¯ $100$
    è®­ç»ƒæ­¥éª¤ä¸­è¿›è¡Œè®¡ç®—ã€‚é‰´äºæˆ‘ä»¬çŸ¥é“çœŸå®çš„å¥–åŠ±åˆ†å¸ƒï¼Œæ‰€æœ‰è¿™äº›é‡éƒ½å¯ä»¥é€šè¿‡åˆ†æè®¡ç®—å¾—å‡ºã€‚
- en: In terms of the training details, we use all 16 data in a single batch and adopts
    SGD as the optimizer, with learning rate of 5e-3. To ensure convergence, we run
    the optimization for a large number of epochs, set to 500,000. For Mallows-$\phi$
    to be 0.05.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: å°±è®­ç»ƒç»†èŠ‚è€Œè¨€ï¼Œæˆ‘ä»¬åœ¨ä¸€ä¸ªæ‰¹æ¬¡ä¸­ä½¿ç”¨æ‰€æœ‰ 16 ä¸ªæ•°æ®ï¼Œå¹¶é‡‡ç”¨ SGD ä½œä¸ºä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡ä¸º 5e-3ã€‚ä¸ºäº†ç¡®ä¿æ”¶æ•›ï¼Œæˆ‘ä»¬å°†ä¼˜åŒ–è¿è¡Œå¤§é‡çš„ epochï¼Œè®¾ç½®ä¸º
    500,000ã€‚å¯¹äº Mallows-$\phi$ è®¾ç½®ä¸º 0.05ã€‚
- en: Table 2. Reward distributions of the five arms.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 2. äº”ä¸ªè‡‚çš„å¥–åŠ±åˆ†å¸ƒã€‚
- en: '| Arm 1 | Arm 2 | Arm 3 | Arm 4 | Arm 5 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| è‡‚ 1 | è‡‚ 2 | è‡‚ 3 | è‡‚ 4 | è‡‚ 5 |'
- en: '| Reward | Prob. | Reward | Prob. | Reward | Prob. | Reward | Prob. | Reward
    | Prob. |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| å¥–åŠ± | æ¦‚ç‡ | å¥–åŠ± | æ¦‚ç‡ | å¥–åŠ± | æ¦‚ç‡ | å¥–åŠ± | æ¦‚ç‡ | å¥–åŠ± | æ¦‚ç‡ |'
- en: '| 20 | 0.5 | 30 | 0.5 | 18 | 0.5 | 15 | 0.99 | 1 | 0.99 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 0.5 | 30 | 0.5 | 18 | 0.5 | 15 | 0.99 | 1 | 0.99 |'
- en: '| 11 | 0.5 | 3 | 0.5 | 15 | 0.5 | 10 | 0.01 | 4 | 0.01 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 0.5 | 3 | 0.5 | 15 | 0.5 | 10 | 0.01 | 4 | 0.01 |'
- en: Table 3. 16 pairs of sampled preference data.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 3. 16 å¯¹é‡‡æ ·çš„åå¥½æ•°æ®ã€‚
- en: '| Win | 3 | 2 | 2 | 1 | 3 | 1 | 1 | 1 | 4 | 2 | 2 | 2 | 1 | 3 | 3 | 4 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| èµ¢ | 3 | 2 | 2 | 1 | 3 | 1 | 1 | 1 | 4 | 2 | 2 | 2 | 1 | 3 | 3 | 4 |'
- en: '| Lose | 5 | 5 | 5 | 2 | 5 | 5 | 4 | 5 | 5 | 4 | 1 | 5 | 3 | 5 | 4 | 2 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| è¾“ | 5 | 5 | 5 | 2 | 5 | 5 | 4 | 5 | 5 | 4 | 1 | 5 | 3 | 5 | 4 | 2 |'
- en: B.2\. Controllable Generation Experiment Details
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2\. å¯æ§ç”Ÿæˆå®éªŒç»†èŠ‚
- en: 'We follow the training setup in [[30](#bib.bib30)], and first fine-tune GPT-2-large
    on the training split of IMDB datasets until convergence to get the SFT model.
    The next step is different from [[30](#bib.bib30)] in that we directly utilize
    the (offline) preference dataset from [[39](#bib.bib39)] instead of generating
    pairwise preferences from the trained SFT model, as in DPO. The rest is the same:
    we use the pairwise preference data to fine-tune the SFT model by either DPO or
    Mallows-DPO. The evaluation metric: accuracy is obtained from a prior sentiment
    classifier as the ground truth reward. By default, we use RMSprop optimizer with
    a learning rate of 1e-6, with a linear learning rate warmup from 0 to 1e-6 over
    the first 150 steps. The training batch size is 64.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éµå¾ªäº†[[30](#bib.bib30)]ä¸­çš„è®­ç»ƒè®¾ç½®ï¼Œé¦–å…ˆåœ¨IMDBæ•°æ®é›†çš„è®­ç»ƒåˆ†å‰²ä¸Šå¯¹GPT-2-largeè¿›è¡Œå¾®è°ƒï¼Œç›´è‡³æ”¶æ•›ï¼Œä»¥è·å¾—SFTæ¨¡å‹ã€‚ä¸‹ä¸€æ­¥ä¸[[30](#bib.bib30)]ä¸åŒçš„æ˜¯ï¼Œæˆ‘ä»¬ç›´æ¥åˆ©ç”¨[[39](#bib.bib39)]çš„ï¼ˆç¦»çº¿ï¼‰åå¥½æ•°æ®é›†ï¼Œè€Œä¸æ˜¯åƒDPOä¸­é‚£æ ·ä»è®­ç»ƒå¥½çš„SFTæ¨¡å‹ä¸­ç”Ÿæˆæˆå¯¹åå¥½æ•°æ®ã€‚å…¶ä½™éƒ¨åˆ†ç›¸åŒï¼šæˆ‘ä»¬ä½¿ç”¨æˆå¯¹åå¥½æ•°æ®é€šè¿‡DPOæˆ–Mallows-DPOå¾®è°ƒSFTæ¨¡å‹ã€‚è¯„ä»·æŒ‡æ ‡ï¼šå‡†ç¡®ç‡æ¥è‡ªå…ˆå‰çš„æƒ…æ„Ÿåˆ†ç±»å™¨ä½œä¸ºçœŸå®å¥–åŠ±ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨å­¦ä¹ ç‡ä¸º1e-6çš„RMSpropä¼˜åŒ–å™¨ï¼Œåœ¨å‰150æ­¥å†…è¿›è¡Œä»0åˆ°1e-6çš„çº¿æ€§å­¦ä¹ ç‡é¢„çƒ­ã€‚è®­ç»ƒæ‰¹é‡å¤§å°ä¸º64ã€‚
- en: B.3\. Language Modeling Experiment Details
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3\. è¯­è¨€å»ºæ¨¡å®éªŒç»†èŠ‚
- en: We follow the training setup in [[30](#bib.bib30)]. By default, we use RMSprop
    optimizer with a learning rate of 1e-6, with a linear learning rate warmup from
    0 to 1e-6 over the first 150 steps. The training batch size is 32.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éµå¾ªäº†[[30](#bib.bib30)]ä¸­çš„è®­ç»ƒè®¾ç½®ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨å­¦ä¹ ç‡ä¸º1e-6çš„RMSpropä¼˜åŒ–å™¨ï¼Œåœ¨å‰150æ­¥å†…è¿›è¡Œä»0åˆ°1e-6çš„çº¿æ€§å­¦ä¹ ç‡é¢„çƒ­ã€‚è®­ç»ƒæ‰¹é‡å¤§å°ä¸º32ã€‚
- en: B.3.1\. GPT-4 Judgement Prompt
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.3.1\. GPT-4åˆ¤æ–­æç¤º
- en: 'Response quality evaluation is completed by GPT-4\. The prompt for instructing
    GPT-4 to evaluate which response is better is particularly important. Thus, we
    use the fastchat package for GPT-4 evaluation, and we used their well-written
    pair-v2 judge prompt. The prompt is shown as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: å“åº”è´¨é‡è¯„ä¼°ç”±GPT-4å®Œæˆã€‚æŒ‡ç¤ºGPT-4è¯„ä¼°å“ªä¸ªå“åº”æ›´å¥½çš„æç¤ºéå¸¸é‡è¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ç”¨äºGPT-4è¯„ä¼°çš„fastchatåŒ…ï¼Œå¹¶ä½¿ç”¨äº†ä»–ä»¬ç¼–å†™è‰¯å¥½çš„pair-v2
    judgeæç¤ºã€‚æç¤ºå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: 'Please act as an impartial judge and evaluate the quality of the responses
    provided by two AI assistants to the user question displayed below. You should
    choose the assistant that follows the userâ€™s instructions and answers the userâ€™s
    question better. Your evaluation should consider factors such as the helpfulness,
    relevance, accuracy, depth, creativity, and level of detail of their responses.
    Begin your evaluation by comparing the two responses and provide a short explanation.
    Avoid any position biases and ensure that the order in which the responses were
    presented does not influence your decision. Do not allow the length of the responses
    to influence your evaluation. Do not favor certain names of the assistants. Be
    as objective as possible. After providing your explanation, output your final
    verdict by strictly following this format: \â€˜â€˜ [[A]]\â€™â€™ if assistant A is better,
    \â€˜â€˜[[B]]\â€™â€™ if assistant B is better, and \â€˜â€˜[[C]]\â€™â€™ for a tie."'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·ä½œä¸ºå…¬æ­£çš„è£åˆ¤ï¼Œè¯„ä¼°ä¸¤ä¸ªAIåŠ©æ‰‹å¯¹ä¸‹åˆ—ç”¨æˆ·é—®é¢˜æä¾›çš„å“åº”è´¨é‡ã€‚ä½ åº”è¯¥é€‰æ‹©éµå¾ªç”¨æˆ·æŒ‡ä»¤å¹¶æ›´å¥½åœ°å›ç­”ç”¨æˆ·é—®é¢˜çš„åŠ©æ‰‹ã€‚ä½ çš„è¯„ä¼°åº”è€ƒè™‘ä»–ä»¬å“åº”çš„æœ‰ç”¨æ€§ã€ç›¸å…³æ€§ã€å‡†ç¡®æ€§ã€æ·±åº¦ã€åˆ›é€ åŠ›å’Œç»†èŠ‚æ°´å¹³ã€‚å¼€å§‹è¯„ä¼°æ—¶è¯·æ¯”è¾ƒä¸¤ä¸ªå“åº”ï¼Œå¹¶æä¾›ç®€çŸ­çš„è§£é‡Šã€‚é¿å…ä»»ä½•ä½ç½®åè§ï¼Œç¡®ä¿å“åº”çš„é¡ºåºä¸ä¼šå½±å“ä½ çš„å†³å®šã€‚ä¸è¦è®©å“åº”çš„é•¿åº¦å½±å“ä½ çš„è¯„ä¼°ã€‚ä¸è¦åè¢’æŸäº›åŠ©æ‰‹çš„åå­—ã€‚å°½å¯èƒ½å®¢è§‚ã€‚åœ¨æä¾›è§£é‡Šåï¼Œä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºæœ€ç»ˆè£å†³ï¼š\â€˜â€˜
    [[A]]\â€™â€™ å¦‚æœåŠ©æ‰‹Aæ›´å¥½ï¼Œ\â€˜â€˜[[B]]\â€™â€™ å¦‚æœåŠ©æ‰‹Bæ›´å¥½ï¼Œ\â€˜â€˜[[C]]\â€™â€™ å¦‚æœå¹³å±€ã€‚
- en: 'To ensure fairness and unbiasedness, for each pairwise input $(x,y_{1},y_{2})$
    wins if and only if it wins both comparisons, or wins one comparison while the
    other is tied. We compute win rate as follows:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºç¡®ä¿å…¬å¹³æ€§å’Œæ— åæ€§ï¼Œå¯¹äºæ¯å¯¹è¾“å…¥$(x,y_{1},y_{2})$ï¼Œä»…å½“å®ƒåœ¨ä¸¤ä¸ªæ¯”è¾ƒä¸­éƒ½è·èƒœï¼Œæˆ–è€…åœ¨ä¸€ä¸ªæ¯”è¾ƒä¸­è·èƒœè€Œå¦ä¸€ä¸ªæ¯”è¾ƒå¹³å±€æ—¶ï¼Œæ‰ç®—èƒœå‡ºã€‚æˆ‘ä»¬æŒ‰å¦‚ä¸‹æ–¹å¼è®¡ç®—èƒœç‡ï¼š
- en: '|  | $\text{Win rate (Model A)}=\frac{\text{Number of samples where Model A
    wins}}{\text{Total number of test samples}}+0.5\times\frac{\text{Number of tied
    samples}}{\text{Total number of test samples}}$ |  |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{èƒœç‡ï¼ˆæ¨¡å‹Aï¼‰}=\frac{\text{æ¨¡å‹Aè·èƒœçš„æ ·æœ¬æ•°é‡}}{\text{æµ‹è¯•æ ·æœ¬æ€»æ•°}}+0.5\times\frac{\text{å¹³å±€æ ·æœ¬æ•°é‡}}{\text{æµ‹è¯•æ ·æœ¬æ€»æ•°}}$
    |  |'
- en: Appendix C Qualitative Examples
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é™„å½•C å®šæ€§ç¤ºä¾‹
- en: 'In this section, we present a series of comparisons between Mallows-DPO variants
    and BT-DPO, as shown in Tables [4](#A3.T4 "Table 4 â€£ Appendix C Qualitative Examples
    â€£ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")â€“[15](#A3.T15 "Table
    15 â€£ Appendix C Qualitative Examples â€£ Mallows-DPO: Fine-Tune Your LLM with Preference
    Dispersions"). These tables demonstrate the qualitative examples of responses
    to in-distribution inputs from the Anthropic-HH test set, to out-of-distribution
    inputs from the SHP test set, and to out-of-distribution inputs from the SE dataset
    respectively.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å‘ˆç°äº†ä¸€ç³»åˆ—å¯¹æ¯”ï¼ŒåŒ…æ‹¬Mallows-DPOå˜ä½“ä¸BT-DPOçš„æ¯”è¾ƒï¼Œå¦‚è¡¨[4](#A3.T4 "è¡¨4 â€£ é™„å½•C å®šæ€§ç¤ºä¾‹ â€£ Mallows-DPO:
    ä½¿ç”¨åå¥½åˆ†æ•£å¾®è°ƒæ‚¨çš„LLM")â€“[15](#A3.T15 "è¡¨15 â€£ é™„å½•C å®šæ€§ç¤ºä¾‹ â€£ Mallows-DPO: ä½¿ç”¨åå¥½åˆ†æ•£å¾®è°ƒæ‚¨çš„LLM")æ‰€ç¤ºã€‚è¿™äº›è¡¨æ ¼å±•ç¤ºäº†å¯¹æ¥è‡ªAnthropic-HHæµ‹è¯•é›†çš„åˆ†å¸ƒå†…è¾“å…¥ã€æ¥è‡ªSHPæµ‹è¯•é›†çš„åˆ†å¸ƒå¤–è¾“å…¥å’Œæ¥è‡ªSEæ•°æ®é›†çš„åˆ†å¸ƒå¤–è¾“å…¥çš„å“åº”çš„å®šæ€§ç¤ºä¾‹ã€‚'
- en: 'To interpret, Tables [4](#A3.T4 "Table 4 â€£ Appendix C Qualitative Examples
    â€£ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")â€“[5](#A3.T5 "Table
    5 â€£ Appendix C Qualitative Examples â€£ Mallows-DPO: Fine-Tune Your LLM with Preference
    Dispersions") show that models fine-tuned by Mallows-$\theta$-DPO and Mallows-$\phi$-DPO,
    their responses do not deviate too much from the user prompt and remain meaningful.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 'è§£é‡Šæ¥è¯´ï¼Œè¡¨[4](#A3.T4 "è¡¨4 â€£ é™„å½•C å®šæ€§ç¤ºä¾‹ â€£ Mallows-DPO: ä½¿ç”¨åå¥½åˆ†æ•£å¾®è°ƒæ‚¨çš„LLM")â€“[5](#A3.T5 "è¡¨5
    â€£ é™„å½•C å®šæ€§ç¤ºä¾‹ â€£ Mallows-DPO: ä½¿ç”¨åå¥½åˆ†æ•£å¾®è°ƒæ‚¨çš„LLM")æ˜¾ç¤ºï¼ŒMallows-$\theta$-DPOå’ŒMallows-$\phi$-DPOå¾®è°ƒçš„æ¨¡å‹ï¼Œå…¶å“åº”ä¸ç”¨æˆ·æç¤ºæ²¡æœ‰å¤ªå¤§åç¦»ï¼Œå¹¶ä¸”ä¿æŒäº†æœ‰æ„ä¹‰æ€§ã€‚'
- en: 'On the contrary, the examples in Tables [8](#A3.T8 "Table 8 â€£ Appendix C Qualitative
    Examples â€£ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")â€“[11](#A3.T11
    "Table 11 â€£ Appendix C Qualitative Examples â€£ Mallows-DPO: Fine-Tune Your LLM
    with Preference Dispersions") indicate that models of Mallows-$\theta$-DPO in
    Tables [8](#A3.T8 "Table 8 â€£ Appendix C Qualitative Examples â€£ Mallows-DPO: Fine-Tune
    Your LLM with Preference Dispersions")â€“[9](#A3.T9 "Table 9 â€£ Appendix C Qualitative
    Examples â€£ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions") include
    peer feedback and additional suggestions, like double majoring, regarding a question
    of major transferring. Tables [10](#A3.T10 "Table 10 â€£ Appendix C Qualitative
    Examples â€£ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")â€“[11](#A3.T11
    "Table 11 â€£ Appendix C Qualitative Examples â€£ Mallows-DPO: Fine-Tune Your LLM
    with Preference Dispersions") show that models fine-tuned by Mallows-$\theta$-DPO
    demonstrate strong knowledge in history and philosophy, by providing more related
    details and supportive arguments, compared to BT-DPO.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç›¸åï¼Œè¡¨[8](#A3.T8 "è¡¨8 â€£ é™„å½•C å®šæ€§ç¤ºä¾‹ â€£ Mallows-DPO: ä½¿ç”¨åå¥½åˆ†æ•£å¾®è°ƒæ‚¨çš„LLM")â€“[11](#A3.T11 "è¡¨11
    â€£ é™„å½•C å®šæ€§ç¤ºä¾‹ â€£ Mallows-DPO: ä½¿ç”¨åå¥½åˆ†æ•£å¾®è°ƒæ‚¨çš„LLM")ä¸­çš„ç¤ºä¾‹è¡¨æ˜ï¼ŒMallows-$\theta$-DPOæ¨¡å‹åœ¨è¡¨[8](#A3.T8
    "è¡¨8 â€£ é™„å½•C å®šæ€§ç¤ºä¾‹ â€£ Mallows-DPO: ä½¿ç”¨åå¥½åˆ†æ•£å¾®è°ƒæ‚¨çš„LLM")â€“[9](#A3.T9 "è¡¨9 â€£ é™„å½•C å®šæ€§ç¤ºä¾‹ â€£ Mallows-DPO:
    ä½¿ç”¨åå¥½åˆ†æ•£å¾®è°ƒæ‚¨çš„LLM")ä¸­åŒ…æ‹¬äº†åŒè¡Œåé¦ˆå’Œé¢å¤–å»ºè®®ï¼Œæ¯”å¦‚åŒå­¦ä½ï¼Œå…³äºä¸“ä¸šè½¬ç§»çš„é—®é¢˜ã€‚è¡¨[10](#A3.T10 "è¡¨10 â€£ é™„å½•C å®šæ€§ç¤ºä¾‹ â€£ Mallows-DPO:
    ä½¿ç”¨åå¥½åˆ†æ•£å¾®è°ƒæ‚¨çš„LLM")â€“[11](#A3.T11 "è¡¨11 â€£ é™„å½•C å®šæ€§ç¤ºä¾‹ â€£ Mallows-DPO: ä½¿ç”¨åå¥½åˆ†æ•£å¾®è°ƒæ‚¨çš„LLM")æ˜¾ç¤ºï¼ŒMallows-$\theta$-DPOå¾®è°ƒçš„æ¨¡å‹å±•ç¤ºäº†åœ¨å†å²å’Œå“²å­¦æ–¹é¢çš„å¼ºå¤§çŸ¥è¯†ï¼Œé€šè¿‡æä¾›æ›´å¤šç›¸å…³ç»†èŠ‚å’Œæ”¯æŒæ€§è®ºæ®ï¼Œä¸BT-DPOç›¸æ¯”è¡¨ç°æ›´ä½³ã€‚'
- en: 'Finally, we also present examples for answering questions regarding coding
    in Tables [12](#A3.T12 "Table 12 â€£ Appendix C Qualitative Examples â€£ Mallows-DPO:
    Fine-Tune Your LLM with Preference Dispersions")â€“[15](#A3.T15 "Table 15 â€£ Appendix
    C Qualitative Examples â€£ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions"),
    among which Tables [12](#A3.T12 "Table 12 â€£ Appendix C Qualitative Examples â€£
    Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")â€“[13](#A3.T13 "Table
    13 â€£ Appendix C Qualitative Examples â€£ Mallows-DPO: Fine-Tune Your LLM with Preference
    Dispersions") show that Mallows-$\theta$-DPO in Table [15](#A3.T15 "Table 15 â€£
    Appendix C Qualitative Examples â€£ Mallows-DPO: Fine-Tune Your LLM with Preference
    Dispersions") provides additional code to support the user, complementing the
    suggestions by BT-DPO.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 'æœ€åï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†å›ç­”ç¼–ç ç›¸å…³é—®é¢˜çš„ç¤ºä¾‹ï¼Œè¿™äº›ç¤ºä¾‹è§è¡¨[12](#A3.T12 "è¡¨12 â€£ é™„å½•C å®šæ€§ç¤ºä¾‹ â€£ Mallows-DPO: ä½¿ç”¨åå¥½åˆ†æ•£å¾®è°ƒæ‚¨çš„LLM")â€“[15](#A3.T15
    "è¡¨15 â€£ é™„å½•C å®šæ€§ç¤ºä¾‹ â€£ Mallows-DPO: ä½¿ç”¨åå¥½åˆ†æ•£å¾®è°ƒæ‚¨çš„LLM")ï¼Œå…¶ä¸­è¡¨[12](#A3.T12 "è¡¨12 â€£ é™„å½•C å®šæ€§ç¤ºä¾‹
    â€£ Mallows-DPO: ä½¿ç”¨åå¥½åˆ†æ•£å¾®è°ƒæ‚¨çš„LLM")â€“[13](#A3.T13 "è¡¨13 â€£ é™„å½•C å®šæ€§ç¤ºä¾‹ â€£ Mallows-DPO: ä½¿ç”¨åå¥½åˆ†æ•£å¾®è°ƒæ‚¨çš„LLM")æ˜¾ç¤ºï¼ŒMallows-$\theta$-DPOåœ¨è¡¨[15](#A3.T15
    "è¡¨15 â€£ é™„å½•C å®šæ€§ç¤ºä¾‹ â€£ Mallows-DPO: ä½¿ç”¨åå¥½åˆ†æ•£å¾®è°ƒæ‚¨çš„LLM")ä¸­æä¾›äº†é¢å¤–çš„ä»£ç æ¥æ”¯æŒç”¨æˆ·ï¼Œè¡¥å……äº†BT-DPOçš„å»ºè®®ã€‚'
- en: Table 4. Sample responses to a prompt from the Anthropic-HH test set. GPT-4
    chooses Mallows-$\theta$.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 4. æ¥è‡ª Anthropic-HH æµ‹è¯•é›†çš„æç¤ºç¤ºä¾‹å“åº”ã€‚GPT-4 é€‰æ‹©äº† Mallows-$\theta$ã€‚
- en: '| Prompt | She likes bingo, knitting, shopping, her grandkids, coffee. |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| æç¤º | å¥¹å–œæ¬¢å®¾æœæ¸¸æˆã€ç¼–ç»‡ã€è´­ç‰©ã€å¥¹çš„å­™å­å­™å¥³ã€å’–å•¡ã€‚ |'
- en: '| Mallows-$\theta$-DPO | Grandma loves receiving special gifts from family
    members, so an elegant, quality, personalized hand knit sweater is a great idea.
    She loves warm, cozy clothing, so a cashmere sweater is a nice gesture, or a cozy
    pullover knit sweater, for a cozy fall or winter wardrobe. Some other ideas are
    a personalized coffee mug, a knitting patterns, and a hand knit tea cozy. Also
    a small ornament from a special hand knit sweater is a nice gift. If you know
    your grandmaâ€™s interests, you can even surprise her by shopping together, so you
    can pick up special gifts for her such as a cookbook for baking, or a knitting
    pattern book, or a coffee mug, or a tea cozy. |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| Mallows-$\theta$-DPO | å¥¶å¥¶å–œæ¬¢æ”¶åˆ°æ¥è‡ªå®¶äººçš„ç‰¹åˆ«ç¤¼ç‰©ï¼Œæ‰€ä»¥ä¸€ä»¶ä¼˜é›…ã€ä¼˜è´¨ã€ä¸ªæ€§åŒ–çš„æ‰‹å·¥ç¼–ç»‡æ¯›è¡£æ˜¯ä¸€ä¸ªä¸é”™çš„ä¸»æ„ã€‚å¥¹å–œæ¬¢æ¸©æš–ã€èˆ’é€‚çš„è¡£ç‰©ï¼Œæ‰€ä»¥ä¸€ä»¶ç¾Šç»’æ¯›è¡£æ˜¯ä¸€ä¸ªä½“è´´çš„é€‰æ‹©ï¼Œæˆ–è€…æ˜¯ä¸€ä»¶èˆ’é€‚çš„å¥—å¤´æ¯›è¡£ï¼Œé€‚åˆç§‹å†¬å­£èŠ‚çš„è¡£æ©±ã€‚å…¶ä»–çš„ä¸€äº›æƒ³æ³•åŒ…æ‹¬ä¸ªæ€§åŒ–çš„å’–å•¡æ¯ã€ç¼–ç»‡å›¾æ¡ˆå’Œæ‰‹å·¥ç¼–ç»‡çš„èŒ¶æ¯å¥—ã€‚æ­¤å¤–ï¼Œä¸€ä»¶ç‰¹åˆ«æ‰‹å·¥ç¼–ç»‡æ¯›è¡£çš„å°é¥°å“ä¹Ÿæ˜¯å¾ˆå¥½çš„ç¤¼ç‰©ã€‚å¦‚æœä½ çŸ¥é“å¥¶å¥¶çš„å…´è¶£çˆ±å¥½ï¼Œä½ ç”šè‡³å¯ä»¥é€šè¿‡ä¸€èµ·è´­ç‰©æ¥ç»™å¥¹æƒŠå–œï¼Œè¿™æ ·ä½ å¯ä»¥ä¸ºå¥¹æŒ‘é€‰ç‰¹åˆ«çš„ç¤¼ç‰©ï¼Œæ¯”å¦‚ä¸€æœ¬çƒ˜ç„™é£Ÿè°±ã€ä¸€æœ¬ç¼–ç»‡å›¾æ¡ˆä¹¦ã€ä¸€åªå’–å•¡æ¯æˆ–ä¸€ä¸ªèŒ¶æ¯å¥—ã€‚
    |'
- en: '| BT-DPO | Grandma loves receiving special gifts from family members, so an
    elegant, quality, personalized hand knit sweater is a great idea. She might enjoy
    a good coffee and a tasty meal, or a book and flowers. If you have a special skill,
    you could knit her a sweater for Christmas or even a sweater for her birthday.
    You can choose a different special occasion, like Valentineâ€™s Day. You could knit
    her a Valentineâ€™s gift like, a Valentineâ€™s ring or bracelet. Or you could make
    a holiday sweater and knit it for Christmas, so she can relax and unwind at the
    end of the year. If she has a lot of friends, you could make her a special hand
    knit sweater and knit it for all her friends to enjoy. If you know her from church
    or play sports, you could knit her a sports themed sweater. Or you could make
    a sweater with a Bible verse on the front, like the sweater Myron did for his
    grandmother. When you make the sweater, you make it all up, so you could even
    make a sweater with a Bible verse on the front, like the sweater that Myron did
    for his grandmother. To make this sweater, you could knit a sweater with the Bible
    verse, you could knit a sweater like your grandmaâ€™s favorite sweater, you could
    knit a sweater with the Bible verse on the front. So Myron made his grandmother
    a sweater with the Bible verse. You could make your grandma a sweater with the
    Bible verse. Grandma loves giving special gifts to family and friends, so I recommend
    a sweater with a Bible verse on the front. Or you could make her a sweater with
    your own special gift. Would you like me to help you with this? Letâ€™s see, you
    want a hand knit sweater, knit by a knitting machine, you want only one sweater
    and you want to knit it for your grandma, how about that? Letâ€™s say you want to
    knit up a sweater and hand it to her. Is that ok? Let me make a list of possibilities
    for your sweater, we can pick one or a few that might be special. Hereâ€™s a list
    of items that are the best match: Bible verse Glover Shawnee Grandma would like
    to know more about the Bible verse, so letâ€™s get specific. There is only one sweater
    that Grandma will prefer, you can pick one, letâ€™s see, could be the one with a
    Bible verse on the front or the one with Glover. So itâ€™s a hand knit sweater with
    a Bible verse on the front. Hereâ€™s a list of specific Bible verses that people
    would like Grandma to learn: â€œI shall tell my lord the king all the prosperity
    that I have. I shall say, I shall say to you of peace. I shall declare the good
    news to the poor. I shall declare the gospel of grace. Here is a list of specific
    Bible verses that people would like Grandma to learn: Let there be light! Let
    there be light! Let there be light! - - - Let there be light! Let [maximum generation
    length reached] |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| BT-DPO | å¥¶å¥¶å–œæ¬¢æ”¶åˆ°å®¶äººçš„ç‰¹åˆ«ç¤¼ç‰©ï¼Œæ‰€ä»¥ä¸€ä»¶ä¼˜é›…ã€ä¼˜è´¨ã€ä¸ªæ€§åŒ–çš„æ‰‹å·¥ç¼–ç»‡æ¯›è¡£æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ä¸»æ„ã€‚å¥¹å¯èƒ½ä¼šå–œæ¬¢ä¸€æ¯å¥½å’–å•¡å’Œç¾å‘³çš„é¤ç‚¹ï¼Œæˆ–è€…ä¸€æœ¬ä¹¦å’Œä¸€äº›èŠ±ã€‚å¦‚æœä½ æœ‰ç‰¹åˆ«çš„æŠ€èƒ½ï¼Œä½ å¯ä»¥åœ¨åœ£è¯èŠ‚ä¸ºå¥¹ç¼–ç»‡ä¸€ä»¶æ¯›è¡£ï¼Œç”šè‡³å¯ä»¥åœ¨å¥¹ç”Ÿæ—¥æ—¶é€å¥¹ä¸€ä»¶æ¯›è¡£ã€‚ä½ ä¹Ÿå¯ä»¥é€‰æ‹©å…¶ä»–ç‰¹åˆ«çš„åœºåˆï¼Œæ¯”å¦‚æƒ…äººèŠ‚ã€‚ä½ å¯ä»¥ä¸ºå¥¹ç¼–ç»‡ä¸€ä¸ªæƒ…äººèŠ‚ç¤¼ç‰©ï¼Œæ¯”å¦‚æƒ…äººèŠ‚æˆ’æŒ‡æˆ–æ‰‹é“¾ã€‚æˆ–è€…ä½ å¯ä»¥åˆ¶ä½œä¸€ä»¶èŠ‚æ—¥æ¯›è¡£ï¼Œä¸ºåœ£è¯èŠ‚ç¼–ç»‡ï¼Œè¿™æ ·å¥¹å¯ä»¥åœ¨å¹´åº•æ”¾æ¾èº«å¿ƒã€‚å¦‚æœå¥¹æœ‰å¾ˆå¤šæœ‹å‹ï¼Œä½ å¯ä»¥ä¸ºå¥¹ç¼–ç»‡ä¸€ä»¶ç‰¹åˆ«çš„æ‰‹å·¥æ¯›è¡£ï¼Œä¾›å¥¹çš„æœ‹å‹ä»¬ä¸€èµ·äº«ç”¨ã€‚å¦‚æœä½ è®¤è¯†å¥¹ï¼Œå¯èƒ½æ˜¯åœ¨æ•™å ‚æˆ–ä½“è‚²æ´»åŠ¨ä¸­ï¼Œä½ å¯ä»¥ä¸ºå¥¹ç¼–ç»‡ä¸€ä»¶ä»¥ä½“è‚²ä¸ºä¸»é¢˜çš„æ¯›è¡£ã€‚æˆ–è€…ä½ å¯ä»¥åœ¨å‰é¢åŠ ä¸Šåœ£ç»ç»æ–‡çš„æ¯›è¡£ï¼Œå°±åƒMyronä¸ºä»–çš„å¥¶å¥¶åšçš„æ¯›è¡£é‚£æ ·ã€‚åˆ¶ä½œè¿™ä»¶æ¯›è¡£æ—¶ï¼Œä½ å¯ä»¥è‡ªç”±å‘æŒ¥ï¼Œæ‰€ä»¥ä½ ç”šè‡³å¯ä»¥åˆ¶ä½œä¸€ä»¶å¸¦æœ‰åœ£ç»ç»æ–‡çš„æ¯›è¡£ï¼Œå°±åƒMyronä¸ºä»–çš„å¥¶å¥¶åšçš„é‚£æ ·ã€‚è¦åˆ¶ä½œè¿™ä»¶æ¯›è¡£ï¼Œä½ å¯ä»¥ç¼–ç»‡ä¸€ä»¶å¸¦æœ‰åœ£ç»ç»æ–‡çš„æ¯›è¡£ï¼Œæˆ–è€…ç¼–ç»‡ä¸€ä»¶åƒå¥¶å¥¶æœ€å–œæ¬¢çš„æ¯›è¡£ä¸€æ ·çš„æ¯›è¡£ï¼Œæˆ–è€…ç¼–ç»‡ä¸€ä»¶å‰é¢æœ‰åœ£ç»ç»æ–‡çš„æ¯›è¡£ã€‚æ‰€ä»¥Myronç»™ä»–çš„å¥¶å¥¶åšäº†ä¸€ä»¶å¸¦æœ‰åœ£ç»ç»æ–‡çš„æ¯›è¡£ã€‚ä½ ä¹Ÿå¯ä»¥ç»™å¥¶å¥¶åšä¸€ä»¶å¸¦æœ‰åœ£ç»ç»æ–‡çš„æ¯›è¡£ã€‚å¥¶å¥¶å–œæ¬¢ç»™å®¶äººå’Œæœ‹å‹é€ç‰¹åˆ«çš„ç¤¼ç‰©ï¼Œæ‰€ä»¥æˆ‘æ¨èä¸€ä»¶å‰é¢æœ‰åœ£ç»ç»æ–‡çš„æ¯›è¡£ã€‚æˆ–è€…ä½ å¯ä»¥åšä¸€ä»¶ä½ è‡ªå·±ç‰¹åˆ«çš„ç¤¼ç‰©çš„æ¯›è¡£ã€‚ä½ æƒ³è®©æˆ‘å¸®å¿™å—ï¼Ÿæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ï¼Œä½ æƒ³è¦ä¸€ä»¶æ‰‹å·¥ç¼–ç»‡çš„æ¯›è¡£ï¼Œç”±ç¼–ç»‡æœºç¼–ç»‡ï¼Œä½ åªæƒ³è¦ä¸€ä»¶æ¯›è¡£ï¼Œå¹¶ä¸”ä½ æƒ³ä¸ºå¥¶å¥¶ç¼–ç»‡ï¼Œæ€ä¹ˆæ ·ï¼Ÿå‡è®¾ä½ æƒ³ç¼–ç»‡ä¸€ä»¶æ¯›è¡£ç„¶åäº²æ‰‹äº¤ç»™å¥¹ã€‚è¿™æ ·å¯ä»¥å—ï¼Ÿè®©æˆ‘åˆ—ä¸€ä¸ªæ¯›è¡£çš„å¯èƒ½æ€§æ¸…å•ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©ä¸€ä¸ªæˆ–å‡ ä¸ªç‰¹åˆ«çš„é€‰é¡¹ã€‚ä»¥ä¸‹æ˜¯æœ€åŒ¹é…çš„é¡¹ç›®æ¸…å•ï¼šåœ£ç»ç»æ–‡
    Glover Shawnee å¥¶å¥¶æƒ³æ›´å¤šåœ°äº†è§£åœ£ç»ç»æ–‡ï¼Œæ‰€ä»¥æˆ‘ä»¬æ¥å…·ä½“çœ‹çœ‹ã€‚å¥¶å¥¶ä¼šæ›´å–œæ¬¢å”¯ä¸€çš„ä¸€ä»¶æ¯›è¡£ï¼Œä½ å¯ä»¥é€‰æ‹©ä¸€ä»¶ï¼Œæ¯”å¦‚å‰é¢æœ‰åœ£ç»ç»æ–‡çš„æ¯›è¡£æˆ–æœ‰Gloverçš„æ¯›è¡£ã€‚æ‰€ä»¥è¿™æ˜¯ä¸€ä»¶å‰é¢æœ‰åœ£ç»ç»æ–‡çš„æ‰‹å·¥ç¼–ç»‡æ¯›è¡£ã€‚ä»¥ä¸‹æ˜¯äººä»¬å¸Œæœ›å¥¶å¥¶å­¦ä¹ çš„å…·ä½“åœ£ç»ç»æ–‡åˆ—è¡¨ï¼šâ€œæˆ‘å°†æŠŠæˆ‘æ‰€æœ‰çš„ç¹è£å‘Šè¯‰æˆ‘çš„ä¸»ç‹ã€‚æˆ‘å°†è¯´ï¼Œæˆ‘å°†å¯¹ä½ è¯´å¹³å®‰ã€‚æˆ‘å°†å‘ç©·äººå®£å‘Šå¥½æ¶ˆæ¯ã€‚æˆ‘å°†å®£æ‰¬æ©å…¸çš„ç¦éŸ³ã€‚ä»¥ä¸‹æ˜¯äººä»¬å¸Œæœ›å¥¶å¥¶å­¦ä¹ çš„å…·ä½“åœ£ç»ç»æ–‡ï¼šè¦æœ‰å…‰ï¼è¦æœ‰å…‰ï¼è¦æœ‰å…‰ï¼-
    - - è¦æœ‰å…‰ï¼è®© [æœ€å¤§ç”Ÿæˆé•¿åº¦å·²è¾¾åˆ°]'
- en: '| Judgment | Assistant Aâ€™s response is more relevant and helpful to the userâ€™s
    statement. The assistant suggests personalized gifts that align with the grandmotherâ€™s
    interests, such as a hand knit sweater, a personalized coffee mug, a knitting
    pattern book, and a tea cozy. The assistant also suggests shopping together as
    a way to spend quality time and pick out special gifts. On the other hand, Assistant
    Bâ€™s response is repetitive and confusing. It focuses heavily on knitting a sweater
    with a Bible verse, which was not mentioned in the userâ€™s statement. The assistant
    also suggests knitting a sweater for the grandmotherâ€™s friends and for people
    from church or sports, which is not relevant to the userâ€™s statement. The assistantâ€™s
    suggestions for Bible verses are also not relevant to the userâ€™s statement. Therefore,
    Assistant Aâ€™s response is more helpful, relevant, and accurate. It provides a
    variety of gift ideas that align with the grandmotherâ€™s interests and suggests
    spending quality time together. Assistant Bâ€™s response is less helpful and relevant,
    as it focuses on irrelevant and repetitive suggestions. |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| åˆ¤æ–­ | åŠ©ç†Açš„å›åº”æ›´ç›¸å…³ä¸”å¯¹ç”¨æˆ·çš„é™ˆè¿°æ›´æœ‰å¸®åŠ©ã€‚åŠ©ç†å»ºè®®äº†ä¸å¥¶å¥¶å…´è¶£ç›¸ç¬¦çš„ä¸ªæ€§åŒ–ç¤¼ç‰©ï¼Œå¦‚æ‰‹å·¥ç¼–ç»‡çš„æ¯›è¡£ã€ä¸ªæ€§åŒ–å’–å•¡æ¯ã€ç¼–ç»‡å›¾æ¡ˆä¹¦å’ŒèŒ¶å£¶å¥—ã€‚åŠ©ç†è¿˜å»ºè®®ä¸€èµ·è´­ç‰©ï¼Œä»¥ä¾¿èŠ±æ—¶é—´æŒ‘é€‰ç‰¹åˆ«çš„ç¤¼ç‰©ã€‚å¦ä¸€æ–¹é¢ï¼ŒåŠ©ç†Bçš„å›åº”é‡å¤ä¸”ä»¤äººå›°æƒ‘ã€‚å®ƒè¿‡äºå…³æ³¨ç¼–ç»‡ä¸€ä»¶å¸¦æœ‰åœ£ç»ç»æ–‡çš„æ¯›è¡£ï¼Œè€Œè¿™åœ¨ç”¨æˆ·çš„é™ˆè¿°ä¸­æ²¡æœ‰æåŠã€‚åŠ©ç†è¿˜å»ºè®®ä¸ºå¥¶å¥¶çš„æœ‹å‹ä»¥åŠæ•™ä¼šæˆ–è¿åŠ¨ä¸­çš„äººç¼–ç»‡æ¯›è¡£ï¼Œè¿™ä¸ç”¨æˆ·çš„é™ˆè¿°æ— å…³ã€‚åŠ©ç†æå‡ºçš„åœ£ç»ç»æ–‡å»ºè®®ä¹Ÿä¸ç”¨æˆ·çš„é™ˆè¿°ä¸ç›¸å…³ã€‚å› æ­¤ï¼ŒåŠ©ç†Açš„å›åº”æ›´ä¸ºæœ‰å¸®åŠ©ã€ç›¸å…³ä¸”å‡†ç¡®ã€‚å®ƒæä¾›äº†ä¸å¥¶å¥¶å…´è¶£ç›¸ç¬¦çš„å¤šç§ç¤¼ç‰©åˆ›æ„ï¼Œå¹¶å»ºè®®ä¸€èµ·åº¦è¿‡ç¾å¥½æ—¶å…‰ã€‚åŠ©ç†Bçš„å›åº”åˆ™è¾ƒå°‘æœ‰å¸®åŠ©å’Œç›¸å…³ï¼Œå› ä¸ºå®ƒé›†ä¸­åœ¨æ— å…³ä¸”é‡å¤çš„å»ºè®®ä¸Šã€‚
    |'
- en: Table 5. Sample responses to a prompt from the Anthropic-HH test set. GPT-4
    chooses Mallows-$\phi$.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨5. æ¥è‡ªAnthropic-HHæµ‹è¯•é›†çš„æç¤ºæ ·æœ¬å›åº”ã€‚GPT-4é€‰æ‹©äº†Mallows-$\phi$ã€‚
- en: '| Prompt | She likes bingo, knitting, shopping, her grandkids, coffee. |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| æç¤º | å¥¹å–œæ¬¢å®¾æœæ¸¸æˆã€ç¼–ç»‡ã€è´­ç‰©ã€å¥¹çš„å­™å­å­™å¥³ã€å’–å•¡ã€‚ |'
- en: '| Mallows-$\phi$-DPO | Grandma loves receiving special gifts from family members,
    so an elegant, quality bingo or knitting set is a great idea. She also loves coffee,
    so a nice coffee table book with coffee-themed photos and a small mug is a nice
    gift. Bingo and knitting sets are well-loved and popular so sheâ€™ll love a set
    of cards and a board. A photo album is a great idea too, so she can look at pictures
    of her family. If you know what sort of games she likes, we could search the internet
    and find a bingo set or a board game sheâ€™d prefer. It also doesnâ€™t hurt to give
    her coffee while you make your gift, so you could leave a cup of hot coffee. |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| Mallows-$\phi$-DPO | å¥¶å¥¶å–œæ¬¢æ”¶åˆ°å®¶äººé€çš„ç‰¹åˆ«ç¤¼ç‰©ï¼Œå› æ­¤ä¸€ä¸ªä¼˜é›…é«˜è´¨é‡çš„å®¾æœæ¸¸æˆæˆ–ç¼–ç»‡å¥—ä»¶æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ä¸»æ„ã€‚å¥¹ä¹Ÿå–œæ¬¢å’–å•¡ï¼Œæ‰€ä»¥ä¸€æœ¬å’–å•¡ä¸»é¢˜çš„ç²¾ç¾å’–å•¡æ¡Œä¹¦å’Œä¸€ä¸ªå°æ¯å­æ˜¯ä¸é”™çš„ç¤¼ç‰©ã€‚å®¾æœæ¸¸æˆå’Œç¼–ç»‡å¥—ä»¶éƒ½éå¸¸å—æ¬¢è¿ï¼Œå¥¹ä¸€å®šä¼šå–œæ¬¢ä¸€å‰¯å¡ç‰‡å’Œä¸€ä¸ªæ¸¸æˆæ¿ã€‚ç›¸å†Œä¹Ÿæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ä¸»æ„ï¼Œå¥¹å¯ä»¥æŸ¥çœ‹å®¶åº­çš„ç…§ç‰‡ã€‚å¦‚æœä½ çŸ¥é“å¥¹å–œæ¬¢ä»€ä¹ˆæ¸¸æˆï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ç½‘ä¸Šå¯»æ‰¾å¥¹å¯èƒ½å–œæ¬¢çš„å®¾æœæ¸¸æˆæˆ–æ£‹ç›˜æ¸¸æˆã€‚åˆ¶ä½œç¤¼ç‰©æ—¶èµ é€å¥¹å’–å•¡ä¹Ÿä¸é”™ï¼Œä½ å¯ä»¥ç•™ä¸‹ä¸€æ¯çƒ­å’–å•¡ã€‚
    |'
- en: '| BT-DPO | Grandma loves receiving special gifts from family members, so an
    elegant, quality, personalized hand knit sweater is a great idea. She might enjoy
    a good coffee and a tasty meal, or a book and flowers. If you have a special skill,
    you could knit her a sweater for Christmas or even a sweater for her birthday.
    You can choose a different special occasion, like Valentineâ€™s Day. You could knit
    her a Valentineâ€™s gift like, a Valentineâ€™s ring or bracelet. Or you could make
    a holiday sweater and knit it for Christmas, so she can relax and unwind at the
    end of the year. If she has a lot of friends, you could make her a special hand
    knit sweater and knit it for all her friends to enjoy. If you know her from church
    or play sports, you could knit her a sports themed sweater. Or you could make
    a sweater with a Bible verse on the front, like the sweater Myron did for his
    grandmother. When you make the sweater, you make it all up, so you could even
    make a sweater with a Bible verse on the front, like the sweater that Myron did
    for his grandmother. To make this sweater, you could knit a sweater with the Bible
    verse, you could knit a sweater like your grandmaâ€™s favorite sweater, you could
    knit a sweater with the Bible verse on the front. So Myron made his grandmother
    a sweater with the Bible verse. You could make your grandma a sweater with the
    Bible verse. Grandma loves giving special gifts to family and friends, so I recommend
    a sweater with a Bible verse on the front. Or you could make her a sweater with
    your own special gift. Would you like me to help you with this? Letâ€™s see, you
    want a hand knit sweater, knit by a knitting machine, you want only one sweater
    and you want to knit it for your grandma, how about that? Letâ€™s say you want to
    knit up a sweater and hand it to her. Is that ok? Let me make a list of possibilities
    for your sweater, we can pick one or a few that might be special. Hereâ€™s a list
    of items that are the best match: Bible verse Glover Shawnee Grandma would like
    to know more about the Bible verse, so letâ€™s get specific. There is only one sweater
    that Grandma will prefer, you can pick one, letâ€™s see, could be the one with a
    Bible verse on the front or the one with Glover. So itâ€™s a hand knit sweater with
    a Bible verse on the front. Hereâ€™s a list of specific Bible verses that people
    would like Grandma to learn: â€œI shall tell my lord the king all the prosperity
    that I have. I shall say, I shall say to you of peace. I shall declare the good
    news to the poor. I shall declare the gospel of grace. Here is a list of specific
    Bible verses that people would like Grandma to learn: Let there be light! Let
    there be light! Let there be light! - - - Let there be light! Let [maximum generation
    length reached] |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| BT-DPO | å¥¶å¥¶å–œæ¬¢æ”¶åˆ°å®¶äººé€çš„ç‰¹åˆ«ç¤¼ç‰©ï¼Œå› æ­¤ä¸€ä»¶ä¼˜é›…ã€é«˜å“è´¨ã€ä¸ªæ€§åŒ–çš„æ‰‹å·¥ç¼–ç»‡æ¯›è¡£æ˜¯ä¸ªå¥½ä¸»æ„ã€‚å¥¹å¯èƒ½ä¼šå–œæ¬¢ä¸€æ¯å¥½å’–å•¡å’Œä¸€é¡¿ç¾é¤ï¼Œæˆ–è€…ä¸€æœ¬ä¹¦å’Œä¸€äº›èŠ±ã€‚å¦‚æœä½ æœ‰ç‰¹åˆ«çš„æŠ€èƒ½ï¼Œä½ å¯ä»¥åœ¨åœ£è¯èŠ‚ä¸ºå¥¹ç¼–ç»‡ä¸€ä»¶æ¯›è¡£ï¼Œç”šè‡³å¯ä»¥ä¸ºå¥¹çš„ç”Ÿæ—¥ç¼–ç»‡ä¸€ä»¶æ¯›è¡£ã€‚ä½ ä¹Ÿå¯ä»¥é€‰æ‹©å…¶ä»–ç‰¹åˆ«çš„èŠ‚æ—¥ï¼Œæ¯”å¦‚æƒ…äººèŠ‚ã€‚ä½ å¯ä»¥ç¼–ç»‡ä¸€ä»½æƒ…äººèŠ‚ç¤¼ç‰©ï¼Œæ¯”å¦‚æƒ…äººèŠ‚æˆ’æŒ‡æˆ–æ‰‹é“¾ã€‚æˆ–è€…ä½ å¯ä»¥åˆ¶ä½œä¸€ä»¶èŠ‚æ—¥æ¯›è¡£ï¼Œç¼–ç»‡æˆåœ£è¯èŠ‚çš„æ¯›è¡£ï¼Œè¿™æ ·å¥¹å¯ä»¥åœ¨å¹´åº•æ”¾æ¾ä¸€ä¸‹ã€‚å¦‚æœå¥¹æœ‰å¾ˆå¤šæœ‹å‹ï¼Œä½ å¯ä»¥ä¸ºå¥¹ç¼–ç»‡ä¸€ä»¶ç‰¹åˆ«çš„æ‰‹å·¥æ¯›è¡£ï¼Œç¼–ç»‡æˆå¥¹æ‰€æœ‰æœ‹å‹éƒ½å–œæ¬¢çš„æ ·å­ã€‚å¦‚æœä½ è®¤è¯†å¥¹æ¥è‡ªæ•™å ‚æˆ–è€…å‚åŠ ä½“è‚²æ´»åŠ¨ï¼Œä½ å¯ä»¥ä¸ºå¥¹ç¼–ç»‡ä¸€ä»¶è¿åŠ¨ä¸»é¢˜çš„æ¯›è¡£ã€‚æˆ–è€…ä½ å¯ä»¥åˆ¶ä½œä¸€ä»¶å‰é¢å°æœ‰åœ£ç»ç»æ–‡çš„æ¯›è¡£ï¼Œå°±åƒè¿ˆä¼¦ä¸ºä»–å¥¶å¥¶åšçš„é‚£ä»¶æ¯›è¡£ä¸€æ ·ã€‚å½“ä½ åˆ¶ä½œæ¯›è¡£æ—¶ï¼Œä½ å¯ä»¥å®Œå…¨è‡ªåˆ›è®¾è®¡ï¼Œå› æ­¤ä½ ç”šè‡³å¯ä»¥åˆ¶ä½œä¸€ä»¶å‰é¢å°æœ‰åœ£ç»ç»æ–‡çš„æ¯›è¡£ï¼Œå°±åƒè¿ˆä¼¦ä¸ºä»–å¥¶å¥¶åšçš„é‚£ä»¶æ¯›è¡£ä¸€æ ·ã€‚ä¸ºäº†åˆ¶ä½œè¿™ä»¶æ¯›è¡£ï¼Œä½ å¯ä»¥ç¼–ç»‡ä¸€ä»¶å¸¦æœ‰åœ£ç»ç»æ–‡çš„æ¯›è¡£ï¼Œæˆ–è€…ç¼–ç»‡ä¸€ä»¶åƒä½ å¥¶å¥¶æœ€å–œæ¬¢çš„æ¯›è¡£é‚£æ ·çš„æ¯›è¡£ï¼Œæˆ–è€…ç¼–ç»‡ä¸€ä»¶å‰é¢å¸¦æœ‰åœ£ç»ç»æ–‡çš„æ¯›è¡£ã€‚å› æ­¤ï¼Œè¿ˆä¼¦ä¸ºä»–çš„å¥¶å¥¶åšäº†ä¸€ä»¶å¸¦æœ‰åœ£ç»ç»æ–‡çš„æ¯›è¡£ã€‚ä½ ä¹Ÿå¯ä»¥ä¸ºä½ çš„å¥¶å¥¶ç¼–ç»‡ä¸€ä»¶å¸¦æœ‰åœ£ç»ç»æ–‡çš„æ¯›è¡£ã€‚å¥¶å¥¶å–œæ¬¢ç»™å®¶äººå’Œæœ‹å‹é€ç‰¹åˆ«çš„ç¤¼ç‰©ï¼Œå› æ­¤æˆ‘æ¨èä¸€ä»¶å‰é¢å¸¦æœ‰åœ£ç»ç»æ–‡çš„æ¯›è¡£ã€‚æˆ–è€…ä½ å¯ä»¥ä¸ºå¥¹åˆ¶ä½œä¸€ä»¶ä½ è‡ªå·±çš„ç‰¹åˆ«ç¤¼ç‰©æ¯›è¡£ã€‚ä½ å¸Œæœ›æˆ‘å¸®å¿™å—ï¼Ÿè®©æˆ‘ä»¬çœ‹çœ‹ï¼Œä½ æƒ³è¦ä¸€ä»¶æ‰‹å·¥ç¼–ç»‡çš„æ¯›è¡£ï¼Œé€šè¿‡ç¼–ç»‡æœºç¼–ç»‡ï¼Œä½ åªè¦ä¸€ä»¶æ¯›è¡£ï¼Œå¹¶ä¸”ä½ æƒ³è¦ä¸ºä½ çš„å¥¶å¥¶ç¼–ç»‡ï¼Œè¿™æ ·æ€ä¹ˆæ ·ï¼Ÿå‡è®¾ä½ æƒ³è¦ç¼–ç»‡ä¸€ä»¶æ¯›è¡£å¹¶äº²æ‰‹é€’ç»™å¥¹ã€‚è¿™æ ·å¯ä»¥å—ï¼Ÿè®©æˆ‘åˆ—ä¸€ä¸ªæ¯›è¡£çš„å¯èƒ½æ€§æ¸…å•ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‘é€‰ä¸€ä¸ªæˆ–å‡ ä¸ªå¯èƒ½ç‰¹åˆ«çš„é€‰æ‹©ã€‚ä»¥ä¸‹æ˜¯æœ€ä½³åŒ¹é…çš„é¡¹ç›®åˆ—è¡¨ï¼šåœ£ç»ç»æ–‡æ ¼æ´›å¼—è‚–å°¼å¥¶å¥¶æƒ³è¦äº†è§£æ›´å¤šçš„åœ£ç»ç»æ–‡ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬å…·ä½“ä¸€ä¸‹ã€‚åªæœ‰ä¸€ä»¶æ¯›è¡£æ˜¯å¥¶å¥¶ä¼šæ›´å–œæ¬¢çš„ï¼Œä½ å¯ä»¥æŒ‘é€‰ä¸€ä¸ªï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ï¼Œå¯èƒ½æ˜¯å‰é¢æœ‰åœ£ç»ç»æ–‡çš„é‚£ä»¶ï¼Œæˆ–è€…æ˜¯æœ‰æ ¼æ´›å¼—çš„é‚£ä»¶ã€‚æ‰€ä»¥æ˜¯ä¸€ä»¶å‰é¢æœ‰åœ£ç»ç»æ–‡çš„æ‰‹å·¥ç¼–ç»‡æ¯›è¡£ã€‚ä»¥ä¸‹æ˜¯äººä»¬å¸Œæœ›å¥¶å¥¶å­¦ä¹ çš„å…·ä½“åœ£ç»ç»æ–‡åˆ—è¡¨ï¼šâ€œæˆ‘å°†å‘Šè¯‰æˆ‘ä¸»ç‹æ‰€æœ‰æˆ‘æ‹¥æœ‰çš„ç¹è£ã€‚æˆ‘å°†è¯´ï¼Œæˆ‘å°†å¯¹ä½ è¯´å¹³å®‰ã€‚æˆ‘å°†å‘ç©·äººä¼ é€’å¥½æ¶ˆæ¯ã€‚æˆ‘å°†å®£è®²æ©å…¸çš„ç¦éŸ³ã€‚â€ä»¥ä¸‹æ˜¯äººä»¬å¸Œæœ›å¥¶å¥¶å­¦ä¹ çš„å…·ä½“åœ£ç»ç»æ–‡åˆ—è¡¨ï¼šè¦æœ‰å…‰ï¼è¦æœ‰å…‰ï¼è¦æœ‰å…‰ï¼
    - - - è¦æœ‰å…‰ï¼[æœ€å¤§ç”Ÿæˆé•¿åº¦å·²è¾¾åˆ°]'
- en: '| Judgment | Assistant Aâ€™s response is more relevant and helpful to the userâ€™s
    statement. The user mentioned that the person in question likes bingo, knitting,
    shopping, her grandkids, and coffee. Assistant A took all these interests into
    account and suggested appropriate gifts such as a bingo or knitting set, a coffee
    table book, a mug, and a photo album. On the other hand, Assistant B focused solely
    on knitting and suggested making a hand-knit sweater with a Bible verse on it.
    This suggestion is not only repetitive but also irrelevant to the userâ€™s statement
    as there was no mention of religious interests. Furthermore, Assistant Bâ€™s response
    is confusing and lacks coherence, making it less helpful to the user. Therefore,
    Assistant A provided a better response. |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| åˆ¤æ–­ | åŠ©æ‰‹ A çš„å›åº”å¯¹ç”¨æˆ·çš„é™ˆè¿°æ›´ç›¸å…³ä¸”æ›´æœ‰å¸®åŠ©ã€‚ç”¨æˆ·æåˆ°å¥¹å–œæ¬¢å®¾æœæ¸¸æˆã€ç¼–ç»‡ã€è´­ç‰©ã€å­™å­å­™å¥³å’Œå’–å•¡ã€‚åŠ©æ‰‹ A è€ƒè™‘äº†æ‰€æœ‰è¿™äº›å…´è¶£ï¼Œå¹¶å»ºè®®äº†åˆé€‚çš„ç¤¼ç‰©ï¼Œä¾‹å¦‚å®¾æœæ¸¸æˆæˆ–ç¼–ç»‡å¥—ä»¶ã€å’–å•¡æ¡Œä¹¦ã€é©¬å…‹æ¯å’Œç›¸å†Œã€‚å¦ä¸€æ–¹é¢ï¼ŒåŠ©æ‰‹
    B ä»…å…³æ³¨ç¼–ç»‡ï¼Œå¹¶å»ºè®®åˆ¶ä½œä¸€ä»¶ä¸Šé¢å°æœ‰åœ£ç»ç»æ–‡çš„æ‰‹å·¥ç¼–ç»‡æ¯›è¡£ã€‚è¿™ä¸ªå»ºè®®ä¸ä»…é‡å¤ï¼Œè€Œä¸”ä¸ç”¨æˆ·çš„é™ˆè¿°æ— å…³ï¼Œå› ä¸ºæ²¡æœ‰æåˆ°å®—æ•™å…´è¶£ã€‚æ­¤å¤–ï¼ŒåŠ©æ‰‹ B çš„å›åº”ä»¤äººå›°æƒ‘ä¸”ç¼ºä¹è¿è´¯æ€§ï¼Œå› æ­¤å¯¹ç”¨æˆ·çš„å¸®åŠ©è¾ƒå°‘ã€‚å› æ­¤ï¼ŒåŠ©æ‰‹
    A æä¾›äº†æ›´å¥½çš„å›åº”ã€‚ |'
- en: Table 6. Sample responses to a prompt from the Anthropic-HH test set. GPT-4
    chooses BT-DPO over Mallows-$\theta$.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 6. æ¥è‡ª Anthropic-HH æµ‹è¯•é›†çš„ç¤ºä¾‹å›åº”ã€‚GPT-4 é€‰æ‹©äº† BT-DPO è€Œé Mallows-$\theta$ã€‚
- en: '| Prompt | She likes bingo, knitting, shopping, her grandkids, coffee. |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| æç¤º | å¥¹å–œæ¬¢å®¾æœæ¸¸æˆã€ç¼–ç»‡ã€è´­ç‰©ã€å­™å­å­™å¥³ã€å’–å•¡ã€‚ |'
- en: '| Mallows-$\theta$-DPO | This is a difficult question! Let me start by saying
    it is true that an elderly person can often enjoy some or all of these things,
    and they might enjoy doing them for many reasons. However, it can sometimes be
    difficult for them to enjoy them for very long, since being able to do them is
    dependent on so many things that can be difficult to take for granted. For example,
    a person canâ€™t get used to the fact that their body is breaking down, or that
    they might not be able to do something at high level for the next few years, so
    they might get bored with some of the things they used to like. However, it is
    still important to have a range of things that they can do and enjoy. If you could
    suggest a Christmas gift that would suit her interests, I could look at some ideas.
    The best gift would have some sort of physical thing that she could use to do,
    like knitting or bingo. Maybe an app would be more like a book that she could
    read? You can help her find games and things like that to play, so it is not just
    about reading? |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| Mallows-$\theta$-DPO | è¿™æ˜¯ä¸€ä¸ªéš¾é¢˜ï¼è®©æˆ‘é¦–å…ˆè¯´ï¼Œçš„ç¡®è€å¹´äººé€šå¸¸ä¼šå–œæ¬¢è¿™äº›ä¸œè¥¿ä¸­çš„ä¸€äº›æˆ–å…¨éƒ¨ï¼Œä»–ä»¬å¯èƒ½å› å¤šç§åŸå› è€Œå–œæ¬¢åšè¿™äº›äº‹ã€‚ç„¶è€Œï¼Œæœ‰æ—¶å€™ä»–ä»¬å¯èƒ½æ— æ³•é•¿æ—¶é—´äº«å—è¿™äº›äº‹ï¼Œå› ä¸ºèƒ½å¤Ÿåšè¿™äº›äº‹ä¾èµ–äºè®¸å¤šéš¾ä»¥ç†æ‰€å½“ç„¶çš„å› ç´ ã€‚ä¾‹å¦‚ï¼Œäººä»¬å¯èƒ½æ— æ³•é€‚åº”èº«ä½“é€æ¸è¡°é€€çš„äº‹å®ï¼Œæˆ–è€…ä»–ä»¬å¯èƒ½åœ¨æœªæ¥å‡ å¹´é‡Œæ— æ³•ä»¥é«˜æ°´å¹³å®ŒæˆæŸäº›äº‹æƒ…ï¼Œå› æ­¤ä»–ä»¬å¯èƒ½ä¼šå¯¹æ›¾ç»å–œæ¬¢çš„äº‹æƒ…æ„Ÿåˆ°åŒå€¦ã€‚ç„¶è€Œï¼Œä»ç„¶é‡è¦çš„æ˜¯æä¾›ä¸€ç³»åˆ—ä»–ä»¬å¯ä»¥åšå’Œäº«å—çš„æ´»åŠ¨ã€‚å¦‚æœä½ èƒ½å»ºè®®ä¸€ä¸ªé€‚åˆå¥¹å…´è¶£çš„åœ£è¯ç¤¼ç‰©ï¼Œæˆ‘å¯ä»¥è€ƒè™‘ä¸€äº›æƒ³æ³•ã€‚æœ€ä½³ç¤¼ç‰©åº”è¯¥åŒ…å«ä¸€äº›å¥¹å¯ä»¥ä½¿ç”¨çš„å®é™…ç‰©å“ï¼Œæ¯”å¦‚ç¼–ç»‡æˆ–å®¾æœæ¸¸æˆã€‚ä¹Ÿè®¸ä¸€ä¸ªåº”ç”¨ç¨‹åºæ›´åƒæ˜¯ä¸€æœ¬å¥¹å¯ä»¥é˜…è¯»çš„ä¹¦ï¼Ÿä½ å¯ä»¥å¸®åŠ©å¥¹æ‰¾åˆ°æ¸¸æˆå’Œç±»ä¼¼çš„ä¸œè¥¿æ¥ç©ï¼Œè¿™æ ·å°±ä¸ä»…ä»…æ˜¯é˜…è¯»äº†ï¼Ÿ'
- en: '| BT-DPO | Well, Iâ€™d say a good gift might be a subscription to a magazine
    or an online magazine, or an online bingo game or an online knitting community.
    Or you could give her a subscription to a physical bingo card, or some yarn, and
    a pattern for a knitting project. Or you could buy her coffee, and put her name
    on her bingo card, or a knitting project. Or you could give her a book about bingo,
    or bingo, or a knitting project. Or you could just make her a present, like a
    pottery bowl. |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| BT-DPO | å¥½å§ï¼Œæˆ‘ä¼šè¯´ä¸€ä¸ªå¥½çš„ç¤¼ç‰©å¯èƒ½æ˜¯è®¢é˜…ä¸€æœ¬æ‚å¿—æˆ–åœ¨çº¿æ‚å¿—ï¼Œæˆ–è€…åœ¨çº¿å®¾æœæ¸¸æˆæˆ–åœ¨çº¿ç¼–ç»‡ç¤¾åŒºã€‚æˆ–è€…ä½ å¯ä»¥é€å¥¹ä¸€ä¸ªå®ä½“å®¾æœå¡çš„è®¢é˜…ï¼Œæˆ–ä¸€äº›çº±çº¿ä»¥åŠä¸€ä¸ªç¼–ç»‡é¡¹ç›®çš„å›¾æ¡ˆã€‚æˆ–è€…ä½ å¯ä»¥ä¹°äº›å’–å•¡ç»™å¥¹ï¼Œå¹¶åœ¨å¥¹çš„å®¾æœå¡æˆ–ç¼–ç»‡é¡¹ç›®ä¸Šå†™ä¸Šå¥¹çš„åå­—ã€‚æˆ–è€…ä½ å¯ä»¥é€å¥¹ä¸€æœ¬å…³äºå®¾æœçš„ä¹¦ï¼Œæˆ–è€…å®¾æœæ¸¸æˆï¼Œæˆ–è€…ä¸€ä¸ªç¼–ç»‡é¡¹ç›®ã€‚æˆ–è€…ä½ å¯ä»¥ç»™å¥¹åšä¸€ä¸ªç¤¼ç‰©ï¼Œæ¯”å¦‚ä¸€ä¸ªé™¶è‰ºç¢—ã€‚
    |'
- en: '| Judgment | Assistant Bâ€™s response is more relevant and helpful to the userâ€™s
    question. Although the userâ€™s question is not explicitly stated, it can be inferred
    that the user is looking for gift suggestions for a person who enjoys bingo, knitting,
    shopping, her grandkids, and coffee. Assistant B provides a variety of gift suggestions
    that align with the personâ€™s interests, such as a subscription to an online magazine,
    a physical bingo card, yarn for knitting, coffee, and a book about bingo. On the
    other hand, Assistant Aâ€™s response is less focused and does not provide as many
    concrete gift suggestions. Therefore, Assistant Bâ€™s response is more helpful and
    relevant. On the other hand, Assistant Aâ€™s response is less focused and does not
    provide as many concrete gift suggestions. Therefore, Assistant Bâ€™s response is
    more helpful and relevant. |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| åˆ¤æ–­ | åŠ©ç† B çš„å›åº”æ›´è´´åˆ‡å¹¶ä¸”å¯¹ç”¨æˆ·çš„é—®é¢˜æ›´æœ‰å¸®åŠ©ã€‚è™½ç„¶ç”¨æˆ·çš„é—®é¢˜æ²¡æœ‰æ˜ç¡®é™ˆè¿°ï¼Œä½†å¯ä»¥æ¨æ–­å‡ºç”¨æˆ·æ­£åœ¨å¯»æ‰¾é€‚åˆå–œæ¬¢å®¾æœæ¸¸æˆã€ç¼–ç»‡ã€è´­ç‰©ã€å­™å­å­™å¥³å’Œå’–å•¡çš„äººç¾¤çš„ç¤¼ç‰©å»ºè®®ã€‚åŠ©ç†
    B æä¾›äº†å¤šç§ç¬¦åˆè¯¥äººå£«å…´è¶£çš„ç¤¼ç‰©å»ºè®®ï¼Œå¦‚åœ¨çº¿æ‚å¿—è®¢é˜…ã€å®ç‰©å®¾æœå¡ã€ç¼–ç»‡ç”¨æ¯›çº¿ã€å’–å•¡å’Œæœ‰å…³å®¾æœçš„ä¹¦ç±ã€‚å¦ä¸€æ–¹é¢ï¼ŒåŠ©ç† A çš„å›åº”è¾ƒä¸ºåˆ†æ•£ï¼Œå¹¶æ²¡æœ‰æä¾›é‚£ä¹ˆå¤šå…·ä½“çš„ç¤¼ç‰©å»ºè®®ã€‚å› æ­¤ï¼ŒåŠ©ç†
    B çš„å›åº”æ›´ä¸ºæœ‰ç”¨å’Œç›¸å…³ã€‚å¦ä¸€æ–¹é¢ï¼ŒåŠ©ç† A çš„å›åº”è¾ƒä¸ºåˆ†æ•£ï¼Œå¹¶æ²¡æœ‰æä¾›é‚£ä¹ˆå¤šå…·ä½“çš„ç¤¼ç‰©å»ºè®®ã€‚å› æ­¤ï¼ŒåŠ©ç† B çš„å›åº”æ›´ä¸ºæœ‰ç”¨å’Œç›¸å…³ã€‚ |'
- en: Table 7. Sample responses to a prompt from the Anthropic-HH test set. GPT-4
    chooses BT-DPO over Mallows-$\phi$.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 7. æ¥è‡ª Anthropic-HH æµ‹è¯•é›†çš„æç¤ºç¤ºä¾‹å“åº”ã€‚GPT-4 é€‰æ‹©äº† BT-DPO è€Œé Mallows-$\phi$ã€‚
- en: '| Prompt | She likes bingo, knitting, shopping, her grandkids, coffee. |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| æç¤º | å¥¹å–œæ¬¢å®¾æœæ¸¸æˆã€ç¼–ç»‡ã€è´­ç‰©ã€å­™å­å­™å¥³ã€å’–å•¡ã€‚ |'
- en: '| Mallows-$\phi$-DPO | This is a good list of categories to check when buying
    for an elderly person, but there are many more options. You could check out whether
    she enjoys gardening and flowers, or if she likes baking and cooking. If you have
    a special skill you can try to learn more about her hobby or skills, and make
    her a special item that she can use to improve her skills. For example, are your
    grandma into sports? Does she like to play a musical instrument? If so, you can
    make a special instrument for her, and the instrument can be a tool to help her
    develop her passion for music. Similarly, if she enjoys baking, you could make
    something for her to make, to use in her kitchen. There are all kinds of ways
    you could find out more about her interests, and make them something sheâ€™ll enjoy.
    |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| Mallows-$\phi$-DPO | è¿™æ˜¯ä¸€ä¸ªåœ¨ä¸ºå¹´é•¿è€…è´­ä¹°ç¤¼ç‰©æ—¶æ£€æŸ¥çš„å¥½ç±»åˆ«åˆ—è¡¨ï¼Œä½†è¿˜æœ‰è®¸å¤šå…¶ä»–é€‰é¡¹ã€‚ä½ å¯ä»¥çœ‹çœ‹å¥¹æ˜¯å¦å–œæ¬¢å›­è‰ºå’ŒèŠ±å‰ï¼Œæˆ–è€…å¥¹æ˜¯å¦å–œæ¬¢çƒ˜ç„™å’Œçƒ¹é¥ªã€‚å¦‚æœä½ æœ‰ç‰¹åˆ«çš„æŠ€èƒ½ï¼Œä½ å¯ä»¥å°è¯•äº†è§£å¥¹çš„çˆ±å¥½æˆ–æŠ€èƒ½ï¼Œå¹¶åˆ¶ä½œä¸€ä¸ªç‰¹åˆ«çš„ç‰©å“ï¼Œå¸®åŠ©å¥¹æé«˜æŠ€èƒ½ã€‚ä¾‹å¦‚ï¼Œä½ çš„å¥¶å¥¶å–œæ¬¢è¿åŠ¨å—ï¼Ÿå¥¹å–œæ¬¢æ¼”å¥ä¹å™¨å—ï¼Ÿå¦‚æœæ˜¯è¿™æ ·ï¼Œä½ å¯ä»¥ä¸ºå¥¹åˆ¶ä½œä¸€ä¸ªç‰¹åˆ«çš„ä¹å™¨ï¼Œè¿™ä¸ªä¹å™¨å¯ä»¥å¸®åŠ©å¥¹å‘å±•å¯¹éŸ³ä¹çš„çƒ­æƒ…ã€‚ç±»ä¼¼åœ°ï¼Œå¦‚æœå¥¹å–œæ¬¢çƒ˜ç„™ï¼Œä½ å¯ä»¥ä¸ºå¥¹åˆ¶ä½œä¸€äº›å¨æˆ¿ç”¨å…·ã€‚ä½ å¯ä»¥é€šè¿‡å„ç§æ–¹å¼äº†è§£å¥¹çš„å…´è¶£ï¼Œå¹¶åˆ¶ä½œä¸€äº›å¥¹ä¼šå–œæ¬¢çš„ä¸œè¥¿ã€‚
    |'
- en: '| BT-DPO | Well, Iâ€™d say a good gift might be a subscription to a magazine
    or an online magazine, or an online bingo game or an online knitting community.
    Or you could give her a subscription to a physical bingo card, or some yarn, and
    a pattern for a knitting project. Or you could buy her coffee, and put her name
    on her bingo card, or a knitting project. Or you could give her a book about bingo,
    or bingo, or a knitting project. Or you could just make her a present, like a
    pottery bowl. |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| BT-DPO | å—¯ï¼Œæˆ‘è§‰å¾—ä¸€ä¸ªå¥½çš„ç¤¼ç‰©å¯èƒ½æ˜¯æ‚å¿—æˆ–åœ¨çº¿æ‚å¿—çš„è®¢é˜…ï¼Œæˆ–è€…åœ¨çº¿å®¾æœæ¸¸æˆæˆ–åœ¨çº¿ç¼–ç»‡ç¤¾åŒºã€‚æˆ–è€…ä½ å¯ä»¥ç»™å¥¹ä¸€ä¸ªå®ç‰©å®¾æœå¡çš„è®¢é˜…ï¼Œæˆ–è€…ä¸€äº›æ¯›çº¿å’Œç¼–ç»‡é¡¹ç›®çš„å›¾æ¡ˆã€‚æˆ–è€…ä½ å¯ä»¥ä¹°å’–å•¡ï¼Œç»™å¥¹çš„å®¾æœå¡ä¸Šå†™ä¸Šå¥¹çš„åå­—ï¼Œæˆ–è€…ç¼–ç»‡é¡¹ç›®ã€‚æˆ–è€…ä½ å¯ä»¥ç»™å¥¹ä¸€æœ¬å…³äºå®¾æœçš„ä¹¦ï¼Œæˆ–è€…å®¾æœæ¸¸æˆï¼Œæˆ–è€…ä¸€ä¸ªç¼–ç»‡é¡¹ç›®ã€‚æˆ–è€…ä½ å¯ä»¥ç»™å¥¹åšä¸€ä¸ªç¤¼ç‰©ï¼Œæ¯”å¦‚ä¸€ä¸ªé™¶ç“·ç¢—ã€‚
    |'
- en: '| Judgment | Assistant Bâ€™s response is more relevant and helpful to the userâ€™s
    question. The user didnâ€™t ask a question, but it seems like theyâ€™re looking for
    gift ideas for a woman who likes bingo, knitting, shopping, her grandkids, and
    coffee. Assistant B provides specific gift ideas that align with the womanâ€™s interests,
    such as a subscription to an online bingo game or knitting community, a physical
    bingo card, yarn and a pattern for a knitting project, coffee, or a book about
    bingo or knitting. On the other hand, Assistant A suggests finding out more about
    the womanâ€™s interests, which doesnâ€™t directly address the userâ€™s implied question.
    Therefore, Assistant Bâ€™s response is more helpful and relevant. |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| åˆ¤æ–­ | åŠ©æ‰‹ B çš„å›åº”å¯¹ç”¨æˆ·çš„é—®é¢˜æ›´ä¸ºç›¸å…³å’Œæœ‰å¸®åŠ©ã€‚ç”¨æˆ·æ²¡æœ‰æ˜ç¡®æé—®ï¼Œä½†ä¼¼ä¹åœ¨å¯»æ‰¾é€‚åˆå–œæ¬¢å®¾æœæ¸¸æˆã€ç¼–ç»‡ã€è´­ç‰©ã€å­™å­å­™å¥³å’Œå’–å•¡çš„å¥³æ€§çš„ç¤¼ç‰©åˆ›æ„ã€‚åŠ©æ‰‹
    B æä¾›äº†ä¸å¥³æ€§å…´è¶£ç›¸ç¬¦çš„å…·ä½“ç¤¼ç‰©åˆ›æ„ï¼Œå¦‚åœ¨çº¿å®¾æœæ¸¸æˆæˆ–ç¼–ç»‡ç¤¾åŒºçš„è®¢é˜…ã€çº¸è´¨å®¾æœå¡ã€ç¼–ç»‡é¡¹ç›®æ‰€éœ€çš„çº±çº¿å’Œå›¾æ¡ˆã€å’–å•¡æˆ–æœ‰å…³å®¾æœæ¸¸æˆæˆ–ç¼–ç»‡çš„ä¹¦ç±ã€‚å¦ä¸€æ–¹é¢ï¼ŒåŠ©æ‰‹
    A å»ºè®®è¿›ä¸€æ­¥äº†è§£å¥³æ€§çš„å…´è¶£ï¼Œè¿™å¹¶æ²¡æœ‰ç›´æ¥è§£å†³ç”¨æˆ·çš„éšå«é—®é¢˜ã€‚å› æ­¤ï¼ŒåŠ©æ‰‹ B çš„å›åº”æ›´ä¸ºæœ‰ç”¨å’Œç›¸å…³ã€‚ |'
- en: Table 8. Sample responses to a prompt from the test set of the Stanford Human
    Preferences Dataset. GPT-4 chooses Mallows-$\theta$ on Anthropic-HH dataset.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 8. æ¥è‡ªæ–¯å¦ç¦äººç±»åå¥½æ•°æ®é›†æµ‹è¯•é›†çš„ç¤ºä¾‹å›åº”ã€‚GPT-4 åœ¨ Anthropic-HH æ•°æ®é›†ä¸­é€‰æ‹©äº† Mallows-$\theta$ã€‚
- en: '| Prompt | I donâ€™t want to be an engineer, I like physics. Hi there, im currently
    in high school and have some financial problems in the family, my parents are
    not satisfied with me doing physics (i mean get a degree and pursue higher studies
    in) as im also considering doing engineering as it has become a need. But i donâ€™t
    want to leave physics, i love atomic physics and cosmology so much. I want to
    ask here, can i continue my passion for physics while Iâ€™m doing engineering (IT)
    , will there be time for my passion. Or else is there any scope to follow my passion
    and take out my family of this? |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| æé—® | æˆ‘ä¸æƒ³æˆä¸ºå·¥ç¨‹å¸ˆï¼Œæˆ‘å–œæ¬¢ç‰©ç†ã€‚ä½ å¥½ï¼Œæˆ‘ç°åœ¨åœ¨é«˜ä¸­ï¼Œå®¶é‡Œæœ‰ä¸€äº›ç»æµé—®é¢˜ï¼Œæˆ‘çš„çˆ¶æ¯å¯¹æˆ‘å­¦ä¹ ç‰©ç†ï¼ˆå³è·å¾—å­¦ä½å¹¶è¿½æ±‚æ›´é«˜çš„å­¦ä¸šï¼‰ä¸æ»¡æ„ï¼Œå› ä¸ºæˆ‘ä¹Ÿåœ¨è€ƒè™‘åšå·¥ç¨‹å­¦ï¼Œå› ä¸ºè¿™å·²ç»æˆä¸ºä¸€ç§éœ€è¦ã€‚ä½†æˆ‘ä¸æƒ³ç¦»å¼€ç‰©ç†ï¼Œæˆ‘éå¸¸çƒ­çˆ±åŸå­ç‰©ç†å’Œå®‡å®™å­¦ã€‚æˆ‘æƒ³åœ¨è¿™é‡Œé—®ä¸€ä¸‹ï¼Œåœ¨æˆ‘å­¦ä¹ å·¥ç¨‹ï¼ˆITï¼‰çš„åŒæ—¶ï¼Œæˆ‘æ˜¯å¦å¯ä»¥ç»§ç»­è¿½æ±‚å¯¹ç‰©ç†çš„çƒ­æƒ…ï¼Œä¼šæœ‰æ—¶é—´æŠ•å…¥åˆ°æˆ‘çš„çƒ­æƒ…ä¸­å—ï¼Ÿæˆ–è€…è¿˜æœ‰æ²¡æœ‰å…¶ä»–é€”å¾„è®©æˆ‘è¿½æ±‚æˆ‘çš„çƒ­æƒ…å¹¶æ”¹å–„å®¶é‡Œçš„ç»æµçŠ¶å†µï¼Ÿ
    |'
- en: '| Mallows-$\theta$-DPO | There is a way you can pursue your interest in physics
    while continuing to do engineering! Letâ€™s take a step back and understand more
    about what motivates you to study physics, what are you learning that you enjoy,
    and what career would you like to go into? Itâ€™s quite possible that you could
    pursue your interest in physics even while working with technologies like computers
    or electronics and electronics, but these fields donâ€™t always allow for the kind
    of hands-on work or the level of interactivity that you are looking for in physics
    and astrophysics. Hereâ€™s some feedback from my peers who have made the transition:
    "I initially studied physics in school to be a medical physicist. When I realized
    I only wanted to pursue a career in physics, I worked full-time in the physics
    field and went to graduate school in another field, to focus on physics." "I studied
    Physics A in university, then switched to engineering to do physics B. Now I want
    to come back to physics but take a break to have more time with my kids. I love
    Physics!" "For me, physics is primarily about learning concepts and techniques
    from a particular area. After taking my first physics exam in university, I fell
    in love with the process and started doing physics outside of school, with friends
    and family and working." You can do these things. So maybe donâ€™t let your family
    hold you back from continuing your passion. You could take courses in basic quantum
    mechanics and relativity and cosmology to learn about the universe and why itâ€™s
    the way it is. Just think about what you want to do and how you could combine
    physics with your interests! |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| Mallows-$\theta$-DPO | æœ‰ä¸€ç§æ–¹æ³•å¯ä»¥è®©ä½ åœ¨ç»§ç»­ä»äº‹å·¥ç¨‹å·¥ä½œæ—¶è¿½æ±‚ä½ å¯¹ç‰©ç†çš„å…´è¶£ï¼è®©æˆ‘ä»¬é€€ä¸€æ­¥ï¼Œæ›´å¤šåœ°äº†è§£æ˜¯ä»€ä¹ˆæ¿€åŠ±ä½ å­¦ä¹ ç‰©ç†ï¼Œä½ å–œæ¬¢å­¦ä¹ å“ªäº›å†…å®¹ï¼Œä»¥åŠä½ å¸Œæœ›ä»äº‹ä»€ä¹ˆèŒä¸šï¼Ÿå³ä½¿åœ¨ä½¿ç”¨è®¡ç®—æœºæˆ–ç”µå­æŠ€æœ¯çš„é¢†åŸŸï¼Œä½ ä¹Ÿæœ‰å¯èƒ½ç»§ç»­è¿½æ±‚å¯¹ç‰©ç†çš„å…´è¶£ï¼Œä½†è¿™äº›é¢†åŸŸå¹¶ä¸æ€»æ˜¯æä¾›ä½ åœ¨ç‰©ç†å’Œå¤©ä½“ç‰©ç†å­¦ä¸­å¯»æ‰¾çš„é‚£ç§åŠ¨æ‰‹æ“ä½œæˆ–äº’åŠ¨æ°´å¹³ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å·²ç»è½¬è¡Œçš„åŒè¡Œçš„åé¦ˆï¼š
    â€œæˆ‘æœ€åˆåœ¨å­¦æ ¡å­¦ä¹ ç‰©ç†æ˜¯ä¸ºäº†æˆä¸ºä¸€ååŒ»å­¦ç‰©ç†å­¦å®¶ã€‚å½“æˆ‘æ„è¯†åˆ°æˆ‘åªæƒ³ä»äº‹ç‰©ç†é¢†åŸŸçš„èŒä¸šæ—¶ï¼Œæˆ‘åœ¨ç‰©ç†é¢†åŸŸå…¨èŒå·¥ä½œï¼Œå¹¶åœ¨å¦ä¸€ä¸ªé¢†åŸŸä¸Šç ”ç©¶ç”Ÿï¼Œä»¥ä¸“æ³¨äºç‰©ç†å­¦ã€‚â€ â€œæˆ‘åœ¨å¤§å­¦å­¦ä¹ äº†ç‰©ç†Aï¼Œç„¶åè½¬åˆ°å·¥ç¨‹å­¦å»å­¦ä¹ ç‰©ç†Bã€‚ç°åœ¨æˆ‘æƒ³å›åˆ°ç‰©ç†é¢†åŸŸï¼Œä½†éœ€è¦ä¼‘æ¯ä¸€ä¸‹ï¼Œä»¥ä¾¿æœ‰æ›´å¤šæ—¶é—´é™ªä¼´æˆ‘çš„å­©å­ã€‚æˆ‘çˆ±ç‰©ç†ï¼â€
    â€œå¯¹æˆ‘æ¥è¯´ï¼Œç‰©ç†ä¸»è¦æ˜¯å­¦ä¹ ç‰¹å®šé¢†åŸŸçš„æ¦‚å¿µå’ŒæŠ€æœ¯ã€‚åœ¨å¤§å­¦å‚åŠ äº†ç¬¬ä¸€æ¬¡ç‰©ç†è€ƒè¯•åï¼Œæˆ‘çˆ±ä¸Šäº†è¿™ä¸ªè¿‡ç¨‹ï¼Œå¹¶å¼€å§‹åœ¨å­¦æ ¡å¤–ä¸æœ‹å‹ã€å®¶äººä¸€èµ·åšç‰©ç†ï¼Œå¹¶å·¥ä½œã€‚â€ ä½ å¯ä»¥åšè¿™äº›äº‹æƒ…ã€‚æ‰€ä»¥ï¼Œä¹Ÿè®¸ä¸è¦è®©ä½ çš„å®¶åº­é˜»ç¢ä½ ç»§ç»­è¿½æ±‚ä½ çš„çƒ­æƒ…ã€‚ä½ å¯ä»¥å­¦ä¹ åŸºæœ¬çš„é‡å­åŠ›å­¦ã€ç›¸å¯¹è®ºå’Œå®‡å®™å­¦è¯¾ç¨‹ï¼Œäº†è§£å®‡å®™åŠå…¶å­˜åœ¨çš„åŸå› ã€‚åªéœ€è€ƒè™‘ä¸€ä¸‹ä½ æƒ³åšä»€ä¹ˆï¼Œä»¥åŠä½ å¦‚ä½•å°†ç‰©ç†ä¸è‡ªå·±çš„å…´è¶£ç»“åˆèµ·æ¥ï¼'
- en: '| BT-DPO | Yes, you can definitely do both physics and engineering. Many people
    do both. At the end of the day itâ€™s about what you like to do with your life,
    and most people find that their passion and work is what they want to do. Itâ€™s
    a great thing, if youâ€™re interested in physics, to be able to pursue it while
    doing something else. Iâ€™m always happy to be a resource and answer any specific
    questions you might have if you donâ€™t yet know how to work it out. Hereâ€™s some
    things to consider: -If you love physics you should start out with an introductory
    course in physics, this is essential for the study of physics. You might go on
    to physics for a year or two, then try physics and engineering for a later year
    or two. But it is also really important to keep coming back to physics - itâ€™s
    like a food for the mind - so if you canâ€™t make it all the way to advanced physics,
    eventually you should aim to do things in the field of physics or engineering
    that you are interested in. -The second thing to consider is that this does take
    up a lot of time - and a lot of money - but we should expect that. I wish I could
    give you some free advice here but I am not a full-time professor and there are
    lots of people with the same questions. You might try asking around at your school
    and perhaps someone can help you find a mentor who can give you a little more
    advice. Just keep in mind that the time you devote to a certain thing - for many
    young people - is mostly because of the pressure of other people and their expectations
    around them. So if there is the feeling that you aren [maximum generation length
    reached] |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| BT-DPO | æ˜¯çš„ï¼Œä½ ç»å¯¹å¯ä»¥åŒæ—¶åšç‰©ç†å’Œå·¥ç¨‹å­¦ã€‚å¾ˆå¤šäººéƒ½è¿™æ ·åšã€‚æœ€ç»ˆè¿™å–å†³äºä½ å–œæ¬¢ç”¨ä»€ä¹ˆæ¥åº¦è¿‡ç”Ÿæ´»ï¼Œå¤§å¤šæ•°äººå‘ç°ä»–ä»¬çš„çƒ­æƒ…å’Œå·¥ä½œæ˜¯ä»–ä»¬æƒ³åšçš„äº‹ã€‚å¦‚æœä½ å¯¹ç‰©ç†æ„Ÿå…´è¶£ï¼Œèƒ½åœ¨åšå…¶ä»–äº‹æƒ…çš„åŒæ—¶è¿½æ±‚ç‰©ç†å­¦æ˜¯ä¸€ä»¶å¾ˆæ£’çš„äº‹ã€‚å¦‚æœä½ ä¸ç¡®å®šå¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘å¾ˆä¹æ„ä½œä¸ºèµ„æºå›ç­”ä½ å¯èƒ½æœ‰çš„ä»»ä½•å…·ä½“é—®é¢˜ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›éœ€è¦è€ƒè™‘çš„äº‹é¡¹ï¼š-
    å¦‚æœä½ å–œæ¬¢ç‰©ç†å­¦ï¼Œä½ åº”è¯¥ä»ç‰©ç†å­¦çš„å…¥é—¨è¯¾ç¨‹å¼€å§‹ï¼Œè¿™å¯¹äºç‰©ç†å­¦çš„å­¦ä¹ è‡³å…³é‡è¦ã€‚ä½ å¯ä»¥å­¦ä¸€ä¸¤å¹´çš„ç‰©ç†ï¼Œç„¶åå°è¯•åœ¨ä»¥åçš„æŸäº›å¹´é‡ŒåŒæ—¶å­¦ä¹ ç‰©ç†å’Œå·¥ç¨‹ã€‚ä½†åŒæ ·é‡è¦çš„æ˜¯è¦ä¸æ–­å›åˆ°ç‰©ç†å­¦ä¸Šâ€”â€”è¿™å°±åƒæ˜¯æ€æƒ³çš„é£Ÿç²®â€”â€”æ‰€ä»¥å³ä½¿ä½ ä¸èƒ½å®Œæˆé«˜çº§ç‰©ç†è¯¾ç¨‹ï¼Œæœ€ç»ˆä½ ä¹Ÿåº”è¯¥è‡´åŠ›äºç‰©ç†å­¦æˆ–å·¥ç¨‹å­¦é¢†åŸŸçš„æ„Ÿå…´è¶£çš„äº‹æƒ…ã€‚-
    éœ€è¦è€ƒè™‘çš„ç¬¬äºŒç‚¹æ˜¯ï¼Œè¿™ç¡®å®éœ€è¦èŠ±è´¹å¤§é‡æ—¶é—´å’Œé‡‘é’±â€”â€”ä½†æˆ‘ä»¬åº”è¯¥é¢„æœŸè¿™äº›ã€‚æˆ‘å¸Œæœ›èƒ½ç»™ä½ ä¸€äº›å…è´¹çš„å»ºè®®ï¼Œä½†æˆ‘ä¸æ˜¯å…¨èŒæ•™æˆï¼Œè¿˜æœ‰å¾ˆå¤šäººæœ‰ç›¸åŒçš„é—®é¢˜ã€‚ä½ å¯ä»¥è¯•ç€åœ¨å­¦æ ¡é‡Œè¯¢é—®ï¼Œä¹Ÿè®¸æœ‰äººå¯ä»¥å¸®åŠ©ä½ æ‰¾åˆ°ä¸€ä¸ªèƒ½ç»™ä½ æ›´å¤šå»ºè®®çš„å¯¼å¸ˆã€‚åªè¦è®°ä½ï¼Œä½ æŠ•å…¥åœ¨æŸä»¶äº‹æƒ…ä¸Šçš„æ—¶é—´â€”â€”å¯¹å¾ˆå¤šå¹´è½»äººæ¥è¯´â€”â€”å¤§å¤šæ˜¯å› ä¸ºåˆ«äººå¯¹ä»–ä»¬çš„æœŸæœ›ã€‚æ‰€ä»¥å¦‚æœæ„Ÿè§‰åˆ°ä½ å¹¶ä¸å®Œå…¨å–œæ¬¢è¿™ä»¶äº‹...'
- en: '| Judgment | Both Assistant A and Assistant B provide helpful and encouraging
    responses to the userâ€™s question. They both suggest that it is possible to pursue
    a passion for physics while studying engineering. However, Assistant Aâ€™s response
    is more detailed and provides more practical advice. It includes feedback from
    people who have made the transition from physics to engineering, which could be
    very helpful for the user. Assistant Bâ€™s response, while supportive, is less specific
    and ends abruptly, leaving the user without a complete answer. Therefore, Assistant
    Aâ€™s response is more helpful and complete. |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| åˆ¤æ–­ | åŠ©æ‰‹ A å’ŒåŠ©æ‰‹ B éƒ½å¯¹ç”¨æˆ·çš„é—®é¢˜æä¾›äº†æœ‰å¸®åŠ©å’Œé¼“åŠ±çš„å›åº”ã€‚ä»–ä»¬éƒ½å»ºè®®åœ¨å­¦ä¹ å·¥ç¨‹çš„åŒæ—¶è¿½æ±‚ç‰©ç†å­¦çš„çƒ­æƒ…æ˜¯å¯èƒ½çš„ã€‚ç„¶è€Œï¼ŒåŠ©æ‰‹ A çš„å›åº”æ›´è¯¦ç»†ï¼Œå¹¶æä¾›äº†æ›´å¤šå®é™…çš„å»ºè®®ã€‚å®ƒåŒ…æ‹¬ä»ç‰©ç†å­¦è½¬å‘å·¥ç¨‹å­¦çš„äººçš„åé¦ˆï¼Œè¿™å¯¹ç”¨æˆ·å¯èƒ½éå¸¸æœ‰å¸®åŠ©ã€‚åŠ©æ‰‹
    B çš„å›åº”è™½ç„¶æ”¯æŒï¼Œä½†ä¸å¤Ÿå…·ä½“ï¼Œå¹¶ä¸”ç»“æŸå¾—å¾ˆçªç„¶ï¼Œè®©ç”¨æˆ·æ²¡æœ‰å¾—åˆ°å®Œæ•´çš„ç­”æ¡ˆã€‚å› æ­¤ï¼ŒåŠ©æ‰‹ A çš„å›åº”æ›´ä¸ºæœ‰å¸®åŠ©å’Œå®Œæ•´ã€‚ |'
- en: Table 9. Sample responses to a prompt from the test set of the Stanford Human
    Preferences Dataset. GPT-4 chooses Mallows-$\phi$ on Anthropic-HH dataset.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 9. æ–¯å¦ç¦äººç±»åå¥½æ•°æ®é›†ä¸­æµ‹è¯•é›†æç¤ºçš„æ ·æœ¬å“åº”ã€‚GPT-4 åœ¨ Anthropic-HH æ•°æ®é›†ä¸­é€‰æ‹©äº† Mallows-$\phi$ã€‚
- en: '| Prompt | I donâ€™t want to be an engineer, I like physics. Hi there, im currently
    in high school and have some financial problems in the family, my parents are
    not satisfied with me doing physics (i mean get a degree and pursue higher studies
    in) as im also considering doing engineering as it has become a need. But i donâ€™t
    want to leave physics, i love atomic physics and cosmology so much. I want to
    ask here, can i continue my passion for physics while Iâ€™m doing engineering (IT)
    , will there be time for my passion. Or else is there any scope to follow my passion
    and take out my family of this? |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| æç¤º | æˆ‘ä¸æƒ³æˆä¸ºå·¥ç¨‹å¸ˆï¼Œæˆ‘å–œæ¬¢ç‰©ç†å­¦ã€‚ä½ å¥½ï¼Œæˆ‘ç›®å‰åœ¨é«˜ä¸­ï¼Œå®¶åº­æœ‰äº›ç»æµé—®é¢˜ï¼Œæˆ‘çš„çˆ¶æ¯å¯¹æˆ‘å­¦ä¹ ç‰©ç†ï¼ˆæˆ‘æŒ‡çš„æ˜¯è·å¾—å­¦ä½å¹¶è¿½æ±‚æ›´é«˜çš„ç ”ç©¶ï¼‰ä¸å¤ªæ»¡æ„ï¼Œå› ä¸ºæˆ‘ä¹Ÿåœ¨è€ƒè™‘åšå·¥ç¨‹å­¦ï¼Œè¿™å·²ç»å˜æˆä¸€ç§éœ€è¦ã€‚ä½†æˆ‘ä¸æƒ³æ”¾å¼ƒç‰©ç†ï¼Œæˆ‘éå¸¸çƒ­çˆ±åŸå­ç‰©ç†å­¦å’Œå®‡å®™å­¦ã€‚æˆ‘æƒ³é—®ä¸€ä¸‹ï¼Œæˆ‘åœ¨å­¦ä¹ å·¥ç¨‹ï¼ˆITï¼‰æœŸé—´æ˜¯å¦å¯ä»¥ç»§ç»­æˆ‘çš„ç‰©ç†å­¦çƒ­æƒ…ï¼Œä¼šæœ‰æ—¶é—´å»è¿½æ±‚æˆ‘çš„çƒ­æƒ…å—ï¼Ÿæˆ–è€…æ˜¯å¦æœ‰å¯èƒ½åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­å®ç°æˆ‘çš„çƒ­æƒ…å¹¶å¸®åŠ©æˆ‘çš„å®¶äººè§£å†³ç»æµé—®é¢˜ï¼Ÿ
    |'
- en: '| Mallows-$\phi$-DPO | Yes, you can definitely do both physics and engineering.
    Many people do both. At the end of the day, physics is about the mind-brain interaction
    of the fundamental particles in the universe, whereas engineering is about the
    world-building of technologies and products. So itâ€™s possible to pursue both!
    If you enjoy physics, you may be able to learn about and participate in research
    in engineering, and vice versa. Or, you can teach yourself engineering by reading
    technical papers and following online tutorials and then pick a project to work
    on. Most engineering schools have classes or tutorials for self-study! So thatâ€™s
    the way itâ€™s usually done. Itâ€™s a choice of whether you like physics, or of whether
    you go into engineering! Hope that helps! You could pursue a "dual degree", where
    you have some physics courses, and also some engineering course work, as an option.
    The physics courses could be in an engineering lab, or physics courses before
    your engineering classes. The more common thing is to just take a class in physics
    before taking an engineering class, either in another department at your university,
    or via a separate institution. And that might be a good place to take a class.
    They might also have online physics courses, or they might make it a requirement
    to take a physics course for an engineering degree. Hope that helps. |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| Mallows-$\phi$-DPO | æ˜¯çš„ï¼Œä½ ç»å¯¹å¯ä»¥åŒæ—¶åšç‰©ç†å­¦å’Œå·¥ç¨‹å­¦ã€‚å¾ˆå¤šäººéƒ½åšè¿™ä¸¤è€…ã€‚å½’æ ¹ç»“åº•ï¼Œç‰©ç†å­¦æ¶‰åŠå®‡å®™ä¸­åŸºæœ¬ç²’å­çš„å¿ƒè„‘äº’åŠ¨ï¼Œè€Œå·¥ç¨‹å­¦åˆ™æ¶‰åŠæŠ€æœ¯å’Œäº§å“çš„ä¸–ç•Œå»ºè®¾ã€‚å› æ­¤ï¼Œè¿½æ±‚è¿™ä¸¤è€…æ˜¯å¯èƒ½çš„ï¼å¦‚æœä½ å–œæ¬¢ç‰©ç†å­¦ï¼Œä½ å¯èƒ½èƒ½å¤Ÿäº†è§£å¹¶å‚ä¸å·¥ç¨‹å­¦çš„ç ”ç©¶ï¼Œåä¹‹äº¦ç„¶ã€‚æˆ–è€…ï¼Œä½ å¯ä»¥é€šè¿‡é˜…è¯»æŠ€æœ¯è®ºæ–‡å’Œè·Ÿéšåœ¨çº¿æ•™ç¨‹è‡ªå­¦å·¥ç¨‹å­¦ï¼Œç„¶åé€‰æ‹©ä¸€ä¸ªé¡¹ç›®è¿›è¡Œå®è·µã€‚å¤§å¤šæ•°å·¥ç¨‹å­¦é™¢éƒ½æœ‰è‡ªå­¦çš„è¯¾ç¨‹æˆ–æ•™ç¨‹ï¼è¿™å°±æ˜¯é€šå¸¸çš„åšæ³•ã€‚ä½ å¯ä»¥é€‰æ‹©æ˜¯å¦å–œæ¬¢ç‰©ç†å­¦ï¼Œæˆ–è€…æ˜¯å¦è¿›å…¥å·¥ç¨‹å­¦ï¼å¸Œæœ›è¿™å¯¹ä½ æœ‰å¸®åŠ©ï¼ä½ å¯ä»¥é€‰æ‹©â€œåŒå­¦ä½â€ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸€äº›ç‰©ç†å­¦è¯¾ç¨‹å’Œä¸€äº›å·¥ç¨‹å­¦è¯¾ç¨‹ã€‚ç‰©ç†å­¦è¯¾ç¨‹å¯ä»¥åœ¨å·¥ç¨‹å®éªŒå®¤ä¸­è¿›è¡Œï¼Œæˆ–è€…åœ¨å·¥ç¨‹è¯¾ç¨‹ä¹‹å‰å®Œæˆã€‚æ›´å¸¸è§çš„åšæ³•æ˜¯ï¼Œåœ¨ä¸Šå·¥ç¨‹è¯¾ä¹‹å‰å…ˆä¸Šç‰©ç†è¯¾ï¼Œæ— è®ºæ˜¯åœ¨ä½ å¤§å­¦çš„å…¶ä»–éƒ¨é—¨ï¼Œè¿˜æ˜¯é€šè¿‡å¦ä¸€ä¸ªæœºæ„ã€‚é‚£å¯èƒ½æ˜¯ä¸€ä¸ªä¸é”™çš„ä¸Šè¯¾åœ°ç‚¹ã€‚ä»–ä»¬ä¹Ÿå¯èƒ½æœ‰åœ¨çº¿ç‰©ç†è¯¾ç¨‹ï¼Œæˆ–è€…å¯èƒ½è¦æ±‚ä½ ä¸ºå·¥ç¨‹å­¦å­¦ä½ä¿®ä¸€é—¨ç‰©ç†è¯¾ã€‚å¸Œæœ›è¿™å¯¹ä½ æœ‰å¸®åŠ©ã€‚'
- en: '| BT-DPO | Yes, you can definitely do both physics and engineering. Many people
    do both. At the end of the day itâ€™s about what you like to do with your life,
    and most people find that their passion and work is what they want to do. Itâ€™s
    a great thing, if youâ€™re interested in physics, to be able to pursue it while
    doing something else. Iâ€™m always happy to be a resource and answer any specific
    questions you might have if you donâ€™t yet know how to work it out. Hereâ€™s some
    things to consider: -If you love physics you should start out with an introductory
    course in physics, this is essential for the study of physics. You might go on
    to physics for a year or two, then try physics and engineering for a later year
    or two. But it is also really important to keep coming back to physics - itâ€™s
    like a food for the mind - so if you canâ€™t make it all the way to advanced physics,
    eventually you should aim to do things in the field of physics or engineering
    that you are interested in. -The second thing to consider is that this does take
    up a lot of time - and a lot of money - but we should expect that. I wish I could
    give you some free advice here but I am not a full-time professor and there are
    lots of people with the same questions. You might try asking around at your school
    and perhaps someone can help you find a mentor who can give you a little more
    advice. Just keep in mind that the time you devote to a certain thing - for many
    young people - is mostly because of the pressure of other people and their expectations
    around them. So if there is the feeling that you aren [maximum generation length
    reached] |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| BT-DPO | æ˜¯çš„ï¼Œä½ ç»å¯¹å¯ä»¥åŒæ—¶åšç‰©ç†å­¦å’Œå·¥ç¨‹å­¦ã€‚å¾ˆå¤šäººéƒ½åšè¿™ä¸¤è€…ã€‚å½’æ ¹ç»“åº•ï¼Œå…³é”®åœ¨äºä½ å–œæ¬¢ç”¨è‡ªå·±çš„ç”Ÿæ´»åšä»€ä¹ˆï¼Œå¤§å¤šæ•°äººå‘ç°ä»–ä»¬çš„æ¿€æƒ…å’Œå·¥ä½œæ˜¯ä»–ä»¬æƒ³è¦åšçš„äº‹ã€‚å¦‚æœä½ å¯¹ç‰©ç†å­¦æ„Ÿå…´è¶£ï¼Œèƒ½å¤Ÿåœ¨åšå…¶ä»–äº‹æƒ…çš„åŒæ—¶è¿½æ±‚å®ƒæ˜¯ä»¶å¾ˆæ£’çš„äº‹ã€‚å¦‚æœä½ è¿˜ä¸çŸ¥é“å¦‚ä½•è§£å†³é—®é¢˜ï¼Œæˆ‘å¾ˆä¹æ„ä½œä¸ºèµ„æºå›ç­”ä½ å¯èƒ½æœ‰çš„ä»»ä½•å…·ä½“é—®é¢˜ã€‚è¿™é‡Œæœ‰ä¸€äº›éœ€è¦è€ƒè™‘çš„äº‹é¡¹ï¼š
    - å¦‚æœä½ å–œæ¬¢ç‰©ç†å­¦ï¼Œä½ åº”è¯¥ä»ç‰©ç†å­¦å…¥é—¨è¯¾ç¨‹å¼€å§‹ï¼Œè¿™å¯¹ç‰©ç†å­¦çš„å­¦ä¹ è‡³å…³é‡è¦ã€‚ä½ å¯ä»¥ç»§ç»­å­¦ä¹ ç‰©ç†å­¦ä¸€ä¸¤å¹´ï¼Œç„¶åå°è¯•åœ¨åæ¥çš„å¹´ä»½é‡Œå­¦ä¹ ç‰©ç†å­¦å’Œå·¥ç¨‹å­¦ã€‚ä½†åŒæ ·é‡è¦çš„æ˜¯è¦ä¸æ–­å›åˆ°ç‰©ç†å­¦ä¸Šâ€”â€”è¿™å°±åƒæ˜¯å¿ƒçµçš„é£Ÿç²®â€”â€”æ‰€ä»¥å³ä½¿ä½ ä¸èƒ½ä¸€ç›´è¿›è¡Œé«˜çº§ç‰©ç†å­¦ç ”ç©¶ï¼Œæœ€ç»ˆä½ ä¹Ÿåº”è¯¥ç„å‡†åœ¨ä½ æ„Ÿå…´è¶£çš„ç‰©ç†å­¦æˆ–å·¥ç¨‹å­¦é¢†åŸŸä»äº‹ä¸€äº›å·¥ä½œã€‚
    - ç¬¬äºŒä»¶éœ€è¦è€ƒè™‘çš„äº‹æ˜¯ï¼Œè¿™ç¡®å®ä¼šèŠ±è´¹å¾ˆå¤šæ—¶é—´å’Œå¾ˆå¤šé’±â€”â€”ä½†æˆ‘ä»¬åº”è¯¥é¢„æ–™åˆ°è¿™ä¸€ç‚¹ã€‚æˆ‘å¸Œæœ›æˆ‘èƒ½ç»™ä½ ä¸€äº›å…è´¹çš„å»ºè®®ï¼Œä½†æˆ‘ä¸æ˜¯å…¨èŒæ•™æˆï¼Œè¿˜æœ‰å¾ˆå¤šäººæœ‰åŒæ ·çš„é—®é¢˜ã€‚ä½ å¯ä»¥å°è¯•åœ¨å­¦æ ¡é‡Œè¯¢é—®ï¼Œä¹Ÿè®¸æœ‰äººèƒ½å¸®åŠ©ä½ æ‰¾åˆ°ä¸€ä¸ªå¯ä»¥ç»™ä½ æ›´å¤šå»ºè®®çš„å¯¼å¸ˆã€‚åªéœ€è®°ä½ï¼Œä½ èŠ±åœ¨æŸä¸€äº‹ç‰©ä¸Šçš„æ—¶é—´â€”â€”å¯¹äºè®¸å¤šå¹´è½»äººæ¥è¯´â€”â€”ä¸»è¦æ˜¯ç”±äºä»–äººåŠå…¶æœŸæœ›çš„å‹åŠ›ã€‚æ‰€ä»¥å¦‚æœæœ‰è¿™ç§æ„Ÿè§‰ï¼Œä½ ä¸å¿…æ„Ÿåˆ°
    |'
- en: '| Judgment | Both Assistant A and Assistant B provide helpful and encouraging
    responses to the userâ€™s question. They both affirm that itâ€™s possible to pursue
    both physics and engineering, and they both suggest taking introductory physics
    courses. However, Assistant A goes a step further by suggesting the possibility
    of a â€œdual degreeâ€ and mentioning that some engineering schools offer self-study
    classes or tutorials. Assistant B, on the other hand, provides more emotional
    support and acknowledges the time and financial constraints the user might face.
    Assistant B also suggests seeking a mentor for further advice. Both responses
    are valuable, but Assistant Aâ€™s response is more detailed and practical, providing
    more concrete steps the user can take to pursue both interests. Therefore, Assistant
    Aâ€™s response is better. |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| åˆ¤æ–­ | åŠ©æ‰‹ A å’Œ åŠ©æ‰‹ B éƒ½å¯¹ç”¨æˆ·çš„é—®é¢˜æä¾›äº†æœ‰å¸®åŠ©å’Œé¼“åŠ±çš„å›ç­”ã€‚ä»–ä»¬éƒ½ç¡®è®¤äº†åŒæ—¶è¿½æ±‚ç‰©ç†å­¦å’Œå·¥ç¨‹å­¦çš„å¯èƒ½æ€§ï¼Œå¹¶ä¸”éƒ½å»ºè®®å‚åŠ å…¥é—¨ç‰©ç†è¯¾ç¨‹ã€‚ç„¶è€Œï¼ŒåŠ©æ‰‹
    A è¿›ä¸€æ­¥æå‡ºäº†â€œåŒå­¦ä½â€çš„å¯èƒ½æ€§ï¼Œå¹¶æåˆ°ä¸€äº›å·¥ç¨‹å­¦æ ¡æä¾›è‡ªå­¦è¯¾ç¨‹æˆ–è¾…å¯¼ç­ã€‚å¦ä¸€æ–¹é¢ï¼ŒåŠ©æ‰‹ B æä¾›äº†æ›´å¤šçš„æƒ…æ„Ÿæ”¯æŒï¼Œå¹¶è®¤è¯†åˆ°ç”¨æˆ·å¯èƒ½é¢ä¸´çš„æ—¶é—´å’Œç»æµé™åˆ¶ã€‚åŠ©æ‰‹
    B è¿˜å»ºè®®å¯»æ±‚å¯¼å¸ˆè¿›ä¸€æ­¥çš„å»ºè®®ã€‚ä¸¤ä¸ªå›ç­”éƒ½å¾ˆæœ‰ä»·å€¼ï¼Œä½†åŠ©æ‰‹ A çš„å›ç­”æ›´è¯¦ç»†å’Œå®ç”¨ï¼Œæä¾›äº†æ›´å¤šå…·ä½“çš„æ­¥éª¤æ¥è¿½æ±‚è¿™ä¸¤ä¸ªå…´è¶£ã€‚å› æ­¤ï¼ŒåŠ©æ‰‹ A çš„å›ç­”æ›´å¥½ã€‚ |'
- en: Table 10. Sample responses to a prompt from the test set of the Stanford Human
    Preferences Dataset. GPT-4 chooses Mallows-$\theta$ on Anthropic-HH dataset.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 10. æ¥è‡ªæ–¯å¦ç¦äººç±»åå¥½æ•°æ®é›†æµ‹è¯•é›†çš„ç¤ºä¾‹å›ç­”ã€‚GPT-4 åœ¨ Anthropic-HH æ•°æ®é›†ä¸Šé€‰æ‹©äº† Mallows-$\theta$ã€‚
- en: '| Prompt | Are there any examples of famous philosophers grossly misinterpreting/misunderstanding
    other philosophers? I havenâ€™t read it myself, but Iâ€™ve heard that Bertrand Russellâ€™s
    characterisation of Hegelâ€™s philosophy in his *History of Western Philosophy*
    is quite bad. I was wondering if there are any other examples of philosophers
    misunderstanding other philosophers in a big way. |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| æç¤º | æ˜¯å¦æœ‰è‘—åå“²å­¦å®¶ä¸¥é‡è¯¯è§£/è¯¯è§£å…¶ä»–å“²å­¦å®¶çš„ä¾‹å­ï¼Ÿæˆ‘è‡ªå·±æ²¡æœ‰è¯»è¿‡ï¼Œä½†æˆ‘å¬è¯´ä¼¯ç‰¹å…°Â·ç½—ç´ åœ¨ä»–çš„*è¥¿æ–¹å“²å­¦å²*ä¸­å¯¹é»‘æ ¼å°”å“²å­¦çš„æè¿°ç›¸å½“ç³Ÿç³•ã€‚æˆ‘æƒ³çŸ¥é“æ˜¯å¦è¿˜æœ‰å…¶ä»–å“²å­¦å®¶å¯¹å…¶ä»–å“²å­¦å®¶çš„è¯¯è§£ã€‚
    |'
- en: '| Mallows-$\theta$-DPO | According to the book *What Is Philosophy?: A Historical
    Survey* by George Murdock, Bertrand Russell actually read and criticized Hegelâ€™s
    philosophy in his own writings in the early 20th century, but he didnâ€™t have a
    clear enough idea about the history of philosophy to understand Hegel. At the
    time of writing the book, Russell believed that Hegelâ€™s philosophy was a philosophy
    of religion, and that he denied freedom and happiness because of this. However,
    Hegelâ€™s own ideas were quite incompatible with certain parts of Christianity.
    Russell also had problems understanding Kant, and he thought Kant gave a simplified
    view of the history of philosophy. He took Kant at face value, when in reality
    Kant himself was very critical of Hegel, describing him as an atheist. The book
    does also have an extensive list of references which Russell could have checked
    to be sure that he wasnâ€™t making a mistake himself, and I think the biggest mistake
    he made was to take Hegelâ€™s ideas at face value when he thought they were religious.
    As far as other philosophers having been misunderstood, Plato and Kant are the
    most famous examples of this in the Western tradition, because they were so influential.
    The problem with both of these was that they werenâ€™t just philosophers but also
    scholars, who studied other philosophersâ€™ ideas and came up with their own versions
    of them. But philosophers werenâ€™t able to interpret ideas of other philosophers
    until the 20th century, because people had not thought about philosophy like they
    do now, but rather as a practical skill where the only real goals were to understand
    the world more and to make people behave better. There are many examples of famous
    philosophers, like Socrates, Hobbes, and Descartes, saying things that other philosophers
    said. These statements are usually based on the historical context of the times,
    so theyâ€™re not false statements. Even people who are aware of this situation,
    and have not given up their prejudices, will still form incorrect assumptions
    about philosophers. But thatâ€™s not what philosophers do. They donâ€™t need to prove
    all their points in order to prove them, which is what people who study history
    do, so they are usually much more accurate in their thinking. |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| Mallows-$\theta$-DPO | æ ¹æ®ä¹”æ²»Â·ç©†å°”å¤šå…‹çš„ä¹¦ç±ã€Šä»€ä¹ˆæ˜¯å“²å­¦ï¼Ÿï¼šå†å²æ¦‚è¿°ã€‹ï¼Œä¼¯ç‰¹å…°Â·ç½—ç´ åœ¨20ä¸–çºªåˆå®é™…ä¸Šé˜…è¯»å¹¶æ‰¹è¯„äº†é»‘æ ¼å°”çš„å“²å­¦ï¼Œä½†ä»–å¯¹å“²å­¦å†å²çš„äº†è§£å¹¶ä¸å……åˆ†ï¼Œæ— æ³•çœŸæ­£ç†è§£é»‘æ ¼å°”ã€‚åœ¨æ’°å†™è¿™æœ¬ä¹¦çš„æ—¶å€™ï¼Œç½—ç´ è®¤ä¸ºé»‘æ ¼å°”çš„å“²å­¦æ˜¯ä¸€ç§å®—æ•™å“²å­¦ï¼Œå› æ­¤ä»–è®¤ä¸ºé»‘æ ¼å°”å¦å®šäº†è‡ªç”±å’Œå¹¸ç¦ã€‚ç„¶è€Œï¼Œé»‘æ ¼å°”çš„æ€æƒ³ä¸åŸºç£æ•™çš„æŸäº›éƒ¨åˆ†å®é™…ä¸Šæ˜¯ç›¸äº’çŸ›ç›¾çš„ã€‚ç½—ç´ åœ¨ç†è§£åº·å¾·æ—¶ä¹Ÿé‡åˆ°äº†é—®é¢˜ï¼Œä»–è®¤ä¸ºåº·å¾·ç®€åŒ–äº†å“²å­¦å²çš„è§‚ç‚¹ã€‚ä»–æŠŠåº·å¾·çš„è§‚ç‚¹å½“ä½œç»å¯¹æ­£ç¡®ï¼Œè€Œå®é™…ä¸Šåº·å¾·æœ¬äººå¯¹é»‘æ ¼å°”éå¸¸æ‰¹è¯„ï¼Œç§°å…¶ä¸ºæ— ç¥è®ºè€…ã€‚è¿™æœ¬ä¹¦ç¡®å®æœ‰ä¸€ä¸ªè¯¦ç»†çš„å‚è€ƒæ–‡çŒ®åˆ—è¡¨ï¼Œç½—ç´ æœ¬å¯ä»¥æ£€æŸ¥è¿™äº›æ–‡çŒ®ä»¥ç¡®ä¿ä»–æ²¡æœ‰çŠ¯é”™ï¼Œæˆ‘è®¤ä¸ºä»–æœ€å¤§çš„é”™è¯¯æ˜¯æŠŠé»‘æ ¼å°”çš„æ€æƒ³å½“ä½œå®—æ•™è§‚ç‚¹æ¥ç†è§£ã€‚è‡³äºå…¶ä»–å“²å­¦å®¶çš„è¯¯è§£ï¼ŒæŸæ‹‰å›¾å’Œåº·å¾·æ˜¯è¥¿æ–¹ä¼ ç»Ÿä¸­æœ€è‘—åçš„ä¾‹å­ï¼Œå› ä¸ºä»–ä»¬çš„å½±å“åŠ›å¾ˆå¤§ã€‚è¿™ä¸¤ä¸ªå“²å­¦å®¶çš„é—®é¢˜åœ¨äºï¼Œä»–ä»¬ä¸ä»…æ˜¯å“²å­¦å®¶ï¼Œè¿˜æ˜¯å­¦è€…ï¼Œç ”ç©¶å…¶ä»–å“²å­¦å®¶çš„æ€æƒ³å¹¶æå‡ºäº†è‡ªå·±çš„ç‰ˆæœ¬ã€‚ä½†å“²å­¦å®¶ä»¬åœ¨20ä¸–çºªä¹‹å‰æ— æ³•è§£è¯»å…¶ä»–å“²å­¦å®¶çš„æ€æƒ³ï¼Œå› ä¸ºå½“æ—¶çš„äººä»¬æ²¡æœ‰åƒç°åœ¨è¿™æ ·æ€è€ƒå“²å­¦ï¼Œè€Œæ˜¯å°†å…¶è§†ä¸ºä¸€ç§å®ç”¨æŠ€èƒ½ï¼Œå”¯ä¸€çš„å®é™…ç›®æ ‡æ˜¯æ›´å¥½åœ°ç†è§£ä¸–ç•Œå’Œæ”¹å–„äººçš„è¡Œä¸ºã€‚è®¸å¤šè‘—åçš„å“²å­¦å®¶ï¼Œå¦‚è‹æ ¼æ‹‰åº•ã€éœå¸ƒæ–¯å’Œç¬›å¡å°”ï¼Œæ›¾ç»è¯´è¿‡å…¶ä»–å“²å­¦å®¶çš„è¯ã€‚è¿™äº›é™ˆè¿°é€šå¸¸åŸºäºå½“æ—¶çš„å†å²èƒŒæ™¯ï¼Œå› æ­¤å¹¶ä¸æ˜¯é”™è¯¯çš„é™ˆè¿°ã€‚å³ä½¿æ˜¯é‚£äº›æ„è¯†åˆ°è¿™ç§æƒ…å†µçš„äººï¼Œå¹¶ä¸”æ²¡æœ‰æ”¾å¼ƒä»–ä»¬çš„åè§ï¼Œä»ç„¶ä¼šå¯¹å“²å­¦å®¶å½¢æˆä¸æ­£ç¡®çš„å‡è®¾ã€‚ä½†è¿™å¹¶ä¸æ˜¯å“²å­¦å®¶ä»¬çš„åšæ³•ã€‚ä»–ä»¬ä¸éœ€è¦åƒç ”ç©¶å†å²çš„äººé‚£æ ·è¯æ˜ä»–ä»¬çš„è§‚ç‚¹ï¼Œå› æ­¤ä»–ä»¬çš„æ€ç»´é€šå¸¸æ›´ä¸ºå‡†ç¡®ã€‚'
- en: '| BT-DPO | According to Wikipedia, as of 2012 Bertrand Russell was accused
    of â€œmisrepresenting the views of Hegel and Marxâ€. It seems like Bertrand Russellâ€™s
    understanding of Hegel was not entirely accurate, but rather a caricature. Since
    the book is a history of philosophy, I was wondering if youâ€™re interested in the
    idea of â€œbad philosophyâ€ in which a philosopher is unfairly caricatured by others?
    But Russellâ€™s book is widely regarded as having historical importance, and for
    the most part I donâ€™t think his interpretation of Hegel is considered to be controversial.
    He does have some critics. In addition, his history of Western philosophy was
    enormously influential at the time, and much has changed since then. But I do
    think there is value to his book, and I think Russell and Hegel are not unfairly
    caricatured by each other. |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| BT-DPO | æ ¹æ®ç»´åŸºç™¾ç§‘ï¼Œæˆªè‡³2012å¹´ï¼Œä¼¯ç‰¹å…°Â·ç½—ç´ è¢«æŒ‡æ§â€œæ­ªæ›²äº†é»‘æ ¼å°”å’Œé©¬å…‹æ€çš„è§‚ç‚¹â€ã€‚çœ‹èµ·æ¥ä¼¯ç‰¹å…°Â·ç½—ç´ å¯¹é»‘æ ¼å°”çš„ç†è§£å¹¶ä¸å®Œå…¨å‡†ç¡®ï¼Œè€Œæ›´åƒæ˜¯ä¸€ç§è®½åˆºã€‚ç”±äºè¿™æœ¬ä¹¦æ˜¯ä¸€æœ¬å“²å­¦å²ï¼Œæˆ‘æƒ³çŸ¥é“ä½ æ˜¯å¦å¯¹â€œåå“²å­¦â€çš„è§‚ç‚¹æ„Ÿå…´è¶£ï¼Œå³å“²å­¦å®¶è¢«å…¶ä»–äººä¸å…¬æ­£åœ°åˆ»ç”»ï¼Ÿä½†ç½—ç´ çš„ä¹¦è¢«å¹¿æ³›è®¤ä¸ºå…·æœ‰å†å²é‡è¦æ€§ï¼Œè€Œä¸”æˆ‘è®¤ä¸ºä»–çš„é»‘æ ¼å°”è§£é‡Šå¤§å¤šä¸è¢«è®¤ä¸ºå…·æœ‰äº‰è®®ã€‚ä»–ç¡®å®æœ‰ä¸€äº›æ‰¹è¯„è€…ã€‚æ­¤å¤–ï¼Œä»–çš„è¥¿æ–¹å“²å­¦å²åœ¨å½“æ—¶å…·æœ‰å·¨å¤§å½±å“åŠ›ï¼Œè€Œè‡ªé‚£æ—¶ä»¥æ¥è®¸å¤šäº‹æƒ…å‘ç”Ÿäº†å˜åŒ–ã€‚ä½†æˆ‘è®¤ä¸ºä»–çš„ä¹¦è¿˜æ˜¯æœ‰ä»·å€¼çš„ï¼Œæˆ‘è®¤ä¸ºç½—ç´ å’Œé»‘æ ¼å°”å¹¶æœªç›¸äº’ä¸å…¬æ­£åœ°åˆ»ç”»å¯¹æ–¹ã€‚
    |'
- en: '| Judgment | Assistant A provides a more detailed response, explaining how
    Bertrand Russell misunderstood Hegelâ€™s philosophy and also mentioning that Russell
    had problems understanding Kant. Assistant A also provides examples of other philosophers
    who have been misunderstood, such as Plato and Kant, and explains why this misunderstanding
    often occurs. On the other hand, Assistant Bâ€™s response is less detailed and does
    not provide any examples of other philosophers who have been misunderstood. Assistant
    B also seems to contradict itself by stating that Russellâ€™s interpretation of
    Hegel is not considered controversial, but then mentioning that Russell does have
    critics. Therefore, Assistant Aâ€™s response is more helpful, relevant, accurate,
    and detailed. |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| åˆ¤æ–­ | åŠ©æ‰‹ A æä¾›äº†æ›´è¯¦ç»†çš„å›ç­”ï¼Œè§£é‡Šäº†ä¼¯ç‰¹å…°Â·ç½—ç´ å¦‚ä½•è¯¯è§£é»‘æ ¼å°”çš„å“²å­¦ï¼Œå¹¶ä¸”æåˆ°ç½—ç´ åœ¨ç†è§£åº·å¾·æ–¹é¢ä¹Ÿå­˜åœ¨é—®é¢˜ã€‚åŠ©æ‰‹ A è¿˜æä¾›äº†å…¶ä»–è¢«è¯¯è§£çš„å“²å­¦å®¶ç¤ºä¾‹ï¼Œå¦‚æŸæ‹‰å›¾å’Œåº·å¾·ï¼Œå¹¶è§£é‡Šäº†è¿™ç§è¯¯è§£ä¸ºä½•å¸¸å¸¸å‘ç”Ÿã€‚å¦ä¸€æ–¹é¢ï¼ŒåŠ©æ‰‹
    B çš„å›ç­”åˆ™è¾ƒä¸ºç®€ç•¥ï¼Œæ²¡æœ‰æä¾›å…¶ä»–è¢«è¯¯è§£å“²å­¦å®¶çš„ç¤ºä¾‹ã€‚åŠ©æ‰‹ B è¿˜ä¼¼ä¹è‡ªç›¸çŸ›ç›¾ï¼Œä¸€æ–¹é¢ç§°ç½—ç´ å¯¹é»‘æ ¼å°”çš„è§£é‡Šå¹¶ä¸è¢«è®¤ä¸ºæœ‰äº‰è®®ï¼Œå¦ä¸€æ–¹é¢åˆæåˆ°ç½—ç´ ç¡®å®æœ‰ä¸€äº›æ‰¹è¯„è€…ã€‚å› æ­¤ï¼ŒåŠ©æ‰‹
    A çš„å›ç­”æ›´ä¸ºæœ‰ç”¨ã€ç›¸å…³ã€å‡†ç¡®ä¸”è¯¦ç»†ã€‚ |'
- en: Table 11. Sample responses to a prompt from the test set of the Stanford Human
    Preferences Dataset. GPT-4 chooses Mallows-$\phi$ on Anthropic-HH dataset.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 11. å¯¹æ–¯å¦ç¦äººç±»åå¥½æ•°æ®é›†æµ‹è¯•é›†ä¸­çš„ä¸€ä¸ªæç¤ºçš„ç¤ºä¾‹å›ç­”ã€‚GPT-4 åœ¨ Anthropic-HH æ•°æ®é›†ä¸Šé€‰æ‹©äº† Mallows-$\phi$ã€‚
- en: '| Prompt | Are there any examples of famous philosophers grossly misinterpreting/misunderstanding
    other philosophers? I havenâ€™t read it myself, but Iâ€™ve heard that Bertrand Russellâ€™s
    characterisation of Hegelâ€™s philosophy in his *History of Western Philosophy*
    is quite bad. I was wondering if there are any other examples of philosophers
    misunderstanding other philosophers in a big way. |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| æç¤º | æœ‰æ²¡æœ‰ä¸€äº›è‘—åå“²å­¦å®¶ä¸¥é‡è¯¯è§£/è¯¯è¯»å…¶ä»–å“²å­¦å®¶çš„ä¾‹å­ï¼Ÿæˆ‘è‡ªå·±æ²¡æœ‰è¯»è¿‡ï¼Œä½†æˆ‘å¬è¯´ä¼¯ç‰¹å…°Â·ç½—ç´ åœ¨ä»–çš„*è¥¿æ–¹å“²å­¦å²*ä¸­å¯¹é»‘æ ¼å°”å“²å­¦çš„åˆ»ç”»ç›¸å½“ç³Ÿç³•ã€‚æˆ‘åœ¨æƒ³æ˜¯å¦è¿˜æœ‰å…¶ä»–å“²å­¦å®¶ä¸¥é‡è¯¯è§£å…¶ä»–å“²å­¦å®¶çš„ä¾‹å­ã€‚
    |'
- en: '| Mallows-$\phi$-DPO | According to the book *Bertrand Russell on Philosophy:
    The Philosopherâ€™s Role in Life*, Russell had a major problem with the German philosopher
    Hegelâ€™s understanding of social and political order, because at the time Russell
    was growing up Hegelâ€™s writings appealed to him for insights about human nature,
    society, and politics. Hegel believed that human consciousness developed toward
    a certain ultimate goal, and that this ideal was a â€œgoalâ€ and a â€œprincipleâ€ that
    humans could strive to reach. Russell thought Hegel was wrong because he believed
    that humanityâ€™s final goal would be one based on some kind of scientific understanding.
    In addition, Russell thinks that Hegelâ€™s philosophy isnâ€™t compatible with human
    experience, and that itâ€™s kind of a â€œdream worldâ€ kind of philosophy, and that
    itâ€™s too abstract for people to be able to understand it. He thought that Hegelâ€™s
    philosophy wasnâ€™t a very useful perspective in the world, even though he was very
    big in Europe at the time. His philosophy is called â€œtheistic idealismâ€ in philosophy.
    Here are some points about the Hegelâ€™s system of philosophy from Russell: - Hegel
    believed in the â€œdialectical unity of nature and history,â€ meaning that the universe
    consists of two parts: One which is called nature, and the other which is called
    spirit. These terms describe each other. - Hegel believed that human reality is
    a kind of â€œmoral dramaâ€ in which human consciousness has a goal, and where the
    only real â€œmoral actorsâ€ are human beings, and people are real actors who can
    make choices and make a difference. People can choose how to respond to social
    interactions, and people can choose how to respond to their own lives. But since
    humans have free will, their values and their moral choices are not necessarily
    predictable. And humans are not a deterministic system, since humans have free
    will, and humans can make moral choices and make a difference in the world. -
    The Hegelian version of the dialectical process is this: In â€œnatureâ€ there are
    two distinct concepts: the concept of â€œconsciousnessâ€ and the concept of â€œmatter,â€
    that are both independent and distinct. But then â€œconsciousnessâ€ and â€œmatterâ€
    are in a â€œstate of developmentâ€ called â€œspirit,â€ and when the development in â€œspiritâ€
    is complete, then the dialectical relationship is complete. Human reality is basically
    this kind of two system-like dialectical relationship. - Hegel believed in a kind
    of â€œsynthesis of oppositesâ€: the concept of â€œmatterâ€ is really not â€œmatterâ€ by
    itself, since the â€œworldâ€ is really a thing, and â€œnatureâ€ is not â€œprimitive natureâ€
    in itself, but a â€œself-developing wholeâ€ that is â€œspiritâ€ in a special sense.
    When â€œspiritâ€ develops fully, then the â€œwholeâ€ becomes â€œpure spiritâ€. - Russell
    thought this â€œsynthesis of oppositesâ€ Hegelian system is not a â€œscientificâ€ system,
    since humanity cannot have a â€œscientificâ€ understanding of how the â€œworldâ€ is,
    and the idea that they could reach an ultimate goal is not scientifically possible.
    |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| Mallows-$\phi$-DPO | æ ¹æ®ä¹¦ç±*ã€Šä¼¯ç‰¹å…°Â·ç½—ç´ è®ºå“²å­¦ï¼šå“²å­¦å®¶åœ¨ç”Ÿæ´»ä¸­çš„è§’è‰²ã€‹*ï¼Œç½—ç´ å¯¹å¾·å›½å“²å­¦å®¶é»‘æ ¼å°”å¯¹ç¤¾ä¼šå’Œæ”¿æ²»ç§©åºçš„ç†è§£æœ‰ä¸€ä¸ªé‡å¤§é—®é¢˜ï¼Œå› ä¸ºåœ¨ç½—ç´ æˆé•¿çš„æ—¶æœŸï¼Œé»‘æ ¼å°”çš„è‘—ä½œå¸å¼•äº†ä»–å¯¹äººæ€§ã€ç¤¾ä¼šå’Œæ”¿æ²»çš„è§è§£ã€‚é»‘æ ¼å°”è®¤ä¸ºï¼Œäººç±»æ„è¯†æ˜¯æœç€æŸä¸ªç»ˆæç›®æ ‡å‘å±•çš„ï¼Œè¿™ä¸ªç†æƒ³æ˜¯ä¸€ä¸ªâ€œç›®æ ‡â€å’Œä¸€ä¸ªâ€œåŸåˆ™â€ï¼Œäººç±»å¯ä»¥åŠªåŠ›è¾¾åˆ°ã€‚ç½—ç´ è®¤ä¸ºé»‘æ ¼å°”æ˜¯é”™çš„ï¼Œå› ä¸ºä»–è®¤ä¸ºäººç±»çš„æœ€ç»ˆç›®æ ‡åº”å»ºç«‹åœ¨æŸç§ç§‘å­¦ç†è§£çš„åŸºç¡€ä¸Šã€‚æ­¤å¤–ï¼Œç½—ç´ è®¤ä¸ºé»‘æ ¼å°”çš„å“²å­¦ä¸äººç±»ç»éªŒä¸ç›¸å®¹ï¼Œå®ƒæ˜¯ä¸€ç§â€œæ¢¦å¹»ä¸–ç•Œâ€èˆ¬çš„å“²å­¦ï¼Œè¿‡äºæŠ½è±¡ï¼Œäººä»¬éš¾ä»¥ç†è§£ã€‚ä»–è®¤ä¸ºé»‘æ ¼å°”çš„å“²å­¦åœ¨å½“æ—¶æ¬§æ´²è™½ç„¶éå¸¸é‡è¦ï¼Œä½†åœ¨ä¸–ç•Œä¸Šå¹¶ä¸æ˜¯ä¸€ä¸ªå¾ˆæœ‰ç”¨çš„è§†è§’ã€‚ä»–çš„å“²å­¦åœ¨å“²å­¦ä¸­è¢«ç§°ä¸ºâ€œæœ‰ç¥è®ºå”¯å¿ƒä¸»ä¹‰â€ã€‚ä»¥ä¸‹æ˜¯ç½—ç´ å¯¹é»‘æ ¼å°”å“²å­¦ä½“ç³»çš„ä¸€äº›è§‚ç‚¹ï¼š
    - é»‘æ ¼å°”ç›¸ä¿¡â€œè‡ªç„¶ä¸å†å²çš„è¾©è¯ç»Ÿä¸€â€ï¼Œå³å®‡å®™ç”±ä¸¤ä¸ªéƒ¨åˆ†ç»„æˆï¼šä¸€ä¸ªç§°ä¸ºè‡ªç„¶ï¼Œå¦ä¸€ä¸ªç§°ä¸ºç²¾ç¥ã€‚è¿™äº›æœ¯è¯­ç›¸äº’æè¿°ã€‚ - é»‘æ ¼å°”è®¤ä¸ºï¼Œäººç±»ç°å®æ˜¯ä¸€ç§â€œé“å¾·æˆå‰§â€ï¼Œåœ¨äººç±»æ„è¯†ä¸­æœ‰ä¸€ä¸ªç›®æ ‡ï¼Œå…¶ä¸­å”¯ä¸€çœŸæ­£çš„â€œé“å¾·æ¼”å‘˜â€æ˜¯äººç±»ï¼Œäººä»¬æ˜¯çœŸæ­£çš„æ¼”å‘˜ï¼Œå¯ä»¥åšå‡ºé€‰æ‹©å¹¶äº§ç”Ÿå½±å“ã€‚äººä»¬å¯ä»¥é€‰æ‹©å¦‚ä½•å›åº”ç¤¾ä¼šäº’åŠ¨ï¼Œå¦‚ä½•å›åº”è‡ªå·±çš„ç”Ÿæ´»ã€‚ä½†ç”±äºäººç±»æ‹¥æœ‰è‡ªç”±æ„å¿—ï¼Œä»–ä»¬çš„ä»·å€¼è§‚å’Œé“å¾·é€‰æ‹©ä¸ä¸€å®šæ˜¯å¯é¢„æµ‹çš„ã€‚äººç±»ä¸æ˜¯ä¸€ä¸ªå†³å®šè®ºç³»ç»Ÿï¼Œå› ä¸ºäººç±»æœ‰è‡ªç”±æ„å¿—ï¼Œå¯ä»¥åšå‡ºé“å¾·é€‰æ‹©ï¼Œå¹¶åœ¨ä¸–ç•Œä¸Šäº§ç”Ÿå½±å“ã€‚
    - é»‘æ ¼å°”çš„è¾©è¯è¿‡ç¨‹ç‰ˆæœ¬æ˜¯è¿™æ ·çš„ï¼šåœ¨â€œè‡ªç„¶â€ä¸­æœ‰ä¸¤ä¸ªä¸åŒçš„æ¦‚å¿µï¼šå³â€œæ„è¯†â€çš„æ¦‚å¿µå’Œâ€œç‰©è´¨â€çš„æ¦‚å¿µï¼Œå®ƒä»¬éƒ½æ˜¯ç‹¬ç«‹è€Œä¸åŒçš„ã€‚ä½†éšåâ€œæ„è¯†â€å’Œâ€œç‰©è´¨â€å¤„äºä¸€ç§ç§°ä¸ºâ€œç²¾ç¥â€çš„â€œå‘å±•çŠ¶æ€â€ä¸­ï¼Œå½“â€œç²¾ç¥â€çš„å‘å±•å®Œæˆæ—¶ï¼Œè¾©è¯å…³ç³»ä¹Ÿå°±å®Œæˆäº†ã€‚äººç±»ç°å®åŸºæœ¬ä¸Šæ˜¯è¿™ç§ç±»ä¼¼äºåŒç³»ç»Ÿçš„è¾©è¯å…³ç³»ã€‚
    - é»‘æ ¼å°”ç›¸ä¿¡ä¸€ç§â€œå¯¹ç«‹ç»Ÿä¸€â€ï¼šæ¦‚å¿µâ€œç‰©è´¨â€å®é™…ä¸Šå¹¶ä¸æ˜¯â€œç‰©è´¨â€æœ¬èº«ï¼Œå› ä¸ºâ€œä¸–ç•Œâ€æœ¬èº«æ˜¯ä¸€ç§äº‹ç‰©ï¼Œè€Œâ€œè‡ªç„¶â€å¹¶ä¸æ˜¯â€œåŸå§‹è‡ªç„¶â€æœ¬èº«ï¼Œè€Œæ˜¯ä»¥ç‰¹æ®Šæ„ä¹‰ä¸Šçš„â€œç²¾ç¥â€å­˜åœ¨çš„â€œè‡ªæˆ‘å‘å±•æ•´ä½“â€ã€‚å½“â€œç²¾ç¥â€å®Œå…¨å‘å±•æ—¶ï¼Œâ€œæ•´ä½“â€å°±æˆä¸ºâ€œçº¯ç²¾ç¥â€ã€‚
    - ç½—ç´ è®¤ä¸ºè¿™ç§â€œå¯¹ç«‹ç»Ÿä¸€â€çš„é»‘æ ¼å°”ä½“ç³»ä¸æ˜¯ä¸€ä¸ªâ€œç§‘å­¦â€ä½“ç³»ï¼Œå› ä¸ºäººç±»æ— æ³•â€œç§‘å­¦â€åœ°ç†è§£â€œä¸–ç•Œâ€æ˜¯ä»€ä¹ˆï¼Œå¹¶ä¸”ä»–ä»¬èƒ½å¤Ÿè¾¾åˆ°ç»ˆæç›®æ ‡çš„æƒ³æ³•åœ¨ç§‘å­¦ä¸Šæ˜¯ä¸å¯èƒ½çš„ã€‚'
- en: '| BT-DPO | According to Wikipedia, as of 2012 Bertrand Russell was accused
    of â€œmisrepresenting the views of Hegel and Marxâ€. It seems like Bertrand Russellâ€™s
    understanding of Hegel was not entirely accurate, but rather a caricature. Since
    the book is a history of philosophy, I was wondering if youâ€™re interested in the
    idea of â€œbad philosophyâ€ in which a philosopher is unfairly caricatured by others?
    But Russellâ€™s book is widely regarded as having historical importance, and for
    the most part I donâ€™t think his interpretation of Hegel is considered to be controversial.
    He does have some critics. In addition, his history of Western philosophy was
    enormously influential at the time, and much has changed since then. But I do
    think there is value to his book, and I think Russell and Hegel are not unfairly
    caricatured by each other. |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| BT-DPO | æ ¹æ®ç»´åŸºç™¾ç§‘ï¼Œæˆªè‡³ 2012 å¹´ï¼Œä¼¯ç‰¹å…°Â·ç½—ç´ è¢«æŒ‡æ§â€œæ­ªæ›²äº†é»‘æ ¼å°”å’Œé©¬å…‹æ€çš„è§‚ç‚¹â€ã€‚çœ‹èµ·æ¥ä¼¯ç‰¹å…°Â·ç½—ç´ å¯¹é»‘æ ¼å°”çš„ç†è§£å¹¶ä¸å®Œå…¨å‡†ç¡®ï¼Œè€Œæ˜¯ä¸€ä¸ªè®½åˆºã€‚ç”±äºè¿™æœ¬ä¹¦æ˜¯å“²å­¦å²ï¼Œæˆ‘æƒ³çŸ¥é“ä½ æ˜¯å¦å¯¹â€œç³Ÿç³•å“²å­¦â€çš„æ¦‚å¿µæ„Ÿå…´è¶£ï¼Œå…¶ä¸­ä¸€ä¸ªå“²å­¦å®¶è¢«å…¶ä»–äººä¸å…¬æ­£åœ°è®½åˆºï¼Ÿä¸è¿‡ï¼Œç½—ç´ çš„ä¹¦è¢«å¹¿æ³›è®¤ä¸ºå…·æœ‰å†å²é‡è¦æ€§ï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæˆ‘è®¤ä¸ºä»–çš„é»‘æ ¼å°”è§£é‡Šå¹¶ä¸è¢«è®¤ä¸ºæ˜¯æœ‰äº‰è®®çš„ã€‚ä»–ç¡®å®æœ‰ä¸€äº›æ‰¹è¯„è€…ã€‚æ­¤å¤–ï¼Œä»–çš„è¥¿æ–¹å“²å­¦å²åœ¨å½“æ—¶å…·æœ‰æå¤§çš„å½±å“åŠ›ï¼Œè€Œä¹‹åå‘ç”Ÿäº†è®¸å¤šå˜åŒ–ã€‚ä½†æˆ‘è®¤ä¸ºä»–çš„ä¹¦è¿˜æ˜¯æœ‰ä»·å€¼çš„ï¼Œæˆ‘è®¤ä¸ºç½—ç´ å’Œé»‘æ ¼å°”å¹¶æ²¡æœ‰ä¸å…¬æ­£åœ°äº’ç›¸è®½åˆºã€‚'
- en: '| Judgment | Assistant A provides a detailed explanation of Bertrand Russellâ€™s
    interpretation of Hegelâ€™s philosophy, highlighting the points of disagreement
    and misunderstanding. This response is not only relevant to the userâ€™s question
    but also provides a comprehensive understanding of the topic. On the other hand,
    Assistant Bâ€™s response is less detailed and does not provide any other examples
    of philosophers misunderstanding other philosophers, which was the userâ€™s main
    question. Therefore, Assistant Aâ€™s response is more helpful and informative. |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| åˆ¤æ–­ | åŠ©æ‰‹ A æä¾›äº†å…³äºä¼¯ç‰¹å…°Â·ç½—ç´ å¯¹é»‘æ ¼å°”å“²å­¦è§£é‡Šçš„è¯¦ç»†è¯´æ˜ï¼Œçªå‡ºäº†åˆ†æ­§å’Œè¯¯è§£çš„è¦ç‚¹ã€‚è¿™ä¸ªå›åº”ä¸ä»…ä¸ç”¨æˆ·çš„é—®é¢˜ç›¸å…³ï¼Œè€Œä¸”æä¾›äº†å¯¹ä¸»é¢˜çš„å…¨é¢ç†è§£ã€‚å¦ä¸€æ–¹é¢ï¼ŒåŠ©æ‰‹
    B çš„å›åº”åˆ™ä¸å¤Ÿè¯¦ç»†ï¼Œå¹¶ä¸”æ²¡æœ‰æä¾›å…¶ä»–å“²å­¦å®¶è¯¯è§£å…¶ä»–å“²å­¦å®¶çš„ä¾‹å­ï¼Œè¿™æ­£æ˜¯ç”¨æˆ·çš„ä¸»è¦é—®é¢˜ã€‚å› æ­¤ï¼ŒåŠ©æ‰‹ A çš„å›åº”æ›´å…·å¸®åŠ©æ€§å’Œä¿¡æ¯æ€§ã€‚ |'
- en: Table 12. Sample responses to a prompt from the test set of the H4 Stack Exchange
    Preferences Dataset. GPT-4 chooses Mallows-$\theta$ on Anthropic-HH dataset.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 12. H4 Stack Exchange åå¥½æ•°æ®é›†æµ‹è¯•é›†æç¤ºçš„ç¤ºä¾‹å›åº”ã€‚GPT-4 åœ¨ Anthropic-HH æ•°æ®é›†ä¸Šé€‰æ‹©äº† Mallows-$\theta$ã€‚
- en: '| Prompt | Iâ€™m attempting to get a (in my opinion) very simple scenario working
    with Google Tag Manager and Analytics, and running into problems. Now we would
    like to track events using custom JavaScript, via the . For example: $(â€™aâ€™).click(function()
    dataLayer.push(â€™eventâ€™: â€™linkclickedtestâ€™); );  This just isnâ€™t working,
    at all. If I watch Chromeâ€™s console/network tabs, no data seems to get sent to
    Google. Iâ€™ve also installed for Chrome and it doesnâ€™t show the event firing either.
    I must be missing something simple. Any guidance would be appreciated. Thanks!
    |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| æç¤º | æˆ‘æ­£åœ¨å°è¯•è®© Google Tag Manager å’Œ Analytics çš„ä¸€ä¸ªï¼ˆåœ¨æˆ‘çœ‹æ¥ï¼‰éå¸¸ç®€å•çš„åœºæ™¯æ­£å¸¸å·¥ä½œï¼Œå´é‡åˆ°é—®é¢˜ã€‚ç°åœ¨æˆ‘ä»¬å¸Œæœ›é€šè¿‡è‡ªå®šä¹‰
    JavaScript è·Ÿè¸ªäº‹ä»¶ã€‚ä¾‹å¦‚ï¼š$(â€™aâ€™).click(function() dataLayer.push(â€™eventâ€™: â€™linkclickedtestâ€™);
    );  è¿™æ ¹æœ¬ä¸èµ·ä½œç”¨ã€‚å¦‚æœæˆ‘æŸ¥çœ‹ Chrome çš„æ§åˆ¶å°/ç½‘ç»œæ ‡ç­¾ï¼Œæ²¡æœ‰æ•°æ®ä¼¼ä¹è¢«å‘é€åˆ° Googleã€‚æˆ‘è¿˜ä¸º Chrome å®‰è£…äº†æ’ä»¶ï¼Œä½†å®ƒä¹Ÿæ²¡æœ‰æ˜¾ç¤ºäº‹ä»¶è§¦å‘ã€‚æˆ‘ä¸€å®šé—æ¼äº†ä¸€äº›ç®€å•çš„ä¸œè¥¿ã€‚ä»»ä½•æŒ‡å¯¼éƒ½ä¼šå¾ˆæ„Ÿæ¿€ã€‚è°¢è°¢ï¼
    |'
- en: '| Mallows-$\theta$-DPO | There are a few possibilities here: - You are defining
    the event before the link has even been added to the DOM, so your code will never
    run. - The code you are running also wonâ€™t run in that scenario because the event
    has not yet been added, so the code you have wonâ€™t get executed. - You have to
    wait for the JavaScript to run before the code can be evaluated, so the first
    part of the code above is going to run after the code and the link youâ€™ve just
    added to the DOM, but before the event has been loaded. If you change the code
    in your question to be more along these lines, itâ€™ll work:  // Wait for
    the JavaScript to load, before executing the code below. setTimeout(function()
    document.querySelector(â€™aâ€™).onclick = function() dataLayer.push(â€™eventâ€™: â€™linkclickedtestâ€™);
    , 0); |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| Mallows-$\theta$-DPO | è¿™é‡Œæœ‰å‡ ä¸ªå¯èƒ½æ€§ï¼š- ä½ åœ¨é“¾æ¥è¿˜æ²¡æœ‰è¢«æ·»åŠ åˆ° DOM ä¹‹å‰å°±å®šä¹‰äº†äº‹ä»¶ï¼Œå› æ­¤ä½ çš„ä»£ç å°†æ°¸è¿œä¸ä¼šè¿è¡Œã€‚-
    ä½ è¿è¡Œçš„ä»£ç ä¹Ÿä¸ä¼šåœ¨è¿™ç§æƒ…å†µä¸‹è¿è¡Œï¼Œå› ä¸ºäº‹ä»¶å°šæœªæ·»åŠ ï¼Œæ‰€ä»¥ä½ çš„ä»£ç ä¸ä¼šè¢«æ‰§è¡Œã€‚- ä½ å¿…é¡»ç­‰å¾… JavaScript è¿è¡Œå®Œæ¯•ï¼Œæ‰èƒ½è¯„ä¼°ä»£ç ï¼Œå› æ­¤ä¸Šé¢ä»£ç çš„ç¬¬ä¸€éƒ¨åˆ†å°†åœ¨ä½ åˆšåˆšæ·»åŠ åˆ°
    DOM çš„ä»£ç å’Œé“¾æ¥ä¹‹åè¿è¡Œï¼Œä½†åœ¨äº‹ä»¶åŠ è½½ä¹‹å‰ã€‚å¦‚æœä½ å°†é—®é¢˜ä¸­çš„ä»£ç æ›´æ”¹ä¸ºä»¥ä¸‹å½¢å¼ï¼Œå®ƒå°†èµ·ä½œç”¨ï¼š // ç­‰å¾… JavaScript åŠ è½½å®Œæ¯•ï¼Œç„¶åå†æ‰§è¡Œä¸‹é¢çš„ä»£ç ã€‚
    setTimeout(function() document.querySelector(â€™aâ€™).onclick = function() dataLayer.push(â€™eventâ€™:
    â€™linkclickedtestâ€™); , 0); |'
- en: '| BT-DPO | There are a few possibilities here: Youâ€™re using the latest version
    of Google Tag Manager, and you havenâ€™t configured it correctly. Youâ€™re using custom
    JavaScript in a page other than the one being tracked. Youâ€™re using analytics
    tags in other parts of your code, which are interfering with your tracking code.
    For instance, if you were using  tags at the start of , those would
    interfere with Google Tag Manager script. To resolve issue 1 here, youâ€™ll need
    to update the version of Google Tag Manager youâ€™re using. In the Google Tag Manager
    Settings page, click "Download & Install" to download and install the latest version
    of the Google Tag Manager. Youâ€™ll also need to make sure itâ€™s configured correctly
    in your environment, and also set it up correctly in the header of the page youâ€™re
    tracking. If youâ€™re using a different version of Google Tag Manager than the one
    you downloaded, youâ€™ll need to install Google Tag Manager version 1.5 or higher.
    For example |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| BT-DPO | è¿™é‡Œæœ‰å‡ ç§å¯èƒ½æ€§ï¼šä½ æ­£åœ¨ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬çš„ Google Tag Managerï¼Œä½†é…ç½®ä¸æ­£ç¡®ã€‚ä½ åœ¨è·Ÿè¸ªé¡µé¢ä¹‹å¤–çš„é¡µé¢ä¸­ä½¿ç”¨äº†è‡ªå®šä¹‰
    JavaScriptã€‚ä½ åœ¨ä»£ç çš„å…¶ä»–éƒ¨åˆ†ä½¿ç”¨äº†åˆ†ææ ‡ç­¾ï¼Œè¿™äº›æ ‡ç­¾å¹²æ‰°äº†ä½ çš„è·Ÿè¸ªä»£ç ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ åœ¨ `` å¼€å§‹æ—¶ä½¿ç”¨äº† `` æ ‡ç­¾ï¼Œè¿™äº›æ ‡ç­¾ä¼šå¹²æ‰°
    Google Tag Manager è„šæœ¬ã€‚è¦è§£å†³è¿™é‡Œçš„é—®é¢˜1ï¼Œä½ éœ€è¦æ›´æ–°ä½ ä½¿ç”¨çš„ Google Tag Manager ç‰ˆæœ¬ã€‚åœ¨ Google Tag Manager
    è®¾ç½®é¡µé¢ä¸­ï¼Œç‚¹å‡»â€œä¸‹è½½ä¸å®‰è£…â€ä»¥ä¸‹è½½å¹¶å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„ Google Tag Managerã€‚ä½ è¿˜éœ€è¦ç¡®ä¿å®ƒåœ¨ä½ çš„ç¯å¢ƒä¸­é…ç½®æ­£ç¡®ï¼Œå¹¶åœ¨ä½ è·Ÿè¸ªçš„é¡µé¢çš„å¤´éƒ¨æ­£ç¡®è®¾ç½®ã€‚å¦‚æœä½ ä½¿ç”¨çš„æ˜¯ä¸ä¸‹è½½ç‰ˆæœ¬ä¸åŒçš„
    Google Tag Manager ç‰ˆæœ¬ï¼Œä½ éœ€è¦å®‰è£… Google Tag Manager ç‰ˆæœ¬ 1.5 æˆ–æ›´é«˜ç‰ˆæœ¬ã€‚ä¾‹å¦‚ |'
- en: '| Judgment | Both assistants provide potential reasons why the userâ€™s code
    might not be working and offer solutions. However, Assistant Aâ€™s response is more
    relevant and directly addresses the userâ€™s question. Assistant A suggests that
    the userâ€™s event might be defined before the link has been added to the DOM, and
    provides a code snippet to resolve this issue. On the other hand, Assistant Bâ€™s
    response is less specific and doesnâ€™t provide a direct solution to the userâ€™s
    problem. Therefore, Assistant Aâ€™s response is more helpful and accurate. |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| Judgment | ä¸¤ä¸ªåŠ©æ‰‹éƒ½æä¾›äº†ç”¨æˆ·ä»£ç å¯èƒ½æ— æ³•å·¥ä½œçš„æ½œåœ¨åŸå› å¹¶æå‡ºäº†è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼ŒåŠ©æ‰‹ A çš„å›åº”æ›´ç›¸å…³ï¼Œç›´æ¥è§£å†³äº†ç”¨æˆ·çš„é—®é¢˜ã€‚åŠ©æ‰‹ A
    å»ºè®®ç”¨æˆ·çš„äº‹ä»¶å¯èƒ½åœ¨é“¾æ¥è¢«æ·»åŠ åˆ° DOM ä¹‹å‰å°±å·²å®šä¹‰ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªä»£ç ç‰‡æ®µæ¥è§£å†³æ­¤é—®é¢˜ã€‚å¦ä¸€æ–¹é¢ï¼ŒåŠ©æ‰‹ B çš„å›åº”ä¸å¤Ÿå…·ä½“ï¼Œæ²¡æœ‰æä¾›ç›´æ¥è§£å†³ç”¨æˆ·é—®é¢˜çš„æ–¹æ¡ˆã€‚å› æ­¤ï¼ŒåŠ©æ‰‹
    A çš„å›åº”æ›´æœ‰å¸®åŠ©ä¸”æ›´å‡†ç¡®ã€‚ |'
- en: Table 13. Sample responses to a prompt from the test set of the H4 Stack Exchange
    Preferences Dataset. GPT-4 chooses Mallows-$\theta$ on Anthropic-HH dataset.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨13. H4 Stack Exchange Preferences Dataset æµ‹è¯•é›†æç¤ºçš„ç¤ºä¾‹å›åº”ã€‚GPT-4 åœ¨ Anthropic-HH æ•°æ®é›†ä¸Šé€‰æ‹©äº†
    Mallows-$\theta$ã€‚
- en: '| Prompt | Iâ€™m attempting to get a (in my opinion) very simple scenario working
    with Google Tag Manager and Analytics, and running into problems. Now we would
    like to track events using custom JavaScript, via the . For example: $(â€™aâ€™).click(function()
    dataLayer.push(â€™eventâ€™: â€™linkclickedtestâ€™); );  This just isnâ€™t working,
    at all. If I watch Chromeâ€™s console/network tabs, no data seems to get sent to
    Google. Iâ€™ve also installed for Chrome and it doesnâ€™t show the event firing either.
    I must be missing something simple. Any guidance would be appreciated. Thanks!
    |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| Prompt | æˆ‘æ­£åœ¨å°è¯•è®©ä¸€ä¸ªï¼ˆåœ¨æˆ‘çœ‹æ¥ï¼‰éå¸¸ç®€å•çš„åœºæ™¯ä¸ Google Tag Manager å’Œ Analytics ä¸€èµ·å·¥ä½œï¼Œä½†é‡åˆ°äº†é—®é¢˜ã€‚ç°åœ¨æˆ‘ä»¬å¸Œæœ›é€šè¿‡è‡ªå®šä¹‰
    JavaScript è·Ÿè¸ªäº‹ä»¶ï¼Œä¾‹å¦‚ï¼š$(â€™aâ€™).click(function() dataLayer.push(â€™eventâ€™: â€™linkclickedtestâ€™);
    );  è¿™æ ¹æœ¬ä¸èµ·ä½œç”¨ã€‚å¦‚æœæˆ‘æŸ¥çœ‹ Chrome çš„æ§åˆ¶å°/ç½‘ç»œé€‰é¡¹å¡ï¼Œä¼¼ä¹æ²¡æœ‰æ•°æ®å‘é€åˆ° Googleã€‚æˆ‘ä¹Ÿä¸º Chrome å®‰è£…äº†æ‰©å±•ï¼Œä½†å®ƒä¹Ÿæ²¡æœ‰æ˜¾ç¤ºäº‹ä»¶è§¦å‘ã€‚æˆ‘ä¸€å®šé—æ¼äº†ä¸€äº›ç®€å•çš„ä¸œè¥¿ã€‚ä»»ä½•æŒ‡å¯¼å°†ä¸èƒœæ„Ÿæ¿€ã€‚è°¢è°¢ï¼'
- en: '| Mallows-$\phi$-DPO | There are a few possibilities here: - You are defining
    the event before the link has even been added to the DOM, so your code will never
    run. Also, check whether your code is running at all or not, that is, whether
    you see any result in the Chrome console. - Your selector is incorrect in the
    code you shared here. - The code is not firing at all on the click event of the
    link because analytics script is not being run at all in the browser. If this
    is the case, you could verify that a click event is actually created from the
    element before trying to push the dataLayer. Have you checked these things? If
    you find out why the code runs on your computer but not on the computer of the
    person you are testing on, can you test it on their computer and tell us what
    happens? If you are just trying to make sure your code works from the example
    you gave above, Iâ€™m afraid Iâ€™m not sure what other details to go over. Iâ€™m |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| Mallows-$\phi$-DPO | è¿™é‡Œæœ‰å‡ ç§å¯èƒ½æ€§ï¼š - ä½ åœ¨é“¾æ¥æ·»åŠ åˆ° DOM ä¹‹å‰å°±å®šä¹‰äº†äº‹ä»¶ï¼Œæ‰€ä»¥ä½ çš„ä»£ç æ°¸è¿œä¸ä¼šè¿è¡Œã€‚æ­¤å¤–ï¼Œæ£€æŸ¥ä¸€ä¸‹ä½ çš„ä»£ç æ˜¯å¦çœŸçš„åœ¨è¿è¡Œï¼Œå³æ˜¯å¦åœ¨
    Chrome æ§åˆ¶å°ä¸­çœ‹åˆ°ä»»ä½•ç»“æœã€‚ - ä½ åœ¨è¿™é‡Œå…±äº«çš„ä»£ç ä¸­çš„é€‰æ‹©å™¨ä¸æ­£ç¡®ã€‚ - ä»£ç åœ¨é“¾æ¥ç‚¹å‡»äº‹ä»¶ä¸Šæ ¹æœ¬æ²¡æœ‰è§¦å‘ï¼Œå› ä¸ºåˆ†æè„šæœ¬æ ¹æœ¬æ²¡æœ‰åœ¨æµè§ˆå™¨ä¸­è¿è¡Œã€‚å¦‚æœæ˜¯è¿™ç§æƒ…å†µï¼Œä½ å¯ä»¥éªŒè¯åœ¨å°è¯•æ¨é€
    dataLayer ä¹‹å‰ï¼Œå…ƒç´ æ˜¯å¦ç¡®å®åˆ›å»ºäº†ç‚¹å‡»äº‹ä»¶ã€‚ä½ æ£€æŸ¥è¿‡è¿™äº›é—®é¢˜å—ï¼Ÿå¦‚æœä½ å‘ç°ä»£ç åœ¨ä½ çš„ç”µè„‘ä¸Šè¿è¡Œä½†åœ¨ä½ æµ‹è¯•çš„äººçš„ç”µè„‘ä¸Šæ²¡æœ‰è¿è¡Œï¼Œå¯ä»¥åœ¨ä»–ä»¬çš„ç”µè„‘ä¸Šæµ‹è¯•ä¸€ä¸‹å¹¶å‘Šè¯‰æˆ‘ä»¬å‘ç”Ÿäº†ä»€ä¹ˆå—ï¼Ÿå¦‚æœä½ åªæ˜¯æƒ³ç¡®ä¿ä½ çš„ä»£ç ä»ä½ ç»™å‡ºçš„ç¤ºä¾‹ä¸­å·¥ä½œï¼Œé‚£ä¹ˆæˆ‘ä¸ç¡®å®šè¿˜éœ€è¦è®¨è®ºå“ªäº›ç»†èŠ‚ã€‚
    |'
- en: '| BT-DPO | There are a few possibilities here: Youâ€™re using the latest version
    of Google Tag Manager, and you havenâ€™t configured it correctly. Youâ€™re using custom
    JavaScript in a page other than the one being tracked. Youâ€™re using analytics
    tags in other parts of your code, which are interfering with your tracking code.
    For instance, if you were using  tags at the start of , those would
    interfere with Google Tag Manager script. To resolve issue 1 here, youâ€™ll need
    to update the version of Google Tag Manager youâ€™re using. In the Google Tag Manager
    Settings page, click "Download & Install" to download and install the latest version
    of the Google Tag Manager. Youâ€™ll also need to make sure itâ€™s configured correctly
    in your environment, and also set it up correctly in the header of the page youâ€™re
    tracking. If youâ€™re using a different version of Google Tag Manager than the one
    you downloaded, youâ€™ll need to install Google Tag Manager version 1.5 or higher.
    For example |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| BT-DPO | è¿™é‡Œæœ‰å‡ ç§å¯èƒ½æ€§ï¼šä½ ä½¿ç”¨çš„æ˜¯æœ€æ–°ç‰ˆæœ¬çš„ Google Tag Managerï¼Œä½†æ²¡æœ‰æ­£ç¡®é…ç½®ã€‚ä½ åœ¨è·Ÿè¸ªé¡µé¢ä¹‹å¤–çš„é¡µé¢ä¸Šä½¿ç”¨äº†è‡ªå®šä¹‰
    JavaScriptã€‚ä½ åœ¨ä»£ç çš„å…¶ä»–éƒ¨åˆ†ä½¿ç”¨äº†åˆ†ææ ‡ç­¾ï¼Œè¿™äº›æ ‡ç­¾å¹²æ‰°äº†ä½ çš„è·Ÿè¸ªä»£ç ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ åœ¨  å¼€å¤´ä½¿ç”¨äº†  æ ‡ç­¾ï¼Œè¿™äº›æ ‡ç­¾ä¼šå¹²æ‰°
    Google Tag Manager è„šæœ¬ã€‚è¦è§£å†³ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œä½ éœ€è¦æ›´æ–°ä½ ä½¿ç”¨çš„ Google Tag Manager ç‰ˆæœ¬ã€‚åœ¨ Google Tag Manager
    è®¾ç½®é¡µé¢ï¼Œç‚¹å‡»â€œä¸‹è½½ä¸å®‰è£…â€ä»¥ä¸‹è½½å’Œå®‰è£…æœ€æ–°ç‰ˆæœ¬çš„ Google Tag Managerã€‚ä½ è¿˜éœ€è¦ç¡®ä¿å®ƒåœ¨ä½ çš„ç¯å¢ƒä¸­é…ç½®æ­£ç¡®ï¼Œå¹¶ä¸”åœ¨ä½ è·Ÿè¸ªçš„é¡µé¢çš„å¤´éƒ¨ä¸­æ­£ç¡®è®¾ç½®ã€‚å¦‚æœä½ ä½¿ç”¨çš„æ˜¯ä¸ä¸‹è½½ç‰ˆæœ¬ä¸åŒçš„
    Google Tag Manager ç‰ˆæœ¬ï¼Œä½ éœ€è¦å®‰è£… Google Tag Manager 1.5 æˆ–æ›´é«˜ç‰ˆæœ¬ã€‚ä¾‹å¦‚ |'
- en: '| Judgment | Both assistants provide useful suggestions for troubleshooting
    the userâ€™s issue with Google Tag Manager and Analytics. However, Assistant Aâ€™s
    response is more directly related to the userâ€™s specific code example and the
    issues that might be causing it not to work, such as the timing of the event definition,
    the accuracy of the selector, and the firing of the analytics script. On the other
    hand, Assistant Bâ€™s response is more general and focuses on potential issues with
    the Google Tag Manager version and configuration, which may not be directly related
    to the userâ€™s problem. Therefore, Assistant Aâ€™s response is more helpful and relevant
    to the userâ€™s question. |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| åˆ¤å®š | ä¸¤ä½åŠ©æ‰‹éƒ½æä¾›äº†æœ‰ç”¨çš„å»ºè®®æ¥è§£å†³ç”¨æˆ·åœ¨ Google Tag Manager å’Œ Analytics ä¸­çš„é—®é¢˜ã€‚ç„¶è€Œï¼ŒåŠ©æ‰‹ A çš„å›åº”ä¸ç”¨æˆ·çš„å…·ä½“ä»£ç ç¤ºä¾‹åŠå¯èƒ½å¯¼è‡´å…¶æ— æ³•å·¥ä½œçš„åŸå› ï¼ˆå¦‚äº‹ä»¶å®šä¹‰çš„æ—¶æœºã€é€‰æ‹©å™¨çš„å‡†ç¡®æ€§ã€ä»¥åŠåˆ†æè„šæœ¬çš„è§¦å‘ï¼‰ç›´æ¥ç›¸å…³ã€‚å¦ä¸€æ–¹é¢ï¼ŒåŠ©æ‰‹
    B çš„å›åº”æ›´ä¸ºä¸€èˆ¬ï¼Œå…³æ³¨äº Google Tag Manager ç‰ˆæœ¬å’Œé…ç½®çš„æ½œåœ¨é—®é¢˜ï¼Œè¿™å¯èƒ½ä¸ç”¨æˆ·çš„é—®é¢˜ä¸ç›´æ¥ç›¸å…³ã€‚å› æ­¤ï¼ŒåŠ©æ‰‹ A çš„å›åº”å¯¹ç”¨æˆ·çš„é—®é¢˜æ›´æœ‰å¸®åŠ©å’Œç›¸å…³ã€‚
    |'
- en: Table 14. Sample responses to a prompt from the test set of the H4 Stack Exchange
    Preferences Dataset. GPT-4 chooses BT-DPO over Mallows-$\theta$ on Anthropic-HH
    dataset.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 14. ä» H4 Stack Exchange Preferences æ•°æ®é›†çš„æµ‹è¯•é›†ä¸­çš„æç¤ºçš„ç¤ºä¾‹å›åº”ã€‚GPT-4 åœ¨ Anthropic-HH
    æ•°æ®é›†ä¸Šé€‰æ‹©äº† BT-DPO è€Œé Mallows-$\theta$ã€‚
- en: '| Prompt | I have an Ajax function which takes some time to respond, due to
    large file. How do I interrupt the response function if I want to stop the Ajax
    request? $("#startAjaxButton").click(function(){ $.ajax({ url:"ajaxRequest.txt",
    success:function(result){ $("#div1").html(result); } }); });  |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| æç¤º | æˆ‘æœ‰ä¸€ä¸ª Ajax å‡½æ•°ï¼Œç”±äºæ–‡ä»¶è¾ƒå¤§ï¼Œå®ƒéœ€è¦ä¸€äº›æ—¶é—´æ‰èƒ½å“åº”ã€‚å¦‚æœæˆ‘æƒ³ä¸­æ–­ Ajax è¯·æ±‚ï¼Œåº”è¯¥å¦‚ä½•ä¸­æ–­å“åº”å‡½æ•°ï¼Ÿ $("#startAjaxButton").click(function(){
    $.ajax({ url:"ajaxRequest.txt", success:function(result){ $("#div1").html(result);
    } }); });  |'
- en: '| Mallows-$\theta$-DPO | That code should pause the Ajax request until you
    click the â€œStartâ€ button. Another approach is to use â€œsetTimeoutâ€ to have JavaScript
    keep the Ajax request active until itâ€™s finished. setTimeout($.ajax(..), 5000);
    Note: If you want to continue on executing the Ajax request after youâ€™ve stopped
    the request, youâ€™ll need to â€œun-set-timeoutâ€ the request. â€œsetTimeout(function(),
    0)â€ will un-set the timeout and start the Ajax request again. |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| Mallows-$\theta$-DPO | è¿™æ®µä»£ç åº”è¯¥ä¼šæš‚åœ Ajax è¯·æ±‚ï¼Œç›´åˆ°ä½ ç‚¹å‡»â€œå¼€å§‹â€æŒ‰é’®ã€‚å¦ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨â€œsetTimeoutâ€è®©
    JavaScript ä¿æŒ Ajax è¯·æ±‚ç›´åˆ°å®ƒå®Œæˆã€‚ setTimeout($.ajax(..), 5000); æ³¨æ„ï¼šå¦‚æœä½ åœ¨åœæ­¢è¯·æ±‚åè¿˜æƒ³ç»§ç»­æ‰§è¡Œ Ajax
    è¯·æ±‚ï¼Œä½ éœ€è¦â€œå–æ¶ˆè®¾ç½®è¶…æ—¶â€è¯·æ±‚ã€‚â€œsetTimeout(function(), 0)â€å°†å–æ¶ˆè®¾ç½®è¶…æ—¶å¹¶é‡æ–°å¯åŠ¨ Ajax è¯·æ±‚ã€‚ |'
- en: '| BT-DPO | That code should pause the Ajax request until you click the â€œStartâ€
    button. Another approach is to use â€œsetTimeoutâ€ to have JavaScript keep the Ajax
    request active until itâ€™s finished. setTimeout($.ajax(..), 5000); Note: If you
    want to interrupt a long request for which you have not waited, I would recommend
    using the â€œsetTimeoutâ€ function to run the Ajax before the long request process
    has finished. To interrupt the request, use the â€œclearTimeoutâ€ function. As an
    alternative, you can also just use JavaScriptâ€™s default wait loop (â€œsetTimeoutâ€
    and â€œclearTimeoutâ€) without having to wait for a specific event to trigger the
    function. |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| BT-DPO | è¿™æ®µä»£ç åº”è¯¥ä¼šæš‚åœ Ajax è¯·æ±‚ï¼Œç›´åˆ°ä½ ç‚¹å‡»â€œå¼€å§‹â€æŒ‰é’®ã€‚å¦ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨â€œsetTimeoutâ€è®© JavaScript ä¿æŒ
    Ajax è¯·æ±‚ç›´åˆ°å®ƒå®Œæˆã€‚ setTimeout($.ajax(..), 5000); æ³¨æ„ï¼šå¦‚æœä½ æƒ³ä¸­æ–­ä¸€ä¸ªä½ æ²¡æœ‰ç­‰å¾…çš„é•¿è¯·æ±‚ï¼Œå»ºè®®ä½¿ç”¨â€œsetTimeoutâ€å‡½æ•°åœ¨é•¿è¯·æ±‚å¤„ç†å®Œæˆä¹‹å‰è¿è¡Œ
    Ajax è¯·æ±‚ã€‚è¦ä¸­æ–­è¯·æ±‚ï¼Œä½¿ç”¨â€œclearTimeoutâ€å‡½æ•°ã€‚ä½œä¸ºæ›¿ä»£æ–¹æ¡ˆï¼Œä½ ä¹Ÿå¯ä»¥åªä½¿ç”¨ JavaScript çš„é»˜è®¤ç­‰å¾…å¾ªç¯ï¼ˆâ€œsetTimeoutâ€å’Œâ€œclearTimeoutâ€ï¼‰ï¼Œæ— éœ€ç­‰å¾…ç‰¹å®šäº‹ä»¶è§¦å‘å‡½æ•°ã€‚
    |'
- en: '| Judgment | Both Assistant A and Assistant B provided similar responses initially,
    suggesting the use of "setTimeout" to keep the Ajax request active until itâ€™s
    finished. However, Assistant B went a step further by explaining how to interrupt
    a long request using the "clearTimeout" function. This directly addresses the
    userâ€™s question about how to interrupt the response function if they want to stop
    the Ajax request. Therefore, Assistant Bâ€™s response is more helpful and relevant
    to the userâ€™s question. |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| åˆ¤æ–­ | åŠ©æ‰‹ A å’ŒåŠ©æ‰‹ B æœ€åˆéƒ½æä¾›äº†ç±»ä¼¼çš„å“åº”ï¼Œå»ºè®®ä½¿ç”¨â€œsetTimeoutâ€ä¿æŒ Ajax è¯·æ±‚ç›´åˆ°å®ƒå®Œæˆã€‚ç„¶è€Œï¼ŒåŠ©æ‰‹ B è¿›ä¸€æ­¥è§£é‡Šäº†å¦‚ä½•ä½¿ç”¨â€œclearTimeoutâ€å‡½æ•°ä¸­æ–­é•¿è¯·æ±‚ã€‚è¿™ç›´æ¥è§£å†³äº†ç”¨æˆ·å…³äºå¦‚ä½•ä¸­æ–­å“åº”å‡½æ•°çš„é—®é¢˜ã€‚å› æ­¤ï¼ŒåŠ©æ‰‹
    B çš„å›ç­”å¯¹ç”¨æˆ·çš„é—®é¢˜æ›´æœ‰å¸®åŠ©å’Œç›¸å…³ã€‚ |'
- en: Table 15. Sample responses to a prompt from the test set of the H4 Stack Exchange
    Preferences Dataset. GPT-4 chooses Mallows-$\phi$ on Anthropic-HH dataset.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 15. æ¥è‡ª H4 Stack Exchange åå¥½æ•°æ®é›†æµ‹è¯•é›†çš„æç¤ºæ ·æœ¬å“åº”ã€‚GPT-4 åœ¨ Anthropic-HH æ•°æ®é›†ä¸Šé€‰æ‹©äº† Mallows-$\phi$ã€‚
- en: '| Prompt | I have an Ajax function which takes some time to respond, due to
    large file. How do I interrupt the response function if I want to stop the Ajax
    request? $("#startAjaxButton").click(function(){ $.ajax({ url:"ajaxRequest.txt",
    success:function(result){ $("#div1").html(result); } }); });  |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| æç¤º | æˆ‘æœ‰ä¸€ä¸ª Ajax å‡½æ•°ï¼Œç”±äºæ–‡ä»¶è¾ƒå¤§ï¼Œå®ƒéœ€è¦ä¸€äº›æ—¶é—´æ‰èƒ½å“åº”ã€‚å¦‚æœæˆ‘æƒ³ä¸­æ–­ Ajax è¯·æ±‚ï¼Œåº”è¯¥å¦‚ä½•ä¸­æ–­å“åº”å‡½æ•°ï¼Ÿ $("#startAjaxButton").click(function(){
    $.ajax({ url:"ajaxRequest.txt", success:function(result){ $("#div1").html(result);
    } }); });  |'
- en: '| Mallows-$\phi$-DPO | That code should pause the Ajax request until you click
    the â€œStartâ€ button. Another approach is to use â€œsetTimeoutâ€ to have JavaScript
    keep the Ajax request active until itâ€™s finished. setTimeout($.ajax(..), 5000);
    Note: If you want to interrupt a long request for which you have not waited a
    long time, you may need to perform a "clearTimeout" on your variable holding the
    time to wait, because the code in the function will not execute until after that
    time has passed. You can do this using: clearTimeout($("#id").data("timeOut"));
    //remove the timeout reference from your element $("#id").data("timeOut", null);
    //and clear all timeout references from your element |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| Mallows-$\phi$-DPO | è¿™æ®µä»£ç åº”è¯¥ä¼šæš‚åœ Ajax è¯·æ±‚ï¼Œç›´åˆ°ä½ ç‚¹å‡»â€œå¼€å§‹â€æŒ‰é’®ã€‚å¦ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨â€œsetTimeoutâ€è®©
    JavaScript ä¿æŒ Ajax è¯·æ±‚æ´»åŠ¨ï¼Œç›´åˆ°å®ƒå®Œæˆã€‚`setTimeout($.ajax(..), 5000);` æ³¨æ„ï¼šå¦‚æœä½ æƒ³ä¸­æ–­ä¸€ä¸ªä½ è¿˜æ²¡æœ‰ç­‰å¾…å¤ªä¹…çš„é•¿è¯·æ±‚ï¼Œä½ å¯èƒ½éœ€è¦å¯¹ä¿å­˜ç­‰å¾…æ—¶é—´çš„å˜é‡æ‰§è¡Œâ€œclearTimeoutâ€ï¼Œå› ä¸ºå‡½æ•°ä¸­çš„ä»£ç ä¸ä¼šåœ¨æ—¶é—´è¿‡å»ä¹‹å‰æ‰§è¡Œã€‚ä½ å¯ä»¥è¿™æ ·åšï¼š`clearTimeout($("#id").data("timeOut"));`
    //ä»ä½ çš„å…ƒç´ ä¸­ç§»é™¤è¶…æ—¶å¼•ç”¨ `$("#id").data("timeOut", null);` //å¹¶æ¸…é™¤ä½ å…ƒç´ ä¸­çš„æ‰€æœ‰è¶…æ—¶å¼•ç”¨ |'
- en: '| BT-DPO | That code should pause the Ajax request until you click the â€œStartâ€
    button. Another approach is to use â€œsetTimeoutâ€ to have JavaScript keep the Ajax
    request active until itâ€™s finished. setTimeout($.ajax(..), 5000); Note: If you
    want to interrupt a long request for which you have not waited, I would recommend
    using the â€œsetTimeoutâ€ function to run the Ajax before the long request process
    has finished. To interrupt the request, use the â€œclearTimeoutâ€ function. As an
    alternative, you can also just use JavaScriptâ€™s default wait loop (â€œsetTimeoutâ€
    and â€œclearTimeoutâ€) without having to wait for a specific event to trigger the
    function. |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| BT-DPO | è¿™æ®µä»£ç åº”è¯¥ä¼šæš‚åœ Ajax è¯·æ±‚ï¼Œç›´åˆ°ä½ ç‚¹å‡»â€œå¼€å§‹â€æŒ‰é’®ã€‚å¦ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨â€œsetTimeoutâ€è®© JavaScript ä¿æŒ
    Ajax è¯·æ±‚æ´»åŠ¨ï¼Œç›´åˆ°å®ƒå®Œæˆã€‚`setTimeout($.ajax(..), 5000);` æ³¨æ„ï¼šå¦‚æœä½ æƒ³ä¸­æ–­ä¸€ä¸ªé•¿æ—¶é—´çš„è¯·æ±‚ï¼Œè€Œä½ è¿˜æ²¡æœ‰ç­‰å¾…å¤ªä¹…ï¼Œæˆ‘å»ºè®®ä½¿ç”¨â€œsetTimeoutâ€å‡½æ•°åœ¨é•¿è¯·æ±‚å¤„ç†å®Œæˆä¹‹å‰è¿è¡Œ
    Ajaxã€‚è¦ä¸­æ–­è¯·æ±‚ï¼Œä½¿ç”¨â€œclearTimeoutâ€å‡½æ•°ã€‚ä½œä¸ºæ›¿ä»£ï¼Œä½ è¿˜å¯ä»¥ä»…ä»…ä½¿ç”¨ JavaScript çš„é»˜è®¤ç­‰å¾…å¾ªç¯ï¼ˆâ€œsetTimeoutâ€å’Œâ€œclearTimeoutâ€ï¼‰ï¼Œè€Œä¸å¿…ç­‰å¾…ç‰¹å®šäº‹ä»¶è§¦å‘å‡½æ•°ã€‚
    |'
- en: '| Judgment | Both Assistant A and Assistant B provided similar responses, suggesting
    the use of â€œsetTimeoutâ€ to keep the Ajax request active until itâ€™s finished. However,
    Assistant Aâ€™s response is more accurate and detailed. Assistant A correctly explains
    that to interrupt a long request, you need to perform a â€œclearTimeoutâ€ on your
    variable holding the time to wait. Assistant A also provides the code to do this.
    On the other hand, Assistant Bâ€™s explanation of using â€œsetTimeoutâ€ to run the
    Ajax before the long request process has finished is not clear and could be misleading.
    Therefore, Assistant Aâ€™s response is more helpful and accurate. |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| Judgment | åŠ©æ‰‹ A å’ŒåŠ©æ‰‹ B éƒ½æä¾›äº†ç±»ä¼¼çš„å›ç­”ï¼Œå»ºè®®ä½¿ç”¨â€œsetTimeoutâ€æ¥ä¿æŒ Ajax è¯·æ±‚æ´»åŠ¨ï¼Œç›´åˆ°å®ƒå®Œæˆã€‚ç„¶è€Œï¼ŒåŠ©æ‰‹
    A çš„å›ç­”æ›´å‡†ç¡®å’Œè¯¦ç»†ã€‚åŠ©æ‰‹ A æ­£ç¡®è§£é‡Šäº†ä¸­æ–­é•¿è¯·æ±‚æ—¶éœ€è¦å¯¹ä¿å­˜ç­‰å¾…æ—¶é—´çš„å˜é‡æ‰§è¡Œâ€œclearTimeoutâ€ã€‚åŠ©æ‰‹ A è¿˜æä¾›äº†æ‰§è¡Œæ­¤æ“ä½œçš„ä»£ç ã€‚å¦ä¸€æ–¹é¢ï¼ŒåŠ©æ‰‹
    B å¯¹äºä½¿ç”¨â€œsetTimeoutâ€åœ¨é•¿è¯·æ±‚å¤„ç†å®Œæˆä¹‹å‰è¿è¡Œ Ajax çš„è§£é‡Šä¸å¤Ÿæ¸…æ™°ï¼Œå¯èƒ½ä¼šè¯¯å¯¼ã€‚å› æ­¤ï¼ŒåŠ©æ‰‹ A çš„å›ç­”æ›´æœ‰å¸®åŠ©å’Œå‡†ç¡®ã€‚ |'
