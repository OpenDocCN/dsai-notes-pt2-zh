- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:39:23'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs for
    Financial Sentiment Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2312.08725](https://ar5iv.labs.arxiv.org/html/2312.08725)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Sorouralsadat Fatemi [sfatem6@uic.edu](mailto:sfatem6@uic.edu) University of
    Illinois at ChicagoUSA  and  Yuheng Hu [yuhenghu@uic.edu](mailto:yuhenghu@uic.edu)
    University of Illinois at ChicagoUSA(2018)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Financial sentiment analysis plays a crucial role in uncovering latent patterns
    and detecting emerging trends, enabling individuals to make well-informed decisions
    that may yield substantial advantages within the constantly changing realm of
    finance. Recently, Large Language Models (LLMs) have demonstrated their effectiveness
    in diverse domains, showcasing remarkable capabilities even in zero-shot and few-shot
    in-context learning for various Natural Language Processing (NLP) tasks. Nevertheless,
    their potential and applicability in the context of financial sentiment analysis
    have not been thoroughly explored yet. To bridge this gap, we employ two approaches:
    in-context learning (with a focus on gpt-3.5-turbo model) and fine-tuning LLMs
    on a finance-domain dataset. Given the computational costs associated with fine-tuning
    LLMs with large parameter sizes, our focus lies on smaller LLMs, spanning from
    250M to 3B parameters for fine-tuning. We then compare the performances with state-of-the-art
    results to evaluate their effectiveness in the finance-domain.'
  prefs: []
  type: TYPE_NORMAL
- en: Our results demonstrate that fine-tuned smaller LLMs can achieve comparable
    performance to state-of-the-art fine-tuned LLMs, even with models having fewer
    parameters and a smaller training dataset. Additionally, the zero-shot and one-shot
    performance of LLMs produces comparable results with fine-tuned smaller LLMs and
    state-of-the-art outcomes. Furthermore, our analysis demonstrates that there is
    no observed enhancement in performance for finance-domain sentiment analysis when
    the number of shots for in-context learning is increased.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentiment Analysis, Finance, Large Language Model, Natural Language Processing^†^†copyright:
    acmcopyright^†^†journalyear: 2018^†^†doi: XXXXXXX.XXXXXXX^†^†conference: Make
    sure to enter the correct conference title from your rights confirmation emai;
    November 27–29, 2018; Woodstock, NY^†^†price: 15.00^†^†isbn: 978-1-4503-XXXX-X/18/06^†^†ccs:
    Computing methodologies Natural language processing'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The performance of various Large Language Models (LLMs) such as GPT3 (Brown
    et al., [2020a](#bib.bib2)), LLaMA (Touvron et al., [2023](#bib.bib20)), and T5
    (Raffel et al., [2020](#bib.bib18)) has been remarkable across a diverse set of
    Natural Language Processing (NLP) tasks. Sentiment analysis is an NLP task that
    utilizes advancements in language modeling to achieve enhanced outcomes. LLMs
    are trained on a wide range of corpora, enabling them to understand language patterns
    and perform tasks using zero-shot or few-shot prompting without explicit supervised
    training (Yang et al., [2023a](#bib.bib26); Brown et al., [2020b](#bib.bib3)).
    Therefore, there exists considerable potential for the application of LLMs in
    sentiment analysis tasks, particularly in the financial domain where a limited
    availability of annotated data.
  prefs: []
  type: TYPE_NORMAL
- en: Recent studies have highlighted the promising performance of LLMs in various
    NLP tasks, particularly in the zero-shot and few-shot settings. Notably, GPT-3
    has shown competitive performance and, in some cases, even outperformed state-of-the-art
    models in few-shot in-context learning (Brown et al., [2020b](#bib.bib3); Zhong
    et al., [2023](#bib.bib29)). However, when it comes to sentiment analysis tasks,
    some studies have explored the zero-shot and in-context learning capabilities
    of LLMs and found that, in certain instances, in-context learning may not achieve
    comparable performance compared to fine-tuned and state-of-the-art models (Liu
    et al., [2022](#bib.bib11); Wang et al., [2023](#bib.bib21)). Therefore, it becomes
    crucial to investigate the zero-shot and few-shot in-context learning capabilities
    of LLMs in the financial domain, as it encompasses specific jargon and unique
    linguistic characteristics that necessitate special consideration. The aforementioned
    aspect has yet to be thoroughly examined in prior research, rendering it an important
    domain for further investigation.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, a number of studies indicate that in-context learning can yield
    inferior performance compared to fine-tuning (Brown et al., [2020b](#bib.bib3);
    Liu et al., [2022](#bib.bib11)). To delve further into this, we extend our experiments
    by fine-tuning LLMs on a finance-domain dataset and evaluate their performance
    on various financial datasets. For this purpose, we focus on a range of smaller
    language models known as FLAN-T5 models, with parameter sizes varying from 770
    million to 3 billion parameters. Our goal is to determine whether their performance
    is comparable to state-of-the-art fine-tuned LLMs with larger parameter sizes.
    To comprehensively assess the performance of LLMs in financial sentiment analysis,
    we conduct comparisons between the fine-tuned models and their counterparts in
    zero-shot and few-shot settings. In summary, our study involves conducting an
    empirical examination of the zero-shot and few-shot learning abilities of LLMs.
    In addition, we extend our investigation by performing fine-tuning on three Flan-T5
    models (base, large, and xl) using finance-specific data. Additionally, we compare
    the results with state-of-the-art models to assess their performance comprehensively.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. related work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sentiment analysis in the financial domain has gained importance in NLP due
    to its impact on the stock prediction task. It plays a crucial role in providing
    valuable insights for investors’ decision-making (Mishev et al., [2020](#bib.bib16)).
    previous literature explores diverse methodologies for conducting financial sentiment
    analysis, including lexicon-based techniques (Chen et al., [2018](#bib.bib4);
    Loughran and McDonald, [2011](#bib.bib14)). However, such approaches often struggle
    to capture the semantic nuances arising from specific word sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, the advent of pre-trained language models, particularly models
    like BERT have revolutionized the field of NLP, including sentiment analysis (Brown
    et al., [2020b](#bib.bib3)). They leverage large-scale datasets and self-supervised
    learning to develop a robust understanding of context. This contextual awareness
    allows them to capture the subtle nuances and idiomatic expressions in sentiment-rich
    text (Devlin et al., [2018](#bib.bib8)). Although these models are trained on
    general domain corpora like news articles and Wikipedia, they lack specific knowledge
    in the finance domain. To address this limitation, researchers have undertaken
    further fine-tuning of BERT specifically using financial text, leading to promising
    outcomes in financial sentiment classification. Nevertheless, it is crucial to
    acknowledge that the fine-tuning procedure for language models such as BERT requires
    a substantial quantity of financial data and computational resources (Brown et al.,
    [2020a](#bib.bib2); Radford et al., [2019](#bib.bib17)). In a specific study,
    BERT performed further pre-training by utilizing a financial communication corpus
    consisting of 4.9 billion finance-domain tokens (Radford et al., [2019](#bib.bib17)).
  prefs: []
  type: TYPE_NORMAL
- en: This problem has been mitigated by the most recent advancements in NLP through
    the utilization of LLMs. These models are trained on vast amounts of textual data,
    spanning various domains and comprising hundreds of millions to billions of words,
    surpassing the scale of previous language models. In addition to leveraging larger
    amounts of pre-training data, large language models (LLMs) integrate various training
    techniques, including instruction tuning and reinforcement learning from human
    feedback (RLHF) (Wei et al., [2022a](#bib.bib22); Christiano et al., [2017](#bib.bib5)).
    This combination enables LLMs to achieve impressive performance in zero-shot or
    few-shot learning scenarios, occasionally surpassing the state-of-the-art benchmarks
    (Brown et al., [2020b](#bib.bib3)).
  prefs: []
  type: TYPE_NORMAL
- en: In light of these advancements, efforts have been made to develop financial-focused
    LLMs. One notable example is BloombergGPT, a 50 billion parameter language model
    specifically trained on a diverse range of financial data (Wu et al., [2023](#bib.bib24)).
    Although, it demonstrated impressive performance in sentiment analysis, its closed-source
    nature limits its use in the research community. To address this, a new study
    proposed an open-source alternative called FinGPT, fine-tuned on extensive financial
    data from various sources (Yang et al., [2023b](#bib.bib25)). FinGPT achieved
    comparable performance in financial sentiment analysis to BloombergGPT, with lower
    adaptation costs and computational requirements.
  prefs: []
  type: TYPE_NORMAL
- en: In a recent study, researchers introduced an instruction-tuned LLaMA-7B model,
    Instruct-FinGPT-7B, designed to overcome challenges faced by LLMs in accurately
    interpreting numerical values and comprehending financial context. The evaluation
    results showed that this model outperformed state-of-the-art methods in financial
    sentiment analysis, underscoring the promising potential of LLMs in this domain
    (Zhang et al., [2023b](#bib.bib27)). However, the focus of the present study is
    on fine-tuning smaller LLMs to achieve comparable performance while minimizing
    computational and memory resources.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, there have been notable efforts to explore the zero-shot and in-context
    learning capabilities of LLMs in sentiment analysis tasks (Zhang et al., [2023a](#bib.bib28);
    Wang et al., [2023](#bib.bib21)). Results indicate that LLMs, like ChatGPT, exhibit
    robust zero-shot performance, even comparable to fine-tuned models like T5-large.
    However, they may not perform as well in few-shot learning, particularly with
    an increasing number of shots, especially on certain datasets like Twitter and
    MR (Zhang et al., [2023a](#bib.bib28)). On the other hand, a separate study suggests
    that long-tail domains could benefit from few-shot prompting (Wang et al., [2023](#bib.bib21)).
    In light of the varying outcomes observed in few-shot learning when applied to
    different sentiment analysis datasets, we aim to conduct a comprehensive examination
    to evaluate the effectiveness of zero-shot and few-shot learning methods for sentiment
    analysis in the finance-domain.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Method and Experimental setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we outline the methodologies and datasets employed to investigate
    the objectives of our study.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Zero-shot and Few-shot Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this investigation, we explore the zero-shot and few-shot learning capabilities
    of LLMs by conducting sentiment analysis inference without any specific training
    or modifying LLM parameters (Li and Liang, [2021](#bib.bib10)). We use three models
    from Flan-T5 with different parameters, namely Flan-T5-Base (250M), Flan-T5-Large
    (780M), and Flan-T5-XL (3B), to study the impact of model size on zero-shot and
    few-shot learning performance. Additionally, we utilize the ChatGPT (gpt-3.5-turbo)
    model from OpenAI, known for its proficiency and cost-effectiveness within the
    GPT-3.5 family. To ensure consistency in our evaluations, we set the temperature
    parameter to zero, resulting in deterministic predictions. The performance of
    Flan-T5 models and ChatGPT is assessed under these conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'zero-shot learning: The model is provided with only task name, task definition
    and label space, which serves as a set of options that enable the model to generate
    its response accordingly.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'few-shot learning: The model is exposed to task definitions and input-output
    pairs containing K randomly selected labeled samples per class. We evaluated the
    few-shot learning capabilities of LLMs across three k-shot settings: 1-shot, 5-shot,
    and 10-shot. In each setting, we sampled K examples for each sentiment label.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.2\. Fine-tuning LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For fine-tuning, we take three Flan-T5 models: Flan-T5 base (250M), Flan-T5-large
    (780M), and Flan-T5-xl (3B parameters). FLAN-T5 is an open-source finetuned version
    of Google’s T5 model with instruct-finetuning (Chung et al., [2022](#bib.bib6)).
    They instruction-finetune T5 model which is a decoder-encoder model on a collection
    of data sources with a variety of instruction template types such as on different
    T5 model sizes. The Flan-T5 series models have indeed demonstrated remarkable
    performance compared to T5 on specific benchmarks. For example, Flan-T5-XL, with
    only 3B parameters, achieved an impressive Massive Multi-task Language Understanding
    (MMLU) score of 52.4%, surpassing the score of GPT-3 with 175B parameters by 8.5%
    (Chung et al., [2022](#bib.bib6)).'
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the authors evaluate Flan instruction tuning as an intermediate
    step before single target fine-tuning, aiming to determine whether Flan-T5 could
    serve as a superior starting checkpoint for subsequent fine-tuning. The results
    indicate that employing Flan-T5 as a starting checkpoint offers an additional
    advantage in terms of training efficiency. Additionally, during single target
    fine-tuning, Flan-T5 converges much faster than T5 and achieves higher accuracies
    (e.g., increased accuracy by 7.8% in the RTE task). These findings strongly suggest
    that Flan-T5 is an excellent choice for further fine-tuning in our sentiment analysis
    task (Longpre et al., [2023](#bib.bib13)).
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Fine-tuning Procedure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fine-tuning pre-trained language models is a widely used technique to adapt
    LLMs for specific tasks by training them on task-specific data. However, this
    process becomes computationally intensive as it requires updating all pre-trained
    model parameters. (Hu et al., [2021](#bib.bib9)). To address this challenge, Microsoft
    researchers introduced a solution known as Low-Rank-Adaption (LoRA). LoRA introduces
    pairs of rank-decomposition weight matrices into the existing weights. During
    fine-tuning, only these newly added weights are trained, while the pre-trained
    model weights remain unchanged (Hu et al., [2021](#bib.bib9)). This method was
    further enhanced by the QLoRA technique, which involves backpropagating gradients
    through a frozen, 4-bit quantized pre-trained language model into LoRA (Dettmers
    et al., [2023](#bib.bib7)). By integrating QLoRA, the fine-tuning process becomes
    more memory-efficient and computationally faster, while outperforming other efficient
    fine-tuning techniques (Dettmers et al., [2023](#bib.bib7)).
  prefs: []
  type: TYPE_NORMAL
- en: Given that we are dealing with LLMs with 250M, 780M, and 3B parameters, we utilize
    the QLoRA method for fine-tuning. This approach, which offers enhanced memory
    efficiency and faster computation, is well-suited for optimizing the fine-tuning
    process for models of such sizes. Table 1 displays the reduced number of trainable
    parameters for each model.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. performance evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we assess the efficacy of fine-tuning smaller LLMs and in-context
    learning of ChatGPT (GPT-3.5-turbo) and Flan-T5 models for financial sentiment
    analysis task. We compare the results with state-of-the-art approaches from previous
    studies, including FinBert and Instruct-FinGPT. In order to assess the effectiveness
    of each model, we report accuracy and F1-Macro for sentiment analysis tasks including
    fin-tuned models and in-context learning settings.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The dataset used for fine-tuning Flan-T5 models is Twitter Financial News Sentiment
    (Twitter Train), which comprises tweets related to financial topics and is accessible
    through HuggingFace. The training dataset consists of 9540 samples, with each
    sample labeled as Positive, Negative, or Neutral based on the sentiment expressed
    in the tweets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Testing dataset consists of two datasets (all datasets are available through
    Hugging Face):'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Twitter financial news sentiment validation (TFSN): The dataset comprises 2,390
    samples of an annotated corpus of finance-related tweets. Each sample is labeled
    as Positive, Negative, or Neutral.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Financial PhraseBank (FPB): The dataset consists of samples randomly extracted
    from financial news articles annotated by a team of 16 domain experts. Additionally,
    the dataset includes information on the agreement levels among the annotators
    for each sentence. We select the sentences with a 50% agreement level between
    annotators which consists of 4845 financial news and their sentiment label of
    positive, negative, or neutral (Malo et al., [2014](#bib.bib15)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We utilize the entire training dataset for fine-tuning. Subsequently, we perform
    inference for fine-tuned model, and in-context settings on both test datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Model Training Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For each Flan-T5 model (base, large, and xl), we follow the same fine-tuning
    procedure with the same hyperparameters. To compress the pretrained language models,
    we load the Flan-T5 models in a 4-bit format. To further reduce memory requirements
    during fine-tuning, we employ LoRa with an attention dimension (r) of 8, an alpha
    parameter of 32, and a dropout probability of 0.05, resulting in a reduction in
    the number of trainable parameters, as illustrated in Table [1](#S4.T1 "Table
    1 ‣ 4.2\. Model Training Details ‣ 4\. performance evaluation ‣ A Comparative
    Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs for Financial Sentiment
    Analysis").
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | All Parameters | Trainable Parameters |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Flan-T5-Base | 248M | 0.88M |'
  prefs: []
  type: TYPE_TB
- en: '| Flan-T5-Large | 785M | 2.36M |'
  prefs: []
  type: TYPE_TB
- en: '| Flan-T5-XL | 2.85B | 4.72M |'
  prefs: []
  type: TYPE_TB
- en: Table 1\. Number of trainable parameters after using QLoRA.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we perform fine-tuning over 3 epochs with a learning rate of 1e-4 and
    a maximum input text length of 256 tokens. To prevent CUDA Out-of-Memory errors,
    we set the batch size to fit into memory automatically through exponential decay.
    We conduct fine-tuning using one A100 GPU, which leads to the following total
    training times for each model: 28 minutes for the base model, 54 minutes for the
    large model, and 65 minutes for the XL model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/edbaaba537c893c1abd12bb72678d50c.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Zero-shot performance of Flan-T5 models and ChatGPT with and without
    label description in the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Metric | FinBert | Instruct-Llama-7B |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Fine-tuned-Flan-T5 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Base &#124; Large &#124; XL &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ChatGPT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0-shot &#124; 1-shot &#124; 5-shot &#124; 10-shot &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| FPB |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Acc. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; F1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; – &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; – &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.758 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.739 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.769 &#124; 0.793 &#124; 0.807 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.726 &#124; 0.759 &#124; 0.780 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.727 &#124; 0.791 &#124; 0.795 &#124; 0.779 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.680 &#124; 0.774 &#124; 0.794 &#124; 0.783 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| TFSN |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Acc. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; F1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.725 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.668 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.881 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.842 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.874 &#124; 0.894 &#124; 0.903 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.838 &#124; 0.867 &#124; 0.878 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.821 &#124; 0.823 &#124; 0.768 &#124; 0.724 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.818 &#124; 0.819 &#124; 0.775 &#124; 0.732 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Table 2\. Performance comparison between fine-tuned Flan-T5 models, in-context
    learning of ChatGPT, and state-of-the-art models. Instruct-FinGPT-7B (Zhang et al.,
    [2023b](#bib.bib27)) results are taken from its respective papers. FinBERT results
    for the FPB dataset are not reported due to its use as the training set. The best
    results are in bold.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Benchmark Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we present outcomes obtained from previous studies that have
    demonstrated superior results.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'FinBERT: This model is a variant of BERT, pre-trained on a financial text corpus
    comprising 1.8M news articles from Reuters TRC2 dataset. For inference, the model
    was accessed via the HuggingFace API pipeline. The sentences are initially tokenized
    and then fed into FinBERT for inference. FinBERT generates sentiment analysis
    outcomes for each textual input, classifying them as positive, negative, or neutral.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Instruct-FinGPT-7B: The model is fine-tuned on LLaMA-7B model, employing the
    instruction tuning dataset. For evaluation purposes, we rely on the results reported
    in the paper that originally introduced Instruct-FinGPT-7B (Zhang et al., [2023b](#bib.bib27)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.4\. Prompt Design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The effectiveness and coherence of prompt formats can vary based on the training
    methodology and data used during the training process. In our experiments, we
    started with a straightforward prompt for zero-shot inference, as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You are an AI language model trained to detect the sentiment of each sentence
    for stock prediction. Analyze the following sentence and determine if the sentiment
    is: positive or negative or neutral. Return only a single word, either Positive
    or Negative or Neutral.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Subsequently, we introduced label descriptions along with various prompt formats¹¹1Examples
    of prompt formats can be found at the following link:[https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api),
    such as ””” to separate the instruction and context, to assess whether they could
    enhance the model’s performance. The example of zero-shot and one-shot prompt
    is shown in Figure [1](#S4.F1 "Figure 1 ‣ 4.2\. Model Training Details ‣ 4\. performance
    evaluation ‣ A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of
    LLMs for Financial Sentiment Analysis").
  prefs: []
  type: TYPE_NORMAL
- en: We evaluated the performance of all three Flan-T5 models and the ChatGPT model
    using zero-shot inference. The results, shown in Figure 1, indicate higher accuracy
    across most models with the prompt containing label descriptions compared to the
    prompt without label descriptions. Therefore, we adopted the prompt format shown
    in Table [4](#A1.T4 "Table 4 ‣ Appendix A Appendices ‣ A Comparative Analysis
    of Fine-Tuned LLMs and Few-Shot Learning of LLMs for Financial Sentiment Analysis")
    for all zero-shot and few-shot settings inference.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model |'
  prefs: []
  type: TYPE_TB
- en: '&#124; TFSN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Acc. &#124; F1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; FPB &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Acc. &#124; F1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Base &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0-Shot &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1-Shot &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 5-Shot &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 10-Shot &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.665 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.694 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.707 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.726 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.593 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.624 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.643 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.659 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.741 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.726 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.734 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.764 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.729 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.727 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.742 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.742 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Large &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0-Shot &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1-Shot &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 5-Shot &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 10-Shot &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.687 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.742 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.765 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.778 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.647 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.688 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.701 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.704 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.797 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.809 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.813 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.804 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.798 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.807 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.810 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.801 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; XL &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0-Shot &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1-Shot &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 5-Shot &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 10-Shot &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.683 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.791 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.760 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.771 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.678 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.753 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.730 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.729 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.815 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.847 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.832 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.780 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.823 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.836 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.833 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.796 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3\. Few-shot performance of Flan-T5 models on TFSB and FPB datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/336eb77d0424b36ed36a5ad039ad14c2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. Few-shot prompting results on TFSN dataset compared with FinBERT,
    Fine-tuned-Flan-T5, and Instruct-FinGPT results. Utilizing the best-performing
    fine-tuned Flan-T5-XL model for ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e2daf5d1f655eb96255f45e7f4478b1f.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. Few-shot prompting results on FPB dataset compared with Fine-tuned-Flan-T5
    and Instruct-FinGPT results. Utilizing the best-performing fine-tuned Flan-T5-XL
    model for ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Evaluation Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned in section [2](#S2 "2\. related work ‣ A Comparative Analysis of
    Fine-Tuned LLMs and Few-Shot Learning of LLMs for Financial Sentiment Analysis"),
    our main focus is on evaluating the zero-shot and few-shot learning performance
    of LLMs, including both smaller and larger models in terms of parameter size,
    in comparison to fine-tuned smaller LLMs. The results of Flan-T5 fine-tuned models
    on both TFSN and FPB datasets, along with the benchmark models, and the zero-shot
    and few-shot performance of ChatGPT, are presented in Table [2](#S4.T2 "Table
    2 ‣ 4.2\. Model Training Details ‣ 4\. performance evaluation ‣ A Comparative
    Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs for Financial Sentiment
    Analysis"). Additionally, the results of the zero-shot and few-shot performance
    of Flan-T5 models are shown in Table [3](#S4.T3 "Table 3 ‣ 4.4\. Prompt Design
    ‣ 4\. performance evaluation ‣ A Comparative Analysis of Fine-Tuned LLMs and Few-Shot
    Learning of LLMs for Financial Sentiment Analysis").
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuned LLMs Results. As the results indicate in Figure [2](#S4.F2 "Figure
    2 ‣ 4.4\. Prompt Design ‣ 4\. performance evaluation ‣ A Comparative Analysis
    of Fine-Tuned LLMs and Few-Shot Learning of LLMs for Financial Sentiment Analysis")
    and Table [2](#S4.T2 "Table 2 ‣ 4.2\. Model Training Details ‣ 4\. performance
    evaluation ‣ A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of
    LLMs for Financial Sentiment Analysis"), on the TFSN dataset, the performance
    of fine-tuned Flan-T5 models is comparable to the state-of-the-art model, Instruct-FinGPT
    ,and significatly outperforms FinBert results. It is noteworthy that we achieved
    this level of performance with significantly fewer computational resources (1
    A100 GPU compared to 8 A100 GPUs) and a comparable or even shorter training time
    by utilizing QLoRA method , especially for the Flan-T5-Base model (28 minutes).
    These findings align with a previous study that highlights the efficiency advantage
    of using Flan-T5 as a starting checkpoint for further fine-tuning, as discussed
    in section [3.2](#S3.SS2 "3.2\. Fine-tuning LLMs ‣ 3\. Method and Experimental
    setup ‣ A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs
    for Financial Sentiment Analysis").
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot Results. As shown in Figure [2](#S4.F2 "Figure 2 ‣ 4.4\. Prompt Design
    ‣ 4\. performance evaluation ‣ A Comparative Analysis of Fine-Tuned LLMs and Few-Shot
    Learning of LLMs for Financial Sentiment Analysis"), the zero-shot learning results
    of Flan-T5 models (Base, Largr, and XL) on the TFSN dataset fall significantly
    behind those of all fine-tuned models (FinBert, Instruct-FinGPT, and all fine-tuned
    Flan-T5 models). Notably, Instruct-FinGPT and fine-tuned Flan-T5 models outperform
    LLMs by a clear margin of roughly 20%. However, the zero-shot performance of ChatGPT
    reaches 82%, surpassing the FinBert model but still remaining inferior to Instruct-FinGPT
    and all fine-tuned Flan-T5 models.
  prefs: []
  type: TYPE_NORMAL
- en: As depicted in Figure [3](#S4.F3 "Figure 3 ‣ 4.4\. Prompt Design ‣ 4\. performance
    evaluation ‣ A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of
    LLMs for Financial Sentiment Analysis") and Table [3](#S4.T3 "Table 3 ‣ 4.4\.
    Prompt Design ‣ 4\. performance evaluation ‣ A Comparative Analysis of Fine-Tuned
    LLMs and Few-Shot Learning of LLMs for Financial Sentiment Analysis"), the zero-shot
    results of all Flan-T5 models on the FPB dataset show more promising outcomes,
    even performing comparably to the corresponding fine-tuned Flan-T5 models. This
    observation aligns with previous research findings (Zhang et al., [2023a](#bib.bib28)).
    Additionally, a noteworthy finding on the FPB dataset is that larger models, with
    a greater number of parameters, tend to outperform the smaller ones in zero-shot
    inference. For instance, when comparing the performance between Flan-T5-Base and
    Flan-T5-XL, there is an increase of roughly 7% in performance.
  prefs: []
  type: TYPE_NORMAL
- en: When comparing the zero-shot performance of Flan-T5 models to ChatGPT, ChatGPT
    appears to be less accurate despite having larger parameters, which contradicts
    the zero-shot inference on the TFSN dataset. This discrepancy in performance can
    be explained by the different language and jargon used in the FPB news headlines
    and the TFSN social media posts. ChatGPT, being trained on a large corpus of social
    media posts, is better equipped to extract the true sentiment from social media
    text, which is reflected in its zero-shot performance on the TFSN dataset. On
    the other hand, smaller LLMs like Flan-T5 models outperform ChatGPT in the zero-shot
    setting on the FPB dataset, possibly due to their large-scale instruction tuning,
    which enhances their reasoning capabilities and allows them to extract sentiment
    from more complex texts.
  prefs: []
  type: TYPE_NORMAL
- en: Few-shot Results. Results comparing few-shot prompting performance with zero-shot
    prompting performance are presented in Figure [2](#S4.F2 "Figure 2 ‣ 4.4\. Prompt
    Design ‣ 4\. performance evaluation ‣ A Comparative Analysis of Fine-Tuned LLMs
    and Few-Shot Learning of LLMs for Financial Sentiment Analysis"), Figure [3](#S4.F3
    "Figure 3 ‣ 4.4\. Prompt Design ‣ 4\. performance evaluation ‣ A Comparative Analysis
    of Fine-Tuned LLMs and Few-Shot Learning of LLMs for Financial Sentiment Analysis"),
    and Table [3](#S4.T3 "Table 3 ‣ 4.4\. Prompt Design ‣ 4\. performance evaluation
    ‣ A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs for
    Financial Sentiment Analysis"). Notably, we observe that 1-shot prompting can
    significantly improve the performance across almost all models and datasets, except
    for Flan-T5-Base (250M), where there is a slight decrease in performance. This
    decrease can be attributed to the impact of a single unrelated example, which
    can have an adverse effect on the smaller LLMs. Interestingly, we find that smaller
    LLMs (Flan-T5) benefit more on average from one demonstration compared to ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: The impact of increasing shots to 5 and 10 exhibited variability across various
    models and datasets. On the TFSN dataset, it was observed that a marginal improvement
    in performance was achieved for both Flan-T5-Base and Flan-T5-Large models with
    an increase in the number of shots. However, the performance of Flan-T5-Large
    and Flan-T5-XL was impeded by the increase in shots. The variation in outcomes
    can be ascribed to the difficulties associated with managing excessively lengthy
    contexts, which have the potential to lead the LLMs misguided, as revealed in
    a new study (Zhang et al., [2023a](#bib.bib28)).
  prefs: []
  type: TYPE_NORMAL
- en: These findings are consistent with a previous study that highlighted the sensitivity
    issue of LLMs when exposed to few-shot examples during inference (Zhang et al.,
    [2023a](#bib.bib28)). In order to mitigate these disparities and obtain consistent
    enhancements in performance, it is imperative to employ more efficacious techniques
    for few-shot learning. For example, a recent study suggested the retrieval of
    examples that possess semantic similarity to a test query sample in order to generate
    its corresponding prompt (Liu et al., [2021](#bib.bib12)). Moreover, the introduction
    of the Chain of Thought (CoT) and Clue And Reasoning Prompting (CARP) methods
    aimed to improve the reasoning capabilities of LLMs. which could be beneficial
    for extracting sentiment from financial corpora (Sun et al., [2023](#bib.bib19);
    Wei et al., [2022b](#bib.bib23)). Our future work will encompass an exploration
    of these approaches
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Conclusion and future directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this study, we extensively compared the performance of fine-tuned LLMs with
    different parameter sizes (ranging from 250M to 3B) and their in-context learning
    capabilities, for financial sentiment analysis. Our experimental results demonstrate
    that fine-tuned LLMs achieved comparable performance to state-of-the-art models
    while utilizing significantly fewer computational resources. The zero-shot and
    one-shot settings performed impressively, especially on the FPB dataset, which
    can be attributed to the corpus used for pre-training the LLMs. Moreover, larger
    LLMs demonstrated better performance in the zero-shot and one-shot settings.
  prefs: []
  type: TYPE_NORMAL
- en: The remarkable results obtained from fine-tuned, zero-shot, and one-shot inferences
    of Flan-T5 models can be attributed to their instruct fine-tuning. However, we
    observed inconsistent performance in the five-shot and ten-shot settings across
    all models and datasets. Notably, increasing the number of shots did not lead
    to improved performance on average. Nevertheless, the models demonstrated reasonable
    performance on both datasets and across all three Flan-T5 models and ChatGPT,
    indicating their potential for financial sentiment analysis considering the scarcity
    of labeled data in finance-domain.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, our study showcases the remarkable capabilities of LLMs, even
    smaller models, in both fine-tuning and in-context learning for financial sentiment
    analysis task. These findings provide valuable insights into potential avenues
    for further investigation in the field of financial sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: For future studies, we plan to assess the new methods of prompt formatting,
    such as CoT and CARP, and prompt selection by retrieving semantically-similar
    examples to the test queries, rather than random sampling for few-shot settings
    (Sun et al., [2023](#bib.bib19); Wei et al., [2022b](#bib.bib23)). The objective
    of this study is to determine the extent to which these methods can consistently
    enhance the performance of LLMs in financial sentiment analysis tasks.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020a) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, et al. 2020a. Language models are few-shot learners. *Advances
    in neural information processing systems* 33 (2020), 1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020b) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, et al. 2020b. Language models are few-shot learners. *Advances
    in neural information processing systems* 33 (2020), 1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2018) Chung-Chi Chen, Hen-Hsen Huang, and Hsin-Hsi Chen. 2018.
    NTUSD-Fin: a market sentiment dictionary for financial social media data applications.
    In *Proceedings of the 1st Financial Narrative Processing Workshop (FNP 2018)*.
    37–43.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Christiano et al. (2017) Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic,
    Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences.
    *Advances in neural information processing systems* 30 (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi
    Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma,
    et al. 2022. Scaling instruction-finetuned language models. *arXiv preprint arXiv:2210.11416*
    (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. *arXiv preprint
    arXiv:2305.14314* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805* (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation
    of large language models. *arXiv preprint arXiv:2106.09685* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li and Liang (2021) Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing
    continuous prompts for generation. *arXiv preprint arXiv:2101.00190* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2022) Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao
    Huang, Mohit Bansal, and Colin A Raffel. 2022. Few-shot parameter-efficient fine-tuning
    is better and cheaper than in-context learning. *Advances in Neural Information
    Processing Systems* 35 (2022), 1950–1965.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2021) Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence
    Carin, and Weizhu Chen. 2021. What Makes Good In-Context Examples for GPT-$3$?
    *arXiv preprint arXiv:2101.06804* (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Longpre et al. (2023) Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won
    Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. 2023. The
    flan collection: Designing data and methods for effective instruction tuning.
    *arXiv preprint arXiv:2301.13688* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loughran and McDonald (2011) Tim Loughran and Bill McDonald. 2011. When is a
    liability not a liability? Textual analysis, dictionaries, and 10-Ks. *The Journal
    of finance* 66, 1 (2011), 35–65.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Malo et al. (2014) Pekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wallenius,
    and Pyry Takala. 2014. Good debt or bad debt: Detecting semantic orientations
    in economic texts. *Journal of the Association for Information Science and Technology*
    65, 4 (2014), 782–796.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mishev et al. (2020) Kostadin Mishev, Ana Gjorgjevikj, Irena Vodenska, Lubomir T
    Chitkushev, and Dimitar Trajanov. 2020. Evaluation of sentiment analysis in finance:
    from lexicons to transformers. *IEEE access* 8 (2020), 131662–131682.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask
    learners. *OpenAI blog* 1, 8 (2019), 9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *The
    Journal of Machine Learning Research* 21, 1 (2020), 5485–5551.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2023) Xiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei Guo, Tianwei
    Zhang, and Guoyin Wang. 2023. Text Classification via Large Language Models. *arXiv
    preprint arXiv:2305.08377* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023) Zengzhi Wang, Qiming Xie, Zixiang Ding, Yi Feng, and Rui
    Xia. 2023. Is ChatGPT a good sentiment analyzer? A preliminary study. *arXiv preprint
    arXiv:2304.04339* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022a) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022a. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems* 35 (2022), 24824–24837.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022b) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022b. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems* 35 (2022), 24824–24837.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2023) Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark
    Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann.
    2023. Bloomberggpt: A large language model for finance. *arXiv preprint arXiv:2303.17564*
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2023b) Hongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. 2023b.
    FinGPT: Open-Source Financial Large Language Models. *arXiv preprint arXiv:2306.06031*
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2023a) Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han,
    Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. 2023a. Harnessing the power
    of llms in practice: A survey on chatgpt and beyond. *arXiv preprint arXiv:2304.13712*
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023b) Boyu Zhang, Hongyang Yang, and Xiao-Yang Liu. 2023b. Instruct-FinGPT:
    Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language
    Models. *arXiv preprint arXiv:2306.12659* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023a) Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, and
    Lidong Bing. 2023a. Sentiment Analysis in the Era of Large Language Models: A
    Reality Check. *arXiv preprint arXiv:2305.15005* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhong et al. (2023) Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and Dacheng
    Tao. 2023. Can chatgpt understand too? a comparative study on chatgpt and fine-tuned
    bert. *arXiv preprint arXiv:2302.10198* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We introduce a zero-shot and 1-shot prompt designed for the financial sentiment
    analysis task, displayed on the subsequent page.
  prefs: []
  type: TYPE_NORMAL
- en: '| Zero-shot prompt |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| You are an AI language model trained to detect the sentiment of sentences
    for stock prediction See below all the possible labels and their descriptions
    |'
  prefs: []
  type: TYPE_TB
- en: '| ””” |'
  prefs: []
  type: TYPE_TB
- en: '| description: Bearish sentiment |'
  prefs: []
  type: TYPE_TB
- en: '| label: Negative |'
  prefs: []
  type: TYPE_TB
- en: '| ””” |'
  prefs: []
  type: TYPE_TB
- en: '| description: neutral sentiment |'
  prefs: []
  type: TYPE_TB
- en: '| label: Neutral |'
  prefs: []
  type: TYPE_TB
- en: '| ””” |'
  prefs: []
  type: TYPE_TB
- en: '| description: Bullish sentiment |'
  prefs: []
  type: TYPE_TB
- en: '| label: Positive |'
  prefs: []
  type: TYPE_TB
- en: '| ””” |'
  prefs: []
  type: TYPE_TB
- en: '| Here is the sentence that needs to be classified |'
  prefs: []
  type: TYPE_TB
- en: '| sentence: {sentence} |'
  prefs: []
  type: TYPE_TB
- en: '| label: |'
  prefs: []
  type: TYPE_TB
- en: '| One-shot Prompt |'
  prefs: []
  type: TYPE_TB
- en: '| You are an AI language model trained to detect the sentiment of sentences
    for stock prediction See below all the possible labels and their descriptions
    |'
  prefs: []
  type: TYPE_TB
- en: '| ””” |'
  prefs: []
  type: TYPE_TB
- en: '| description: Bearish sentiment |'
  prefs: []
  type: TYPE_TB
- en: '| label: Negative |'
  prefs: []
  type: TYPE_TB
- en: '| ””” |'
  prefs: []
  type: TYPE_TB
- en: '| description: neutral sentiment |'
  prefs: []
  type: TYPE_TB
- en: '| label: Neutral |'
  prefs: []
  type: TYPE_TB
- en: '| ””” |'
  prefs: []
  type: TYPE_TB
- en: '| description: Bullish sentiment |'
  prefs: []
  type: TYPE_TB
- en: '| label: Positive |'
  prefs: []
  type: TYPE_TB
- en: '| ””” |'
  prefs: []
  type: TYPE_TB
- en: '| See below a couple of examples |'
  prefs: []
  type: TYPE_TB
- en: '| ””” |'
  prefs: []
  type: TYPE_TB
- en: '| text: Cemex cut at Credit Suisse, J.P. Morgan on weak building outlook |'
  prefs: []
  type: TYPE_TB
- en: '| label: Negative |'
  prefs: []
  type: TYPE_TB
- en: '| ””” |'
  prefs: []
  type: TYPE_TB
- en: '| text: ululemon Falls on Conservative View But Analysts Keep Faith |'
  prefs: []
  type: TYPE_TB
- en: '| label: Neutral |'
  prefs: []
  type: TYPE_TB
- en: '| ””” |'
  prefs: []
  type: TYPE_TB
- en: '| text: Wells Fargo Downgrades Netflix $NFLX to Underperform but sees as a
    takeover target. NFLX could get acquired |'
  prefs: []
  type: TYPE_TB
- en: '| label: Positive |'
  prefs: []
  type: TYPE_TB
- en: '| ””” |'
  prefs: []
  type: TYPE_TB
- en: '| Here is the sentence that needs to be classified |'
  prefs: []
  type: TYPE_TB
- en: '| sentence: {sentence} |'
  prefs: []
  type: TYPE_TB
- en: '| label: |'
  prefs: []
  type: TYPE_TB
- en: Table 4\. Prompts for zero-shot and one-shot settings. In the Five-Shot and
    Ten-Shot settings, 5 and 10 samples were added for each class, respectively.
  prefs: []
  type: TYPE_NORMAL
