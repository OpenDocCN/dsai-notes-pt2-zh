- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†ç±»ï¼šæœªåˆ†ç±»
- en: 'date: 2024-09-08 18:39:41'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¥æœŸï¼š2024-09-08 18:39:41
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task
    Medical Applications'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'MOELoRA: ä¸€ç§åŸºäºMOEçš„å¤šä»»åŠ¡åŒ»å­¦åº”ç”¨å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•'
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2310.18339](https://ar5iv.labs.arxiv.org/html/2310.18339)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2310.18339](https://ar5iv.labs.arxiv.org/html/2310.18339)
- en: Qidong Liu Xiâ€™an Jiaotong University
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: é‚±ä¸œ åˆ˜ è¥¿å®‰äº¤é€šå¤§å­¦
- en: City University of Hong Kong Xiâ€™an, China
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: é¦™æ¸¯åŸå¸‚å¤§å­¦ è¥¿å®‰ï¼Œä¸­å›½
- en: liuqidong@stu.xjtu.edu.cn â€ƒâ€ƒ Xian Wu ğŸ–‚Â Corresponding Authors Jarvis Research
    Center, Tencent YouTu Lab Shenzhen, China
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: liuqidong@stu.xjtu.edu.cn â€ƒâ€ƒ å´æ˜¾ ğŸ–‚Â é€šè®¯ä½œè€… è´¾ç»´æ–¯ç ”ç©¶ä¸­å¿ƒï¼Œè…¾è®¯ä¼˜å›¾å®éªŒå®¤ æ·±åœ³ï¼Œä¸­å›½
- en: kevinxwu@tencent.com â€ƒâ€ƒ Xiangyu Zhao ğŸ–‚ {@IEEEauthorhalign} Yuanshao Zhu
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: kevinxwu@tencent.com â€ƒâ€ƒ èµµå‘å®‡ ğŸ–‚ {@IEEEauthorhalign} æœ±å…ƒç»
- en: Feng Tian City University of Hong Kong Hong Kong, China
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ç”°é”‹ é¦™æ¸¯åŸå¸‚å¤§å­¦ é¦™æ¸¯ï¼Œä¸­å›½
- en: xianzhao@cityu.edu.hk Southern University of Science and Technology
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: xianzhao@cityu.edu.hk å—æ–¹ç§‘æŠ€å¤§å­¦
- en: City University of Hong Kong Shenzhen, China
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: é¦™æ¸¯åŸå¸‚å¤§å­¦ æ·±åœ³ï¼Œä¸­å›½
- en: zhuys2019@mail.sustech.edu.cn Xiaâ€™an Jiaotong University Xiâ€™an, China
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: zhuys2019@mail.sustech.edu.cn è¥¿å®‰äº¤é€šå¤§å­¦ è¥¿å®‰ï¼Œä¸­å›½
- en: fengtian@mail.xjtu.edu.cn â€ƒâ€ƒ Derong Xu
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: fengtian@mail.xjtu.edu.cn â€ƒâ€ƒ å¾·è£ å¾
- en: Yefeng Zheng University of Science and Technology of China
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: éƒ‘ä¸šé”‹ ä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦
- en: City University of Hong Kong Hefei, China
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: é¦™æ¸¯åŸå¸‚å¤§å­¦ åˆè‚¥ï¼Œä¸­å›½
- en: derongxu@mail.ustc.edu.cn Jarvis Research Center, Tencent YouTu Lab Shenzhen,
    China
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: derongxu@mail.ustc.edu.cn è´¾ç»´æ–¯ç ”ç©¶ä¸­å¿ƒï¼Œè…¾è®¯ä¼˜å›¾å®éªŒå®¤ æ·±åœ³ï¼Œä¸­å›½
- en: yefengzheng@tencent.com
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: yefengzheng@tencent.com
- en: Abstract
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: The recent surge in the field of Large Language Models (LLMs) has gained significant
    attention in numerous domains. In order to tailor an LLM to a specific domain
    such as a web-based healthcare system, fine-tuning with domain knowledge is necessary.
    However, two issues arise during fine-tuning LLMs for medical applications. The
    first is the problem of task variety, where there are numerous distinct tasks
    in real-world medical scenarios. This diversity often results in suboptimal fine-tuning
    due to data imbalance and seesawing problems. Additionally, the high cost of fine-tuning
    can be prohibitive, impeding the application of LLMs. The large number of parameters
    in LLMs results in enormous time and computational consumption during fine-tuning,
    which is difficult to justify. To address these two issues simultaneously, we
    propose a novel parameter-efficient fine-tuning framework for multi-task medical
    applications called MOELoRA. The framework aims to capitalize on the benefits
    of both MOE for multi-task learning and LoRA for parameter-efficient fine-tuning.
    To unify MOE and LoRA, we devise multiple experts as the trainable parameters,
    where each expert consists of a pair of low-rank matrices to maintain a small
    number of trainable parameters. Additionally, we propose a task-motivated gate
    function for all MOELoRA layers that can regulate the contributions of each expert
    and generate distinct parameters for various tasks. To validate the effectiveness
    and practicality of the proposed method, we conducted comprehensive experiments
    on a public multi-task Chinese medical dataset. The experimental results demonstrate
    that MOELoRA outperforms existing parameter-efficient fine-tuning methods. The
    implementation is available online for convenient reproduction of our experimentsÂ¹Â¹1https://github.com/liuqidong07/MOELoRA-peft.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢†åŸŸçš„è¿…çŒ›å‘å±•åœ¨ä¼—å¤šé¢†åŸŸå¼•èµ·äº†æ˜¾è‘—å…³æ³¨ã€‚ä¸ºäº†å°†LLMé‡èº«å®šåˆ¶ä¸ºç‰¹å®šé¢†åŸŸï¼Œå¦‚åŸºäºç½‘ç»œçš„åŒ»ç–—ç³»ç»Ÿï¼Œéœ€è¦è¿›è¡Œé¢†åŸŸçŸ¥è¯†çš„å¾®è°ƒã€‚ç„¶è€Œï¼Œåœ¨åŒ»å­¦åº”ç”¨ä¸­å¾®è°ƒLLMsæ—¶ä¼šå‡ºç°ä¸¤ä¸ªé—®é¢˜ã€‚ç¬¬ä¸€ä¸ªæ˜¯ä»»åŠ¡å¤šæ ·æ€§é—®é¢˜ï¼Œå®é™…åŒ»å­¦åœºæ™¯ä¸­æœ‰è®¸å¤šä¸åŒçš„ä»»åŠ¡ã€‚è¿™ç§å¤šæ ·æ€§å¸¸å¯¼è‡´æ•°æ®ä¸å¹³è¡¡å’Œè··è··æ¿é—®é¢˜ï¼Œä»è€Œå¯¼è‡´å¾®è°ƒæ•ˆæœä¸ä½³ã€‚æ­¤å¤–ï¼Œå¾®è°ƒçš„é«˜æˆæœ¬å¯èƒ½ä¼šæˆä¸ºé˜»ç¢LLMåº”ç”¨çš„å› ç´ ã€‚LLMä¸­çš„å¤§é‡å‚æ•°åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ä¼šæ¶ˆè€—å¤§é‡æ—¶é—´å’Œè®¡ç®—èµ„æºï¼Œè¿™å¾ˆéš¾å¾—åˆ°åˆç†
    justificationã€‚ä¸ºåŒæ—¶è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ¡†æ¶ï¼Œç”¨äºå¤šä»»åŠ¡åŒ»å­¦åº”ç”¨ï¼Œç§°ä¸ºMOELoRAã€‚è¯¥æ¡†æ¶æ—¨åœ¨åˆ©ç”¨MOEçš„å¤šä»»åŠ¡å­¦ä¹ ä¼˜åŠ¿å’ŒLoRAçš„å‚æ•°é«˜æ•ˆå¾®è°ƒä¼˜åŠ¿ã€‚ä¸ºäº†ç»Ÿä¸€MOEå’ŒLoRAï¼Œæˆ‘ä»¬è®¾è®¡äº†å¤šä¸ªä¸“å®¶ä½œä¸ºå¯è®­ç»ƒå‚æ•°ï¼Œæ¯ä¸ªä¸“å®¶ç”±ä¸€å¯¹ä½ç§©çŸ©é˜µç»„æˆï¼Œä»¥ä¿æŒå°‘é‡çš„å¯è®­ç»ƒå‚æ•°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸ºæ‰€æœ‰MOELoRAå±‚æå‡ºäº†ä¸€ä¸ªä»»åŠ¡é©±åŠ¨çš„é—¨æ§å‡½æ•°ï¼Œè¯¥å‡½æ•°å¯ä»¥è°ƒèŠ‚æ¯ä¸ªä¸“å®¶çš„è´¡çŒ®ï¼Œå¹¶ä¸ºå„ç§ä»»åŠ¡ç”Ÿæˆä¸åŒçš„å‚æ•°ã€‚ä¸ºäº†éªŒè¯æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ï¼Œæˆ‘ä»¬åœ¨ä¸€ä¸ªå…¬å¼€çš„å¤šä»»åŠ¡ä¸­æ–‡åŒ»å­¦æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢å®éªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMOELoRAä¼˜äºç°æœ‰çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ã€‚å®ç°ä»£ç å¯åœ¨çº¿è·å–ï¼Œä»¥æ–¹ä¾¿é‡ç°æˆ‘ä»¬çš„å®éªŒÂ¹Â¹1https://github.com/liuqidong07/MOELoRA-peftã€‚
- en: 'Index Terms:'
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å…³é”®è¯ï¼š
- en: Medical Applications; Large Language Model; Multi-task Learning;
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: åŒ»å­¦åº”ç”¨ï¼›å¤§å‹è¯­è¨€æ¨¡å‹ï¼›å¤šä»»åŠ¡å­¦ä¹ ï¼›
- en: I Introduction
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I å¼•è¨€
- en: Due to the impressive capabilities in language understanding and generation,
    the Large Language Models (LLMs) such as ChatGPTÂ [[1](#bib.bib1)] and ChatGLMÂ [[2](#bib.bib2)]
    have gained extensive interest from both academia and industry. Many efforts have
    been devoted to investigating the potential applications of LLMs across various
    domainsÂ [[3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5)]. One particularly suitable
    domain for LLMs is the medical domain, as the application of LLMs can benefit
    both patients and doctors. For patients, the LLM-enabled online Chatbot can provide
    convenient access to medical knowledge; For doctors, the LLM-enabled Clinical
    Decision Supporting Systems (CDSS) can relieve their heavy workload and improve
    diagnosis efficiency.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºåœ¨è¯­è¨€ç†è§£å’Œç”Ÿæˆæ–¹é¢çš„å“è¶Šèƒ½åŠ›ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ChatGPT[[1](#bib.bib1)]å’ŒChatGLM[[2](#bib.bib2)]å—åˆ°äº†å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œçš„å¹¿æ³›å…³æ³¨ã€‚è®¸å¤šç ”ç©¶å·²è‡´åŠ›äºæ¢ç´¢LLMsåœ¨å„ä¸ªé¢†åŸŸçš„æ½œåœ¨åº”ç”¨[[3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5)]ã€‚LLMsç‰¹åˆ«é€‚åˆçš„ä¸€ä¸ªé¢†åŸŸæ˜¯åŒ»ç–—é¢†åŸŸï¼Œå› ä¸ºLLMsçš„åº”ç”¨å¯ä»¥åŒæ—¶æƒ åŠæ‚£è€…å’ŒåŒ»ç”Ÿã€‚å¯¹äºæ‚£è€…ï¼ŒLLMé©±åŠ¨çš„åœ¨çº¿èŠå¤©æœºå™¨äººå¯ä»¥æä¾›ä¾¿æ·çš„åŒ»ç–—çŸ¥è¯†è·å–ï¼›å¯¹äºåŒ»ç”Ÿï¼ŒLLMé©±åŠ¨çš„ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿï¼ˆCDSSï¼‰å¯ä»¥å‡è½»ä»–ä»¬çš„å·¥ä½œè´Ÿæ‹…ï¼Œæé«˜è¯Šæ–­æ•ˆç‡ã€‚
- en: However, the majority of LLMs are trained for general-purpose and are not customized
    for the medical domain. As a result, the general LLMs often fall short in medical
    tasks due to a lack of specialized medical knowledgeÂ [[6](#bib.bib6)]. To empower
    LLMs with medical capabilities, a straightforward manner is to fine-tune LLMs
    with medical tasks. For large LLMs with more than $100$ billion parameters, they
    are usually closed-source and extremely costly for fine-tuningÂ [[7](#bib.bib7)].
    Therefore, in this paper, we focus on the open-source LLMs and fine-tuning them
    with medical knowledge and clinical tasksÂ [[8](#bib.bib8), [6](#bib.bib6)].
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå¤§å¤šæ•°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯ä¸ºé€šç”¨ç›®çš„è®­ç»ƒçš„ï¼Œå¹¶æœªé’ˆå¯¹åŒ»ç–—é¢†åŸŸè¿›è¡Œå®šåˆ¶ã€‚å› æ­¤ï¼Œç”±äºç¼ºä¹ä¸“ä¸šçš„åŒ»ç–—çŸ¥è¯†ï¼Œä¸€èˆ¬çš„LLMsåœ¨åŒ»ç–—ä»»åŠ¡ä¸­å¾€å¾€è¡¨ç°ä¸ä½³[[6](#bib.bib6)]ã€‚ä¸ºäº†èµ‹äºˆLLMsåŒ»ç–—èƒ½åŠ›ï¼Œä¸€ä¸ªç®€å•çš„æ–¹æ³•æ˜¯å¯¹LLMsè¿›è¡ŒåŒ»ç–—ä»»åŠ¡çš„å¾®è°ƒã€‚å¯¹äºå‚æ•°è¶…è¿‡$100$äº¿çš„å¤§å‹LLMsï¼Œå®ƒä»¬é€šå¸¸æ˜¯é—­æºçš„ï¼Œä¸”å¾®è°ƒæˆæœ¬æé«˜[[7](#bib.bib7)]ã€‚å› æ­¤ï¼Œåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†é‡ç‚¹å…³æ³¨å¼€æºLLMsï¼Œå¹¶ç”¨åŒ»ç–—çŸ¥è¯†å’Œä¸´åºŠä»»åŠ¡å¯¹å…¶è¿›è¡Œå¾®è°ƒ[[8](#bib.bib8),
    [6](#bib.bib6)]ã€‚
- en: 'Fine-tuning LLMs for the medical domain usually involves two primary challenges:
    (i) Task Variety Problem: In real-world clinics, the LLMs can be applied to a
    large range of tasks, like doctor recommendationÂ [[9](#bib.bib9)], diagnosis predictionÂ [[10](#bib.bib10)],
    medicine recommendationÂ [[11](#bib.bib11)], medical named entity recognitionÂ [[12](#bib.bib12),
    [13](#bib.bib13)], clinical report generationÂ [[14](#bib.bib14)] and *etc.* Since
    the input and output of these tasks are quite different, it is difficult to fine-tune
    a unified model for all tasks. Given the diversity of these tasks, fine-tuning
    a single model for each specific task is feasible but demands extensive expertise
    and labor. An integrated multi-task learning framework could potentially address
    this issue. However, much of the existing research on LLMs, as seen in studies
    likeÂ [[8](#bib.bib8), [15](#bib.bib15)], predominantly centers on medical dialogue.
    Such over-attention ignores the variety of tasks, resulting in multi-task fine-tuning
    remains underexplored. (ii) High Tuning Cost: While fine-tuning all model parameters
    was a standard approach during the era of BertÂ [[16](#bib.bib16)], it becomes
    challenging for LLMs due to their sheer size. The vast number of parameters in
    LLMs can lead to prohibitive time and computational expenses in practiceÂ [[17](#bib.bib17)].
    As such, there is an urgent need for parameter efficient fine-tuning methodologies.
    To address these two challenges, the community urgently calls for developing a
    multi-tasking parameter efficient fine-tuning framework for LLM-driven medical
    applications.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºåŒ»å­¦é¢†åŸŸçš„LLMså¾®è°ƒé€šå¸¸æ¶‰åŠä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šï¼ˆiï¼‰ä»»åŠ¡å¤šæ ·æ€§é—®é¢˜ï¼šåœ¨ç°å®ä¸–ç•Œçš„è¯Šæ‰€ä¸­ï¼ŒLLMså¯ä»¥åº”ç”¨äºå¹¿æ³›çš„ä»»åŠ¡ï¼Œä¾‹å¦‚åŒ»ç”Ÿæ¨è[[9](#bib.bib9)]ã€è¯Šæ–­é¢„æµ‹[[10](#bib.bib10)]ã€è¯ç‰©æ¨è[[11](#bib.bib11)]ã€åŒ»å­¦å‘½åå®ä½“è¯†åˆ«[[12](#bib.bib12),
    [13](#bib.bib13)]ã€ä¸´åºŠæŠ¥å‘Šç”Ÿæˆ[[14](#bib.bib14)]ä»¥åŠ*ç­‰ç­‰*ã€‚ç”±äºè¿™äº›ä»»åŠ¡çš„è¾“å…¥å’Œè¾“å‡ºå·®å¼‚å¾ˆå¤§ï¼Œå¾®è°ƒä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹ä»¥é€‚åº”æ‰€æœ‰ä»»åŠ¡å˜å¾—å›°éš¾ã€‚é‰´äºè¿™äº›ä»»åŠ¡çš„å¤šæ ·æ€§ï¼Œä¸ºæ¯ä¸ªç‰¹å®šä»»åŠ¡å¾®è°ƒå•ä¸€æ¨¡å‹æ˜¯å¯è¡Œçš„ï¼Œä½†éœ€è¦å¹¿æ³›çš„ä¸“ä¸šçŸ¥è¯†å’ŒåŠ³åŠ¨ã€‚ä¸€ä¸ªé›†æˆçš„å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶å¯èƒ½æœ‰åŠ©äºè§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMç ”ç©¶å¤§å¤šé›†ä¸­äºåŒ»å­¦å¯¹è¯ï¼Œå¦‚[[8](#bib.bib8),
    [15](#bib.bib15)]æ‰€ç¤ºã€‚è¿™ç§è¿‡åº¦å…³æ³¨å¿½è§†äº†ä»»åŠ¡çš„å¤šæ ·æ€§ï¼Œå¯¼è‡´å¤šä»»åŠ¡å¾®è°ƒä»æœªè¢«å……åˆ†æ¢è®¨ã€‚ï¼ˆiiï¼‰é«˜è°ƒä¼˜æˆæœ¬ï¼šè™½ç„¶åœ¨Bertæ—¶ä»£[[16](#bib.bib16)]å¾®è°ƒæ‰€æœ‰æ¨¡å‹å‚æ•°æ˜¯ä¸€ç§æ ‡å‡†æ–¹æ³•ï¼Œä½†å¯¹äºLLMsè€Œè¨€ï¼Œç”±äºå…¶åºå¤§çš„è§„æ¨¡ï¼Œè¿™ä¸€æ–¹æ³•å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚LLMsä¸­çš„å¤§é‡å‚æ•°å¯èƒ½ä¼šå¯¼è‡´å®è·µä¸­æ—¶é—´å’Œè®¡ç®—è´¹ç”¨é«˜æ˜‚[[17](#bib.bib17)]ã€‚å› æ­¤ï¼Œè¿«åˆ‡éœ€è¦é«˜æ•ˆçš„å‚æ•°å¾®è°ƒæ–¹æ³•ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸¤ä¸ªæŒ‘æˆ˜ï¼Œç¤¾åŒºè¿«åˆ‡å‘¼åå¼€å‘é’ˆå¯¹LLMé©±åŠ¨çš„åŒ»ç–—åº”ç”¨çš„å¤šä»»åŠ¡é«˜æ•ˆå‚æ•°å¾®è°ƒæ¡†æ¶ã€‚
- en: 'For task variety problem, several multi-task learning frameworks have been
    proposedÂ [[18](#bib.bib18), [19](#bib.bib19)]. A standout among these is Mixture-of-Experts
    (MOE)Â [[20](#bib.bib20)], which designs multiple separate experts to learn task-shared
    and -specific knowledge, and integrates a gate function to modulate the contributions
    of each expert. While existing frameworks adeptly consolidate multiple tasks for
    classical neural network architectures, they are primarily compatible with full
    fine-tuning, which is associated with high tuning costs. Correspondingly, the
    emergence of parameter efficient fine-tuning (PEFT) methods (P-TuningÂ [[1](#bib.bib1)],
    LoRAÂ [[21](#bib.bib21)], etc.) has offered a potential solution to the problem
    of high tuning cost. These methods typically tune a limited number of parameters,
    keeping the pre-trained LLM parameters frozen. For instance, LoRAÂ [[21](#bib.bib21)]
    proposes to only train pairs of low-rank matrices for fitting the parameter updates
    of dense layers in LLMs. However, the existing PEFT is limited to fine-tuning
    either multiple sets of parameters for each task separately or a singular set
    across all tasks. Though separate training can fit each task well, this strategy
    is laborious and lacks task-shared knowledge. While fine-tuning a set of parameters
    is feasible, it may hurt performance due to issues such as data imbalance and
    seesaw effectsÂ [[19](#bib.bib19), [22](#bib.bib22)]. For illustration, we analyze
    the data distribution of a Chinese medical dataset, PromptCBLUE, in FigureÂ [1](#S1.F1
    "Figure 1 â€£ I Introduction â€£ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning
    Method for Multi-task Medical Applications"). Our analysis reveals significant
    disparities: while some tasks boast nearly $5,000$. This imbalance can skew the
    uniquely fine-tuned parameters towards tasks with more samples, inadvertently
    undermining the performance on tasks with fewer samples. Therefore, parameter
    efficient fine-tuning of separate parameters for multi-task by a unique training
    process can alleviate both problems simultaneously.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 'é’ˆå¯¹ä»»åŠ¡å¤šæ ·æ€§é—®é¢˜ï¼Œå·²æå‡ºè‹¥å¹²å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶[[18](#bib.bib18), [19](#bib.bib19)]ã€‚å…¶ä¸­ä¸€ä¸ªçªå‡ºçš„æ–¹æ³•æ˜¯Mixture-of-Experts
    (MOE) [[20](#bib.bib20)]ï¼Œè¯¥æ–¹æ³•è®¾è®¡äº†å¤šä¸ªç‹¬ç«‹çš„ä¸“å®¶ä»¥å­¦ä¹ ä»»åŠ¡å…±äº«å’Œç‰¹å®šçš„çŸ¥è¯†ï¼Œå¹¶é›†æˆäº†ä¸€ä¸ªé—¨æ§å‡½æ•°æ¥è°ƒèŠ‚æ¯ä¸ªä¸“å®¶çš„è´¡çŒ®ã€‚å°½ç®¡ç°æœ‰æ¡†æ¶åœ¨ç»å…¸ç¥ç»ç½‘ç»œæ¶æ„ä¸­ç†Ÿç»ƒåœ°æ•´åˆäº†å¤šä¸ªä»»åŠ¡ï¼Œä½†å®ƒä»¬ä¸»è¦å…¼å®¹å®Œå…¨å¾®è°ƒï¼Œè¿™ä¸é«˜è°ƒä¼˜æˆæœ¬ç›¸å…³ã€‚å› æ­¤ï¼Œå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ï¼ˆå¦‚P-Tuning
    [[1](#bib.bib1)]ã€LoRA [[21](#bib.bib21)]ç­‰ï¼‰çš„å‡ºç°ä¸ºé«˜è°ƒä¼˜æˆæœ¬é—®é¢˜æä¾›äº†æ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚è¿™äº›æ–¹æ³•é€šå¸¸åªè°ƒèŠ‚æœ‰é™æ•°é‡çš„å‚æ•°ï¼Œè€Œä¿æŒé¢„è®­ç»ƒLLMå‚æ•°ä¸å˜ã€‚ä¾‹å¦‚ï¼ŒLoRA
    [[21](#bib.bib21)]å»ºè®®ä»…è®­ç»ƒä½ç§©çŸ©é˜µå¯¹ä»¥é€‚åº”LLMä¸­å¯†é›†å±‚çš„å‚æ•°æ›´æ–°ã€‚ç„¶è€Œï¼Œç°æœ‰çš„PEFTæ–¹æ³•ä»…é™äºä¸ºæ¯ä¸ªä»»åŠ¡åˆ†åˆ«å¾®è°ƒå¤šä¸ªå‚æ•°é›†æˆ–åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­å¾®è°ƒå•ä¸€å‚æ•°é›†ã€‚è™½ç„¶å•ç‹¬è®­ç»ƒå¯ä»¥å¾ˆå¥½åœ°é€‚åº”æ¯ä¸ªä»»åŠ¡ï¼Œä½†è¿™ç§ç­–ç•¥æ—¢ç¹çåˆç¼ºä¹ä»»åŠ¡å…±äº«çŸ¥è¯†ã€‚è™½ç„¶å¾®è°ƒä¸€ä¸ªå‚æ•°é›†æ˜¯å¯è¡Œçš„ï¼Œä½†ç”±äºæ•°æ®ä¸å¹³è¡¡å’Œè··è··æ¿æ•ˆåº”[[19](#bib.bib19),
    [22](#bib.bib22)]ç­‰é—®é¢˜ï¼Œå¯èƒ½ä¼šå½±å“æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬åœ¨å›¾[1](#S1.F1 "Figure 1 â€£ I Introduction â€£ MOELoRA:
    An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications")ä¸­åˆ†æäº†ä¸­æ–‡åŒ»å­¦æ•°æ®é›†PromptCBLUEçš„æ•°æ®åˆ†å¸ƒã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†æ˜¾è‘—çš„å·®å¼‚ï¼šè™½ç„¶æŸäº›ä»»åŠ¡çš„æ ·æœ¬é‡æ¥è¿‘$5,000$ï¼Œè¿™ç§ä¸å¹³è¡¡å¯èƒ½ä¼šä½¿å¾—å”¯ä¸€å¾®è°ƒçš„å‚æ•°åå‘äºæ ·æœ¬æ›´å¤šçš„ä»»åŠ¡ï¼Œä»è€Œæ— æ„ä¸­å‰Šå¼±äº†æ ·æœ¬è¾ƒå°‘ä»»åŠ¡çš„æ€§èƒ½ã€‚å› æ­¤ï¼Œé€šè¿‡ç‹¬ç‰¹çš„è®­ç»ƒè¿‡ç¨‹å¯¹å¤šä»»åŠ¡çš„å•ç‹¬å‚æ•°è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒå¯ä»¥åŒæ—¶ç¼“è§£è¿™ä¸¤ä¸ªé—®é¢˜ã€‚'
- en: '![Refer to caption](img/12316866ebbf0cb43a0c2444993760fe.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜æ–‡å­—](img/12316866ebbf0cb43a0c2444993760fe.png)'
- en: 'Figure 1: The illustration for data imbalance problem of various medical tasks.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1ï¼šå„ç§åŒ»å­¦ä»»åŠ¡æ•°æ®ä¸å¹³è¡¡é—®é¢˜çš„ç¤ºæ„å›¾ã€‚
- en: To address the challenges of task variety and high tuning costs, we propose
    a unified parameter efficient fine-tuning framework to learn separate parameters
    for various tasks, dubbed MOELoRA. Our framework follows the basic scheme of LoRA
    for the parameter efficiency, *i.e.,* only fine-tuning small size of parameters
    parallel to the dense layers in LLMs. However, as discussed previously, existing
    unified LoRA fine-tuning faces the challenge of a singular set of parameters across
    all tasks. Thus, in our approach, we first design several experts as the trainable
    part rather than a singular pair of low-rank matrices. On the one hand, inspired
    by MOEÂ [[20](#bib.bib20)], separate experts can help learn task-specific knowledge
    under one unique training process. On the other hand, such design gives the chance
    to produce several distinct sets of parameters. Besides, for parameter efficiency,
    we devise each expert as two low-rank matrices. Then, to learn separate sets of
    parameters for each task, we propose a task-motivated gate function. In specific,
    the gate function absorbs the task identity and outputs corresponding expert weights.
    By the expert weights for one specific task and the parameters of multiple experts,
    we can get the unique updated parameters for this task.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åº”å¯¹ä»»åŠ¡å¤šæ ·æ€§å’Œé«˜è°ƒä¼˜æˆæœ¬çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ¡†æ¶ï¼Œç§°ä¸ºMOELoRAã€‚æˆ‘ä»¬çš„æ¡†æ¶éµå¾ªLoRAåœ¨å‚æ•°é«˜æ•ˆæ€§æ–¹é¢çš„åŸºæœ¬æ–¹æ¡ˆï¼Œ*å³*ï¼Œä»…å¯¹LLMsä¸­å¯†é›†å±‚çš„å°‘é‡å‚æ•°è¿›è¡Œå¾®è°ƒã€‚ç„¶è€Œï¼Œå¦‚å‰æ‰€è¿°ï¼Œç°æœ‰çš„ç»Ÿä¸€LoRAå¾®è°ƒé¢ä¸´æ‰€æœ‰ä»»åŠ¡å…±äº«å•ä¸€å‚æ•°é›†çš„æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œåœ¨æˆ‘ä»¬çš„æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†å¤šä¸ªä¸“å®¶ä½œä¸ºå¯è®­ç»ƒéƒ¨åˆ†ï¼Œè€Œä¸æ˜¯å•ä¸€å¯¹ä½ç§©çŸ©é˜µã€‚ä¸€æ–¹é¢ï¼Œå—MOE
    [[20](#bib.bib20)] çš„å¯å‘ï¼Œç‹¬ç«‹ä¸“å®¶å¯ä»¥åœ¨ä¸€ä¸ªç‹¬ç‰¹çš„è®­ç»ƒè¿‡ç¨‹ä¸­å¸®åŠ©å­¦ä¹ ä»»åŠ¡ç‰¹å®šçš„çŸ¥è¯†ã€‚å¦ä¸€æ–¹é¢ï¼Œè¿™ç§è®¾è®¡æä¾›äº†ç”Ÿæˆå¤šä¸ªä¸åŒå‚æ•°é›†çš„æœºä¼šã€‚æ­¤å¤–ï¼Œä¸ºäº†æé«˜å‚æ•°æ•ˆç‡ï¼Œæˆ‘ä»¬å°†æ¯ä¸ªä¸“å®¶è®¾è®¡ä¸ºä¸¤ä¸ªä½ç§©çŸ©é˜µã€‚ç„¶åï¼Œä¸ºäº†ä¸ºæ¯ä¸ªä»»åŠ¡å­¦ä¹ ç‹¬ç«‹çš„å‚æ•°é›†ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä»»åŠ¡é©±åŠ¨çš„é—¨æ§å‡½æ•°ã€‚å…·ä½“æ¥è¯´ï¼Œé—¨æ§å‡½æ•°å¸æ”¶ä»»åŠ¡èº«ä»½å¹¶è¾“å‡ºç›¸åº”çš„ä¸“å®¶æƒé‡ã€‚é€šè¿‡ä¸ºç‰¹å®šä»»åŠ¡çš„ä¸“å®¶æƒé‡å’Œå¤šä¸ªä¸“å®¶çš„å‚æ•°ï¼Œæˆ‘ä»¬å¯ä»¥è·å¾—è¯¥ä»»åŠ¡çš„ç‹¬ç‰¹æ›´æ–°å‚æ•°ã€‚
- en: 'In summary, the contributions of this paper are as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“æ¥è¯´ï¼Œæœ¬æ–‡çš„è´¡çŒ®å¦‚ä¸‹ï¼š
- en: â€¢
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: We introduce MOELoRA, a novel multi-task PEFT framework that combines the strengths
    of both MOE and LoRA. Additionally, we design a task-motivated gate function to
    facilitate the tuning of distinct parameter sets for each task.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»‹ç»äº†MOELoRAï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„å¤šä»»åŠ¡PEFTæ¡†æ¶ï¼Œç»“åˆäº†MOEå’ŒLoRAçš„ä¼˜ç‚¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä»»åŠ¡é©±åŠ¨çš„é—¨æ§å‡½æ•°ï¼Œä»¥ä¿ƒè¿›ä¸ºæ¯ä¸ªä»»åŠ¡è°ƒæ•´ä¸åŒå‚æ•°é›†ã€‚
- en: â€¢
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: We conduct comprehensive experiments on a public multi-task Chinese medical
    dataset, with the results underscoring the superiority of the proposed MOELoRA
    framework.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨ä¸€ä¸ªå…¬å…±çš„å¤šä»»åŠ¡ä¸­æ–‡åŒ»å­¦æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼Œç»“æœå¼ºè°ƒäº†æ‰€æå‡ºçš„MOELoRAæ¡†æ¶çš„ä¼˜è¶Šæ€§ã€‚
- en: â€¢
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: To our knowledge, this research represents the first endeavor to delve into
    multi-task parameter efficient fine-tuning techniques for LLM-driven medical applications.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™é¡¹ç ”ç©¶ä»£è¡¨äº†é¦–æ¬¡æ·±å…¥æ¢è®¨LLMé©±åŠ¨çš„åŒ»å­¦åº”ç”¨ä¸­çš„å¤šä»»åŠ¡å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯çš„å°è¯•ã€‚
- en: II Preliminary
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II åˆæ­¥
- en: '![Refer to caption](img/fb2a16f93d6c3ab66c1094eaa402ac66.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/fb2a16f93d6c3ab66c1094eaa402ac66.png)'
- en: 'Figure 2: The medical name entity recognition example for illustration of how
    to use LLMs to complete medical tasks.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2ï¼šåŒ»å­¦å‘½åå®ä½“è¯†åˆ«ç¤ºä¾‹ï¼Œè¯´æ˜å¦‚ä½•ä½¿ç”¨LLMså®ŒæˆåŒ»å­¦ä»»åŠ¡ã€‚
- en: In this section, we first briefly introduce how LLMs are adopted for medical
    applications. Then, we give the problem definition of multi-task medical applications.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆç®€è¦ä»‹ç»LLMså¦‚ä½•åº”ç”¨äºåŒ»å­¦é¢†åŸŸã€‚ç„¶åï¼Œæˆ‘ä»¬ç»™å‡ºå¤šä»»åŠ¡åŒ»å­¦åº”ç”¨çš„é—®é¢˜å®šä¹‰ã€‚
- en: II-A LLMs for Medical Applications
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A LLMsåœ¨åŒ»å­¦åº”ç”¨ä¸­çš„ä½œç”¨
- en: 'Intelligent medical systems have become increasingly prevalent in contemporary
    web-based healthcare settings. Numerous studies have sought to standardize medical
    tasks by defining consistent input and output patterns, thereby streamlining the
    model design process. As the example of medical named entity recognition (NER)Â [[12](#bib.bib12),
    [13](#bib.bib13)] illustrated in FigureÂ [2](#S2.F2 "Figure 2 â€£ II Preliminary
    â€£ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task
    Medical Applications"), traditional models typically process medical texts, denoted
    as $I_{M}$. However, the integration of LLMs into medical tasks introduces a distinct
    paradigm. Given that both the input and output of LLMs are typically linguistic
    in nature, there is a necessity to reformulate medical tasks to be compatible
    with LLMs.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ™ºèƒ½åŒ»ç–—ç³»ç»Ÿåœ¨å½“ä»£åŸºäºç½‘ç»œçš„åŒ»ç–—ç¯å¢ƒä¸­å˜å¾—è¶Šæ¥è¶Šæ™®éã€‚è®¸å¤šç ”ç©¶æ—¨åœ¨é€šè¿‡å®šä¹‰ä¸€è‡´çš„è¾“å…¥å’Œè¾“å‡ºæ¨¡å¼æ¥æ ‡å‡†åŒ–åŒ»å­¦ä»»åŠ¡ï¼Œä»è€Œç®€åŒ–æ¨¡å‹è®¾è®¡è¿‡ç¨‹ã€‚å¦‚å›¾[2](#S2.F2
    "Figure 2 â€£ II Preliminary â€£ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning
    Method for Multi-task Medical Applications")æ‰€ç¤ºçš„åŒ»å­¦å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰[[12](#bib.bib12),
    [13](#bib.bib13)]çš„ä¾‹å­ä¸­ï¼Œä¼ ç»Ÿæ¨¡å‹é€šå¸¸å¤„ç†æ ‡è®°ä¸º$I_{M}$çš„åŒ»å­¦æ–‡æœ¬ã€‚ç„¶è€Œï¼Œå°†LLMsé›†æˆåˆ°åŒ»å­¦ä»»åŠ¡ä¸­å¼•å…¥äº†ä¸€ä¸ªä¸åŒçš„èŒƒå¼ã€‚é‰´äºLLMsçš„è¾“å…¥å’Œè¾“å‡ºé€šå¸¸æ˜¯è¯­è¨€æ€§è´¨çš„ï¼Œæœ‰å¿…è¦é‡æ–°æ„å»ºåŒ»å­¦ä»»åŠ¡ä»¥ä¸LLMså…¼å®¹ã€‚'
- en: 'To adapt medical tasks for LLMs, we need to restructure both the input and
    output patterns. Input Modification: We incorporate instruction templates into
    the original medical texts to guide LLMs in executing the relevant tasksÂ [[23](#bib.bib23)].
    Taking medical NER as an example, as depicted in FigureÂ [2](#S2.F2 "Figure 2 â€£
    II Preliminary â€£ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method
    for Multi-task Medical Applications"), we employ the template: Please recognize
    the medical name entity in this sentence: â€œ[Medical Text]â€, where â€œ[Medical Text]â€
    serves as a placeholder for the raw medical text $I_{M}$ and $TP^{A}_{NER}$, respectively.
    With these modifications in place, the process by which LLMs undertake the NER
    task can be described as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¸ºäº†ä½¿åŒ»å­¦ä»»åŠ¡é€‚åº”LLMsï¼Œæˆ‘ä»¬éœ€è¦é‡æ„è¾“å…¥å’Œè¾“å‡ºæ¨¡å¼ã€‚è¾“å…¥ä¿®æ”¹ï¼šæˆ‘ä»¬å°†æŒ‡ä»¤æ¨¡æ¿çº³å…¥åŸå§‹åŒ»å­¦æ–‡æœ¬ä¸­ï¼Œä»¥æŒ‡å¯¼LLMsæ‰§è¡Œç›¸å…³ä»»åŠ¡[[23](#bib.bib23)]ã€‚ä»¥åŒ»å­¦NERä¸ºä¾‹ï¼Œå¦‚å›¾[2](#S2.F2
    "Figure 2 â€£ II Preliminary â€£ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning
    Method for Multi-task Medical Applications")æ‰€ç¤ºï¼Œæˆ‘ä»¬ä½¿ç”¨æ¨¡æ¿ï¼šè¯·è¯†åˆ«å¥å­ä¸­çš„åŒ»å­¦åç§°å®ä½“ï¼šâ€œ[Medical
    Text]â€ï¼Œå…¶ä¸­â€œ[Medical Text]â€ä½œä¸ºåŸå§‹åŒ»å­¦æ–‡æœ¬$I_{M}$å’Œ$TP^{A}_{NER}$çš„å ä½ç¬¦ã€‚è¿›è¡Œè¿™äº›ä¿®æ”¹åï¼ŒLLMsæ‰§è¡ŒNERä»»åŠ¡çš„è¿‡ç¨‹å¯ä»¥æè¿°å¦‚ä¸‹ï¼š'
- en: '|  | $1$2 |  | (1) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: After the task reformulation for LLMs, we can use the purely lingual data to
    fine-tune the foundation large language model, such as LlaMAÂ [[24](#bib.bib24)],
    ChatGLMÂ [[25](#bib.bib25)] and *etc.*. Then, the fine-tuned model completes the
    medical tasks by generating the regulated answers.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¯¹LLMsè¿›è¡Œä»»åŠ¡é‡æ–°æ„å»ºåï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨çº¯è¯­è¨€æ•°æ®æ¥å¾®è°ƒåŸºç¡€çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œå¦‚LlaMA[[24](#bib.bib24)]ã€ChatGLM[[25](#bib.bib25)]å’Œ*ç­‰*ã€‚ç„¶åï¼Œç»è¿‡å¾®è°ƒçš„æ¨¡å‹é€šè¿‡ç”Ÿæˆè§„èŒƒåŒ–çš„ç­”æ¡ˆæ¥å®ŒæˆåŒ»å­¦ä»»åŠ¡ã€‚
- en: II-B Multi-task Fine-tuning
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B å¤šä»»åŠ¡å¾®è°ƒ
- en: As previously mentioned, medical applications often encompass a variety of tasks,
    such as name entity recognition, medical inquiry, etc. Our goal is to fine-tune
    LLMs to gain robust performance for each task and thus can also benefit the whole
    healthcare system. For multi-task fine-tuning, we consider a set of medical tasks
    represented as $\mathbb{T}=\{\mathcal{T}_{1},\ldots,\mathcal{T}_{j},\ldots,\mathcal{T}_{M}\}$
    and $LO$, optimize the parameters $\Phi$.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼ŒåŒ»ç–—åº”ç”¨é€šå¸¸æ¶‰åŠå¤šç§ä»»åŠ¡ï¼Œå¦‚å‘½åå®ä½“è¯†åˆ«ã€åŒ»å­¦æŸ¥è¯¢ç­‰ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¾®è°ƒLLMsï¼Œä»¥ä¾¿åœ¨æ¯ä¸ªä»»åŠ¡ä¸Šè·å¾—ç¨³å¥çš„æ€§èƒ½ï¼Œä»è€Œä¹Ÿèƒ½æƒ åŠæ•´ä¸ªåŒ»ç–—ä¿å¥ç³»ç»Ÿã€‚å¯¹äºå¤šä»»åŠ¡å¾®è°ƒï¼Œæˆ‘ä»¬è€ƒè™‘ä¸€ç»„åŒ»å­¦ä»»åŠ¡è¡¨ç¤ºä¸º$\mathbb{T}=\{\mathcal{T}_{1},\ldots,\mathcal{T}_{j},\ldots,\mathcal{T}_{M}\}$å’Œ$LO$ï¼Œä¼˜åŒ–å‚æ•°$\Phi$ã€‚
- en: 'Since the data from diverse tasks are standardized into a consistent linguistic
    format, we straightforwardly employ the conditional language modeling objectivesÂ [[25](#bib.bib25)]
    for all training instances. Furthermore, with the intent to assimilate shared
    medical knowledge and be free from the labor of adjusting fine-tuning for several
    tasks, data from all tasks are incorporated into the unique optimization process.
    Consequently, the objective function for multi-task fine-tuning can be formulated
    as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæ¥è‡ªä¸åŒä»»åŠ¡çš„æ•°æ®è¢«æ ‡å‡†åŒ–ä¸ºä¸€è‡´çš„è¯­è¨€æ ¼å¼ï¼Œæˆ‘ä»¬ç›´æ¥é‡‡ç”¨æ¡ä»¶è¯­è¨€å»ºæ¨¡ç›®æ ‡[[25](#bib.bib25)]æ¥å¤„ç†æ‰€æœ‰è®­ç»ƒå®ä¾‹ã€‚æ­¤å¤–ï¼Œæ—¨åœ¨å¸æ”¶å…±äº«åŒ»å­¦çŸ¥è¯†å¹¶å…äºè°ƒæ•´å¤šä¸ªä»»åŠ¡çš„å¾®è°ƒå·¥ä½œï¼Œæ‰€æœ‰ä»»åŠ¡çš„æ•°æ®è¢«æ•´åˆåˆ°ç‹¬ç‰¹çš„ä¼˜åŒ–è¿‡ç¨‹ä¸­ã€‚å› æ­¤ï¼Œå¤šä»»åŠ¡å¾®è°ƒçš„ç›®æ ‡å‡½æ•°å¯ä»¥è¡¨ç¤ºå¦‚ä¸‹ï¼š
- en: '|  | $1$2 |  | (2) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: '![Refer to caption](img/ad777d5a480cfa5c2bafb6bdb57dffba.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒå›¾æ³¨](img/ad777d5a480cfa5c2bafb6bdb57dffba.png)'
- en: 'Figure 3: The overview of parameter efficient fine-tuning using MOELoRA.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3ï¼šä½¿ç”¨MOELoRAçš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ¦‚è§ˆã€‚
- en: III Method
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III æ–¹æ³•
- en: 'In this section, we provide a comprehensive description of our proposed framework.
    We begin with an overview in SectionÂ [III-A](#S3.SS1 "III-A Overview â€£ III Method
    â€£ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task
    Medical Applications"). Subsequently, SectionÂ [III-B](#S3.SS2 "III-B MOELoRA â€£
    III Method â€£ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for
    Multi-task Medical Applications") delves into how MOELoRA seamlessly integrates
    the processes of MOE and LoRA, harnessing the strengths of both. The task-motivated
    gate function is detailed in SectionÂ [III-C](#S3.SS3 "III-C Task Motivated Gate
    Function â€£ III Method â€£ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning
    Method for Multi-task Medical Applications"), where we also discuss the recovery
    of unique fine-tuned LLM parameters for each task. Lastly, SectionÂ [III-D](#S3.SS4
    "III-D Optimization and Inference â€£ III Method â€£ MOELoRA: An MOE-based Parameter
    Efficient Fine-Tuning Method for Multi-task Medical Applications") elaborates
    on the optimization and inference processes.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å¯¹æå‡ºçš„æ¡†æ¶è¿›è¡Œå…¨é¢æè¿°ã€‚æˆ‘ä»¬ä»ç¬¬[III-A](#S3.SS1 "III-A æ¦‚è¿° â€£ III æ–¹æ³• â€£ MOELoRA: ä¸€ç§åŸºäºMOEçš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ç”¨äºå¤šä»»åŠ¡åŒ»å­¦åº”ç”¨")èŠ‚çš„æ¦‚è¿°å¼€å§‹ã€‚éšåï¼Œç¬¬[III-B](#S3.SS2
    "III-B MOELoRA â€£ III æ–¹æ³• â€£ MOELoRA: ä¸€ç§åŸºäºMOEçš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ç”¨äºå¤šä»»åŠ¡åŒ»å­¦åº”ç”¨")èŠ‚æ·±å…¥æ¢è®¨äº†MOELoRAå¦‚ä½•æ— ç¼åœ°æ•´åˆMOEå’ŒLoRAçš„è¿‡ç¨‹ï¼Œå……åˆ†åˆ©ç”¨ä¸¤è€…çš„ä¼˜åŠ¿ã€‚ä»»åŠ¡é©±åŠ¨çš„é—¨æ§å‡½æ•°åœ¨ç¬¬[III-C](#S3.SS3
    "III-C ä»»åŠ¡é©±åŠ¨é—¨æ§å‡½æ•° â€£ III æ–¹æ³• â€£ MOELoRA: ä¸€ç§åŸºäºMOEçš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ç”¨äºå¤šä»»åŠ¡åŒ»å­¦åº”ç”¨")èŠ‚ä¸­è¯¦ç»†æè¿°ï¼Œæˆ‘ä»¬è¿˜è®¨è®ºäº†æ¯ä¸ªä»»åŠ¡çš„ç‹¬ç‰¹å¾®è°ƒLLMå‚æ•°çš„æ¢å¤ã€‚æœ€åï¼Œç¬¬[III-D](#S3.SS4
    "III-D ä¼˜åŒ–ä¸æ¨ç† â€£ III æ–¹æ³• â€£ MOELoRA: ä¸€ç§åŸºäºMOEçš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ç”¨äºå¤šä»»åŠ¡åŒ»å­¦åº”ç”¨")èŠ‚è¯¦ç»†é˜è¿°äº†ä¼˜åŒ–å’Œæ¨ç†è¿‡ç¨‹ã€‚'
- en: III-A Overview
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A æ¦‚è¿°
- en: 'Figure [3](#S2.F3 "Figure 3 â€£ II-B Multi-task Fine-tuning â€£ II Preliminary
    â€£ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task
    Medical Applications") provides a visual representation of the parameter efficient
    fine-tuning process of LLMs using MOELoRA. In the realm of parameter-efficient
    fine-tuning, LoRAÂ [[21](#bib.bib21)] introduces the concept of training only two
    low-rank matrices as a substitute for updates in dense layers. Building on this,
    our approach integrates MOELoRA layers into each dense layer, enabling them to
    acquire keys, queries, and values, as well as facilitating the feed-forward process.
    A significant advantage of our method is that we only fine-tune the parameters
    of the MOELoRA layers, keeping the rest of the original LLM parameters frozen.
    This approach substantially reduces the often prohibitive costs associated with
    tuning.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾[3](#S2.F3 "å›¾ 3 â€£ II-B å¤šä»»åŠ¡å¾®è°ƒ â€£ II åˆæ­¥ â€£ MOELoRA: ä¸€ç§åŸºäºMOEçš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ç”¨äºå¤šä»»åŠ¡åŒ»å­¦åº”ç”¨")å±•ç¤ºäº†ä½¿ç”¨MOELoRAå¯¹LLMè¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒçš„è¿‡ç¨‹ã€‚åœ¨å‚æ•°é«˜æ•ˆå¾®è°ƒé¢†åŸŸï¼ŒLoRA
    [[21](#bib.bib21)] å¼•å…¥äº†åªè®­ç»ƒä¸¤ä¸ªä½ç§©çŸ©é˜µä½œä¸ºå¯†é›†å±‚æ›´æ–°çš„æ›¿ä»£æ¦‚å¿µã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†MOELoRAå±‚é›†æˆåˆ°æ¯ä¸ªå¯†é›†å±‚ä¸­ï¼Œä½¿å…¶èƒ½å¤Ÿè·å–é”®ã€æŸ¥è¯¢å’Œå€¼ï¼ŒåŒæ—¶ä¿ƒè¿›å‰å‘ä¼ æ’­è¿‡ç¨‹ã€‚æˆ‘ä»¬æ–¹æ³•çš„ä¸€ä¸ªæ˜¾è‘—ä¼˜ç‚¹æ˜¯åªå¾®è°ƒMOELoRAå±‚çš„å‚æ•°ï¼Œå…¶ä½™åŸå§‹LLMå‚æ•°ä¿æŒä¸å˜ã€‚è¿™ç§æ–¹æ³•æ˜¾è‘—é™ä½äº†å¾®è°ƒæ—¶å¸¸è§çš„é«˜æˆæœ¬ã€‚'
- en: 'Furthermore, each MOELoRA layer incorporates multiple shared experts. These
    experts are designed to capture diverse knowledge across various medical domains,
    a concept we will delve deeper into in SectionÂ [III-B](#S3.SS2 "III-B MOELoRA
    â€£ III Method â€£ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for
    Multi-task Medical Applications"). We introduce a task-motivated gate function
    to ensure that unique parameter sets are learned for each task. This function
    determines the contribution weights of experts across all MOELoRA layers, enabling
    the generation of distinct updated parameters tailored to different tasks. It
    is worth noting that we employ a single gate function for all MOELoRA layers,
    rather than having a one-to-one correspondence between gates and MOELoRA layers.
    This design choice is intentional, aiming to reduce the number of tunable parameters
    and mitigate the risk of over-parameterization.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ­¤å¤–ï¼Œæ¯ä¸ª MOELoRA å±‚éƒ½åŒ…å«å¤šä¸ªå…±äº«çš„ä¸“å®¶ã€‚è¿™äº›ä¸“å®¶æ—¨åœ¨æ•æ‰å„ç§åŒ»å­¦é¢†åŸŸä¸­çš„å¤šæ ·çŸ¥è¯†ï¼Œæˆ‘ä»¬å°†åœ¨ç¬¬ [III-B](#S3.SS2 "III-B
    MOELoRA â€£ III Method â€£ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method
    for Multi-task Medical Applications") èŠ‚ä¸­è¯¦ç»†æ¢è®¨è¿™ä¸€æ¦‚å¿µã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªä»»åŠ¡é©±åŠ¨çš„é—¨æ§å‡½æ•°ï¼Œä»¥ç¡®ä¿ä¸ºæ¯ä¸ªä»»åŠ¡å­¦ä¹ ç‹¬ç‰¹çš„å‚æ•°é›†ã€‚è¯¥å‡½æ•°ç¡®å®šæ‰€æœ‰
    MOELoRA å±‚ä¸­ä¸“å®¶çš„è´¡çŒ®æƒé‡ï¼Œä»è€Œç”Ÿæˆé€‚åˆä¸åŒä»»åŠ¡çš„ä¸åŒæ›´æ–°å‚æ•°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å¯¹æ‰€æœ‰ MOELoRA å±‚ä½¿ç”¨ä¸€ä¸ªé—¨æ§å‡½æ•°ï¼Œè€Œä¸æ˜¯åœ¨é—¨æ§å‡½æ•°å’Œ MOELoRA
    å±‚ä¹‹é—´ä¸€ä¸€å¯¹åº”ã€‚è¿™ä¸€è®¾è®¡é€‰æ‹©æ—¨åœ¨å‡å°‘å¯è°ƒå‚æ•°çš„æ•°é‡ï¼Œå¹¶é™ä½è¿‡åº¦å‚æ•°åŒ–çš„é£é™©ã€‚'
- en: '![Refer to caption](img/3f86160213bb6a0eb96dac8f07a008ff.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/3f86160213bb6a0eb96dac8f07a008ff.png)'
- en: 'Figure 4: The architecture of the proposed MOELoRA.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4ï¼šæ‰€æå‡ºçš„ MOELoRA çš„æ¶æ„ã€‚
- en: III-B MOELoRA
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B MOELoRA
- en: 'Low-rank Adaptation (LoRA)Â [[21](#bib.bib21)] has demonstrated both its effectiveness
    and efficiency in fine-tuning LLMs. As our approach builds upon the foundational
    principles of LoRA for parameter efficiency, it is pertinent to provide a brief
    overview of its workings. LoRA is inspired by the low intrinsic dimension characteristicÂ [[26](#bib.bib26)],
    which reformulates the parameter fine-tuning process in LLMs as a low-rank decomposition.
    Specifically, the equation $\mathbf{W}_{0}+\Delta\mathbf{W}=\mathbf{W}+\mathbf{B}\mathbf{A}$
    and $\mathbf{A}\in\mathbb{R}^{r\times d_{out}}$ are low-rank and trainable. Given
    this setup, the forward process of a linear layer paired with a LoRA layer can
    be expressed as:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ç§©é€‚åº” (LoRA) [[21](#bib.bib21)] å·²è¯æ˜åœ¨å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹ (LLMs) æ—¶å…·æœ‰æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ã€‚ç”±äºæˆ‘ä»¬çš„æ–¹æ³•åŸºäº LoRA
    çš„åŸºæœ¬åŸç†ä»¥å®ç°å‚æ•°æ•ˆç‡ï¼Œå› æ­¤æœ‰å¿…è¦ç®€è¦æ¦‚è¿°å…¶å·¥ä½œåŸç†ã€‚LoRA å—åˆ°ä½å›ºæœ‰ç»´åº¦ç‰¹æ€§çš„å¯å‘ [[26](#bib.bib26)]ï¼Œå°† LLMs ä¸­çš„å‚æ•°å¾®è°ƒè¿‡ç¨‹é‡æ–°è¡¨è¿°ä¸ºä½ç§©åˆ†è§£ã€‚å…·ä½“è€Œè¨€ï¼Œæ–¹ç¨‹
    $\mathbf{W}_{0}+\Delta\mathbf{W}=\mathbf{W}+\mathbf{B}\mathbf{A}$ å’Œ $\mathbf{A}\in\mathbb{R}^{r\times
    d_{out}}$ æ˜¯ä½ç§©ä¸”å¯è®­ç»ƒçš„ã€‚é‰´äºæ­¤è®¾ç½®ï¼Œçº¿æ€§å±‚ä¸ LoRA å±‚é…å¯¹çš„å‰å‘è¿‡ç¨‹å¯ä»¥è¡¨ç¤ºä¸ºï¼š
- en: '|  | $\displaystyle\mathbf{h}$ |  | (3) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{h}$ |  | (3) |'
- en: '|  |  | $\displaystyle=\mathbf{W}_{0}\mathbf{x}+\frac{\alpha}{r}\cdot\mathbf{B}\mathbf{A}\mathbf{x}$
    |  |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbf{W}_{0}\mathbf{x}+\frac{\alpha}{r}\cdot\mathbf{B}\mathbf{A}\mathbf{x}$
    |  |'
- en: where $\mathbf{x}$. The rank of the trainable low-rank matrices is denoted by
    $r$, $\mathbf{W}k$ and $\mathbf{B}$ and $\mathbf{B}$. Such characteristic results
    in achieving parameter efficiency for the fine-tuning process.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\mathbf{x}$ã€‚å¯è®­ç»ƒçš„ä½ç§©çŸ©é˜µçš„ç§©ç”± $r$ è¡¨ç¤ºï¼Œ$\mathbf{W}k$ å’Œ $\mathbf{B}$ ä»¥åŠ $\mathbf{B}$ã€‚è¿™ç§ç‰¹æ€§æœ‰åŠ©äºåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å®ç°å‚æ•°æ•ˆç‡ã€‚
- en: 'However, the integral parameters are fine-tuned for all tasks in the original
    LoRA, which causes difficulty in learning the various aspects of medical knowledge.
    A potential solution to this challenge is to segment the entire parameter set
    into several parts and derive various combinations. The Mixture-of-Expert (MOE)
    modelÂ [[20](#bib.bib20)] suggests employing multiple expert networks to capture
    different facets of multi-task information, aligning with the combination concept.
    This insight leads us to design MOELoRA, which seamlessly integrates the advantages
    of both LoRA and MOE. To harmonize the distinct forward processes of LoRA and
    MOE, we introduce a set of experts, denoted as $\{E_{i}\}_{i=1}^{N}$ is expressed
    as:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œåœ¨åŸå§‹ LoRA ä¸­ï¼Œæ‰€æœ‰ä»»åŠ¡çš„ç§¯åˆ†å‚æ•°éƒ½è¿›è¡Œäº†å¾®è°ƒï¼Œè¿™ä½¿å¾—å­¦ä¹ åŒ»å­¦çŸ¥è¯†çš„å„ä¸ªæ–¹é¢å˜å¾—å›°éš¾ã€‚ä¸€ä¸ªæ½œåœ¨çš„è§£å†³æ–¹æ¡ˆæ˜¯å°†æ•´ä¸ªå‚æ•°é›†åˆ’åˆ†ä¸ºå¤šä¸ªéƒ¨åˆ†ï¼Œå¹¶æ¨å¯¼å‡ºå„ç§ç»„åˆã€‚Mixture-of-Expert
    (MOE) æ¨¡å‹ [[20](#bib.bib20)] å»ºè®®é‡‡ç”¨å¤šä¸ªä¸“å®¶ç½‘ç»œæ¥æ•æ‰å¤šä»»åŠ¡ä¿¡æ¯çš„ä¸åŒæ–¹é¢ï¼Œè¿™ä¸ç»„åˆæ¦‚å¿µç›¸ä¸€è‡´ã€‚è¿™ä¸€è§è§£ä¿ƒä½¿æˆ‘ä»¬è®¾è®¡äº† MOELoRAï¼Œå®ƒæ— ç¼é›†æˆäº†
    LoRA å’Œ MOE çš„ä¼˜ç‚¹ã€‚ä¸ºäº†åè°ƒ LoRA å’Œ MOE çš„ä¸åŒå‰å‘è¿‡ç¨‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç»„ä¸“å®¶ï¼Œè®°ä½œ $\{E_{i}\}_{i=1}^{N}$ï¼Œå…¶è¡¨è¾¾å¼ä¸ºï¼š
- en: '|  | $\displaystyle\mathbf{h}_{j}$ |  | (4) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{h}_{j}$ |  | (4) |'
- en: '|  |  | $\displaystyle=\mathbf{W}_{0}\mathbf{x}_{j}+\frac{\alpha}{r}\cdot\sum_{i=1}^{N}\omega_{ji}\cdot
    E_{i}(\mathbf{x}_{j})$ |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  |  | `$\displaystyle=\mathbf{W}_{0}\mathbf{x}_{j}+\frac{\alpha}{r}\cdot\sum_{i=1}^{N}\omega_{ji}\cdot
    E_{i}(\mathbf{x}_{j})$` |  |'
- en: '|  |  | $\displaystyle=\mathbf{W}_{0}\mathbf{x}_{j}+\frac{\alpha}{r}\cdot\sum_{i=1}^{N}\omega_{ji}\cdot\mathbf{B}_{i}\mathbf{A}_{i}\mathbf{x}_{j}$
    |  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  |  | `$\displaystyle=\mathbf{W}_{0}\mathbf{x}_{j}+\frac{\alpha}{r}\cdot\sum_{i=1}^{N}\omega_{ji}\cdot\mathbf{B}_{i}\mathbf{A}_{i}\mathbf{x}_{j}$`
    |  |'
- en: where $\mathbf{h}_{j}$ and $\mathbf{A}_{i}\in\mathbb{R}^{\frac{r}{N}\times d_{out}}$
    and $B$. This weight is determined by our proposed gate function, which we will
    detail in following section.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­`$\mathbf{h}_{j}$`å’Œ`$\mathbf{A}_{i}\in\mathbb{R}^{\frac{r}{N}\times d_{out}}$`åŠ`$B$`ã€‚è¿™ä¸ªæƒé‡ç”±æˆ‘ä»¬æå‡ºçš„é—¨æ§å‡½æ•°ç¡®å®šï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚è¯¦ç»†ä»‹ç»ã€‚
- en: Here, we will discuss the number of trainable parameters for LoRA and MOELoRA.
    In terms of LoRA, the two low-rank matrices $\mathbf{B}\in\mathbb{R}^{d_{in}\times
    r}$ trainable experts and each expert own $\frac{r}{N}\times(d_{in}+d{out})$.
    As a conclusion, the MOELoRA has the same number of trainable parameters as LoRA,
    which indicates high efficiency.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†è®¨è®ºLoRAå’ŒMOELoRAçš„å¯è®­ç»ƒå‚æ•°æ•°é‡ã€‚åœ¨LoRAæ–¹é¢ï¼Œä¸¤ä¸ªä½ç§©çŸ©é˜µ`$\mathbf{B}\in\mathbb{R}^{d_{in}\times
    r}$`å¯è®­ç»ƒä¸“å®¶ï¼Œæ¯ä¸ªä¸“å®¶æ‹¥æœ‰`$\frac{r}{N}\times(d_{in}+d_{out})$`ã€‚æ€»ä¹‹ï¼ŒMOELoRAçš„å¯è®­ç»ƒå‚æ•°æ•°é‡ä¸LoRAç›¸åŒï¼Œè¿™è¡¨æ˜å…¶é«˜æ•ˆæ€§ã€‚
- en: III-C Task Motivated Gate Function
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C ä»»åŠ¡é©±åŠ¨çš„é—¨æ§å‡½æ•°
- en: 'In this section, we detail the intricacies of our task-motivated gate function.
    As previously emphasized, the contribution of each expert should be tailored to
    specific tasks. To regulate these contributions, we introduce a gate function.
    Since these weights are inherently task-specific, our gate function is designed
    to take the task identity as input. To facilitate this, we employ a task embedding
    matrix, denoted as $\mathbf{E}\in\mathbb{R}^{|\mathbb{T}|\times d_{T}}$-th column
    of $\mathbf{E}$, we apply a linear transformation. This computation is captured
    by the following equation:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬è¯¦ç»†ä»‹ç»äº†æˆ‘ä»¬çš„ä»»åŠ¡é©±åŠ¨çš„é—¨æ§å‡½æ•°çš„å¤æ‚æ€§ã€‚å¦‚å‰æ‰€è¿°ï¼Œæ¯ä¸ªä¸“å®¶çš„è´¡çŒ®åº”é’ˆå¯¹ç‰¹å®šä»»åŠ¡é‡èº«å®šåˆ¶ã€‚ä¸ºäº†è°ƒèŠ‚è¿™äº›è´¡çŒ®ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé—¨æ§å‡½æ•°ã€‚ç”±äºè¿™äº›æƒé‡æœ¬è´¨ä¸Šæ˜¯ä»»åŠ¡ç‰¹å®šçš„ï¼Œæˆ‘ä»¬çš„é—¨æ§å‡½æ•°è®¾è®¡ä¸ºä»¥ä»»åŠ¡èº«ä»½ä½œä¸ºè¾“å…¥ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ä¸ªä»»åŠ¡åµŒå…¥çŸ©é˜µï¼Œè®°ä½œ`$\mathbf{E}\in\mathbb{R}^{|\mathbb{T}|\times
    d_{T}}$`-thåˆ—çš„`$\mathbf{E}$`ï¼Œæˆ‘ä»¬åº”ç”¨çº¿æ€§å˜æ¢ã€‚è¿™ä¸ªè®¡ç®—ç”±ä»¥ä¸‹æ–¹ç¨‹æ•è·ï¼š
- en: '|  | $\bm{\omega}_{j}={\rm Softmax}(\mathbf{W}_{T}\mathbf{e}_{j})$ |  | (5)
    |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | `$\bm{\omega}_{j}={\rm Softmax}(\mathbf{W}_{T}\mathbf{e}_{j})$` |  | (5)
    |'
- en: Here, $\bm{\omega}_{j}\in\mathbb{R}^{|\mathbb{T}|}$. To prevent any disproportionately
    large weights, we employ a softmax operation to normalize the contribution weights.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œ`$\bm{\omega}_{j}\in\mathbb{R}^{|\mathbb{T}|}$`ã€‚ä¸ºäº†é˜²æ­¢æƒé‡è¿‡å¤§ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†softmaxæ“ä½œæ¥å½’ä¸€åŒ–è´¡çŒ®æƒé‡ã€‚
- en: 'Next, we elucidate the mechanism to retrieve the distinct parameters learned
    for each task. While the conventional design of MOE directly feeds the input vector
    $\mathbf{x}$, the process can be articulated as:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é˜æ˜å¦‚ä½•æ£€ç´¢ä¸ºæ¯ä¸ªä»»åŠ¡å­¦ä¹ çš„ä¸åŒå‚æ•°ã€‚è™½ç„¶ä¼ ç»Ÿçš„MOEè®¾è®¡ç›´æ¥å°†è¾“å…¥å‘é‡`$\mathbf{x}$`é¦ˆé€è¿›å»ï¼Œä½†è¿™ä¸ªè¿‡ç¨‹å¯ä»¥æè¿°ä¸ºï¼š
- en: '|  | $\displaystyle\mathbf{W}_{j}$ |  | (6) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | `$\displaystyle\mathbf{W}_{j}$` |  | (6) |'
- en: '|  |  | $\displaystyle=\mathbf{W}_{0}+\frac{\alpha}{r}\cdot\sum_{i=1}^{N}\omega_{ji}\cdot
    E_{i}$ |  |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  |  | `$\displaystyle=\mathbf{W}_{0}+\frac{\alpha}{r}\cdot\sum_{i=1}^{N}\omega_{ji}\cdot
    E_{i}$` |  |'
- en: '|  |  | $\displaystyle=\mathbf{W}_{0}+\frac{\alpha}{r}\cdot\sum_{i=1}^{N}\omega_{ji}\cdot\mathbf{B}_{i}\mathbf{A}_{i}$
    |  |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  |  | `$\displaystyle=\mathbf{W}_{0}+\frac{\alpha}{r}\cdot\sum_{i=1}^{N}\omega_{ji}\cdot\mathbf{B}_{i}\mathbf{A}_{i}$`
    |  |'
- en: 'In contrast, if the gate function is driven by the input vector $\mathbf{x}$,
    leading to a sample-specific fine-tuned parameter matrix. This design would render
    the parameters non-retrievable on a per-task basis. The ability to retrieve parameters
    for each task offers two primary advantages: 1) Customization for Task: Each task
    is fine-tuned with a set of parameters, which can help learn more task-specific
    information and alleviate the problem of data imbalance. 2) Efficiency in Inference:
    The retrieved, fine-tuned LLM exhibits reduced inference latency. This is attributed
    to the elimination of the need for the additional forward computation associated
    with the LoRA layer.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¦‚æœé—¨æ§å‡½æ•°ç”±è¾“å…¥å‘é‡`$\mathbf{x}$`é©±åŠ¨ï¼Œä¼šå¯¼è‡´æ ·æœ¬ç‰¹å®šçš„ç²¾ç»†è°ƒæ•´å‚æ•°çŸ©é˜µã€‚è¿™ç§è®¾è®¡å°†ä½¿å‚æ•°åœ¨æ¯ä¸ªä»»åŠ¡åŸºç¡€ä¸Šæ— æ³•æ£€ç´¢ã€‚èƒ½å¤Ÿä¸ºæ¯ä¸ªä»»åŠ¡æ£€ç´¢å‚æ•°æä¾›äº†ä¸¤ä¸ªä¸»è¦ä¼˜åŠ¿ï¼š1)
    ä»»åŠ¡å®šåˆ¶ï¼šæ¯ä¸ªä»»åŠ¡éƒ½ç”¨ä¸€ç»„å‚æ•°è¿›è¡Œç²¾ç»†è°ƒæ•´ï¼Œè¿™æœ‰åŠ©äºå­¦ä¹ æ›´å¤šä»»åŠ¡ç‰¹å®šçš„ä¿¡æ¯ï¼Œå¹¶ç¼“è§£æ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚2) æ¨ç†æ•ˆç‡ï¼šæ£€ç´¢åˆ°çš„ç²¾ç»†è°ƒæ•´çš„LLMè¡¨ç°å‡ºå‡å°‘çš„æ¨ç†å»¶è¿Ÿã€‚è¿™å½’å› äºæ¶ˆé™¤äº†ä¸LoRAå±‚ç›¸å…³çš„é¢å¤–å‰å‘è®¡ç®—éœ€æ±‚ã€‚
- en: Algorithm 1 Train and inference process of MOELoRA
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ç®—æ³•1 MOELoRAçš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹
- en: 1:Indicate the LLM and the layers that need MOELoRA fine-tuning.2:Indicate the
    rank value $r$ of MOELoRA.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '1: æŒ‡å®šéœ€è¦ MOELoRA å¾®è°ƒçš„ LLM å’Œå±‚æ•°ã€‚2: æŒ‡å®š MOELoRA çš„ç§©å€¼ $r$ã€‚'
- en: Optimization Process
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–è¿‡ç¨‹
- en: 4:Freeze all parameters in pre-trained LLM, *e.g.,* $\mathbf{W}_{q}$ in $\mathcal{D}$9:endÂ for
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '4: å†»ç»“é¢„è®­ç»ƒ LLM ä¸­çš„æ‰€æœ‰å‚æ•°ï¼Œä¾‹å¦‚ $\mathbf{W}_{q}$ åœ¨ $\mathcal{D}$9: ç»“æŸ'
- en: Inference Process
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨ç†è¿‡ç¨‹
- en: 10:forÂ $\mathcal{T}_{j}$, apply the corresponding parameters of LLM for pediction.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '10: å¯¹äº $\mathcal{T}_{j}$ï¼Œåº”ç”¨ç›¸åº”çš„ LLM å‚æ•°è¿›è¡Œé¢„æµ‹ã€‚'
- en: III-D Optimization and Inference
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D ä¼˜åŒ–å’Œæ¨ç†
- en: 'In this section, we refer to the optimization and inference process of MOELoRA.
    For better readability, we also conclude the whole procedure in AlgorithmÂ [1](#alg1
    "Algorithm 1 â€£ III-C Task Motivated Gate Function â€£ III Method â€£ MOELoRA: An MOE-based
    Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications").'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å‚è€ƒäº† MOELoRA çš„ä¼˜åŒ–å’Œæ¨ç†è¿‡ç¨‹ã€‚ä¸ºäº†æ›´å¥½çš„å¯è¯»æ€§ï¼Œæˆ‘ä»¬è¿˜åœ¨ç®—æ³• [1](#alg1 "Algorithm 1 â€£ III-C
    Task Motivated Gate Function â€£ III Method â€£ MOELoRA: An MOE-based Parameter Efficient
    Fine-Tuning Method for Multi-task Medical Applications") ä¸­æ€»ç»“äº†æ•´ä¸ªè¿‡ç¨‹ã€‚'
- en: Optimization. We first configure the MOELoRA according to the specified layers
    in LLM and several hyper-parameters (line 1-3). Then, for the parameter efficient
    fine-tuning, all pre-trained parameters in LLM (line 4) are frozen. During the
    optimization, we randomly sample a batch of data from all tasks iteratively, instead
    of grouping the samples from the same task into one batch as some multi-task researchesÂ [[27](#bib.bib27),
    [28](#bib.bib28)] do. We choose the random sampling for batch by the performance
    comparison in experiments. Using the batch of data, we can conduct the forward
    process and compute the loss for training (line 6-7). For parameter update, it
    is worth noting that we only fine-tune the parameters of MOELoRA and task motivated
    gate function, *i.e.,* $\{\mathbf{A}_{i},\mathbf{B}_{i}\}_{i=1}^{N}$.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–ã€‚æˆ‘ä»¬é¦–å…ˆæ ¹æ® LLM ä¸­æŒ‡å®šçš„å±‚æ•°å’Œå‡ ä¸ªè¶…å‚æ•°ï¼ˆç¬¬ 1-3 è¡Œï¼‰é…ç½® MOELoRAã€‚ç„¶åï¼Œä¸ºäº†å‚æ•°é«˜æ•ˆå¾®è°ƒï¼Œæˆ‘ä»¬å°† LLM ä¸­æ‰€æœ‰é¢„è®­ç»ƒçš„å‚æ•°ï¼ˆç¬¬
    4 è¡Œï¼‰å†»ç»“ã€‚åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä»æ‰€æœ‰ä»»åŠ¡ä¸­éšæœºæŠ½å–ä¸€æ‰¹æ•°æ®è¿›è¡Œè¿­ä»£ï¼Œè€Œä¸æ˜¯åƒæŸäº›å¤šä»»åŠ¡ç ”ç©¶ [[27](#bib.bib27), [28](#bib.bib28)]
    é‚£æ ·å°†æ¥è‡ªåŒä¸€ä»»åŠ¡çš„æ ·æœ¬åˆ†ç»„ä¸ºä¸€æ‰¹ã€‚æˆ‘ä»¬é€‰æ‹©éšæœºæŠ½æ ·è¿›è¡Œæ‰¹å¤„ç†æ˜¯æ ¹æ®å®éªŒä¸­çš„æ€§èƒ½æ¯”è¾ƒç»“æœæ¥å†³å®šçš„ã€‚ä½¿ç”¨è¿™ä¸€æ‰¹æ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥è¿›è¡Œå‰å‘è¿‡ç¨‹å¹¶è®¡ç®—è®­ç»ƒæŸå¤±ï¼ˆç¬¬ 6-7
    è¡Œï¼‰ã€‚åœ¨å‚æ•°æ›´æ–°æ—¶ï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯æˆ‘ä»¬ä»…å¾®è°ƒ MOELoRA å’Œä»»åŠ¡é©±åŠ¨çš„é—¨æ§å‡½æ•°çš„å‚æ•°ï¼Œå³ $\{\mathbf{A}_{i},\mathbf{B}_{i}\}_{i=1}^{N}$ã€‚
- en: 'Inference. As discussed in SectionÂ [III-C](#S3.SS3 "III-C Task Motivated Gate
    Function â€£ III Method â€£ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning
    Method for Multi-task Medical Applications"), the MOELoRA can recover the fine-tuned
    parameter matrices for each task by EquationÂ ([6](#S3.E6 "In III-C Task Motivated
    Gate Function â€£ III Method â€£ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning
    Method for Multi-task Medical Applications")). For inference, we first recover
    the trained parameters in LLM for each task (line 10-13), which indicates that
    each task has its own LLM parameters. Then, we can apply the corresponding LLM
    to complete the specified task.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ¨ç†ã€‚æ­£å¦‚åœ¨ç¬¬ [III-C](#S3.SS3 "III-C Task Motivated Gate Function â€£ III Method â€£
    MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical
    Applications") èŠ‚ä¸­è®¨è®ºçš„ï¼ŒMOELoRA å¯ä»¥é€šè¿‡æ–¹ç¨‹ ([6](#S3.E6 "In III-C Task Motivated Gate
    Function â€£ III Method â€£ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning
    Method for Multi-task Medical Applications")) æ¢å¤æ¯ä¸ªä»»åŠ¡çš„å¾®è°ƒå‚æ•°çŸ©é˜µã€‚å¯¹äºæ¨ç†ï¼Œæˆ‘ä»¬é¦–å…ˆæ¢å¤ LLM ä¸­æ¯ä¸ªä»»åŠ¡çš„è®­ç»ƒå‚æ•°ï¼ˆç¬¬
    10-13 è¡Œï¼‰ï¼Œè¿™è¡¨æ˜æ¯ä¸ªä»»åŠ¡éƒ½æœ‰å…¶è‡ªå·±çš„ LLM å‚æ•°ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥åº”ç”¨ç›¸åº”çš„ LLM æ¥å®ŒæˆæŒ‡å®šçš„ä»»åŠ¡ã€‚'
- en: IV Experiment
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV å®éªŒ
- en: 'In this section, we present comprehensive experiments conducted on a multi-task
    Chinese medical dataset. Through a detailed analysis of the experimental results,
    we seek to address the following Research Questions (RQ):'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åœ¨ä¸€ä¸ªå¤šä»»åŠ¡ä¸­æ–‡åŒ»å­¦æ•°æ®é›†ä¸Šè¿›è¡Œçš„ç»¼åˆå®éªŒã€‚é€šè¿‡å¯¹å®éªŒç»“æœçš„è¯¦ç»†åˆ†æï¼Œæˆ‘ä»¬æ—¨åœ¨è§£å†³ä»¥ä¸‹ç ”ç©¶é—®é¢˜ï¼ˆRQï¼‰ï¼š
- en: â€¢
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'RQ1: How does MOELoRA compare to other parameter-efficient fine-tuning strategies
    and cross-task generalization methods in terms of performance?'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'RQ1: MOELoRA ä¸å…¶ä»–å‚æ•°é«˜æ•ˆå¾®è°ƒç­–ç•¥å’Œè·¨ä»»åŠ¡æ³›åŒ–æ–¹æ³•åœ¨æ€§èƒ½æ–¹é¢ç›¸æ¯”å¦‚ä½•ï¼Ÿ'
- en: â€¢
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'RQ2: What impact do the MOE architecture and the gate function have on the
    fine-tuning process? Additionally, how do different training strategies influence
    the performance of MOELoRA?'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'RQ2: MOE æ¶æ„å’Œé—¨æ§å‡½æ•°å¯¹å¾®è°ƒè¿‡ç¨‹æœ‰ä»€ä¹ˆå½±å“ï¼Ÿæ­¤å¤–ï¼Œä¸åŒçš„è®­ç»ƒç­–ç•¥å¦‚ä½•å½±å“ MOELoRA çš„æ€§èƒ½ï¼Ÿ'
- en: â€¢
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'RQ3: How do the number of experts and the rank of MOELoRA influence performance
    outcomes?'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'RQ3: ä¸“å®¶æ•°é‡å’Œ MOELoRA çš„ç§©å¦‚ä½•å½±å“æ€§èƒ½ç»“æœï¼Ÿ'
- en: â€¢
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'RQ4: Are the experts specialized in capturing specific aspects of knowledge?'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'RQ4: ä¸“å®¶æ˜¯å¦ä¸“æ³¨äºæ•æ‰ç‰¹å®šæ–¹é¢çš„çŸ¥è¯†ï¼Ÿ'
- en: IV-A Experimental Settings
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A å®éªŒè®¾ç½®
- en: IV-A1 Dataset
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A1 æ•°æ®é›†
- en: 'Our experiments are conducted on the PromptCBLUE datasetÂ²Â²2https://tianchi.aliyun.com/competition/entrance/532084/information,
    a Chinese multi-task medical dataset recently made available on the Tianchi Competition
    PlatformÂ³Â³3https://tianchi.aliyun.com/competition/activeList. This dataset encompasses
    $16$ distinct medical tasks, each of which has been transformed into pure text
    format using specific prompts, ensuring compatibility with LLMs. To the best of
    our knowledge, PromptCBLUE is the only medical multi-task dataset tailored for
    LLMs. Specifically, the dataset includes tasks such as medical named entity recognition,
    diagnosis report generation, etc. Due to computational constraints, we have chosen
    eight tasks at random for our experiments. For pre-processing, we eliminated duplicate
    samples. Since the test set remains unreleased, we opt to use the development
    set as our test set. The validation set is derived from the training set, with
    its size matching that of the test set. The statistics of the pre-processed dataset
    are concluded in TableÂ [I](#S4.T1 "TABLE I â€£ IV-A1 Dataset â€£ IV-A Experimental
    Settings â€£ IV Experiment â€£ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning
    Method for Multi-task Medical Applications").'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 'æˆ‘ä»¬çš„å®éªŒæ˜¯åœ¨PromptCBLUEæ•°æ®é›†Â²Â²2https://tianchi.aliyun.com/competition/entrance/532084/informationä¸Šè¿›è¡Œçš„ï¼Œè¿™æ˜¯ä¸€ä¸ªæœ€è¿‘åœ¨å¤©æ± ç«èµ›å¹³å°ä¸Šæä¾›çš„ä¸­æ–‡å¤šä»»åŠ¡åŒ»ç–—æ•°æ®é›†Â³Â³3https://tianchi.aliyun.com/competition/activeListã€‚è¯¥æ•°æ®é›†åŒ…å«$16$ä¸ªä¸åŒçš„åŒ»ç–—ä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡å·²ä½¿ç”¨ç‰¹å®šçš„æç¤ºè½¬æ¢ä¸ºçº¯æ–‡æœ¬æ ¼å¼ï¼Œç¡®ä¿ä¸LLMså…¼å®¹ã€‚æ ¹æ®æˆ‘ä»¬çš„äº†è§£ï¼ŒPromptCBLUEæ˜¯å”¯ä¸€ä¸€ä¸ªä¸“ä¸ºLLMsé‡èº«å®šåˆ¶çš„åŒ»ç–—å¤šä»»åŠ¡æ•°æ®é›†ã€‚å…·ä½“è€Œè¨€ï¼Œæ•°æ®é›†åŒ…æ‹¬åŒ»ç–—å‘½åå®ä½“è¯†åˆ«ã€è¯Šæ–­æŠ¥å‘Šç”Ÿæˆç­‰ä»»åŠ¡ã€‚ç”±äºè®¡ç®—é™åˆ¶ï¼Œæˆ‘ä»¬éšæœºé€‰æ‹©äº†å…«ä¸ªä»»åŠ¡è¿›è¡Œå®éªŒã€‚å¯¹äºé¢„å¤„ç†ï¼Œæˆ‘ä»¬å»é™¤äº†é‡å¤æ ·æœ¬ã€‚ç”±äºæµ‹è¯•é›†å°šæœªå‘å¸ƒï¼Œæˆ‘ä»¬é€‰æ‹©ä½¿ç”¨å¼€å‘é›†ä½œä¸ºæˆ‘ä»¬çš„æµ‹è¯•é›†ã€‚éªŒè¯é›†æ¥æºäºè®­ç»ƒé›†ï¼Œå…¶å¤§å°ä¸æµ‹è¯•é›†ç›¸åŒ¹é…ã€‚é¢„å¤„ç†æ•°æ®é›†çš„ç»Ÿè®¡ä¿¡æ¯æ€»ç»“åœ¨è¡¨[I](#S4.T1
    "TABLE I â€£ IV-A1 Dataset â€£ IV-A Experimental Settings â€£ IV Experiment â€£ MOELoRA:
    An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications")ä¸­ã€‚'
- en: 'TABLE I: The brief description and statistics of the dataset PromptCBLUE.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ Iï¼šPromptCBLUEæ•°æ®é›†çš„ç®€è¦æè¿°å’Œç»Ÿè®¡ä¿¡æ¯ã€‚
- en: '| Task | Description | # Train | # Validation | # Test |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| ä»»åŠ¡ | æè¿° | # è®­ç»ƒ | # éªŒè¯ | # æµ‹è¯• |'
- en: '| CMeIE | Name Entity Recognition | 2,828 | 600 | 600 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| CMeIE | å®ä½“è¯†åˆ« | 2,828 | 600 | 600 |'
- en: '| CHIP-CDN | Normalization | 2,381 | 600 | 600 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| CHIP-CDN | æ ‡å‡†åŒ– | 2,381 | 600 | 600 |'
- en: '| CHIP-CDEE | Attribute Extraction | 1,562 | 600 | 600 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| CHIP-CDEE | å±æ€§æå– | 1,562 | 600 | 600 |'
- en: '| CHIP-MDCFNPC | Clinic Entity Discovery | 4,935 | 600 | 600 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| CHIP-MDCFNPC | ä¸´åºŠå®ä½“å‘ç° | 4,935 | 600 | 600 |'
- en: '| CHIP-CTC | Medical Text Classification | 3,622 | 1,100 | 1,100 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| CHIP-CTC | åŒ»å­¦æ–‡æœ¬åˆ†ç±» | 3,622 | 1,100 | 1,100 |'
- en: '| KUAKE-QIC | Query Intention | 3,279 | 660 | 660 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| KUAKE-QIC | æŸ¥è¯¢æ„å›¾ | 3,279 | 660 | 660 |'
- en: '| IMCS-V2-MRG | Report Generation | 1,799 | 600 | 600 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| IMCS-V2-MRG | æŠ¥å‘Šç”Ÿæˆ | 1,799 | 600 | 600 |'
- en: '| MedDG | Doctor Dialogue | 4,964 | 600 | 600 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| MedDG | åŒ»ç”Ÿå¯¹è¯ | 4,964 | 600 | 600 |'
- en: IV-A2 Baselines
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A2 åŸºå‡†
- en: 'In our experiments, we benchmark against three distinct groups of baselines,
    namely: (i) LLM without Fine-tuning, (ii) LLM with Fine-tuning, and (iii) Cross-task
    Generalization. The latter two groups utilize ChatGLM-6BÂ [[25](#bib.bib25)], renowned
    for its prowess in Chinese text generation. A brief description of each baseline
    group is as follows.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬ä»¥ä¸‰ç»„ä¸åŒçš„åŸºå‡†è¿›è¡Œå¯¹æ¯”ï¼Œå³ï¼šï¼ˆiï¼‰LLMæ— å¾®è°ƒï¼Œï¼ˆiiï¼‰LLMå¾®è°ƒï¼Œä»¥åŠï¼ˆiiiï¼‰è·¨ä»»åŠ¡æ³›åŒ–ã€‚åä¸¤ç»„ä½¿ç”¨ChatGLM-6BÂ [[25](#bib.bib25)]ï¼Œä»¥å…¶åœ¨ä¸­æ–‡æ–‡æœ¬ç”Ÿæˆä¸­çš„å“è¶Šè¡¨ç°è€Œé—»åã€‚æ¯ä¸ªåŸºå‡†ç»„çš„ç®€è¦æè¿°å¦‚ä¸‹ã€‚
- en: â€¢
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'LLM without Fine-tuing: For this group, we employ In-Context LearningÂ [[29](#bib.bib29)]
    to guide the LLM in accomplishing the relevant tasks. Specifically, we provide
    a task description accompanied by $3$ randomly sampled examples for in-context
    learning. Models such as ChatGPT[[30](#bib.bib30)] and HuatuoÂ [[8](#bib.bib8)]
    serve as the foundational models.'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLMæ— å¾®è°ƒï¼šå¯¹äºè¿™ä¸€ç»„ï¼Œæˆ‘ä»¬ä½¿ç”¨In-Context LearningÂ [[29](#bib.bib29)]æ¥æŒ‡å¯¼LLMå®Œæˆç›¸å…³ä»»åŠ¡ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªä»»åŠ¡æè¿°ï¼Œå¹¶é™„å¸¦$3$ä¸ªéšæœºé‡‡æ ·çš„ç¤ºä¾‹è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚æ¨¡å‹å¦‚ChatGPT[[30](#bib.bib30)]å’ŒHuatuoÂ [[8](#bib.bib8)]ä½œä¸ºåŸºç¡€æ¨¡å‹ã€‚
- en: â€¢
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'LLM with Fine-tuning: This group encompasses various fine-tuning strategies.
    One is P-TuningÂ [[1](#bib.bib1)], which inserts continuous trainable prompt vectors
    to the start of the sequence. The other competitors are rooted in LoRAÂ [[21](#bib.bib21)].
    For the multi-task medical tasks, we implement two straightforward strategies:
    one that fine-tunes a single LoRA for all tasks (denoted as LoRA (Full)) and another
    that fine-tunes a distinct LoRA for each task (denoted as LoRA (Single)). Additionally,
    we introduce a variant of LoRA (Full) that incorporates a basic task description
    in the prompt, labeled as LoRA (Full+TP).'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¾®è°ƒçš„ LLMï¼šè¿™ä¸€ç»„åŒ…æ‹¬å„ç§å¾®è°ƒç­–ç•¥ã€‚ä¸€ä¸ªæ˜¯ P-Tuning [[1](#bib.bib1)]ï¼Œå®ƒå°†è¿ç»­çš„å¯è®­ç»ƒæç¤ºå‘é‡æ’å…¥åˆ°åºåˆ—çš„å¼€å§‹ã€‚å…¶ä»–ç«äº‰æ–¹æ³•åŸºäº
    LoRA [[21](#bib.bib21)]ã€‚å¯¹äºå¤šä»»åŠ¡åŒ»ç–—ä»»åŠ¡ï¼Œæˆ‘ä»¬å®ç°äº†ä¸¤ç§ç®€å•çš„ç­–ç•¥ï¼šä¸€ç§æ˜¯ä¸ºæ‰€æœ‰ä»»åŠ¡å¾®è°ƒä¸€ä¸ª LoRAï¼ˆæ ‡è®°ä¸º LoRA (Full)ï¼‰ï¼Œå¦ä¸€ç§æ˜¯ä¸ºæ¯ä¸ªä»»åŠ¡å¾®è°ƒä¸€ä¸ªç‹¬ç‰¹çš„
    LoRAï¼ˆæ ‡è®°ä¸º LoRA (Single)ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ LoRA (Full) çš„å˜ä½“ï¼Œå®ƒåœ¨æç¤ºä¸­åŒ…å«åŸºæœ¬ä»»åŠ¡æè¿°ï¼Œæ ‡è®°ä¸º LoRA (Full+TP)ã€‚
- en: â€¢
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Cross-task Generalization: To assess the applicability of cross-task generalization
    to multi-task fine-tuning, we also evaluate a recent approach, namely LoRAHubÂ [[31](#bib.bib31)].'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è·¨ä»»åŠ¡æ³›åŒ–ï¼šä¸ºäº†è¯„ä¼°è·¨ä»»åŠ¡æ³›åŒ–åœ¨å¤šä»»åŠ¡å¾®è°ƒä¸­çš„é€‚ç”¨æ€§ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†ä¸€ç§æœ€æ–°çš„æ–¹æ³•ï¼Œå³ LoRAHub [[31](#bib.bib31)]ã€‚
- en: IV-A3 Implementation Details
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A3 å®æ–½ç»†èŠ‚
- en: 'Our experiments are simulated by PyTorch 1.12.0 and Python 3.6.5\. We run the
    code on Tesla V100 GPUs for acceleration. The LLM ChatGLM-6BÂ [[25](#bib.bib25)],
    recognized for its proficiency in Chinese language processing, serves as the foundational
    model for fine-tuning. To summarize, the configuration details are shown as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„å®éªŒåœ¨ PyTorch 1.12.0 å’Œ Python 3.6.5 ä¸‹æ¨¡æ‹Ÿè¿›è¡Œã€‚æˆ‘ä»¬åœ¨ Tesla V100 GPU ä¸Šè¿è¡Œä»£ç ä»¥åŠ é€Ÿã€‚LLM ChatGLM-6B
    [[25](#bib.bib25)]ï¼Œä»¥å…¶åœ¨ä¸­æ–‡å¤„ç†ä¸­çš„ä¼˜è¶Šæ€§è€Œé—»åï¼Œä½œä¸ºå¾®è°ƒçš„åŸºç¡€æ¨¡å‹ã€‚æ€»ä¹‹ï¼Œé…ç½®ç»†èŠ‚å¦‚ä¸‹ï¼š
- en: â€¢
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Model Layers: For all LoRA fine-tuning baselines and the proposed MOELoRA,
    we designate trainable layers within the self-attention and linear layers of ChatGLM.
    These layers are identified as â€œquery_key_valueâ€, â€œdenseâ€, â€œdense_h_to_4hâ€, and
    â€œdense_4h_to_hâ€.'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹å±‚ï¼šå¯¹äºæ‰€æœ‰ LoRA å¾®è°ƒåŸºçº¿å’Œæå‡ºçš„ MOELoRAï¼Œæˆ‘ä»¬æŒ‡å®š ChatGLM çš„è‡ªæ³¨æ„åŠ›å’Œçº¿æ€§å±‚ä¸­çš„å¯è®­ç»ƒå±‚ã€‚è¿™äº›å±‚è¢«æ ‡è¯†ä¸ºâ€œquery_key_valueâ€ã€â€œdenseâ€ã€â€œdense_h_to_4hâ€å’Œâ€œdense_4h_to_hâ€ã€‚
- en: â€¢
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Input/Output Length: The maximum input and output lengths were configured to
    $1,024$, respectively.'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¾“å…¥/è¾“å‡ºé•¿åº¦ï¼šæœ€å¤§è¾“å…¥å’Œè¾“å‡ºé•¿åº¦åˆ†åˆ«é…ç½®ä¸º $1,024$ã€‚
- en: â€¢
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Parameters: We set batch size to $64$, with a LoRA dropout $\alpha=0.1$.'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å‚æ•°ï¼šæˆ‘ä»¬å°†æ‰¹é‡å¤§å°è®¾ç½®ä¸º $64$ï¼ŒLoRA dropout ä¸º $\alpha=0.1$ã€‚
- en: Our MOELoRA implementationâ´â´4https://github.com/liuqidong07/MOELoRA-peft is
    compatible with the PEFT packageâµâµ5https://github.com/huggingface/peft, facilitating
    easier adoption and utilization of MOELoRA.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ MOELoRA å®ç°â´â´4https://github.com/liuqidong07/MOELoRA-peft ä¸ PEFT åŒ…âµâµ5https://github.com/huggingface/peft
    å…¼å®¹ï¼Œä¾¿äº MOELoRA çš„é‡‡çº³å’Œä½¿ç”¨ã€‚
- en: IV-A4 Evaluation Metrics
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A4 è¯„ä¼°æŒ‡æ ‡
- en: For our evaluations, we employ a variety of metrics tailored to the nature of
    each task. Specifically, the Micro-F1 score is used for CMeIE, CHIP-CDN, CHIP-CDEE
    and CHIP-MDCFNPC, while the Macro-F1 score is for CHIP-CTC and KUAKE-QIC. As for
    text generation tasks, *i.e.,* report generation and doctor dialogue, the Rouge-LÂ [[32](#bib.bib32)]
    is applied. Also, the average score across all tasks is used for evaluating the
    overall performance.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæˆ‘ä»¬çš„è¯„ä¼°ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å¤šç§é’ˆå¯¹æ¯ä¸ªä»»åŠ¡æ€§è´¨çš„æŒ‡æ ‡ã€‚å…·ä½“æ¥è¯´ï¼ŒMicro-F1 åˆ†æ•°ç”¨äº CMeIEã€CHIP-CDNã€CHIP-CDEE å’Œ CHIP-MDCFNPCï¼Œè€Œ
    Macro-F1 åˆ†æ•°ç”¨äº CHIP-CTC å’Œ KUAKE-QICã€‚è‡³äºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œå³æŠ¥å‘Šç”Ÿæˆå’ŒåŒ»ç”Ÿå¯¹è¯ï¼Œåº”ç”¨äº† Rouge-L [[32](#bib.bib32)]ã€‚å¦å¤–ï¼Œæ‰€æœ‰ä»»åŠ¡çš„å¹³å‡åˆ†æ•°ç”¨äºè¯„ä¼°æ•´ä½“æ€§èƒ½ã€‚
- en: 'TABLE II: The overall results of competing baselines and MOELoRA on PromptCBLUE.
    The boldface refers to the highest score and the underline indicates the best
    result of the baselines. â€œ*â€ indicates the statistically significant improvements
    (*i.e.,* two-sided t-test with $p<0.05$) over the best baseline.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ IIï¼šç«äº‰åŸºçº¿å’Œ MOELoRA åœ¨ PromptCBLUE ä¸Šçš„æ•´ä½“ç»“æœã€‚ç²—ä½“è¡¨ç¤ºæœ€é«˜åˆ†æ•°ï¼Œä¸‹åˆ’çº¿è¡¨ç¤ºåŸºçº¿çš„æœ€ä½³ç»“æœã€‚â€œ*â€è¡¨ç¤ºåœ¨æœ€ä½³åŸºçº¿ä¸Šçš„ç»Ÿè®¡æ˜¾è‘—æ€§æ”¹è¿›ï¼ˆ*å³*ï¼ŒåŒä¾§
    t æ£€éªŒ $p<0.05$ï¼‰ã€‚
- en: '| Model | CMeIE | CHIP-CDN | CHIP-CDEE | CHIP-MDCFNPC | CHIP-CTC | KUAKE-QIC
    | IMCS-V2-MRG | MedDG | Avg. |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| æ¨¡å‹ | CMeIE | CHIP-CDN | CHIP-CDEE | CHIP-MDCFNPC | CHIP-CTC | KUAKE-QIC |
    IMCS-V2-MRG | MedDG | å¹³å‡å€¼ |'
- en: '| ChatGPT | 0.3058 | 0.6069 | 0.2838 | 0.5854 | 0.5253 | 0.4851 | 0.3253 |
    0.1361 | 0.4067 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | 0.3058 | 0.6069 | 0.2838 | 0.5854 | 0.5253 | 0.4851 | 0.3253 |
    0.1361 | 0.4067 |'
- en: '| Huatuo | 0.1826 | 0.3610 | 0.1658 | 0.3487 | 0.1909 | 0.1454 | 0.2401 | 0.1308
    | 0.2207 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Huatuo | 0.1826 | 0.3610 | 0.1658 | 0.3487 | 0.1909 | 0.1454 | 0.2401 | 0.1308
    | 0.2207 |'
- en: '| P-Tuning | 0.4552 | 0.8687 | 0.5256 | 0.7423 | 0.8275 | 0.8377 | 0.3155 |
    0.0901 | 0.5828 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| P-Tuning | 0.4552 | 0.8687 | 0.5256 | 0.7423 | 0.8275 | 0.8377 | 0.3155 |
    0.0901 | 0.5828 |'
- en: '| LoRA (Full) | 0.5089 | 0.8748 | 0.5464 | 0.7780 | 0.8758 | 0.8615 | 0.3678
    | 0.1113 | 0.6155 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| LoRA (å®Œæ•´) | 0.5089 | 0.8748 | 0.5464 | 0.7780 | 0.8758 | 0.8615 | 0.3678
    | 0.1113 | 0.6155 |'
- en: '| LoRA (Single) | 0.4984 | 0.8882 | 0.5528 | 0.7765 | 0.8694 | 0.8524 | 0.3583
    | 0.1143 | 0.6138 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| LoRA (å•ä¸€) | 0.4984 | 0.8882 | 0.5528 | 0.7765 | 0.8694 | 0.8524 | 0.3583
    | 0.1143 | 0.6138 |'
- en: '| LoRA (Full+TP) | 0.4933 | 0.8814 | 0.5450 | 0.7705 | 0.8755 | 0.8664 | 0.3556
    | 0.1160 | 0.6130 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| LoRA (å®Œæ•´+TP) | 0.4933 | 0.8814 | 0.5450 | 0.7705 | 0.8755 | 0.8664 | 0.3556
    | 0.1160 | 0.6130 |'
- en: '| LoRAHub | 0.4411 | 0.8442 | 0.5041 | 0.7177 | 0.8564 | 0.8502 | 0.3061 |
    0.1192 | 0.5799 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| LoRAHub | 0.4411 | 0.8442 | 0.5041 | 0.7177 | 0.8564 | 0.8502 | 0.3061 |
    0.1192 | 0.5799 |'
- en: '| MOELoRA | 0.5193* | 0.8928* | 0.5697* | 0.7933* | 0.8691 | 0.8675 | 0.3681
    | 0.1089 | 0.6236* |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| MOELoRA | 0.5193* | 0.8928* | 0.5697* | 0.7933* | 0.8691 | 0.8675 | 0.3681
    | 0.1089 | 0.6236* |'
- en: 'TABLE III: The experimental results of ablation study for MOELoRA. The boldface
    refers to the highest score and the underline indicates the best result of the
    baselines. â€œ*â€ indicates the statistically significant improvements (*i.e.,* two-sided
    t-test with $p<0.05$) over the best baseline.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ IIIï¼šMOELoRA çš„æ¶ˆèç ”ç©¶å®éªŒç»“æœã€‚ç²—ä½“å­—è¡¨ç¤ºæœ€é«˜åˆ†æ•°ï¼Œä¸‹åˆ’çº¿è¡¨ç¤ºåŸºçº¿ä¸­çš„æœ€ä½³ç»“æœã€‚â€œ*â€è¡¨ç¤ºåœ¨æœ€ä½³åŸºçº¿ä¸Šçš„ç»Ÿè®¡æ˜¾è‘—æ”¹è¿›ï¼ˆ*å³ï¼Œ* åŒè¾¹ t
    æ£€éªŒ $p<0.05$ï¼‰ã€‚
- en: '| Model | CMeIE | CHIP-CDN | CHIP-CDEE | CHIP-MDCFNPC | CHIP-CTC | KUAKE-QIC
    | IMCS-V2-MRG | MedDG | Avg. |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| æ¨¡å‹ | CMeIE | CHIP-CDN | CHIP-CDEE | CHIP-MDCFNPC | CHIP-CTC | KUAKE-QIC |
    IMCS-V2-MRG | MedDG | å¹³å‡ |'
- en: '| w/o MOE | 0.5089 | 0.8748 | 0.5464 | 0.7780 | 0.8758 | 0.8615 | 0.3678 |
    0.1113 | 0.6155 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| æ—  MOE | 0.5089 | 0.8748 | 0.5464 | 0.7780 | 0.8758 | 0.8615 | 0.3678 | 0.1113
    | 0.6155 |'
- en: '| w/o gate | 0.5015 | 0.8840 | 0.5378 | 0.7789 | 0.8818 | 0.8699 | 0.3709 |
    0.1140 | 0.6174 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| æ— é—¨ | 0.5015 | 0.8840 | 0.5378 | 0.7789 | 0.8818 | 0.8699 | 0.3709 | 0.1140
    | 0.6174 |'
- en: '| w multiple gate | 0.4994 | 0.8840 | 0.5692 | 0.7842 | 0.8764 | 0.8675 | 0.3632
    | 0.1130 | 0.6196 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| w å¤šé‡é—¨ | 0.4994 | 0.8840 | 0.5692 | 0.7842 | 0.8764 | 0.8675 | 0.3632 | 0.1130
    | 0.6196 |'
- en: '| w BT | 0.4817 | 0.8806 | 0.5712 | 0.7713 | 0.8682 | 0.8643 | 0.3522 | 0.1110
    | 0.6126 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| w BT | 0.4817 | 0.8806 | 0.5712 | 0.7713 | 0.8682 | 0.8643 | 0.3522 | 0.1110
    | 0.6126 |'
- en: '| w RBT | 0.4769 | 0.8930 | 0.5600 | 0.7741 | 0.8636 | 0.8795 | 0.3541 | 0.1135
    | 0.6144 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| w RBT | 0.4769 | 0.8930 | 0.5600 | 0.7741 | 0.8636 | 0.8795 | 0.3541 | 0.1135
    | 0.6144 |'
- en: '| MOELoRA | 0.5193* | 0.8928* | 0.5697 | 0.7933* | 0.8691 | 0.8675 | 0.3681
    | 0.1089 | 0.6236* |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| MOELoRA | 0.5193* | 0.8928* | 0.5697 | 0.7933* | 0.8691 | 0.8675 | 0.3681
    | 0.1089 | 0.6236* |'
- en: IV-B Overall Performance
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B æ•´ä½“æ€§èƒ½
- en: 'The comprehensive experimental results of MOELoRA and the competing baselines
    are presented in TableÂ [II](#S4.T2 "TABLE II â€£ IV-A4 Evaluation Metrics â€£ IV-A
    Experimental Settings â€£ IV Experiment â€£ MOELoRA: An MOE-based Parameter Efficient
    Fine-Tuning Method for Multi-task Medical Applications"). Analyzing the average
    scores across all tasks, it is evident that MOELoRA consistently outperforms all
    other methods. A detailed analysis of the results is as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 'MOELoRA å’Œç«äº‰åŸºçº¿çš„ç»¼åˆå®éªŒç»“æœè§è¡¨[II](#S4.T2 "TABLE II â€£ IV-A4 Evaluation Metrics â€£ IV-A
    Experimental Settings â€£ IV Experiment â€£ MOELoRA: An MOE-based Parameter Efficient
    Fine-Tuning Method for Multi-task Medical Applications")ã€‚åˆ†ææ‰€æœ‰ä»»åŠ¡çš„å¹³å‡åˆ†æ•°å¯ä»¥çœ‹å‡ºï¼ŒMOELoRA
    å§‹ç»ˆä¼˜äºå…¶ä»–æ‰€æœ‰æ–¹æ³•ã€‚ç»“æœçš„è¯¦ç»†åˆ†æå¦‚ä¸‹ï¼š'
- en: â€¢
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'LLMs without Fine-tuning: The first group of baselines, which are LLMs without
    any fine-tuning, significantly lag behind the other groups. This highlights the
    importance of fine-tuning LLMs to incorporate task-specific medical knowledge.
    Notably, ChatGPT outperforms Huatuo on most tasks, suggesting that the LLMs only
    in large parameter scales can be well motivated by the in-context learningÂ [[29](#bib.bib29)].'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æœªå¾®è°ƒçš„ LLMsï¼šç¬¬ä¸€ç»„åŸºçº¿æ˜¯æœªç»è¿‡ä»»ä½•å¾®è°ƒçš„ LLMsï¼Œæ˜æ˜¾è½åäºå…¶ä»–ç»„ã€‚è¿™çªæ˜¾äº†å¾®è°ƒ LLMs ä»¥èå…¥ä»»åŠ¡ç‰¹å®šåŒ»ç–—çŸ¥è¯†çš„é‡è¦æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒChatGPT
    åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸­ä¼˜äºåæ‹“ï¼Œè¿™è¡¨æ˜ä»…ä»…æ‹¥æœ‰å¤§è§„æ¨¡å‚æ•°çš„ LLMs å¯ä»¥é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ å¾—åˆ°å¾ˆå¥½çš„æ¿€åŠ±[[29](#bib.bib29)]ã€‚
- en: â€¢
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Parameter Efficient Fine-tuning Strategies: Among the parameter-efficient fine-tuning
    strategies, LoRA-based methods clearly surpass P-Tuning. While LoRA (Full) and
    LoRA (Full+TP) both utilize data from all tasks, LoRA (Full+TP) slightly underperforms.
    This might be attributed to the addition of task prompts, which extend the input
    texts, leading to potential truncation of informative words due to input length
    constraints. LoRA (Single), which fine-tunes for individual tasks, also does not
    match the performance of LoRA (Full), underscoring the value of shared knowledge
    across tasks.'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å‚æ•°é«˜æ•ˆå¾®è°ƒç­–ç•¥ï¼šåœ¨å‚æ•°é«˜æ•ˆå¾®è°ƒç­–ç•¥ä¸­ï¼ŒåŸºäº LoRA çš„æ–¹æ³•æ˜æ˜¾ä¼˜äº P-Tuningã€‚è™½ç„¶ LoRAï¼ˆå®Œæ•´ï¼‰å’Œ LoRAï¼ˆå®Œæ•´+TPï¼‰éƒ½ä½¿ç”¨äº†æ¥è‡ªæ‰€æœ‰ä»»åŠ¡çš„æ•°æ®ï¼Œä½†
    LoRAï¼ˆå®Œæ•´+TPï¼‰çš„è¡¨ç°ç•¥é€Šä¸€ç­¹ã€‚è¿™å¯èƒ½å½’å› äºä»»åŠ¡æç¤ºçš„å¢åŠ ï¼Œè¿™æ‰©å±•äº†è¾“å…¥æ–‡æœ¬ï¼Œå¯¼è‡´ç”±äºè¾“å…¥é•¿åº¦é™åˆ¶è€Œå¯èƒ½æˆªæ–­æœ‰ç”¨è¯æ±‡ã€‚LoRAï¼ˆå•ä¸€ï¼‰è™½ç„¶ä¸ºå•ç‹¬ä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼Œä½†å…¶è¡¨ç°ä¹Ÿä¸å¦‚
    LoRAï¼ˆå®Œæ•´ï¼‰ï¼Œçªæ˜¾äº†è·¨ä»»åŠ¡å…±äº«çŸ¥è¯†çš„ä»·å€¼ã€‚
- en: â€¢
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Cross-task Generalization: We benchmark against a recent cross-task generalization
    method, LoRAHub. Despite its impressive performance in cross-task generalization
    settings, it requires a vast amount of task data, which conflicts with the multi-task
    setting. In our experiments, we only consider 8 tasks, which might explain its
    relative underperformance.'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è·¨ä»»åŠ¡æ³›åŒ–ï¼šæˆ‘ä»¬ä¸è¿‘æœŸçš„è·¨ä»»åŠ¡æ³›åŒ–æ–¹æ³•LoRAHubè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚å°½ç®¡åœ¨è·¨ä»»åŠ¡æ³›åŒ–è®¾ç½®ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒéœ€è¦å¤§é‡ä»»åŠ¡æ•°æ®ï¼Œè¿™ä¸å¤šä»»åŠ¡è®¾ç½®ç›¸å†²çªã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬åªè€ƒè™‘äº†8ä¸ªä»»åŠ¡ï¼Œè¿™å¯èƒ½è§£é‡Šäº†å…¶ç›¸å¯¹çš„è¡¨ç°ä¸ä½³ã€‚
- en: â€¢
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Task-specific Observations: Performance variations are evident across tasks.
    For instance, LoRA (Full) and LoRA (Full+TP) excel in tasks with larger datasets,
    while LoRA (Single) shines in tasks with fewer samples, highlighting the data
    imbalance issue. MOELoRA consistently achieves optimal performance in most tasks,
    demonstrating its ability to effectively address this imbalance. For MedDG tasks,
    the inherent dialog capability of ChatGPT and Huatuo gives them an advantage over
    other approaches.'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»»åŠ¡ç‰¹å®šè§‚å¯Ÿï¼šä¸åŒä»»åŠ¡ä¹‹é—´çš„æ€§èƒ½å·®å¼‚æ˜¾è‘—ã€‚ä¾‹å¦‚ï¼ŒLoRA (Full)å’ŒLoRA (Full+TP)åœ¨æ•°æ®é›†è¾ƒå¤§çš„ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè€ŒLoRA (Single)åœ¨æ ·æœ¬è¾ƒå°‘çš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œçªæ˜¾äº†æ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚MOELoRAåœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸­å§‹ç»ˆèƒ½è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆè§£å†³è¿™ä¸€ä¸å¹³è¡¡çš„èƒ½åŠ›ã€‚åœ¨MedDGä»»åŠ¡ä¸­ï¼ŒChatGPTå’ŒHuatuoå›ºæœ‰çš„å¯¹è¯èƒ½åŠ›ä½¿å®ƒä»¬åœ¨å…¶ä»–æ–¹æ³•ä¸­å…·æœ‰ä¼˜åŠ¿ã€‚
- en: In response to RQ1, MOELoRA demonstrates superior performance compared to other
    parameter-efficient strategies and cross-task generalization methods.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: é’ˆå¯¹RQ1ï¼ŒMOELoRAåœ¨å‚æ•°æ•ˆç‡ç­–ç•¥å’Œè·¨ä»»åŠ¡æ³›åŒ–æ–¹æ³•ä¸­è¡¨ç°ä¼˜è¶Šã€‚
- en: IV-C Ablation Study
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C æ¶ˆèç ”ç©¶
- en: 'To delve deeper into RQ2 and understand the contributions of each component
    in MOELoRA, we present the results of our ablation study in TableÂ [III](#S4.T3
    "TABLE III â€£ IV-A4 Evaluation Metrics â€£ IV-A Experimental Settings â€£ IV Experiment
    â€£ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task
    Medical Applications"). The variant w/o MOE (essentially reverts to LoRA (Full))
    excludes the MOE architecture. It demonstrates inferior performance compared to
    the full-fledged MOELoRA, underscoring the significance of the MOE architecture.
    Similarly, the w/o gate variant, which employs uniform expert weights bypassing
    the gate function, also lags behind MOELoRA, highlighting the gate functionâ€™s
    effectiveness. The w multiple gate variant, uses a unique gate function for each
    MOELoRA layer. We can see that it achieves comparable results on several tasks
    and is slightly outperformed by the single gate function design due to over-parameterization.
    Besides, multiple gate functions also incur a higher count of trainable parameters,
    leading to diminished efficiency compared to a single gate function setup.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¸ºäº†*æ·±å…¥*äº†è§£RQ2å¹¶ç†è§£MOELoRAä¸­æ¯ä¸ªç»„ä»¶çš„è´¡çŒ®ï¼Œæˆ‘ä»¬åœ¨è¡¨[III](#S4.T3 "TABLE III â€£ IV-A4 Evaluation
    Metrics â€£ IV-A Experimental Settings â€£ IV Experiment â€£ MOELoRA: An MOE-based Parameter
    Efficient Fine-Tuning Method for Multi-task Medical Applications")ä¸­å±•ç¤ºäº†æˆ‘ä»¬çš„æ¶ˆèç ”ç©¶ç»“æœã€‚w/o
    MOEå˜ä½“ï¼ˆå®è´¨ä¸Šå›åˆ°LoRA (Full)ï¼‰æ’é™¤äº†MOEæ¶æ„ã€‚ä¸å®Œæ•´çš„MOELoRAç›¸æ¯”ï¼Œå…¶æ€§èƒ½è¾ƒå·®ï¼Œçªæ˜¾äº†MOEæ¶æ„çš„é‡è¦æ€§ã€‚ç±»ä¼¼åœ°ï¼Œw/o gateå˜ä½“é‡‡ç”¨å‡åŒ€çš„ä¸“å®¶æƒé‡ç»•è¿‡é—¨æ§å‡½æ•°ï¼Œä¹Ÿè½åäºMOELoRAï¼Œçªæ˜¾äº†é—¨æ§å‡½æ•°çš„æœ‰æ•ˆæ€§ã€‚w
    multiple gateå˜ä½“ä¸ºæ¯ä¸ªMOELoRAå±‚ä½¿ç”¨ç‹¬ç‰¹çš„é—¨æ§å‡½æ•°ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒåœ¨å‡ ä¸ªä»»åŠ¡ä¸Šå–å¾—äº†ç›¸å½“çš„ç»“æœï¼Œä½†ç”±äºè¿‡åº¦å‚æ•°åŒ–ï¼Œç•¥é€Šäºå•ä¸€é—¨æ§å‡½æ•°è®¾è®¡ã€‚æ­¤å¤–ï¼Œå¤šé‡é—¨æ§å‡½æ•°è¿˜ä¼šå¯¼è‡´æ›´å¤šå¯è®­ç»ƒå‚æ•°ï¼Œä»è€Œé™ä½äº†æ•ˆç‡ã€‚'
- en: Additionally, we analyze the impact of different training strategies. Specifically,
    the w BT methodÂ [[27](#bib.bib27)] consolidates samples from the same task into
    one batch. The w RBT approachÂ [[28](#bib.bib28)] randomly selects a task for each
    batch of data. Both of them prove to be less conducive for MOELoRA, resulting
    in performance degradation. These findings underscore the critical roles of both
    the MOE architecture and the gate function in the MOELoRA model, as well as the
    influence of specific training patterns.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ†æäº†ä¸åŒè®­ç»ƒç­–ç•¥çš„å½±å“ã€‚å…·ä½“è€Œè¨€ï¼Œw BT æ–¹æ³•[[27](#bib.bib27)]å°†åŒä¸€ä»»åŠ¡çš„æ ·æœ¬åˆå¹¶ä¸ºä¸€ä¸ªæ‰¹æ¬¡ã€‚w RBT æ–¹æ³•[[28](#bib.bib28)]åˆ™éšæœºé€‰æ‹©æ¯æ‰¹æ•°æ®çš„ä»»åŠ¡ã€‚è¿™ä¸¤ç§æ–¹æ³•éƒ½è¯æ˜å¯¹MOELoRAä¸å¤ªæœ‰åˆ©ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚è¿™äº›å‘ç°çªæ˜¾äº†MOELoRAæ¨¡å‹ä¸­MOEæ¶æ„å’Œé—¨æ§å‡½æ•°çš„å…³é”®ä½œç”¨ï¼Œä»¥åŠç‰¹å®šè®­ç»ƒæ¨¡å¼çš„å½±å“ã€‚
- en: '![Refer to caption](img/bbb02dfe735ada014f5e9c4fc01e7105.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/bbb02dfe735ada014f5e9c4fc01e7105.png)'
- en: (a)
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/b2b545cf1b24e845d33abc21ee43a131.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/b2b545cf1b24e845d33abc21ee43a131.png)'
- en: (b)
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 5: The results of experiments for hyper-parameters, *i.e.,* expert number
    $N$.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5ï¼šè¶…å‚æ•°å®éªŒç»“æœï¼Œ*å³*ï¼Œä¸“å®¶æ•°é‡$N$ã€‚
- en: IV-D Hyper-parameter Analysis
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D è¶…å‚æ•°åˆ†æ
- en: 'To answer RQ3, we delve into the impact of hyper-parameters on the performance
    of MOELoRA. Specifically, we examine how variations in the expert number $M$ to
    $8$, ensuring that the size of trainable parameters remains relatively stable.
    Subsequently, we observe from FigureÂ [5b](#S4.F5.sf2 "In Figure 5 â€£ IV-C Ablation
    Study â€£ IV Experiment â€£ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning
    Method for Multi-task Medical Applications") that while an increase in $r$.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å›ç­” RQ3ï¼Œæˆ‘ä»¬**æ·±å…¥æ¢è®¨**äº†è¶…å‚æ•°å¯¹ MOELoRA æ€§èƒ½çš„å½±å“ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ£€æŸ¥äº†ä¸“å®¶æ•°é‡ $M$ ä» 8 çš„å˜åŒ–æƒ…å†µï¼ŒåŒæ—¶ç¡®ä¿å¯è®­ç»ƒå‚æ•°çš„å¤§å°ä¿æŒç›¸å¯¹ç¨³å®šã€‚éšåï¼Œæˆ‘ä»¬ä»å›¾
    [5b](#S4.F5.sf2 "åœ¨å›¾ 5 â€£ IV-C æ¶ˆèç ”ç©¶ â€£ IV å®éªŒ â€£ MOELoRAï¼šä¸€ç§åŸºäº MOE çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œç”¨äºå¤šä»»åŠ¡åŒ»ç–—åº”ç”¨")
    ä¸­è§‚å¯Ÿåˆ°ï¼Œå°½ç®¡ $r$ å¢åŠ ã€‚
- en: IV-E Case Study
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-E æ¡ˆä¾‹ç ”ç©¶
- en: 'For RQ4, we present a visualization of the expert weights across four tasks
    in FigureÂ [6](#S4.F6 "Figure 6 â€£ IV-E Case Study â€£ IV Experiment â€£ MOELoRA: An
    MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications").
    For each task, the length of the bar in different colors represents the weights
    for the corresponding expert. Since the expert weights are normalized to 1, the
    lengths of the bar for each task are the same. At a macro level, it is evident
    that the contributions from each expert vary significantly, underscoring the idea
    that different experts specialize in distinct facets of medical knowledge. Moreover,
    the pronounced disparities in weights across tasks highlight the diverse nature
    of medical applications. Taking a closer look at the tasks CHIP-CDN and KUAKE-QIC,
    we observe that their expert weights are largely congruent, with exceptions in
    experts 3 and 4. Given that CHIP-CDN can be viewed as a precursor to KUAKE-QICâ€”since
    diagnostic word normalization can bolster inquiry classificationâ€”the similarity
    in expert weights suggests that MOELoRA is adept at harnessing shared knowledge
    to benefit intrinsically related tasks.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº RQ4ï¼Œæˆ‘ä»¬åœ¨å›¾ [6](#S4.F6 "å›¾ 6 â€£ IV-E æ¡ˆä¾‹ç ”ç©¶ â€£ IV å®éªŒ â€£ MOELoRAï¼šä¸€ç§åŸºäº MOE çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œç”¨äºå¤šä»»åŠ¡åŒ»ç–—åº”ç”¨")
    ä¸­å±•ç¤ºäº†å››ä¸ªä»»åŠ¡çš„ä¸“å®¶æƒé‡çš„å¯è§†åŒ–ã€‚å¯¹äºæ¯ä¸ªä»»åŠ¡ï¼Œä¸åŒé¢œè‰²çš„æ¡å½¢é•¿åº¦è¡¨ç¤ºç›¸åº”ä¸“å®¶çš„æƒé‡ã€‚ç”±äºä¸“å®¶æƒé‡è¢«æ ‡å‡†åŒ–ä¸º 1ï¼Œå› æ­¤æ¯ä¸ªä»»åŠ¡çš„æ¡å½¢é•¿åº¦æ˜¯ç›¸åŒçš„ã€‚ä»å®è§‚å±‚é¢æ¥çœ‹ï¼Œå„ä¸“å®¶çš„è´¡çŒ®æ˜¾è‘—ä¸åŒï¼Œçªæ˜¾äº†ä¸åŒä¸“å®¶åœ¨åŒ»ç–—çŸ¥è¯†ä¸åŒæ–¹é¢çš„ä¸“é•¿ã€‚æ­¤å¤–ï¼Œä»»åŠ¡ä¹‹é—´æƒé‡çš„æ˜æ˜¾å·®å¼‚çªæ˜¾äº†åŒ»ç–—åº”ç”¨çš„å¤šæ ·æ€§ã€‚ä»”ç»†æŸ¥çœ‹ä»»åŠ¡
    CHIP-CDN å’Œ KUAKE-QICï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å®ƒä»¬çš„ä¸“å®¶æƒé‡å¤§ä½“ä¸€è‡´ï¼Œåªæœ‰ä¸“å®¶ 3 å’Œ 4 æœ‰ä¾‹å¤–ã€‚è€ƒè™‘åˆ° CHIP-CDN å¯ä»¥è¢«è§†ä¸º KUAKE-QIC
    çš„å‰èº«â€”â€”å› ä¸ºè¯Šæ–­è¯è§„èŒƒåŒ–å¯ä»¥æå‡æŸ¥è¯¢åˆ†ç±»â€”â€”ä¸“å®¶æƒé‡çš„ç›¸ä¼¼æ€§è¡¨æ˜ MOELoRA èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨å…±äº«çŸ¥è¯†ï¼Œé€ ç¦äºæœ¬è´¨ä¸Šç›¸å…³çš„ä»»åŠ¡ã€‚
- en: '![Refer to caption](img/6686ec5e867e1500ea35c838d4ce914d.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/6686ec5e867e1500ea35c838d4ce914d.png)'
- en: 'Figure 6: The visualization of expert weights for various tasks. In each task,
    the length of the bar in different colors represents the weights for the corresponding
    expert.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 6ï¼šå„ç§ä»»åŠ¡çš„ä¸“å®¶æƒé‡å¯è§†åŒ–ã€‚åœ¨æ¯ä¸ªä»»åŠ¡ä¸­ï¼Œä¸åŒé¢œè‰²çš„æ¡å½¢é•¿åº¦è¡¨ç¤ºç›¸åº”ä¸“å®¶çš„æƒé‡ã€‚
- en: V Related Works
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V ç›¸å…³å·¥ä½œ
- en: V-A LLM for Medical Applications
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A åŒ»ç–—åº”ç”¨ä¸­çš„ LLM
- en: Recently, there has been a surge of work in the medical field leveraging the
    powerful capabilities of LLM. For instance, Med-PaLM [[15](#bib.bib15)] proposes
    a new benchmark called MultiMedQA, which combines seven medical question-answering
    datasets to address the challenges of evaluating the clinical knowledge of LLM.
    Med-PaLM2 [[33](#bib.bib33)] has further improved upon Med-PaLM by introducing
    a new prompting strategy called ensemble refinement. This strategy is based on
    CoT [[34](#bib.bib34)] and self-consistency [[35](#bib.bib35)] and has shown significant
    improvements in MedQA. Whatâ€™s more, ChatDoctor [[6](#bib.bib6)] constructs a dataset
    of 100,000 patient-doctor dialogues collected from a widely used online medical
    consultation platform. The dataset is fine-tuned on LLaMA and an automated information
    retrieval method is proposed to utilize online information, like Wikipedia. Besides,
    HuaTuo [[8](#bib.bib8)] is based on LLaMaÂ [[24](#bib.bib24)] and fine-tuned using
    Chinese medical knowledge from CMeKG [[36](#bib.bib36)]. Additionally, HuaTuo
    has introduced a new evaluation metric called SUS for Safety, Usability, and Smoothness.
    Previous studies have demonstrated the potential of LLMs in the medical field.
    However, they tend to focus on medical dialogue while neglecting other equally
    important tasks (such as medical NER and diagnosis report generation) and usually
    demand a significant fine-tuning cost for achieving generalization ability.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘ï¼ŒåŒ»å­¦é¢†åŸŸçš„å·¥ä½œå‡ºç°äº†æ¿€å¢ï¼Œåˆ©ç”¨äº†LLMçš„å¼ºå¤§èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼ŒMed-PaLM [[15](#bib.bib15)] æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œç§°ä¸ºMultiMedQAï¼Œè¯¥æµ‹è¯•ç»“åˆäº†ä¸ƒä¸ªåŒ»å­¦é—®ç­”æ•°æ®é›†ï¼Œä»¥åº”å¯¹è¯„ä¼°LLMä¸´åºŠçŸ¥è¯†çš„æŒ‘æˆ˜ã€‚Med-PaLM2
    [[33](#bib.bib33)] é€šè¿‡å¼•å…¥ä¸€ç§æ–°çš„æç¤ºç­–ç•¥ç§°ä¸ºé›†æˆç»†åŒ–ï¼Œè¿›ä¸€æ­¥æ”¹è¿›äº†Med-PaLMã€‚è¿™ç§ç­–ç•¥åŸºäºCoT [[34](#bib.bib34)]
    å’Œè‡ªä¸€è‡´æ€§ [[35](#bib.bib35)]ï¼Œåœ¨MedQAä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼ŒChatDoctor [[6](#bib.bib6)] æ„å»ºäº†ä¸€ä¸ªç”±ä»å¹¿æ³›ä½¿ç”¨çš„åœ¨çº¿åŒ»ç–—å’¨è¯¢å¹³å°æ”¶é›†çš„100,000ä¸ªæ‚£è€…-åŒ»ç”Ÿå¯¹è¯ç»„æˆçš„æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åœ¨LLaMAä¸Šè¿›è¡Œäº†å¾®è°ƒï¼Œå¹¶æå‡ºäº†ä¸€ç§è‡ªåŠ¨ä¿¡æ¯æ£€ç´¢æ–¹æ³•ï¼Œä»¥åˆ©ç”¨åœ¨çº¿ä¿¡æ¯ï¼Œå¦‚Wikipediaã€‚æ­¤å¤–ï¼ŒHuaTuo
    [[8](#bib.bib8)] åŸºäºLLaMa [[24](#bib.bib24)] å¹¶ä½¿ç”¨æ¥è‡ªCMeKG [[36](#bib.bib36)] çš„ä¸­æ–‡åŒ»å­¦çŸ¥è¯†è¿›è¡Œäº†å¾®è°ƒã€‚æ­¤å¤–ï¼ŒHuaTuoå¼•å…¥äº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼Œç§°ä¸ºSUSï¼Œç”¨äºå®‰å…¨æ€§ã€å¯ç”¨æ€§å’Œæµç•…æ€§ã€‚ä»¥å¾€çš„ç ”ç©¶å·²å±•ç¤ºäº†LLMåœ¨åŒ»å­¦é¢†åŸŸçš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¾€å¾€ä¸“æ³¨äºåŒ»å­¦å¯¹è¯ï¼ŒåŒæ—¶å¿½è§†å…¶ä»–åŒæ ·é‡è¦çš„ä»»åŠ¡ï¼ˆå¦‚åŒ»å­¦NERå’Œè¯Šæ–­æŠ¥å‘Šç”Ÿæˆï¼‰ï¼Œå¹¶ä¸”é€šå¸¸éœ€è¦æ˜¾è‘—çš„å¾®è°ƒæˆæœ¬ä»¥å®ç°æ³›åŒ–èƒ½åŠ›ã€‚
- en: V-B Parameter-Efficient Fine-tuning
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B å‚æ•°é«˜æ•ˆå¾®è°ƒ
- en: Parameter efficient fine-tuning (PEFT) aims to improve the performance of LLMs
    on new tasks by minimizing the number of fine-tuning parameters and computational
    complexity. Adapter Tuning [[37](#bib.bib37)] first introduces a lightweight adapter
    module, which has only a few trainable parameters and has shown comparable results
    to fine-tuning on the top layers of LLMs. On the other hand, prefix-tuning [[38](#bib.bib38)]
    and P-Tuning [[39](#bib.bib39)] both construct a task-specific virtual token that
    adds trainable, continuous prompts or embeddings to the original text sequence,
    making optimization more feasible than with discrete prompts. However, using prompts
    can be challenging for training and can also limit the available sequence length
    of the model. LoRA [[21](#bib.bib21)] is inspired by the discovery that low intrinsic
    dimensionÂ [[26](#bib.bib26)] in the large parameters is the key role of LLMs,
    and introduces two trainable low-rank matrices into each dense layer. It has been
    shown to achieve comparable performance to full fine-tuning while requiring no
    additional computation during inference. Nevertheless, the LoRA fine-tuning performs
    inferiorly for multi-task medical applications. It can only learn integral updated
    parameters for all tasks, which loses the vital task-specific information. In
    recent times, a thread of research named cross-task generalizationÂ [[31](#bib.bib31),
    [28](#bib.bib28), [40](#bib.bib40), [41](#bib.bib41)] emerges, which proposes
    various parameter efficient fine-tuning strategies for multi-task. However, different
    from the multi-task setting in this paper, they first train the model on too many
    tasks and aim to transfer the ability to unseen tasks. Due to the distinct setting,
    their method is difficult to be adapted to our problem. In a word, the multi-task
    parameter efficient fine-tuning for LLM-driven medical applications is still underexplored,
    and we take the first step.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰çš„ç›®çš„æ˜¯é€šè¿‡æœ€å°åŒ–å¾®è°ƒå‚æ•°å’Œè®¡ç®—å¤æ‚æ€§æ¥æé«˜LLMsåœ¨æ–°ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚Adapter Tuning [[37](#bib.bib37)]
    é¦–å…ˆå¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„é€‚é…å™¨æ¨¡å—ï¼Œè¯¥æ¨¡å—åªæœ‰å°‘é‡å¯è®­ç»ƒçš„å‚æ•°ï¼Œå¹¶ä¸”æ˜¾ç¤ºå‡ºä¸LLMsé¡¶å±‚å¾®è°ƒçš„ç»“æœç›¸å½“ã€‚å¦ä¸€æ–¹é¢ï¼Œprefix-tuning [[38](#bib.bib38)]
    å’ŒP-Tuning [[39](#bib.bib39)] éƒ½æ„å»ºäº†ä¸€ä¸ªä»»åŠ¡ç‰¹å®šçš„è™šæ‹Ÿä»¤ç‰Œï¼Œè¿™ä¸ªä»¤ç‰Œå‘åŸå§‹æ–‡æœ¬åºåˆ—ä¸­æ·»åŠ äº†å¯è®­ç»ƒçš„ã€è¿ç»­çš„æç¤ºæˆ–åµŒå…¥ï¼Œä½¿å¾—ä¼˜åŒ–æ¯”ç¦»æ•£æç¤ºæ›´ä¸ºå¯è¡Œã€‚ç„¶è€Œï¼Œä½¿ç”¨æç¤ºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯èƒ½ä¼šé¢ä¸´æŒ‘æˆ˜ï¼Œå¹¶ä¸”ä¹Ÿå¯èƒ½é™åˆ¶æ¨¡å‹çš„åºåˆ—é•¿åº¦ã€‚LoRA
    [[21](#bib.bib21)] çš„çµæ„Ÿæ¥æºäºä½å†…åœ¨ç»´åº¦ [[26](#bib.bib26)] åœ¨å¤§å‚æ•°ä¸­çš„å‘ç°ï¼Œè¿™è¢«è®¤ä¸ºæ˜¯LLMsçš„å…³é”®ä½œç”¨ï¼Œå¹¶åœ¨æ¯ä¸ªå¯†é›†å±‚ä¸­å¼•å…¥äº†ä¸¤ä¸ªå¯è®­ç»ƒçš„ä½ç§©çŸ©é˜µã€‚å®ƒå·²è¢«è¯æ˜åœ¨æ€§èƒ½ä¸Šä¸å®Œæ•´å¾®è°ƒç›¸å½“ï¼ŒåŒæ—¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ— éœ€é¢å¤–è®¡ç®—ã€‚ç„¶è€Œï¼ŒLoRAå¾®è°ƒåœ¨å¤šä»»åŠ¡åŒ»ç–—åº”ç”¨ä¸­çš„è¡¨ç°ä¸ä½³ã€‚å®ƒåªèƒ½ä¸ºæ‰€æœ‰ä»»åŠ¡å­¦ä¹ æ•´ä½“æ›´æ–°çš„å‚æ•°ï¼Œè¿™ä¼šä¸§å¤±å…³é”®çš„ä»»åŠ¡ç‰¹å®šä¿¡æ¯ã€‚è¿‘å¹´æ¥ï¼Œå‡ºç°äº†ä¸€ç§åä¸ºè·¨ä»»åŠ¡æ³›åŒ–çš„ç ”ç©¶
    [[31](#bib.bib31), [28](#bib.bib28), [40](#bib.bib40), [41](#bib.bib41)]ï¼Œæå‡ºäº†å„ç§å¤šä»»åŠ¡çš„å‚æ•°é«˜æ•ˆå¾®è°ƒç­–ç•¥ã€‚ç„¶è€Œï¼Œä¸æœ¬æ–‡ä¸­çš„å¤šä»»åŠ¡è®¾ç½®ä¸åŒçš„æ˜¯ï¼Œå®ƒä»¬é¦–å…ˆåœ¨è¿‡å¤šçš„ä»»åŠ¡ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œå¹¶æ—¨åœ¨å°†èƒ½åŠ›è½¬ç§»åˆ°æœªè§è¿‡çš„ä»»åŠ¡ä¸Šã€‚ç”±äºè®¾ç½®çš„ä¸åŒï¼Œå®ƒä»¬çš„æ–¹æ³•éš¾ä»¥é€‚åº”æˆ‘ä»¬çš„éš¾é¢˜ã€‚æ€»è€Œè¨€ä¹‹ï¼ŒLLMé©±åŠ¨çš„åŒ»ç–—åº”ç”¨ä¸­çš„å¤šä»»åŠ¡å‚æ•°é«˜æ•ˆå¾®è°ƒä»ç„¶æ˜¯ä¸€ä¸ªæœªè¢«å……åˆ†æ¢ç´¢çš„é¢†åŸŸï¼Œæˆ‘ä»¬è¿ˆå‡ºäº†ç¬¬ä¸€æ­¥ã€‚
- en: VI Conclusion
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI ç»“è®º
- en: In this paper, we first take the step to explore the multi-task parameter efficient
    fine-tuning method for LLM-driven medical applications. To satisfy the requirements
    of efficiency for fine-tuning and effectiveness for multi-task, we propose a novel
    multi-task fine-tuning framework. Specifically, we design the MOELoRA architecture,
    which consists of several low-rank experts as the trainable parameters to learn
    task-related knowledge and retain high efficiency. Besides, a task motivated gate
    function is devised to produce distinct fine-tuned parameters for various tasks.
    By the comprehensive experiments on a multi-task Chinese medical dataset, we verify
    the effectiveness of the proposed MOELoRA. In the future, we will further explore
    how to combine explicit medical knowledge, such as knowledge graphs, with LLMs
    by fine-tuning.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆæ¢ç´¢äº†LLMé©±åŠ¨åŒ»ç–—åº”ç”¨ä¸­çš„å¤šä»»åŠ¡å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ã€‚ä¸ºäº†æ»¡è¶³å¾®è°ƒçš„æ•ˆç‡è¦æ±‚å’Œå¤šä»»åŠ¡çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„å¤šä»»åŠ¡å¾®è°ƒæ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†MOELoRAæ¶æ„ï¼Œå®ƒç”±å‡ ä¸ªä½ç§©ä¸“å®¶ç»„æˆï¼Œä½œä¸ºå¯è®­ç»ƒçš„å‚æ•°ä»¥å­¦ä¹ ä»»åŠ¡ç›¸å…³çŸ¥è¯†å¹¶ä¿æŒé«˜æ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ä¸ªä»»åŠ¡é©±åŠ¨çš„é—¨æ§å‡½æ•°ï¼Œä»¥ç”Ÿæˆä¸åŒä»»åŠ¡çš„å¾®è°ƒå‚æ•°ã€‚é€šè¿‡åœ¨å¤šä»»åŠ¡ä¸­æ–‡åŒ»ç–—æ•°æ®é›†ä¸Šçš„å…¨é¢å®éªŒï¼Œæˆ‘ä»¬éªŒè¯äº†æ‰€æå‡ºçš„MOELoRAçš„æœ‰æ•ˆæ€§ã€‚åœ¨æœªæ¥ï¼Œæˆ‘ä»¬å°†è¿›ä¸€æ­¥æ¢ç´¢å¦‚ä½•é€šè¿‡å¾®è°ƒå°†æ˜¾å¼åŒ»ç–—çŸ¥è¯†ï¼Œå¦‚çŸ¥è¯†å›¾è°±ï¼Œä¸LLMsç»“åˆã€‚
- en: References
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] X.Â Liu, Y.Â Zheng, Z.Â Du, M.Â Ding, Y.Â Qian, Z.Â Yang, and J.Â Tang, â€œGpt understands,
    too,â€ *AI Open*, 2023.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] X. Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang, å’Œ J. Tang, â€œGpt understands,
    too,â€ *AI Open*, 2023.'
- en: '[2] A.Â Zeng, X.Â Liu, Z.Â Du, Z.Â Wang, H.Â Lai, M.Â Ding, Z.Â Yang, Y.Â Xu, W.Â Zheng,
    X.Â Xia *etÂ al.*, â€œGlm-130b: An open bilingual pre-trained model,â€ in *The Eleventh
    International Conference on Learning Representations*, 2022.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] A.Â Zeng, X.Â Liu, Z.Â Du, Z.Â Wang, H.Â Lai, M.Â Ding, Z.Â Yang, Y.Â Xu, W.Â Zheng,
    X.Â Xia *ç­‰*ï¼Œâ€œGLM-130Bï¼šä¸€ç§å¼€æ”¾çš„åŒè¯­é¢„è®­ç»ƒæ¨¡å‹â€ï¼Œè§äº*ç¬¬åä¸€å±Šå›½é™…å­¦ä¹ è¡¨å¾ä¼šè®®*ï¼Œ2022ã€‚'
- en: '[3] W.Â Fan, Z.Â Zhao, J.Â Li, Y.Â Liu, X.Â Mei, Y.Â Wang, J.Â Tang, and Q.Â Li, â€œRecommender
    systems in the era of large language models (llms),â€ *arXiv preprint arXiv:2307.02046*,
    2023.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] W.Â Fan, Z.Â Zhao, J.Â Li, Y.Â Liu, X.Â Mei, Y.Â Wang, J.Â Tang, å’Œ Q.Â Liï¼Œâ€œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ—¶ä»£çš„æ¨èç³»ç»Ÿâ€ï¼Œ*arXiv
    é¢„å°æœ¬ arXiv:2307.02046*ï¼Œ2023ã€‚'
- en: '[4] L.Â Wang, C.Â Ma, X.Â Feng, Z.Â Zhang, H.Â Yang, J.Â Zhang, Z.Â Chen, J.Â Tang,
    X.Â Chen, Y.Â Lin *etÂ al.*, â€œA survey on large language model based autonomous agents,â€
    *arXiv preprint arXiv:2308.11432*, 2023.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] L.Â Wang, C.Â Ma, X.Â Feng, Z.Â Zhang, H.Â Yang, J.Â Zhang, Z.Â Chen, J.Â Tang,
    X.Â Chen, Y.Â Lin *ç­‰*ï¼Œâ€œå…³äºåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªä¸»ä»£ç†çš„è°ƒæŸ¥â€ï¼Œ*arXiv é¢„å°æœ¬ arXiv:2308.11432*ï¼Œ2023ã€‚'
- en: '[5] M.Â U. Hadi, R.Â Qureshi, A.Â Shah, M.Â Irfan, A.Â Zafar, M.Â Shaikh, N.Â Akhtar,
    J.Â Wu, and S.Â Mirjalili, â€œA survey on large language models: Applications, challenges,
    limitations, and practical usage,â€ *TechRxiv*, 2023.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] M.Â U. Hadi, R.Â Qureshi, A.Â Shah, M.Â Irfan, A.Â Zafar, M.Â Shaikh, N.Â Akhtar,
    J.Â Wu, å’Œ S.Â Mirjaliliï¼Œâ€œå…³äºå¤§å‹è¯­è¨€æ¨¡å‹çš„è°ƒæŸ¥ï¼šåº”ç”¨ã€æŒ‘æˆ˜ã€å±€é™æ€§å’Œå®é™…ä½¿ç”¨â€ï¼Œ*TechRxiv*ï¼Œ2023ã€‚'
- en: '[6] L.Â Yunxiang, L.Â Zihan, Z.Â Kai, D.Â Ruilong, and Z.Â You, â€œChatdoctor: A medical
    chat model fine-tuned on llama model using medical domain knowledge,â€ *arXiv preprint
    arXiv:2303.14070*, 2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] L.Â Yunxiang, L.Â Zihan, Z.Â Kai, D.Â Ruilong, å’Œ Z.Â Youï¼Œâ€œChatdoctorï¼šä¸€ä¸ªä½¿ç”¨åŒ»å­¦é¢†åŸŸçŸ¥è¯†åœ¨LLAMAæ¨¡å‹ä¸Šå¾®è°ƒçš„åŒ»å­¦èŠå¤©æ¨¡å‹â€ï¼Œ*arXiv
    é¢„å°æœ¬ arXiv:2303.14070*ï¼Œ2023ã€‚'
- en: '[7] OpenAI, â€œGpt-4 technical report,â€ *arXiv preprint arXiv:2303.08774*, 2023.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] OpenAIï¼Œâ€œGPT-4æŠ€æœ¯æŠ¥å‘Šâ€ï¼Œ*arXiv é¢„å°æœ¬ arXiv:2303.08774*ï¼Œ2023ã€‚'
- en: '[8] H.Â Wang, C.Â Liu, N.Â Xi, Z.Â Qiang, S.Â Zhao, B.Â Qin, and T.Â Liu, â€œHuatuo:
    Tuning llama model with chinese medical knowledge,â€ 2023.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] H.Â Wang, C.Â Liu, N.Â Xi, Z.Â Qiang, S.Â Zhao, B.Â Qin, å’Œ T.Â Liuï¼Œâ€œåä½—ï¼šç”¨ä¸­æ–‡åŒ»å­¦çŸ¥è¯†è°ƒæ•´LLAMAæ¨¡å‹â€ï¼Œ2023ã€‚'
- en: '[9] Z.Â Zheng, Z.Â Qiu, H.Â Xiong, X.Â Wu, T.Â Xu, E.Â Chen, and X.Â Zhao, â€œDdr: Dialogue
    based doctor recommendation for online medical service,â€ in *Proceedings of the
    28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*, ser. KDD â€™22.Â Â Â New
    York, NY, USA: Association for Computing Machinery, 2022, p. 4592â€“4600\. [Online].
    Available: https://doi.org/10.1145/3534678.3539201'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Z.Â Zheng, Z.Â Qiu, H.Â Xiong, X.Â Wu, T.Â Xu, E.Â Chen, å’Œ X.Â Zhaoï¼Œâ€œDDRï¼šåŸºäºå¯¹è¯çš„åœ¨çº¿åŒ»ç–—æœåŠ¡åŒ»ç”Ÿæ¨èâ€ï¼Œè§äº*ç¬¬28å±ŠACM
    SIGKDDçŸ¥è¯†å‘ç°ä¸æ•°æ®æŒ–æ˜å¤§ä¼šè®ºæ–‡é›†*ï¼Œç³»åˆ— KDD â€™22ã€‚çº½çº¦ï¼Œç¾å›½ï¼šè®¡ç®—æœºåä¼šï¼Œ2022ï¼Œç¬¬4592â€“4600é¡µã€‚[åœ¨çº¿]ã€‚å¯ç”¨é“¾æ¥: https://doi.org/10.1145/3534678.3539201'
- en: '[10] Z.Â Qiao, X.Â Wu, S.Â Ge, and W.Â Fan, â€œMnn: Multimodal attentional neural
    networks for diagnosis prediction,â€ in *International Joint Conference on Artificial
    Intelligence*, 2019\. [Online]. Available: https://api.semanticscholar.org/CorpusID:199466261'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Z.Â Qiao, X.Â Wu, S.Â Ge, å’Œ W.Â Fanï¼Œâ€œMNNï¼šç”¨äºè¯Šæ–­é¢„æµ‹çš„å¤šæ¨¡æ€æ³¨æ„åŠ›ç¥ç»ç½‘ç»œâ€ï¼Œè§äº*å›½é™…äººå·¥æ™ºèƒ½è”åˆä¼šè®®*ï¼Œ2019ã€‚[åœ¨çº¿]ã€‚å¯ç”¨é“¾æ¥:
    https://api.semanticscholar.org/CorpusID:199466261'
- en: '[11] Y.Â Zhang, X.Â Wu, Q.Â Fang, S.Â Qian, and C.Â Xu, â€œKnowledge-enhanced attributed
    multi-task learning for medicine recommendation,â€ *ACM Trans. Inf. Syst.*, jan
    2023.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Y.Â Zhang, X.Â Wu, Q.Â Fang, S.Â Qian, å’Œ C.Â Xuï¼Œâ€œç”¨äºåŒ»å­¦æ¨èçš„çŸ¥è¯†å¢å¼ºå±æ€§å¤šä»»åŠ¡å­¦ä¹ â€ï¼Œ*ACM ä¿¡æ¯ç³»ç»Ÿæ±‡åˆŠ*ï¼Œ2023å¹´1æœˆã€‚'
- en: '[12] S.Â Zhao, T.Â Liu, S.Â Zhao, and F.Â Wang, â€œA neural multi-task learning framework
    to jointly model medical named entity recognition and normalization,â€ in *Proceedings
    of the AAAI Conference on Artificial Intelligence*, vol.Â 33, no.Â 01, 2019, pp.
    817â€“824.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] S.Â Zhao, T.Â Liu, S.Â Zhao, å’Œ F.Â Wangï¼Œâ€œä¸€ä¸ªç¥ç»å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ç”¨äºè”åˆå»ºæ¨¡åŒ»å­¦å‘½åå®ä½“è¯†åˆ«å’Œæ ‡å‡†åŒ–â€ï¼Œè§äº*AAAI
    äººå·¥æ™ºèƒ½ä¼šè®®è®ºæ–‡é›†*ï¼Œç¬¬33å·ï¼Œç¬¬01æœŸï¼Œ2019ï¼Œç¬¬817â€“824é¡µã€‚'
- en: '[13] S.Â Rezayi, H.Â Dai, Z.Â Liu, Z.Â Wu, A.Â Hebbar, A.Â H. Burns, L.Â Zhao, D.Â Zhu,
    Q.Â Li, W.Â Liu *etÂ al.*, â€œClinicalradiobert: Knowledge-infused few shot learning
    for clinical notes named entity recognition,â€ in *International Workshop on Machine
    Learning in Medical Imaging*.Â Â Â Springer, 2022, pp. 269â€“278.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] S.Â Rezayi, H.Â Dai, Z.Â Liu, Z.Â Wu, A.Â Hebbar, A.Â H. Burns, L.Â Zhao, D.Â Zhu,
    Q.Â Li, W.Â Liu *ç­‰*ï¼Œâ€œä¸´åºŠæ”¾å°„ä½“ï¼šç”¨äºä¸´åºŠç¬”è®°å‘½åå®ä½“è¯†åˆ«çš„çŸ¥è¯†æ³¨å…¥å¼å°‘æ ·æœ¬å­¦ä¹ â€ï¼Œè§äº*å›½é™…åŒ»å­¦å½±åƒæœºå™¨å­¦ä¹ ç ”è®¨ä¼š*ã€‚Springerï¼Œ2022ï¼Œç¬¬269â€“278é¡µã€‚'
- en: '[14] Y.Â Miura, Y.Â Zhang, E.Â B. Tsai, C.Â P. Langlotz, and D.Â Jurafsky, â€œImproving
    factual completeness and consistency of image-to-text radiology report generation,â€
    *arXiv preprint arXiv:2010.10042*, 2020.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Y.Â Miura, Y.Â Zhang, E.Â B. Tsai, C.Â P. Langlotz, å’Œ D.Â Jurafskyï¼Œâ€œæé«˜å›¾åƒåˆ°æ–‡æœ¬æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆçš„äº‹å®å®Œæ•´æ€§å’Œä¸€è‡´æ€§â€ï¼Œ*arXiv
    é¢„å°æœ¬ arXiv:2010.10042*ï¼Œ2020ã€‚'
- en: '[15] K.Â Singhal, S.Â Azizi, T.Â Tu, S.Â S. Mahdavi, J.Â Wei, H.Â W. Chung, N.Â Scales,
    A.Â Tanwani, H.Â Cole-Lewis, S.Â Pfohl *etÂ al.*, â€œLarge language models encode clinical
    knowledge,â€ *Nature*, vol. 620, no. 7972, pp. 172â€“180, 2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales,
    A. Tanwani, H. Cole-Lewis, S. Pfohl *ç­‰*ï¼Œâ€œå¤§å‹è¯­è¨€æ¨¡å‹ç¼–ç ä¸´åºŠçŸ¥è¯†â€ï¼Œ*Nature*ï¼Œç¬¬620å·ï¼Œç¬¬7972æœŸï¼Œç¬¬172â€“180é¡µï¼Œ2023å¹´ã€‚'
- en: '[16] J.Â D. M.-W.Â C. Kenton and L.Â K. Toutanova, â€œBert: Pre-training of deep
    bidirectional transformers for language understanding,â€ in *Proceedings of NAACL-HLT*,
    2019, pp. 4171â€“4186.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] J. D. M.-W. C. Kenton å’Œ L. K. Toutanovaï¼Œâ€œBertï¼šç”¨äºè¯­è¨€ç†è§£çš„æ·±åº¦åŒå‘å˜æ¢å™¨é¢„è®­ç»ƒâ€ï¼Œæ”¶å½•äº*NAACL-HLTä¼šè®®è®ºæ–‡é›†*ï¼Œ2019å¹´ï¼Œç¬¬4171â€“4186é¡µã€‚'
- en: '[17] A.Â Yang, B.Â Xiao, B.Â Wang, B.Â Zhang, C.Â Yin, C.Â Lv, D.Â Pan, D.Â Wang, D.Â Yan,
    F.Â Yang *etÂ al.*, â€œBaichuan 2: Open large-scale language models,â€ *arXiv preprint
    arXiv:2309.10305*, 2023.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] A. Yang, B. Xiao, B. Wang, B. Zhang, C. Yin, C. Lv, D. Pan, D. Wang, D.
    Yan, F. Yang *ç­‰*ï¼Œâ€œBaichuan 2ï¼šå¼€æ”¾çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹â€ï¼Œ*arXiv é¢„å°æœ¬ arXiv:2309.10305*ï¼Œ2023å¹´ã€‚'
- en: '[18] Y.Â Zhang and Q.Â Yang, â€œA survey on multi-task learning,â€ *IEEE Transactions
    on Knowledge and Data Engineering*, vol.Â 34, no.Â 12, pp. 5586â€“5609, 2021.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Y. Zhang å’Œ Q. Yangï¼Œâ€œå¤šä»»åŠ¡å­¦ä¹ ç»¼è¿°â€ï¼Œ*IEEEçŸ¥è¯†ä¸æ•°æ®å·¥ç¨‹å­¦æŠ¥*ï¼Œç¬¬34å·ï¼Œç¬¬12æœŸï¼Œç¬¬5586â€“5609é¡µï¼Œ2021å¹´ã€‚'
- en: '[19] M.Â Crawshaw, â€œMulti-task learning with deep neural networks: A survey,â€
    *arXiv preprint arXiv:2009.09796*, 2020.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] M. Crawshawï¼Œâ€œæ·±åº¦ç¥ç»ç½‘ç»œçš„å¤šä»»åŠ¡å­¦ä¹ ï¼šç»¼è¿°â€ï¼Œ*arXiv é¢„å°æœ¬ arXiv:2009.09796*ï¼Œ2020å¹´ã€‚'
- en: '[20] N.Â Shazeer, A.Â Mirhoseini, K.Â Maziarz, A.Â Davis, Q.Â Le, G.Â Hinton, and
    J.Â Dean, â€œOutrageously large neural networks: The sparsely-gated mixture-of-experts
    layer,â€ *arXiv preprint arXiv:1701.06538*, 2017.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, å’Œ J.
    Deanï¼Œâ€œæå…¶åºå¤§çš„ç¥ç»ç½‘ç»œï¼šç¨€ç–é—¨æ§ä¸“å®¶æ··åˆå±‚â€ï¼Œ*arXiv é¢„å°æœ¬ arXiv:1701.06538*ï¼Œ2017å¹´ã€‚'
- en: '[21] E.Â J. Hu, P.Â Wallis, Z.Â Allen-Zhu, Y.Â Li, S.Â Wang, L.Â Wang, W.Â Chen *etÂ al.*,
    â€œLora: Low-rank adaptation of large language models,â€ in *International Conference
    on Learning Representations*, 2021.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] E. J. Hu, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen *ç­‰*ï¼Œâ€œLoraï¼šå¤§å‹è¯­è¨€æ¨¡å‹çš„ä½ç§©é€‚é…â€ï¼Œæ”¶å½•äº*å›½é™…å­¦ä¹ è¡¨å¾ä¼šè®®*ï¼Œ2021å¹´ã€‚'
- en: '[22] X.Â Li, X.Â Sun, Y.Â Meng, J.Â Liang, F.Â Wu, and J.Â Li, â€œDice loss for data-imbalanced
    nlp tasks,â€ in *Proceedings of the 58th Annual Meeting of the Association for
    Computational Linguistics*, 2020, pp. 465â€“476.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] X. Li, X. Sun, Y. Meng, J. Liang, F. Wu, å’Œ J. Liï¼Œâ€œæ•°æ®ä¸å¹³è¡¡ NLP ä»»åŠ¡çš„ Dice æŸå¤±â€ï¼Œæ”¶å½•äº*ç¬¬58å±Šè®¡ç®—è¯­è¨€å­¦åä¼šå¹´ä¼šè®ºæ–‡é›†*ï¼Œ2020å¹´ï¼Œç¬¬465â€“476é¡µã€‚'
- en: '[23] S.Â Zhang, L.Â Dong, X.Â Li, S.Â Zhang, X.Â Sun, S.Â Wang, J.Â Li, R.Â Hu, T.Â Zhang,
    F.Â Wu *etÂ al.*, â€œInstruction tuning for large language models: A survey,â€ *arXiv
    preprint arXiv:2308.10792*, 2023.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] S. Zhang, L. Dong, X. Li, S. Zhang, X. Sun, S. Wang, J. Li, R. Hu, T.
    Zhang, F. Wu *ç­‰*ï¼Œâ€œå¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤è°ƒä¼˜ï¼šç»¼è¿°â€ï¼Œ*arXiv é¢„å°æœ¬ arXiv:2308.10792*ï¼Œ2023å¹´ã€‚'
- en: '[24] H.Â Touvron, T.Â Lavril, G.Â Izacard, X.Â Martinet, M.-A. Lachaux, T.Â Lacroix,
    B.Â RoziÃ¨re, N.Â Goyal, E.Â Hambro, F.Â Azhar, A.Â Rodriguez, A.Â Joulin, E.Â Grave,
    and G.Â Lample, â€œLlama: Open and efficient foundation language models,â€ *arXiv
    preprint arXiv:2302.13971*, 2023.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,
    B. RoziÃ¨re, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave,
    å’Œ G. Lampleï¼Œâ€œLlamaï¼šå¼€æ”¾å’Œé«˜æ•ˆçš„åŸºç¡€è¯­è¨€æ¨¡å‹â€ï¼Œ*arXiv é¢„å°æœ¬ arXiv:2302.13971*ï¼Œ2023å¹´ã€‚'
- en: '[25] Z.Â Du, Y.Â Qian, X.Â Liu, M.Â Ding, J.Â Qiu, Z.Â Yang, and J.Â Tang, â€œGlm: General
    language model pretraining with autoregressive blank infilling,â€ in *Proceedings
    of the 60th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, 2022, pp. 320â€“335.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, å’Œ J. Tangï¼Œâ€œGlmï¼šè‡ªå›å½’ç©ºç™½å¡«å……çš„é€šç”¨è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒâ€ï¼Œæ”¶å½•äº*ç¬¬60å±Šè®¡ç®—è¯­è¨€å­¦åä¼šå¹´ä¼šè®ºæ–‡é›†ï¼ˆç¬¬1å·ï¼šé•¿ç¯‡è®ºæ–‡ï¼‰*ï¼Œ2022å¹´ï¼Œç¬¬320â€“335é¡µã€‚'
- en: '[26] A.Â Aghajanyan, S.Â Gupta, and L.Â Zettlemoyer, â€œIntrinsic dimensionality
    explains the effectiveness of language model fine-tuning,â€ in *Proceedings of
    the 59th Annual Meeting of the Association for Computational Linguistics and the
    11th International Joint Conference on Natural Language Processing (Volume 1:
    Long Papers)*, 2021, pp. 7319â€“7328.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] A. Aghajanyan, S. Gupta, å’Œ L. Zettlemoyerï¼Œâ€œå†…åœ¨ç»´åº¦è§£é‡Šäº†è¯­è¨€æ¨¡å‹å¾®è°ƒçš„æœ‰æ•ˆæ€§â€ï¼Œæ”¶å½•äº*ç¬¬59å±Šè®¡ç®—è¯­è¨€å­¦åä¼šå¹´ä¼šåŠç¬¬11å±Šå›½é™…è‡ªç„¶è¯­è¨€å¤„ç†è”åˆä¼šè®®è®ºæ–‡é›†ï¼ˆç¬¬1å·ï¼šé•¿ç¯‡è®ºæ–‡ï¼‰*ï¼Œ2021å¹´ï¼Œç¬¬7319â€“7328é¡µã€‚'
- en: '[27] X.-R. Sheng, L.Â Zhao, G.Â Zhou, X.Â Ding, B.Â Dai, Q.Â Luo, S.Â Yang, J.Â Lv,
    C.Â Zhang, H.Â Deng *etÂ al.*, â€œOne model to serve all: Star topology adaptive recommender
    for multi-domain ctr prediction,â€ in *Proceedings of the 30th ACM International
    Conference on Information & Knowledge Management*, 2021, pp. 4104â€“4113.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] X.-R. Sheng, L. Zhao, G. Zhou, X. Ding, B. Dai, Q. Luo, S. Yang, J. Lv,
    C. Zhang, H. Deng *ç­‰*ï¼Œâ€œä¸€ä¸ªæ¨¡å‹æœåŠ¡æ‰€æœ‰ï¼šå¤šé¢†åŸŸCTRé¢„æµ‹çš„æ˜Ÿå½¢æ‹“æ‰‘è‡ªé€‚åº”æ¨èå™¨â€ï¼Œæ”¶å½•äº*ç¬¬30å±ŠACMå›½é™…ä¿¡æ¯ä¸çŸ¥è¯†ç®¡ç†ä¼šè®®è®ºæ–‡é›†*ï¼Œ2021å¹´ï¼Œç¬¬4104â€“4113é¡µã€‚'
- en: '[28] T.Â Sun, Z.Â He, Q.Â Zhu, X.Â Qiu, and X.-J. Huang, â€œMultitask pre-training
    of modular prompt for chinese few-shot learning,â€ in *Proceedings of the 61st
    Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
    Papers)*, 2023, pp. 11â€‰156â€“11â€‰172.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] T. Sun, Z. He, Q. Zhu, X. Qiu, å’Œ X.-J. Huangï¼Œâ€œä¸­æ–‡å°‘æ ·æœ¬å­¦ä¹ çš„æ¨¡å—åŒ–æç¤ºå¤šä»»åŠ¡é¢„è®­ç»ƒï¼Œâ€ æ”¶å½•äº
    *ç¬¬61å±Šè®¡ç®—è¯­è¨€å­¦åä¼šå¹´ä¼šï¼ˆç¬¬1å·ï¼šé•¿ç¯‡è®ºæ–‡ï¼‰*ï¼Œ2023å¹´ï¼Œç¬¬11â€‰156â€“11â€‰172é¡µã€‚'
- en: '[29] Q.Â Dong, L.Â Li, D.Â Dai, C.Â Zheng, Z.Â Wu, B.Â Chang, X.Â Sun, J.Â Xu, and
    Z.Â Sui, â€œA survey for in-context learning,â€ *arXiv preprint arXiv:2301.00234*,
    2022.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, å’Œ Z.
    Suiï¼Œâ€œå…³äºä¸Šä¸‹æ–‡å­¦ä¹ çš„è°ƒæŸ¥ï¼Œâ€ *arXivé¢„å°æœ¬ arXiv:2301.00234*ï¼Œ2022å¹´ã€‚'
- en: '[30] T.Â Brown, B.Â Mann, N.Â Ryder, M.Â Subbiah, J.Â D. Kaplan, P.Â Dhariwal, A.Â Neelakantan,
    P.Â Shyam, G.Â Sastry, A.Â Askell *etÂ al.*, â€œLanguage models are few-shot learners,â€
    *Advances in neural information processing systems*, vol.Â 33, pp. 1877â€“1901, 2020.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A.
    Neelakantan, P. Shyam, G. Sastry, A. Askell *ç­‰*ï¼Œâ€œè¯­è¨€æ¨¡å‹æ˜¯å°‘é‡å­¦ä¹ è€…ï¼Œâ€ *ã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹*ï¼Œç¬¬33å·ï¼Œç¬¬1877â€“1901é¡µï¼Œ2020å¹´ã€‚'
- en: '[31] C.Â Huang, Q.Â Liu, B.Â Y. Lin, T.Â Pang, C.Â Du, and M.Â Lin, â€œLorahub: Efficient
    cross-task generalization via dynamic lora composition,â€ *arXiv preprint arXiv:2307.13269*,
    2023.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] C. Huang, Q. Liu, B. Y. Lin, T. Pang, C. Du, å’Œ M. Linï¼Œâ€œLorahubï¼šé€šè¿‡åŠ¨æ€Loraç»„åˆå®ç°é«˜æ•ˆè·¨ä»»åŠ¡æ³›åŒ–ï¼Œâ€
    *arXivé¢„å°æœ¬ arXiv:2307.13269*ï¼Œ2023å¹´ã€‚'
- en: '[32] C.-Y. Lin and F.Â J. Och, â€œAutomatic evaluation of machine translation
    quality using longest common subsequence and skip-bigram statistics,â€ in *Proceedings
    of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)*,
    2004, pp. 605â€“612.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] C.-Y. Lin å’Œ F. J. Ochï¼Œâ€œä½¿ç”¨æœ€é•¿å…¬å…±å­åºåˆ—å’Œè·³å­—å¯¹ç»Ÿè®¡è¿›è¡Œæœºå™¨ç¿»è¯‘è´¨é‡çš„è‡ªåŠ¨è¯„ä¼°ï¼Œâ€ æ”¶å½•äº *ç¬¬42å±Šè®¡ç®—è¯­è¨€å­¦åä¼šå¹´ä¼šï¼ˆACL-04ï¼‰ä¼šè®®è®ºæ–‡é›†*ï¼Œ2004å¹´ï¼Œç¬¬605â€“612é¡µã€‚'
- en: '[33] K.Â Singhal, T.Â Tu, J.Â Gottweis, R.Â Sayres, E.Â Wulczyn, L.Â Hou, K.Â Clark,
    S.Â Pfohl, H.Â Cole-Lewis, D.Â Neal *etÂ al.*, â€œTowards expert-level medical question
    answering with large language models,â€ *arXiv preprint arXiv:2305.09617*, 2023.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou, K. Clark,
    S. Pfohl, H. Cole-Lewis, D. Neal *ç­‰*ï¼Œâ€œæœç€ä¸“å®¶çº§åŒ»ç–—é—®ç­”è¿ˆè¿›ï¼Œå€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œâ€ *arXivé¢„å°æœ¬ arXiv:2305.09617*ï¼Œ2023å¹´ã€‚'
- en: '[34] J.Â Wei, X.Â Wang, D.Â Schuurmans, M.Â Bosma, F.Â Xia, E.Â Chi, Q.Â V. Le, D.Â Zhou
    *etÂ al.*, â€œChain-of-thought prompting elicits reasoning in large language models,â€
    *Advances in Neural Information Processing Systems*, vol.Â 35, pp. 24â€‰824â€“24â€‰837,
    2022.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D.
    Zhou *ç­‰*ï¼Œâ€œé“¾å¼æ€è€ƒæç¤ºåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¼•å‘æ¨ç†ï¼Œâ€ *ã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹*ï¼Œç¬¬35å·ï¼Œç¬¬24â€‰824â€“24â€‰837é¡µï¼Œ2022å¹´ã€‚'
- en: '[35] X.Â Wang, J.Â Wei, D.Â Schuurmans, Q.Â Le, E.Â Chi, S.Â Narang, A.Â Chowdhery,
    and D.Â Zhou, â€œSelf-consistency improves chain of thought reasoning in language
    models,â€ *arXiv preprint arXiv:2203.11171*, 2022.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery,
    å’Œ D. Zhouï¼Œâ€œè‡ªä¸€è‡´æ€§æå‡äº†è¯­è¨€æ¨¡å‹ä¸­çš„æ€ç»´é“¾æ¨ç†ï¼Œâ€ *arXivé¢„å°æœ¬ arXiv:2203.11171*ï¼Œ2022å¹´ã€‚'
- en: '[36] O.Â Byambasuren, Y.Â Yang, Z.Â Sui, D.Â Dai, B.Â Chang, S.Â Li, and H.Â Zan,
    â€œPreliminary study on the construction of chinese medical knowledge graph,â€ *Journal
    of Chinese Information Processing*, vol.Â 33, no.Â 10, pp. 1â€“9, 2019.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] O. Byambasuren, Y. Yang, Z. Sui, D. Dai, B. Chang, S. Li, å’Œ H. Zanï¼Œâ€œå¯¹ä¸­æ–‡åŒ»å­¦çŸ¥è¯†å›¾è°±æ„å»ºçš„åˆæ­¥ç ”ç©¶ï¼Œâ€
    *ã€Šä¸­æ–‡ä¿¡æ¯å¤„ç†å­¦æŠ¥ã€‹*ï¼Œç¬¬33å·ï¼Œç¬¬10æœŸï¼Œç¬¬1â€“9é¡µï¼Œ2019å¹´ã€‚'
- en: '[37] N.Â Houlsby, A.Â Giurgiu, S.Â Jastrzebski, B.Â Morrone, Q.Â DeÂ Laroussilhe,
    A.Â Gesmundo, M.Â Attariyan, and S.Â Gelly, â€œParameter-efficient transfer learning
    for nlp,â€ in *International Conference on Machine Learning*.Â Â Â PMLR, 2019, pp.
    2790â€“2799.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe,
    A. Gesmundo, M. Attariyan, å’Œ S. Gellyï¼Œâ€œé¢å‘è‡ªç„¶è¯­è¨€å¤„ç†çš„å‚æ•°é«˜æ•ˆè¿ç§»å­¦ä¹ ï¼Œâ€ æ”¶å½•äº *å›½é™…æœºå™¨å­¦ä¹ ä¼šè®®*ã€‚PMLRï¼Œ2019å¹´ï¼Œç¬¬2790â€“2799é¡µã€‚'
- en: '[38] X.Â L. Li and P.Â Liang, â€œPrefix-tuning: Optimizing continuous prompts for
    generation,â€ in *Proceedings of the 59th Annual Meeting of the Association for
    Computational Linguistics and the 11th International Joint Conference on Natural
    Language Processing (Volume 1: Long Papers)*.Â Â Â Online: Association for Computational
    Linguistics, Aug. 2021, pp. 4582â€“4597\. [Online]. Available: https://aclanthology.org/2021.acl-long.353'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] X. L. Li å’Œ P. Liangï¼Œâ€œå‰ç¼€è°ƒä¼˜ï¼šä¼˜åŒ–è¿ç»­æç¤ºç”Ÿæˆï¼Œâ€ æ”¶å½•äº *ç¬¬59å±Šè®¡ç®—è¯­è¨€å­¦åä¼šå¹´ä¼šæš¨ç¬¬11å±Šå›½é™…è‡ªç„¶è¯­è¨€å¤„ç†è”åˆä¼šè®®ï¼ˆç¬¬1å·ï¼šé•¿ç¯‡è®ºæ–‡ï¼‰*ã€‚åœ¨çº¿ï¼šè®¡ç®—è¯­è¨€å­¦åä¼šï¼Œ2021å¹´8æœˆï¼Œç¬¬4582â€“4597é¡µã€‚[åœ¨çº¿]ã€‚å¯ç”¨é“¾æ¥ï¼š
    https://aclanthology.org/2021.acl-long.353'
- en: '[39] X.Â Liu, K.Â Ji, Y.Â Fu, W.Â Tam, Z.Â Du, Z.Â Yang, and J.Â Tang, â€œP-tuning:
    Prompt tuning can be comparable to fine-tuning across scales and tasks,â€ in *Proceedings
    of the 60th Annual Meeting of the Association for Computational Linguistics (Volume
    2: Short Papers)*.Â Â Â Dublin, Ireland: Association for Computational Linguistics,
    May 2022, pp. 61â€“68\. [Online]. Available: https://aclanthology.org/2022.acl-short.8'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] X. Liu, K. Ji, Y. Fu, W. Tam, Z. Du, Z. Yang, å’Œ J. Tangï¼Œâ€œP-tuning: æç¤ºè°ƒä¼˜åœ¨ä¸åŒå°ºåº¦å’Œä»»åŠ¡ä¸Šçš„æ•ˆæœå¯ä¸å¾®è°ƒç›¸åª²ç¾ï¼Œâ€
    æ”¶å½•äº *ç¬¬60å±Šè®¡ç®—è¯­è¨€å­¦åä¼šå¹´ä¼šï¼ˆç¬¬2å·ï¼šçŸ­ç¯‡è®ºæ–‡é›†ï¼‰*ã€‚éƒ½æŸæ—ï¼Œçˆ±å°”å…°ï¼šè®¡ç®—è¯­è¨€å­¦åä¼šï¼Œ2022å¹´5æœˆï¼Œç¬¬61â€“68é¡µã€‚ [åœ¨çº¿]. å¯ç”¨ç½‘å€: https://aclanthology.org/2022.acl-short.8'
- en: '[40] A.Â Asai, M.Â Salehi, M.Â E. Peters, and H.Â Hajishirzi, â€œAttempt: Parameter-efficient
    multi-task tuning via attentional mixtures of soft prompts,â€ in *Proceedings of
    the 2022 Conference on Empirical Methods in Natural Language Processing*, 2022,
    pp. 6655â€“6672.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] A. Asai, M. Salehi, M. E. Peters, å’Œ H. Hajishirziï¼Œâ€œAttempt: é€šè¿‡æ³¨æ„åŠ›æ··åˆè½¯æç¤ºçš„å‚æ•°é«˜æ•ˆå¤šä»»åŠ¡è°ƒä¼˜ï¼Œâ€
    æ”¶å½•äº *2022å¹´è‡ªç„¶è¯­è¨€å¤„ç†ç»éªŒæ–¹æ³•ä¼šè®®è®ºæ–‡é›†*ï¼Œ2022å¹´ï¼Œç¬¬6655â€“6672é¡µã€‚'
- en: '[41] A.Â ÃœstÃ¼n, A.Â Bisazza, G.Â Bouma, G.Â van Noord, and S.Â Ruder, â€œHyper-x:
    A unified hypernetwork for multi-task multilingual transfer,â€ *arXiv preprint
    arXiv:2205.12148*, 2022.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] A. ÃœstÃ¼n, A. Bisazza, G. Bouma, G. van Noord, å’Œ S. Ruderï¼Œâ€œHyper-x: å¤šä»»åŠ¡å¤šè¯­è¨€è¿ç§»çš„ç»Ÿä¸€è¶…ç½‘ç»œï¼Œâ€
    *arXiv é¢„å°æœ¬ arXiv:2205.12148*ï¼Œ2022ã€‚'
