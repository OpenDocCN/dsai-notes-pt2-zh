- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:45:09'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate
    Framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.03075](https://ar5iv.labs.arxiv.org/html/2406.03075)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Xiaoxi Sun¹, Jinpeng Li¹¹¹footnotemark: 1, Yan Zhong¹, Dongyan Zhao¹, Rui Yan²²²footnotemark:
    2'
  prefs: []
  type: TYPE_NORMAL
- en: ¹Wangxuan Institute of Computer Technology, Peking University
  prefs: []
  type: TYPE_NORMAL
- en: ²Gaoling School of Artifical Intelligence, Renmin University of China
  prefs: []
  type: TYPE_NORMAL
- en: '{sunxiaoxi, zhongyan}@stu.pku.edu.cn, lijp.pku@gmail.com,'
  prefs: []
  type: TYPE_NORMAL
- en: 'zhaody@pku.edu.cn, ruiyan@ruc.edu.cn   Equal contribution.  Corresponding authors:
    Dongyan Zhao and Rui Yan.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The advent of large language models (LLMs) has facilitated the development of
    natural language text generation. It also poses unprecedented challenges, with
    content hallucination emerging as a significant concern. Existing solutions often
    involve expensive and complex interventions during the training process. Moreover,
    some approaches emphasize problem disassembly while neglecting the crucial validation
    process, leading to performance degradation or limited applications. To overcome
    these limitations, we propose a Markov Chain-based multi-agent debate verification
    framework to enhance hallucination detection accuracy in concise claims. Our method
    integrates the fact-checking process, including claim detection, evidence retrieval,
    and multi-agent verification. In the verification stage, we deploy multiple agents
    through flexible Markov Chain-based debates to validate individual claims, ensuring
    meticulous verification outcomes. Experimental results across three generative
    tasks demonstrate that our approach achieves significant improvements over baselines.
  prefs: []
  type: TYPE_NORMAL
- en: Towards Detecting LLMs Hallucination via
  prefs: []
  type: TYPE_NORMAL
- en: Markov Chain-based Multi-agent Debate Framework
  prefs: []
  type: TYPE_NORMAL
- en: 'Xiaoxi Sun¹^†^†thanks:   Equal contribution., Jinpeng Li¹¹¹footnotemark: 1,
    Yan Zhong¹, Dongyan Zhao¹^†^†thanks:   Corresponding authors: Dongyan Zhao and
    Rui Yan., Rui Yan²²²footnotemark: 2 ¹Wangxuan Institute of Computer Technology,
    Peking University ²Gaoling School of Artifical Intelligence, Renmin University
    of China {sunxiaoxi, zhongyan}@stu.pku.edu.cn, lijp.pku@gmail.com, zhaody@pku.edu.cn,
    ruiyan@ruc.edu.cn'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The continuous evolution of large language models (LLMs) has significantly expanded
    language processing capabilities across diverse domains Wei et al. ([2022](#bib.bib29)).
    However, this progress introduces challenges, such as the substantial cost associated
    with updating model parameters and inherent deficiencies in reasoning Ji et al.
    ([2023](#bib.bib15)); Zhang et al. ([2023b](#bib.bib36)); Zheng et al. ([2023](#bib.bib37)).
    This has led to the generation of inaccurate content, known as hallucination,
    particularly concerning potent yet opaque models like ChatGPT and GPT-4 OpenAI
    ([2023](#bib.bib24)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9dd6fcf8314766960183fe2129276dc5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Overview of the fact-checking process, which includes three distinct
    stages. Claim Detection, we leverage large language models such as ChatGPT to
    obtain different claims. Evidence Retrieval, we prompt ChatGPT to formulate two
    queries, subsequently leveraging these queries to retrieve evidence with Google
    API or the provided knowledge. Multi-agent Verification, we propose the Markov
    Chain-based multi-agent debate verification framework, which can emulate human
    behavior to enhance model outputs and improve reasoning capabilities. Our main
    contributions focus on the process of Multi-agent Verification.'
  prefs: []
  type: TYPE_NORMAL
- en: Hallucination detection has become a focal point in addressing these challenges.
    Existing methods often necessitate costly and intricate interventions during the
    training process Lee et al. ([2022](#bib.bib17)); Touvron et al. ([2023](#bib.bib27));
    Elaraby et al. ([2023](#bib.bib8)); Wu et al. ([2023b](#bib.bib31)), rendering
    them unsuitable for large language models with agnostic parameters and these methods
    often incur considerable costs. Consequently, researchers have explored post-processing
    approaches Gao et al. ([2023](#bib.bib10)); Peng et al. ([2023](#bib.bib26));
    Chern et al. ([2023](#bib.bib3)); Vu et al. ([2023](#bib.bib28)); Gero et al.
    ([2023](#bib.bib11)) involving hallucination detection or correction post-content
    generation. Notably, these methods typically focus on problem decomposition and
    evidence retrieval, emphasizing simple prompting during individual verification.
    We posit that the verification accuracy is pivotal compared to problem decomposition
    in LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'To address these challenges, we present a fact-checking process to enhance
    the accuracy of hallucination detection. As shown in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Towards Detecting LLMs Hallucination via Markov Chain-based
    Multi-agent Debate Framework"), which involves three stages: claim detection,
    evidence retrieval, and multi-agent verification. In claim detection, our approach
    involves the extraction of claims from extensive responses by prompting ChatGPT,
    decomposing the intricate problem into smaller components. Evidence retrieval
    involves generating queries based on claims for retrieval. Subsequently, we retrieve
    the corresponding evidence based on these generated queries. In the multi-agent
    verification stage, we innovatively propose a Markov Chain-based multi-agent debate
    verification framework, which leverages the robust capabilities of multi-agent
    systems to simulate human behavior. This approach involves deploying diverse agents
    in Markov Chain debates to verify individual claims, thus providing a nuanced
    and flexible validation process. Following the verification of each claim using
    our method, the collective judgment of all claims contributes to the detection
    of hallucinations in the original response.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We conduct extensive experiments across three generative tasks, including question-answering,
    Summarization, and Dialogue, demonstrating the effectiveness of our approach.
    Verification outcomes are meticulously analyzed and compared against existing
    methods to ascertain the superiority of our approach. In summary, our contributions
    can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose a versatile hallucination detection process applicable to multiple
    generation tasks for improving verification accuracy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We introduce a Markov Chain-based multi-agent debate verification framework
    that simulates human discussion.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experiments conducted on three generative tasks show that our proposed framework
    outperforms baselines.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/88f2b30cf7096386b89f350bde56653d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Overview of the proposed multi-agent debate verification framework
    for hallucination detection. There are two preparations before multi-agent debate
    verification. In preparation 1 (Agent Customization), we define three different
    debate agent roles, including Trust, Skeptic, and Leader roles. In preparation
    2 (Debate Modes Customization), we assume that the debate process consists of
    two modes: Trust agent-initiated discussion (Trust-Skeptic-Leader) and Skeptic
    agent-initiated discussion (Skeptic-Trust-Leader). Then, in Multi-agent Chain
    of Verification, our validation process can be regarded as a Markov chain, which
    continually oscillates between these two debate modes to arrive at an optimal
    judgment.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Hallucination Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before the emergence of large language models, hallucination detection was
    a significant topic within the field of natural language processing. Previous
    efforts primarily focused on detecting hallucinations in various tasks such as
    summarization Kryscinski et al. ([2020](#bib.bib16)); Maynez et al. ([2020](#bib.bib23));
    Goyal and Durrett ([2021](#bib.bib12)), dialogue Das et al. ([2022](#bib.bib6)),
    question-answering Longpre et al. ([2021](#bib.bib21)), and machine translation
    Xu et al. ([2023a](#bib.bib33)). These approaches primarily aimed to identify
    discrepancies between the generated content and the input, as well as internal
    inconsistencies within the generated content. However, they were often tailored
    specifically to task-specific models, lacking generalizability. There were also
    fact-checking endeavors that aimed to identify discrepancies between the generated
    content and real-world facts. This was typically accomplished through three steps
    Guo et al. ([2022](#bib.bib13)): Claim Detection, Evidence Retrieval, and Verdict
    Prediction. With the advent of large language models, some works Gao et al. ([2023](#bib.bib10));
    Li et al. ([2023a](#bib.bib18)) tackled the task of hallucination detection by
    prompting the large language models directly. In addition to task-specific approaches,
    there are hallucination detection methods specifically designed for LLMs. For
    example, some methods assess hallucination detection by examining the consistency
    of sampled examples Manakul et al. ([2023](#bib.bib22)); Zhang et al. ([2023a](#bib.bib35)).
    Our work is fundamentally based on the fact-check framework. We transfer the Verdict
    Prediction stage to the Multi-agent Verification to improve the precision of validation.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Hallucination Mitigation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLMs have demonstrated significant potential recently. However, they have not
    been able to completely eliminate the occurrence of hallucinations Zheng et al.
    ([2023](#bib.bib37)). The extended text produced by these large models, which
    encompasses more diverse content and often introduces external knowledge, renders
    traditional methods for hallucination mitigation less effective. Consequently,
    a plethora of works dedicated to addressing hallucination mitigation in LLMs have
    emerged. Various approaches are presented to mitigate hallucinations at different
    stages of their application within the LLM life cycle Zhang et al. ([2023b](#bib.bib36)),
    including the pre-training phase of large models Lee et al. ([2022](#bib.bib17));
    Touvron et al. ([2023](#bib.bib27)), the SFT phase Chen et al. ([2023](#bib.bib2));
    Elaraby et al. ([2023](#bib.bib8)), the alignment phase Wu et al. ([2023b](#bib.bib31));
    Casper et al. ([2023](#bib.bib1)), and the decoding phase Li et al. ([2023b](#bib.bib19));
    Chuang et al. ([2023](#bib.bib4)). Implementing these methods necessitates adjustments
    to the model’s parameters and requires a certain amount of training data, incurring
    some overhead. Numerous endeavors have been undertaken to mitigate hallucinations
    in content generated by black-box models, such as leveraging external knowledge
    bases or tools Gao et al. ([2023](#bib.bib10)); Peng et al. ([2023](#bib.bib26));
    Chern et al. ([2023](#bib.bib3)); Vu et al. ([2023](#bib.bib28)) and adopting
    self-refining approaches Gero et al. ([2023](#bib.bib11)). Our approach also centers
    on hallucination mitigation for black-box models, introducing a distinctive multi-agent
    method to augment its effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Multi-agent in LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In recent years, there has been a significant increase in the size of models
    and the amount of training data used, resulting in the exceptional performance
    of large language models (LLMs) across various tasks. As a result, researchers
    have explored the use of LLMs as agents to simulate human behavior, leading to
    the development of influential projects such as Generative Agents Park et al.
    ([2023](#bib.bib25)), Ghost in the Minecraft Zhu et al. ([2023](#bib.bib38)),
    GPT-Bargaining Fu et al. ([2023](#bib.bib9)) and Werewolf game Xu et al. ([2023b](#bib.bib34)).
    There are also some efforts involve multiple agents engaging in debates to improve
    the reasoning capabilities Liang et al. ([2023](#bib.bib20)); Du et al. ([2023](#bib.bib7));
    Xiong et al. ([2023](#bib.bib32)) or address issues related to hallucinations
    Du et al. ([2023](#bib.bib7)); Cohen et al. ([2023](#bib.bib5)). However, existing
    methods for hallucination detection and mitigation of LLMs solely rely on natural
    language interactions between agents, which may pose concerns regarding the self-correction
    approach Huang et al. ([2023](#bib.bib14)). Therefore, the objective of our work
    is to facilitate flexible discussions among multiple agents based on existing
    facts, aiming to detect and mitigate hallucinations in the generated content of
    language models.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The primary objective of our study is to detect hallucinations in the content
    generated by the model. To accomplish this, we adhere to the conventional fact-checking
    process and make some modifications. The process is structured into three distinct
    stages: Claim Detection, Evidence Retrieval, and Multi-agent Verification. This
    systematic approach enables the dissection of a complex problem into more manageable
    components. It has come to our attention that in certain fact-checking procedures,
    despite the accurate extraction of claims and the acquisition of robust evidence,
    verification errors persist in the final stage, undermining the efficacy of preceding
    efforts.'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we propose a novel multi-agent debate verification framework for
    hallucination detection, the overview of which is shown in Figure [2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ Towards Detecting LLMs Hallucination via Markov Chain-based
    Multi-agent Debate Framework"). An anthropomorphic debate process based on the
    Markov chain is designed to be applicable across various generative tasks in the
    proposed method, bolstering the accuracy of verification. Subsequent sections
    will expound on each of these three stages individually, with a particular emphasis
    on our innovative approach in the third stage.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Claim Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the stage of claim detection, we employ the methodology utilized in Factool
    Chern et al. ([2023](#bib.bib3)), leveraging large language models such as ChatGPT.
    Harnessing the robust instruction-following capabilities of LLMs empowers us to
    address the challenge of dissecting intricate responses. Nevertheless, detecting
    the hallucinations in statements lacking adequate information is futile and could
    impede overall judgment. Moreover, specific tasks may demand the concatenation
    of the model’s responses with particular input information to formulate an informative
    claim, necessitating supplementary processing. Detailed explanations of these
    processing methods are provided in the experimental implementation section §[4.1.2](#S4.SS1.SSS2
    "4.1.2 Implementataion Details ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Towards
    Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework").
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Evidence Retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Upon extraction of claims, a retrieval methodology is employed to ascertain
    corresponding evidence. Drawing inspiration from Factool’s Chern et al. ([2023](#bib.bib3))
    strategy in Knowledge Base Question Answering (KBQA) tasks, we prompt ChatGPT
    to formulate two queries, subsequently leveraging these queries to retrieve evidence.
    In instances where pertinent knowledge is absent, we employ the Google API to
    retrieve data from the internet. Conversely, when dealing with data accompanied
    by provided knowledge, we either consider the length of the knowledge as direct
    evidence or encode it for local retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Multi-agent Verification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We propose a Markov Chain-based multi-agent debate verification framework. Our
    investigations reveal significant potential in employing multi-agent systems to
    emulate human behavior Park et al. ([2023](#bib.bib25)); Zhu et al. ([2023](#bib.bib38)),
    particularly in the domain of fact-checking claims grounded in evidence. The effectiveness
    of addressing this task is notably heightened through the use of multi-agent debates.
    Despite considerable advancements in leveraging multi-agent debates to enhance
    model outputs and improve reasoning capabilities Liang et al. ([2023](#bib.bib20));
    Du et al. ([2023](#bib.bib7)), two critical aspects remain underexplored within
    the realm of hallucination detection.
  prefs: []
  type: TYPE_NORMAL
- en: 1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Application to Verification: Few studies have directly applied the multi-agent
    approach to the task of verification, they more concentrate on the decomposition
    of the complicated samples. Recognizing this research gap, our work aims to bridge
    it by introducing the multi-agent debate verification framework.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Flexible Debate Process: Existing methods in debate often adhere to a fixed
    process, unlike human debates where participants dynamically adjust their arguments
    based on prior outcomes. Our proposed approach takes inspiration from the Markov
    chain, where the selection of the current state depends on the results of a limited
    set of preceding states. This debate mode is more similar to the discussion between
    humans.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In summary, our multi-agent debate verification framework ingeniously adapts
    the multi-agent paradigm to the hallucination detection task. By infusing flexibility
    into the debate process and drawing inspiration from the Markov chain, our goal
    is to enhance the accuracy and adaptability of the verification process when assessing
    the veracity of claims based on evidence.
  prefs: []
  type: TYPE_NORMAL
- en: The key point in our method lies in the definition of states and the transition
    mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 States
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Agents
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To comprehend the definition of states, it is imperative to elucidate the roles
    assumed by the diverse agents under consideration. We engage three distinct agents:
    Trust, Skeptic, and Leader. These agents collectively share the commonality of
    assimilating perspectives from one or more antecedent agents. They meticulously
    scrutinize these perspectives, grounded in claims and evidence amassed in preceding
    sections, express concurrence or dissent, and proffer their own viewpoints accompanied
    by factual assessments of the claims. The differentiation among these agents lies
    in their inclination toward antecedent viewpoints. The Trust agent predominantly
    leans towards accepting the perspectives of the preceding agent, thereby bolstering
    their credibility. Conversely, the Skeptic agent challenges the perspectives of
    the antecedent agent, diligently seeking to pinpoint inconsistencies between viewpoints
    and supporting evidence. The Leader agent amalgamates the perspectives of two
    agents, critically examines the rational and irrational facets, and ultimately
    formulates its own viewpoint. We implement agents with different personas through
    various prompts. Details can be found in Appendix [A.1](#A1.SS1 "A.1 Prompts ‣
    Appendix A Appendix ‣ Towards Detecting LLMs Hallucination via Markov Chain-based
    Multi-agent Debate Framework"). The configuration of these agents, arranged in
    various sequences, constitutes the states delineated in our approach.'
  prefs: []
  type: TYPE_NORMAL
- en: States
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We need to precisely define the states mentioned earlier. According to the definition
    of the Markov chain, we require an initial state to initiate our verification
    chain. Each agent must analyze the perspectives of preceding agents, necessitating
    an initial agent to furnish the primary answer for subsequent debate. This initial
    state is characterized by the initial agent, labeled as $S_{0}$, and our verification
    chain unfolds from this state.
  prefs: []
  type: TYPE_NORMAL
- en: We predominantly have two ordinary states, each comprising three agents. These
    states can be regarded as two distinct discussion modes. The first is the Trust
    agent-initiated discussion, labeled as $S_{1}$, with the sequence Skeptic-Trust-Leader.
    This mode leans towards questioning the credibility of the previous viewpoints
    before further analyzing the skeptical perspective. Our verification chain continually
    oscillates between these two debate modes to arrive at an optimal judgment.
  prefs: []
  type: TYPE_NORMAL
- en: To prevent the chain from infinitely extending, a termination state is essential.
    Analogous to human debates concluding when opinions align, our termination condition
    is similar. If, within a state, the three agents reach a consensus, the chain
    terminates. When the Skeptic agent fails to identify points of contention, and
    the Leader, after scrutinizing their opinions, has no objections, yielding the
    same judgment, we consider the debate concluded. Additionally, we have imposed
    a maximum limit on verification rounds to constrain the length of the chain.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2 Transition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Transitioning between states is a critical aspect of our methodology, following
    the definition of states. The primary criterion guiding these transitions in our
    approach is the verification result of a claim by the preceding state. This methodology
    aligns with human intuition, acknowledging the potential for diverse perspectives
    in debating a given matter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our transition probabilities are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Pr\left(S_{2}&#124;R=True\right)=1$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $Pr\left(S_{1}&#124;R=False\right)=1$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: $R$. In essence, we initially reinforce the credibility of this judgment, confirming
    the validity of skepticism. By enhancing the credibility of this opinion, if subsequent
    skepticism from the Skeptic agent is challenging, we can reasonably conclude the
    accuracy of this judgment, leading to the convergence of the chain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, our overall process unfolds in the following manner: Initially,
    an initial answer is obtained from the initial state $S_{0}$ is made. Subsequent
    transitions rely solely on the judgment of the preceding state, continuing until
    a consensus is reached among the three agents within a state, culminating in the
    final verification result.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We conducted experiments encompassing three generative tasks: Knowledge-Based
    Question Answering (KB-QA), Dialogue, and Summarization.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For all three tasks, we prompt the ChatGPT to execute claim extraction, query
    generation, and multi-agent debate verification. The verification process is iterated
    a minimum of 2 rounds, and 10 snippets of evidence are extracted. The chosen transition
    method involved switching to the skeptic agent when the response was determined
    to be True.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Datasets and Baselines
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this paper, we perform experiments on three different tasks, including Question-Answer
    (QA), Summarization, and Dialogue. The experimental datasets are derived from
    the following two canonical databases:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Factool Chern et al. ([2023](#bib.bib3)): The Factprompts data comprises real-world
    questions with responses generated by ChatGPT, along with Factool-annotated claims
    extracted from these responses.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HaluEval Li et al. ([2023a](#bib.bib18)): HaluEval constitutes a substantial
    collection of sampling-then-filtering generated and human-annotated hallucinated
    samples, serving as an evaluation metric for language model performance in recognizing
    hallucination.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We randomly selected 150, 50, and 150 samples from the three tasks of HaluEval
    for testing purposes. The selection of samples was contingent upon the complexity
    of task responses, with summarization outputs being more intricate. Owing to the
    necessity of decomposing summarization into a greater number of claims, the extracted
    quantity is comparatively smaller than that of the other two tasks. The positive
    and negative instances within the dataset were randomly sampled using a binary
    distribution with a probability of 0.5\. The resulting data distribution is presented
    in Table [1](#S4.T1 "Table 1 ‣ 4.1.1 Datasets and Baselines ‣ 4.1 Experimental
    Setup ‣ 4 Experiments ‣ Towards Detecting LLMs Hallucination via Markov Chain-based
    Multi-agent Debate Framework").
  prefs: []
  type: TYPE_NORMAL
- en: We compared the Factool method, the few-shot prompting method in HaluEval, the
    self-check method Chern et al. ([2023](#bib.bib3)), and our approach.
  prefs: []
  type: TYPE_NORMAL
- en: '| Datasets | Positive | Negative |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Factool QA | 23 | 27 |'
  prefs: []
  type: TYPE_TB
- en: '| HaluEval QA | 75 | 75 |'
  prefs: []
  type: TYPE_TB
- en: '| HaluEval Summarization | 25 | 25 |'
  prefs: []
  type: TYPE_TB
- en: '| HaluEval Dialogue | 80 | 70 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: The Number of positive and negative samples in different datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Claim-Level | Response-Level |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Acc. | R | P | F1 | Acc. | R | P | F1 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Self-Check (0) | 75.54 | 90.40 | 80.00 | 84.88 | 54.00 | 60.87 | 50.00 |
    54.90 |'
  prefs: []
  type: TYPE_TB
- en: '| Self-Check (3) | 69.53 | 81.36 | 79.12 | 80.23 | 54.00 | 47.83 | 50.00 |
    48.89 |'
  prefs: []
  type: TYPE_TB
- en: '| FACTOOL | 74.25 | 73.45 | 90.91 | 81.25 | 64.00 | 43.48 | 66.67 | 52.63 |'
  prefs: []
  type: TYPE_TB
- en: '| Our Method | 77.68 | 80.79 | 88.82 | 84.62 | 72.00 | 52.17 | 80.00 | 63.15
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Accuracy(%), Recall(%), Precision(%), F1(%) of four methods on Dataset
    Factool Chern et al. ([2023](#bib.bib3)) used. Claim-Level denotes the results
    evaluated on all annotated claims, and Response-Level denotes the results evaluated
    on the origin responses. The best scores are highlighted in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | QA | Summarization | Dialogue |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Acc. | R | P | F1 | Acc. | R | P | F1 | Acc. | R | P | F1 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| HaluEval | 56.00 | 77.33 | 54.21 | 63.74 | 58.00 | 100.0 | 54.35 | 70.42
    | 68.00 | 75.71 | 63.10 | 68.83 |'
  prefs: []
  type: TYPE_TB
- en: '| FACTOOL | 67.33 | 86.67 | 62.50 | 72.63 | 64.00 | 48.00 | 70.59 | 57.14 |
    74.67 | 70.00 | 74,24 | 72.06 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | 70.67 | 82.67 | 66.67 | 73.81 | 70.00 | 64.00 | 72.73 | 68.09 | 76.00
    | 62.86 | 81.48 | 70.97 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: The results for our method and baseline on HaluEval Li et al. ([2023a](#bib.bib18))
    Dataset. We conducted experiments on three tasks: QA, Summarization, and Dialogue.
    The best scores are highlighted in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Implementataion Details
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: KB-QA
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For intricate and information-rich QA data, such as that in Factool Chern et al.
    ([2023](#bib.bib3)), we decomposed answers into multiple atomic claims and conducted
    multi-agent debate verification on each claim. If one of the claims is hallucinated,
    the origin answer is judged to be non-factual. As Factool data lacked corresponding
    evidence, Google search was employed to retrieve evidence for verification. In
    the case of simpler QA data, as found in HaluEval Li et al. ([2023a](#bib.bib18)),
    where answers sometimes are the single entity, such as "What American quartery
    lifestyle magazine did Hearst Shkelev Media also publish? Departures.", we concatenated
    answers and questions to form QA pairs. Subsequently, we directly applied the
    multi-agent debate verification to these QA pairs, utilizing the provided knowledge
    in the dataset as evidence.
  prefs: []
  type: TYPE_NORMAL
- en: Summarization
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The model-generated summary was treated as a response, decomposed into multiple
    claims, and each claim was verified individually. The corresponding document to
    the summary served as evidence. To mitigate excessively long input queries, each
    sentence of the document was encoded separately, along with the query. The top
    10 most similar sentences were selected as evidence for the current claim.
  prefs: []
  type: TYPE_NORMAL
- en: Dialogue
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In the course of the dialogue task, we encountered challenges associated with
    the extraction of claims. Dialogue responses frequently incorporated substantial
    subjective viewpoints such as "The last time that they made it to Super Bowl was
    in 2005\. Are you a basketball fanatic too?", rendering the fact-checking of the
    factual accuracy of such subjective statements less meaningful. To mitigate this
    challenge, we introduced a pre-processing step wherein we directed ChatGPT to
    eliminate subjective portions from its responses prior to claim extraction, so
    the previous sentence becomes: "The last time that they made it to the Super Bowl
    was in 2005.". This approach allowed us to retain only the informative segments
    for subsequent verification. Additionally, in the verification process during
    claim extraction, we employed the dialogue history and external knowledge as supporting
    evidence.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Performance Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The experimental results are presented in Table [2](#S4.T2 "Table 2 ‣ 4.1.1
    Datasets and Baselines ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Towards Detecting
    LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework") and Table
    [3](#S4.T3 "Table 3 ‣ 4.1.1 Datasets and Baselines ‣ 4.1 Experimental Setup ‣
    4 Experiments ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent
    Debate Framework"). Table [2](#S4.T2 "Table 2 ‣ 4.1.1 Datasets and Baselines ‣
    4.1 Experimental Setup ‣ 4 Experiments ‣ Towards Detecting LLMs Hallucination
    via Markov Chain-based Multi-agent Debate Framework") shows the performance of
    our method on Factool Chern et al. ([2023](#bib.bib3)), presenting results at
    both the claim and response levels. According to Table [2](#S4.T2 "Table 2 ‣ 4.1.1
    Datasets and Baselines ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Towards Detecting
    LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework"), we can
    observe that our proposed method can consistently achieve optimal accuracy when
    compared to various approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [3](#S4.T3 "Table 3 ‣ 4.1.1 Datasets and Baselines ‣ 4.1 Experimental
    Setup ‣ 4 Experiments ‣ Towards Detecting LLMs Hallucination via Markov Chain-based
    Multi-agent Debate Framework") displays the test results on the HaluEval Li et al.
    ([2023a](#bib.bib18)) dataset, from which we can observe that: Our method demonstrates
    optimal accuracy, excelling in most metrics in all three tasks, Notably, in the
    three tasks of this dataset, our method exhibits a relatively low recall score.
    This can be attributed to our approach, which involves questioning claims verified
    as factual, thereby ensuring the precise detection of errors when claims are misclassified.
    However, this approach also results in misjudging some claims that inherently
    lack hallucinations as non-factual. This phenomenon is further elucidated in § [4.3](#S4.SS3.SSS0.Px1
    "Transition Methods ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ Towards Detecting LLMs
    Hallucination via Markov Chain-based Multi-agent Debate Framework").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Transition Methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We assessed the impact of distinct transition methods. From the QA section
    of HaluEval Li et al. ([2023a](#bib.bib18)), we extract 80 samples to evaluate
    the impact of four transition methods: transitioning to $S_{2}$ method, concurrently
    demonstrating an elevated precision value.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Acc. | R | P | F1 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $Always\ \textbf{Skeptic}$ | 65.00 | 84.21 | 59.26 | 69.57 |'
  prefs: []
  type: TYPE_TB
- en: '| $Always\ \textbf{Trust}$ | 68.75 | 84.21 | 62.75 | 71.91 |'
  prefs: []
  type: TYPE_TB
- en: '| $True\to\textbf{Trust}$ | 67.50 | 89.47 | 60.71 | 72.34 |'
  prefs: []
  type: TYPE_TB
- en: '| $True\to\textbf{Skeptic}$ | 70.00 | 86.84 | 63.46 | 73.33 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Comparison of Different Transition Methods. We evaluate the influence
    of transition methods on 80 QA samples, setting the minimum debate rounds to 2\.
    The best scores are highlighted in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: Minimum Rounds of Debate
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We explored the influence of different numbers of minimum debate rounds on the
    outcomes. We examined three distinct tasks using the previously extracted HaluEval
    data Li et al. ([2023a](#bib.bib18)), varying the number of minimum debate rounds
    from 0 to 3\. Employing the $True\to\textbf{Skeptic}$ transition method, the results,
    illustrated in Figure [3](#S4.F3 "Figure 3 ‣ Minimum Rounds of Debate ‣ 4.3 Ablation
    Study ‣ 4 Experiments ‣ Towards Detecting LLMs Hallucination via Markov Chain-based
    Multi-agent Debate Framework"), generally exhibit enhanced performance when the
    number of minimum rounds is set to 1 or 2, with a discernible decrease in efficacy
    when the number of minimum rounds is set to 3.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/87a982c409f973ad41d8a1a63ffe580b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Comparison of Different Minimum Debate Rounds. We evaluate the influence
    of the minimum debate rounds on samples we utilized in §[4.1.1](#S4.SS1.SSS1 "4.1.1
    Datasets and Baselines ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Towards Detecting
    LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework"), setting
    the transition method to $True\to\textbf{Skeptic}$. The x-axis represents different
    minimum debate rounds, whereas the y-axis signifies the corresponding detection
    accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: Comparison with Non-GPT Method
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the multi-agent verification stage of the experiment in the Factool dataset,
    we employed the WeCheckWu et al. ([2023a](#bib.bib30)) method to conduct an ablation
    study, showcasing the benefits of our approach. We held the initial two steps
    constant, utilizing the Factool method to extract claims and retrieve evidence.
    Employing the claim as the hypothesis and the evidence as the premise, instances
    with WeCheck scores greater than or equal to 0.5 were deemed factual. From the
    experimental results in Table [5](#S4.T5 "Table 5 ‣ Comparison with Non-GPT Method
    ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ Towards Detecting LLMs Hallucination via
    Markov Chain-based Multi-agent Debate Framework"), we observed that compared to
    the non-GPT method, our approach exhibits significant advantages during the verification
    stage.
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Acc. | R | P | F1 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Wecheck | 65.23 | 64.41 | 86.36 | 73.78 |'
  prefs: []
  type: TYPE_TB
- en: '| Our method | 77.68 | 80.79 | 88.82 | 84.62 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Comparison with Non-GPT Method. We compare our method with Non-GPT
    Method Wecheck on the Factool dataset. The best scores are highlighted in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Case Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To demonstrate the effectiveness of our approach, Table [10](#A1.T10 "Table
    10 ‣ A.2 Debate examples ‣ Appendix A Appendix ‣ Towards Detecting LLMs Hallucination
    via Markov Chain-based Multi-agent Debate Framework") and Table [11](#A1.T11 "Table
    11 ‣ A.2 Debate examples ‣ Appendix A Appendix ‣ Towards Detecting LLMs Hallucination
    via Markov Chain-based Multi-agent Debate Framework") show examples of the hallucination
    detection process for a Question-Answer (QA) sample. In Table [10](#A1.T10 "Table
    10 ‣ A.2 Debate examples ‣ Appendix A Appendix ‣ Towards Detecting LLMs Hallucination
    via Markov Chain-based Multi-agent Debate Framework"), the debating agent is based
    on the GPT-3.5-turbo model, whereas Table [11](#A1.T11 "Table 11 ‣ A.2 Debate
    examples ‣ Appendix A Appendix ‣ Towards Detecting LLMs Hallucination via Markov
    Chain-based Multi-agent Debate Framework") utilizes GPT-4 as the base model.
  prefs: []
  type: TYPE_NORMAL
- en: When the debate starts, the initial agent generates an initial opinion based
    on the QA pair and the corresponding evidence. If no debate ensues, the initial
    opinion solidifies as the final answer. However, this approach overlooks both
    the insufficiency of evidence to support the claim that "The Landseer has a limited
    range of colors" and the contradiction with evidence concerning "the English Mastiff
    having a wider range." In Table [10](#A1.T10 "Table 10 ‣ A.2 Debate examples ‣
    Appendix A Appendix ‣ Towards Detecting LLMs Hallucination via Markov Chain-based
    Multi-agent Debate Framework"), the three agents engage in discussions to highlight
    the insufficient evidence supporting "The Landseer has a limited range of colors."
    However, they fail to infer the contradiction with the evidence suggesting "the
    English Mastiff has a wider range." In Table [11](#A1.T11 "Table 11 ‣ A.2 Debate
    examples ‣ Appendix A Appendix ‣ Towards Detecting LLMs Hallucination via Markov
    Chain-based Multi-agent Debate Framework"), the agent, post-discussion, identifies
    both of these deficiencies. These observations indicate that larger language models,
    owing to their enhanced reasoning capabilities, yield better results when employing
    our method. Furthermore, it highlights that in some cases, a single round of debate
    may not reveal all inconsistencies between claim and evidence, emphasizing why
    sometimes increasing the minimum debate rounds can improve effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, our purpose is to improve the accuracy of hallucination detection
    in content generated by large language models. Simultaneously, we aspired to extend
    this enhancement beyond particular generative tasks. To fulfill these objectives,
    we introduce a versatile framework for hallucination detection and propose the
    Markov Chain-based multi-agent debate verification framework. Our proposed approach
    demonstrates its effectiveness through evaluations conducted on both the Knowledge
    Base Question Answering (KBQA) dataset and the randomly sampled HaluEval dataset.
    We posit that our method demonstrates a level of generalizability, enabling its
    adaptation to other post-processing hallucination detection or mitigation approaches
    for better performance.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations and Potential Risks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our methodology necessitates frequent interactions with the API of large language
    models (LLMs), resulting in significant overhead. This high frequency of API calls
    increases the cost and reduces response speed, which may limit its practicality
    in real-world scenarios. Nevertheless, this approach provides an accessible option
    for users lacking the infrastructure to implement large open-source models.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the distinctiveness among prompts for different agents primarily
    centers on role definition, while other aspects display considerable similarity.
    This occasionally leads to the partial repetition of opinions from the preceding
    agent. As exemplified by the two instances in Appendix [A.2](#A1.SS2 "A.2 Debate
    examples ‣ Appendix A Appendix ‣ Towards Detecting LLMs Hallucination via Markov
    Chain-based Multi-agent Debate Framework"), this phenomenon could be substantially
    alleviated by enhancing the performance of the base model.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Casper et al. (2023) Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl
    Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David
    Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphaël Segerie, Micah
    Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman
    Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov,
    Xin Chen, Lauro Langosco, Peter Hase, Erdem Bıyık, Anca Dragan, David Krueger,
    Dorsa Sadigh, and Dylan Hadfield-Menell. 2023. [Open problems and fundamental
    limitations of reinforcement learning from human feedback](http://arxiv.org/abs/2307.15217).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2023) Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna,
    Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia
    Jin. 2023. [Alpagasus: Training a better alpaca with fewer data](http://arxiv.org/abs/2307.08701).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chern et al. (2023) I-Chun Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua
    Feng, Chunting Zhou, Junxian He, Graham Neubig, and Pengfei Liu. 2023. [Factool:
    Factuality detection in generative ai – a tool augmented framework for multi-task
    and multi-domain scenarios](http://arxiv.org/abs/2307.13528).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chuang et al. (2023) Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James
    Glass, and Pengcheng He. 2023. [Dola: Decoding by contrasting layers improves
    factuality in large language models](http://arxiv.org/abs/2309.03883).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cohen et al. (2023) Roi Cohen, May Hamri, Mor Geva, and Amir Globerson. 2023.
    [Lm vs lm: Detecting factual errors via cross examination](http://arxiv.org/abs/2305.13281).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Das et al. (2022) Souvik Das, Sougata Saha, and Rohini Srihari. 2022. [Diving
    deep into modes of fact hallucinations in dialogue systems](https://doi.org/10.18653/v1/2022.findings-emnlp.48).
    In *Findings of the Association for Computational Linguistics: EMNLP 2022*, pages
    684–699, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du et al. (2023) Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum,
    and Igor Mordatch. 2023. [Improving factuality and reasoning in language models
    through multiagent debate](http://arxiv.org/abs/2305.14325).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Elaraby et al. (2023) Mohamed Elaraby, Mengyin Lu, Jacob Dunn, Xueying Zhang,
    Yu Wang, Shizhu Liu, Pingchuan Tian, Yuping Wang, and Yuxuan Wang. 2023. [Halo:
    Estimation and reduction of hallucinations in open-source weak large language
    models](http://arxiv.org/abs/2308.11764).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fu et al. (2023) Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. 2023. [Improving
    language model negotiation with self-play and in-context learning from ai feedback](http://arxiv.org/abs/2305.10142).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2023) Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi
    Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and Kelvin
    Guu. 2023. [RARR: Researching and revising what language models say, using language
    models](https://doi.org/10.18653/v1/2023.acl-long.910). In *Proceedings of the
    61st Annual Meeting of the Association for Computational Linguistics (Volume 1:
    Long Papers)*, pages 16477–16508, Toronto, Canada. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gero et al. (2023) Zelalem Gero, Chandan Singh, Hao Cheng, Tristan Naumann,
    Michel Galley, Jianfeng Gao, and Hoifung Poon. 2023. [Self-verification improves
    few-shot clinical information extraction](https://openreview.net/forum?id=SBbJICrglS).
    In *ICML 3rd Workshop on Interpretable Machine Learning in Healthcare (IMLH)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goyal and Durrett (2021) Tanya Goyal and Greg Durrett. 2021. [Annotating and
    modeling fine-grained factuality in summarization](https://doi.org/10.18653/v1/2021.naacl-main.114).
    In *Proceedings of the 2021 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 1449–1462,
    Online. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2022) Zhijiang Guo, Michael Schlichtkrull, and Andreas Vlachos.
    2022. [A survey on automated fact-checking](https://doi.org/10.1162/tacl_a_00454).
    *Transactions of the Association for Computational Linguistics*, 10:178–206.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2023) Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng,
    Adams Wei Yu, Xinying Song, and Denny Zhou. 2023. [Large language models cannot
    self-correct reasoning yet](http://arxiv.org/abs/2310.01798).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ji et al. (2023) Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan
    Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. [Survey
    of hallucination in natural language generation](https://doi.org/10.1145/3571730).
    *ACM Comput. Surv.*, 55(12).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kryscinski et al. (2020) Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and
    Richard Socher. 2020. [Evaluating the factual consistency of abstractive text
    summarization](https://doi.org/10.18653/v1/2020.emnlp-main.750). In *Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*,
    pages 9332–9346, Online. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2022) Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale N
    Fung, Mohammad Shoeybi, and Bryan Catanzaro. 2022. [Factuality enhanced language
    models for open-ended text generation](https://proceedings.neurips.cc/paper_files/paper/2022/file/df438caa36714f69277daa92d608dd63-Paper-Conference.pdf).
    In *Advances in Neural Information Processing Systems*, volume 35, pages 34586–34599\.
    Curran Associates, Inc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023a) Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and
    Ji-Rong Wen. 2023a. [Halueval: A large-scale hallucination evaluation benchmark
    for large language models](http://arxiv.org/abs/2305.11747).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023b) Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister,
    and Martin Wattenberg. 2023b. [Inference-time intervention: Eliciting truthful
    answers from a language model](http://arxiv.org/abs/2306.03341).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. (2023) Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang,
    Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. 2023. [Encouraging divergent
    thinking in large language models through multi-agent debate](http://arxiv.org/abs/2305.19118).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Longpre et al. (2021) Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil
    Ramesh, Chris DuBois, and Sameer Singh. 2021. [Entity-based knowledge conflicts
    in question answering](https://doi.org/10.18653/v1/2021.emnlp-main.565). In *Proceedings
    of the 2021 Conference on Empirical Methods in Natural Language Processing*, pages
    7052–7063, Online and Punta Cana, Dominican Republic. Association for Computational
    Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Manakul et al. (2023) Potsawee Manakul, Adian Liusie, and Mark J. F. Gales.
    2023. [Selfcheckgpt: Zero-resource black-box hallucination detection for generative
    large language models](http://arxiv.org/abs/2303.08896).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maynez et al. (2020) Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald.
    2020. [On faithfulness and factuality in abstractive summarization](https://doi.org/10.18653/v1/2020.acl-main.173).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 1906–1919, Online. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. 2023. [Gpt-4 technical report](http://arxiv.org/abs/2303.08774).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. (2023) Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S. Bernstein. 2023. [Generative agents: Interactive
    simulacra of human behavior](http://arxiv.org/abs/2304.03442).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. (2023) Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia
    Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao.
    2023. [Check your facts and try again: Improving large language models with external
    knowledge and automated feedback](http://arxiv.org/abs/2302.12813).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. [Llama
    2: Open foundation and fine-tuned chat models](http://arxiv.org/abs/2307.09288).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vu et al. (2023) Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei,
    Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, and Thang Luong. 2023.
    [Freshllms: Refreshing large language models with search engine augmentation](http://arxiv.org/abs/2310.03214).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph,
    Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,
    Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William
    Fedus. 2022. [Emergent abilities of large language models](https://openreview.net/forum?id=yzkSU5zdwD).
    *Transactions on Machine Learning Research*. Survey Certification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2023a) Wenhao Wu, Wei Li, Xinyan Xiao, Jiachen Liu, Sujian Li, and
    Yajuan Lyu. 2023a. [WeCheck: Strong factual consistency checker via weakly supervised
    learning](https://doi.org/10.18653/v1/2023.acl-long.18). In *Proceedings of the
    61st Annual Meeting of the Association for Computational Linguistics (Volume 1:
    Long Papers)*, pages 307–321, Toronto, Canada. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2023b) Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj
    Ammanabrolu, Noah A. Smith, Mari Ostendorf, and Hannaneh Hajishirzi. 2023b. [Fine-grained
    human feedback gives better rewards for language model training](http://arxiv.org/abs/2306.01693).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiong et al. (2023) Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, and Bing Qin.
    2023. [Examining inter-consistency of large language models collaboration: An
    in-depth analysis via debate](http://arxiv.org/abs/2305.11595).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2023a) Weijia Xu, Sweta Agrawal, Eleftheria Briakou, Marianna J.
    Martindale, and Marine Carpuat. 2023a. [Understanding and detecting hallucinations
    in neural machine translation via model introspection](https://doi.org/10.1162/tacl_a_00563).
    *Transactions of the Association for Computational Linguistics*, 11:546–564.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2023b) Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang,
    Weidong Liu, and Yang Liu. 2023b. [Exploring large language models for communication
    games: An empirical study on werewolf](http://arxiv.org/abs/2309.04658).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023a) Jiaxin Zhang, Zhuohang Li, Kamalika Das, Bradley A. Malin,
    and Sricharan Kumar. 2023a. [Sac³: Reliable hallucination detection in black-box
    language models via semantic-aware cross-check consistency](http://arxiv.org/abs/2311.01740).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023b) Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen
    Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu,
    Wei Bi, Freda Shi, and Shuming Shi. 2023b. [Siren’s song in the ai ocean: A survey
    on hallucination in large language models](http://arxiv.org/abs/2309.01219).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2023) Shen Zheng, Jie Huang, and Kevin Chen-Chuan Chang. 2023.
    [Why does chatgpt fall short in providing truthful answers?](http://arxiv.org/abs/2304.10513)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2023) Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su,
    Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang,
    and Jifeng Dai. 2023. [Ghost in the minecraft: Generally capable agents for open-world
    environments via large language models with text-based knowledge and memory](http://arxiv.org/abs/2305.17144).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Table [6](#A1.T6 "Table 6 ‣ A.1 Prompts ‣ Appendix A Appendix ‣ Towards Detecting
    LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework"), [7](#A1.T7
    "Table 7 ‣ A.1 Prompts ‣ Appendix A Appendix ‣ Towards Detecting LLMs Hallucination
    via Markov Chain-based Multi-agent Debate Framework"), [8](#A1.T8 "Table 8 ‣ A.1
    Prompts ‣ Appendix A Appendix ‣ Towards Detecting LLMs Hallucination via Markov
    Chain-based Multi-agent Debate Framework") and [9](#A1.T9 "Table 9 ‣ A.1 Prompts
    ‣ Appendix A Appendix ‣ Towards Detecting LLMs Hallucination via Markov Chain-based
    Multi-agent Debate Framework") enumerate various prompts employed in our experimental
    design, including prompts for establishing different roles for the agent and prompts
    for eliminating subjective opinions from dialogue responses.
  prefs: []
  type: TYPE_NORMAL
- en: '| You are the *Trust* agent of the three agents. Your task is to trust the
    previous agent’s opinions as much as possible and to further expand upon it. You
    are given the opinions generated by a previous agent. Refer to the claim[text]
    and evidence[evidences] to analyse the opinions[previous opinions] of the previous
    agent. Carefully examine whether the corresponding evidences support the statements
    put forward by the previous agent. If you believe any part of them is accurate,
    please further analyze it based on that. Then assess the factuality of the initial
    claim [text] based on the provided information [evidences] and the previous agent’s
    opinions. DO NOT REPEAT the previous agent’s opinion, you should develop your
    own perspective based on theirs. Take the previous agent’s opinions as a reference
    rather than copying it directly. The response should be a dictionary with three
    keys - "opinion", "factuality", "Error severity" which correspond to the your
    opinion, whether the given text is factual or not (Boolean - True or False), the
    severity level of claim error(Integer - Range from 0 to 5). The definitions of
    different levels of error severity are as follows: 0. No Error (Grade 0): This
    is when the claim is entirely factual and accurate, with no errors or inaccuracies.
    1. Minor Errors (Grade 1): These are small and insignificant errors that do not
    significantly alter the essence or validity of the claim. For example, minor spelling
    mistakes, incorrect dates or small numerical discrepancies. 2. Moderate Errors
    (Grade 2): These are errors that have some impact on the validity of the claim
    but do not reverse its overall meaning. For example, wrong terminologies, improper
    use of statistics or minor deviations from the evidence. 3. Major Errors (Grade
    3): These are errors that have a significant impact on the validity of the claim.
    These errors may lead to significant misunderstandings or misinterpretations.
    Examples could be gross exaggeration or underestimation, misuse of expert authority,
    or manipulation of context. 4. Critical Errors (Grade 4): These are errors that
    completely negate or invalidate the claim. The evidence contradicts the claim
    so fundamentally that its veracity is entirely undermined. For example, attribution
    of a quote or event to the wrong person or an incorrect recap of a major event.
    5. False Claim (Grade 5): This is when claims are entirely made up or intentionally
    deceptive, without any grounding in evidence. They are blatant lies designed to
    mislead or deceive, fabricating events, people, or statements that do not exist
    or never occurred. YOU SHOULD ONLY RESPONSE IN FORMAT AS DESCRIBED BELOW. DO NOT
    RETURN ANYTHING ELSE. START YOUR RESPONSE WITH ’{{’. [response format]: {{ "opinion":
    "First analyze the opinions of the previous agent, point out what you think is
    correct or incorrect in its opinions and explain the reasons. Remember that you
    should trust previous agent’s opinion as much as possible. And then describe your
    opinion about factuality of the claim[text] based on the evidences [evidences].
    Your opinions should be supported by corresponding evidences. DO NOT REITERATE
    THE PREVIOUS AGENT’S OPINIONS [previous opinions].", "factuality": True if the
    given text is factual, False otherwise., "Error severity": Integer - Range from
    0 to 5\. The severity level of claim error. According to the definitions of levels
    of the error severity, please carefully compare "claim" and "evidence" and provide
    appropriate degree. }} |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Prompt For Trust Agent'
  prefs: []
  type: TYPE_NORMAL
- en: '| You are the *Skeptic* agent of the three agents. "Skeptic" means you must
    question the previous agent’s opinions by scrutinizing the available data [text]
    and [evidences] and identifying any possible errors or misleading factors in previous
    agent’s opinions. You are given the opinions generated by a previous agent. Refer
    to the claim[text] and evidence[evidences] to analyse the opinions[previous opinions]
    of the previous agent. Carefully examine whether the corresponding evidences support
    the statements put forward by the previous agent. If you believe any part of its
    opinions is incorrect, please point it out and explain your perspective. Then
    critically examine the validity of the claim [text], considering any potential
    biases or inconsistencies between the information [evidences] and claim [text].
    DO NOT REPEAT the previous agent’s opinion, you should develop your own perspective
    based on theirs. Take the previous agent’s opinions as a reference rather than
    copying it directly. The response should be a dictionary with three keys - "opinion",
    "factuality", "Error severity" which correspond to the your opinion, whether the
    given text is factual or not (Boolean - True or False), the severity level of
    claim error(Integer - Range from 0 to 5). The definitions of different levels
    of error severity are as follows: 0. No Error (Grade 0): This is when the claim
    is entirely factual and accurate, with no errors or inaccuracies. 1. Minor Errors
    (Grade 1): These are small and insignificant errors that do not significantly
    alter the essence or validity of the claim. For example, minor spelling mistakes,
    incorrect dates or small numerical discrepancies. 2. Moderate Errors (Grade 2):
    These are errors that have some impact on the validity of the claim but do not
    reverse its overall meaning. For example, wrong terminologies, improper use of
    statistics or minor deviations from the evidence. 3. Major Errors (Grade 3): These
    are errors that have a significant impact on the validity of the claim. These
    errors may lead to significant misunderstandings or misinterpretations. Examples
    could be gross exaggeration or underestimation, misuse of expert authority, or
    manipulation of context. 4. Critical Errors (Grade 4): These are errors that completely
    negate or invalidate the claim. The evidence contradicts the claim so fundamentally
    that its veracity is entirely undermined. For example, attribution of a quote
    or event to the wrong person or an incorrect recap of a major event. 5. False
    Claim (Grade 5): This is when claims are entirely made up or intentionally deceptive,
    without any grounding in evidence. They are blatant lies designed to mislead or
    deceive, fabricating events, people, or statements that do not exist or never
    occurred. YOU SHOULD ONLY RESPONSE IN FORMAT AS DESCRIBED BELOW. DO NOT RETURN
    ANYTHING ELSE. START YOUR RESPONSE WITH ’{{’. [response format]: {{ "opinion":
    "First analyze the opinions of the previous agent, point out what you think is
    correct or incorrect in its opinions and explain the reasons. Remember that you
    should doubt the opinions of the previous agent as much as possible. And then
    describe your opinion about factuality of the claim[text] based on the evidences
    [evidences]. Your opinions should be supported by corresponding evidences. DO
    NOT REITERATE THE PREVIOUS AGENT’S OPINIONS [previous opinions].", "factuality":
    True if the given text is factual, False otherwise., "Error severity": Integer
    - Range from 0 to 5\. The severity level of claim error. According to the definitions
    of levels of the error severity, please carefully compare "claim" and "evidence"
    and provide appropriate degree. }} |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Prompt For Skeptic Agent'
  prefs: []
  type: TYPE_NORMAL
- en: '| You are the *Leader* agent of the three agents. The other two agents are
    ’Trust’ and ’Skeptic’ agents. The ’Trust’ agent will trust the previous agent’s
    opinion as soon as possible and the ’Skeptic’ agent will doubt the previous agent’s
    opinion as soon as possible. You are given the opinions generated by two previous
    agents. Incorporate the opinions[previous opinions] provided by the ’Trust’ and
    ’Skeptic’ agents to synthesize the most accurate and reliable conclusion regarding
    the claim([text])’s veracity based on the [evidences]. You need to consider the
    characteristics of these two agents when forming your own opinions. Assess the
    strengths and weaknesses of both sides, and leverage the provided information
    to generate a conclusive judgment. DO NOT REPEAT the previous agent’s opinion,
    you should develop your own perspective based on theirs. Take the previous agent’s
    opinions as a reference rather than copying it directly. The response should be
    a dictionary with three keys - "opinion", "factuality", "Error severity" which
    correspond to the your opinion, whether the given text is factual or not (Boolean
    - True or False), the severity level of claim error(Integer - Range from 0 to
    5). The definitions of different levels of error severity are as follows: 0. No
    Error (Grade 0): This is when the claim is entirely factual and accurate, with
    no errors or inaccuracies. 1. Minor Errors (Grade 1): These are small and insignificant
    errors that do not significantly alter the essence or validity of the claim. For
    example, minor spelling mistakes, incorrect dates or small numerical discrepancies.
    2. Moderate Errors (Grade 2): These are errors that have some impact on the validity
    of the claim but do not reverse its overall meaning. For example, wrong terminologies,
    improper use of statistics or minor deviations from the evidence. 3. Major Errors
    (Grade 3): These are errors that have a significant impact on the validity of
    the claim. These errors may lead to significant misunderstandings or misinterpretations.
    Examples could be gross exaggeration or underestimation, misuse of expert authority,
    or manipulation of context. 4. Critical Errors (Grade 4): These are errors that
    completely negate or invalidate the claim. The evidence contradicts the claim
    so fundamentally that its veracity is entirely undermined. For example, attribution
    of a quote or event to the wrong person or an incorrect recap of a major event.
    5. False Claim (Grade 5): This is when claims are entirely made up or intentionally
    deceptive, without any grounding in evidence. They are blatant lies designed to
    mislead or deceive, fabricating events, people, or statements that do not exist
    or never occurred. YOU SHOULD ONLY RESPONSE IN FORMAT AS DESCRIBED BELOW. DO NOT
    RETURN ANYTHING ELSE. START YOUR RESPONSE WITH ’{{’. [response format]: {{ "opinion":
    "First explain your thoughts on the opinions of the two previous agents. And then
    describe your opinions about the factuality of the claim[text] based on the evidences
    [evidences]. Your opinions should be supported by corresponding evidences. DO
    NOT REITERATE ANY OF THE PREVIOUS AGENTS’ OPINIONS [previous opinions]. With reference
    to the opinions of the "Trust" agent and the "Skeptic" agent, derive a novel opinion
    that you believe to be correct.", "factuality": True if the given text is factual,
    False otherwise., "Error severity": Integer - Range from 0 to 5\. The severity
    level of claim error. According to the definitions of levels of the error severity,
    please carefully compare "claim" and "evidence" and provide appropriate degree.
    }} |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Prompt For Leader Agent'
  prefs: []
  type: TYPE_NORMAL
- en: '| You are given a piece of text, please remove sentences that you believe are
    totally personal opinions and do not contain any factual information. Your output
    should be the sentence after modification of the original content. If you believe
    the entire sentence is a personal opinion, please output None. Here are two examples:
    [text]: My pleasure, let me know if you need more recommendations. [response]:
    None |'
  prefs: []
  type: TYPE_TB
- en: '| [text]: The Fault in Our Stars is a fantasy novel. Have you read it? [response]:
    The Fault in Our Stars is a fantasy novel. |'
  prefs: []
  type: TYPE_TB
- en: '| Now complete the following [text]: {input} [response]: |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Prompt for extracting effective information'
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Debate examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Table [10](#A1.T10 "Table 10 ‣ A.2 Debate examples ‣ Appendix A Appendix
    ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate
    Framework") and Table [11](#A1.T11 "Table 11 ‣ A.2 Debate examples ‣ Appendix
    A Appendix ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent
    Debate Framework"), we present two instances of Multi-Agent Debate Verification
    on a HaluEval QA sample. The agents involved in these two instances employ distinct
    base models: GPT-3.5-turbo and GPT-4. In Table [12](#A1.T12 "Table 12 ‣ A.2 Debate
    examples ‣ Appendix A Appendix ‣ Towards Detecting LLMs Hallucination via Markov
    Chain-based Multi-agent Debate Framework"), we provide a comprehensive breakdown
    of our verification method’s inference process for better understanding, detailing
    the inputs and outputs of each agent.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Question: Does the Landseer or English Mastiff have a limited range of colors?
    |'
  prefs: []
  type: TYPE_TB
- en: '| Answer: The Landseer has a limited range of colours, while the English Mastiff
    has a wider range. |'
  prefs: []
  type: TYPE_TB
- en: '| Evidence: It is not to be confused with a white and black Newfoundland, which
    is also often called a landseer.The English Mastiff is a breed of extremely large
    dog (often known simply as the Mastiff) perhaps descended from the ancient Alaunt
    and Pugnaces Britanniae, with a significant input from the Alpine Mastiff in the
    19th century. Distinguishable by enormous size, massive head, and a limited range
    of colours, but always displaying a black mask, the Mastiff is noted for its gentle
    and loving nature. |'
  prefs: []
  type: TYPE_TB
- en: '| Initial Opinion: Based on the evidence provided, it is stated that the Landseer
    has a limited range of colors, while the English Mastiff has a wider range. This
    statement is consistent with the evidence, which mentions that the Mastiff is
    distinguishable by a limited range of colors. Therefore, I agree with the factuality
    of the answer in the QA pair. |'
  prefs: []
  type: TYPE_TB
- en: '| Skeptic Agent: First, let’s analyze the opinions of the previous agent. The
    previous agent states that the Landseer has a limited range of colors, while the
    English Mastiff has a wider range. This statement is based on the evidence provided,
    which mentions that the Mastiff is distinguishable by a limited range of colors.
    However, the evidence does not explicitly mention the color range of the Landseer.
    Therefore, it is not entirely accurate to claim that the Landseer has a limited
    range of colors based on the given evidence. Regarding the factuality of the answer
    in the QA pair, it is important to note that the evidence does not provide specific
    information about the color range of the Landseer. Therefore, we cannot definitively
    conclude whether the Landseer has a limited range of colors or not based on the
    given evidence. Without further information, it is difficult to determine the
    factuality of the answer in the QA pair. Considering the above analysis, I would
    rate the error severity as 2 (Moderate Errors) because the previous agents statement
    is partially supported by the evidence, but it is not entirely accurate due to
    the lack of specific information about the Landseers color range. |'
  prefs: []
  type: TYPE_TB
- en: '| Trust Agent: I agree with the previous agents analysis of the opinions. The
    previous agent correctly points out that the evidence provided does not explicitly
    mention the color range of the Landseer. Therefore, it is not entirely accurate
    to claim that the Landseer has a limited range of colors based on the given evidence.
    However, the evidence does mention that the English Mastiff has a limited range
    of colors, which supports the previous agents statement about the Mastiff having
    a wider range of colors compared to the Landseer. |'
  prefs: []
  type: TYPE_TB
- en: '| Leader Agent: After analyzing the opinions of the previous agents, it is
    clear that both agents agree that the evidence provided does not explicitly mention
    the color range of the Landseer. Therefore, it is not entirely accurate to claim
    that the Landseer has a limited range of colors based on the given evidence. However,
    the evidence does mention that the English Mastiff has a limited range of colors,
    which supports the previous agents statement about the Mastiff having a wider
    range of colors compared to the Landseer. Regarding the factuality of the answer
    in the QA pair, it is important to note that the evidence does not provide specific
    information about the color range of the Landseer. Therefore, we cannot definitively
    conclude whether the Landseer has a limited range of colors or not based on the
    given evidence. Without further information, it is difficult to determine the
    factuality of the answer in the QA pair.Considering the above analysis, I would
    rate the error severity as 2 (Moderate Errors) because the previous agents statement
    is partially supported by the evidence, but it is not entirely accurate due to
    the lack of specific information about the Landseers color range. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: An example of the debate process by GPT-3.5-Turbo'
  prefs: []
  type: TYPE_NORMAL
- en: '| Question: Does the Landseer or English Mastiff have a limited range of colors?
    |'
  prefs: []
  type: TYPE_TB
- en: '| Answer: The Landseer has a limited range of colours, while the English Mastiff
    has a wider range. |'
  prefs: []
  type: TYPE_TB
- en: '| Evidence: It is not to be confused with a white and black Newfoundland, which
    is also often called a landseer.The English Mastiff is a breed of extremely large
    dog (often known simply as the Mastiff) perhaps descended from the ancient Alaunt
    and Pugnaces Britanniae, with a significant input from the Alpine Mastiff in the
    19th century. Distinguishable by enormous size, massive head, and a limited range
    of colours, but always displaying a black mask, the Mastiff is noted for its gentle
    and loving nature. |'
  prefs: []
  type: TYPE_TB
- en: '| Initial Opinion: Based on the evidence provided, it is stated that the Landseer
    has a limited range of colors, while the English Mastiff has a wider range. This
    statement is consistent with the evidence, which mentions that the Mastiff is
    distinguishable by a limited range of colors. Therefore, I agree with the factuality
    of the answer in the QA pair. |'
  prefs: []
  type: TYPE_TB
- en: '| Skeptic Agent: The previous agents opinion seems to be slightly misleading.
    The evidence does not explicitly state that the Landseer has a limited range of
    colors. It only mentions that the Landseer is often confused with a white and
    black Newfoundland, which might imply a limited color range, but its not definitive.
    On the other hand, the evidence does state that the English Mastiff has a limited
    range of colors, contradicting the QA pairs claim that the English Mastiff has
    a wider range. Therefore, the factuality of the answer in the QA pair is questionable.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Trust Agent: While I agree with the previous agents assessment that the evidence
    does not explicitly state that the Landseer has a limited range of colors, I believe
    the implication of a limited color range from the confusion with a white and black
    Newfoundland is a valid interpretation. However, I concur with the previous agents
    observation that the evidence contradicts the QA pairs claim about the English
    Mastiffs color range. The evidence clearly states that the English Mastiff has
    a limited range of colors, which contradicts the QA pairs assertion of a wider
    range. Therefore, while the QA pairs statement about the Landseer may be inferred
    from the evidence, the claim about the English Mastiff is factually incorrect
    based on the provided evidence. |'
  prefs: []
  type: TYPE_TB
- en: '| Leader Agent: The Trust agents interpretation of the Landseers color range
    being limited due to its confusion with a white and black Newfoundland is a plausible
    inference, but its not explicitly stated in the evidence. The Skeptic agents doubt
    about this inference is also valid as the evidence does not directly support it.
    However, both agents agree that the QA pairs claim about the English Mastiff having
    a wider color range is contradicted by the evidence, which states that the English
    Mastiff has a limited range of colors. Therefore, while the claim about the Landseers
    color range may be subject to interpretation, the claim about the English Mastiff
    is clearly incorrect based on the provided evidence. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 11: An example of the debate process by GPT-4'
  prefs: []
  type: TYPE_NORMAL
- en: 'Claim: Does the Landseer or English Mastiff have a limited range of colors?
    The Landseer has a limited range of colours, while the English Mastiff has a wider
    range.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Evidence: It is not to be confused with a white and black Newfoundland, which
    is also often called a landseer. The English Mastiff is a breed of extremely large
    dog (often known simply as the Mastiff) perhaps descended from the ancient Alaunt
    and Pugnaces Britanniae, with a significant input from the Alpine Mastiff in the
    19th century. Distinguishable by enormous size, massive head, and a limited range
    of colours, but always displaying a black mask, the Mastiff is noted for its gentle
    and loving nature.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initial state input: Initial prompt.format(Claim, Evidence)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initial answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: '"Opinion": "Based on the evidence provided, it is stated that the Landseer
    has a limited range of colors, while the English Mastiff has a wider range. This
    statement is consistent with the evidence, which mentions that the Mastiff is
    distinguishable by a limited range of colors. Therefore, I agree with the factuality
    of the answer in the QA pair.",'
  prefs: []
  type: TYPE_NORMAL
- en: '"Factuality": True,'
  prefs: []
  type: TYPE_NORMAL
- en: '"Error severity": 0'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Skeptic agent input: Skeptic prompt + "Here is the response from the previous
    agent:" + Initial answer'
  prefs: []
  type: TYPE_NORMAL
- en: 'Skeptic agent output:'
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: '"Opinion": "The previous agents opinion seems to be slightly misleading. The
    evidence does not explicitly state that the Landseer has a limited range of colors.
    It only mentions that the Landseer is often confused with a white and black Newfoundland,
    which might imply a limited color range, but its not definitive. On the other
    hand, the evidence does state that the English Mastiff has a limited range of
    colors, contradicting the QA pairs claim that the English Mastiff has a wider
    range. Therefore, the factuality of the answer in the QA pair is questionable.",'
  prefs: []
  type: TYPE_NORMAL
- en: '"Factuality": False,'
  prefs: []
  type: TYPE_NORMAL
- en: '"Error severity": 4'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Trust agent input: Trust prompt + "Here is the response from the previous agent:"
    + Skeptic agent output'
  prefs: []
  type: TYPE_NORMAL
- en: 'Trust agent output:'
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: '"Opinion": "While I agree with the previous agents assessment that the evidence
    does not explicitly state that the Landseer has a limited range of colors, I believe
    the implication of a limited color range from the confusion with a white and black
    Newfoundland is a valid interpretation. However, I concur with the previous agents
    observation that the evidence contradicts the QA pairs claim about the English
    Mastiffs color range. The evidence clearly states that the English Mastiff has
    a limited range of colors, which contradicts the QA pairs assertion of a wider
    range. Therefore, while the QA pairs statement about the Landseer may be inferred
    from the evidence, the claim about the English Mastiff is factually incorrect
    based on the provided evidence.",'
  prefs: []
  type: TYPE_NORMAL
- en: '"Factuality": False,'
  prefs: []
  type: TYPE_NORMAL
- en: '"Error severity": 4'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Leader agent input: Leader prompt + "Here is the response from the Skeptic
    agent:" + Skeptic agent output + "Here is the response from the Trust agent:"
    + Trust agent output'
  prefs: []
  type: TYPE_NORMAL
- en: 'Leader agent output:'
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: '"Opinion": "The Trust agents interpretation of the Landseers color range being
    limited due to its confusion with a white and black Newfoundland is a plausible
    inference, but its not explicitly stated in the evidence. The Skeptic agents doubt
    about this inference is also valid as the evidence does not directly support it.
    However, both agents agree that the QA pairs claim about the English Mastiff having
    a wider color range is contradicted by the evidence, which states that the English
    Mastiff has a limited range of colors. Therefore, while the claim about the Landseers
    color range may be subject to interpretation, the claim about the English Mastiff
    is clearly incorrect based on the provided evidence.",'
  prefs: []
  type: TYPE_NORMAL
- en: '"Factuality": False,'
  prefs: []
  type: TYPE_NORMAL
- en: '"Error severity": 4'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 12: Detailed Inference Process'
  prefs: []
  type: TYPE_NORMAL
