- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:44:00'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Identifying Performance-Sensitive Configurations in Software Systems through
    Code Analysis with LLM Agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.12806](https://ar5iv.labs.arxiv.org/html/2406.12806)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Zehao Wang, Dong Jae Kim, Tse-Husn (Peter) Chen
  prefs: []
  type: TYPE_NORMAL
- en: Software PErformance, Analysis and Reliability (SPEAR) Lab
  prefs: []
  type: TYPE_NORMAL
- en: Concordia University, Montreal, Canada
  prefs: []
  type: TYPE_NORMAL
- en: w_zeha@encs.concordia.ca, k_dongja@encs.concordia.ca, peterc@encs.concordia.ca
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Configuration settings are essential for tailoring software behavior to meet
    specific performance requirements. However, incorrect configurations are widespread,
    and identifying those that impact system performance is challenging due to the
    vast number and complexity of possible settings. In this work, we present PerfSense,
    a lightweight framework that leverages Large Language Models (LLMs) to efficiently
    identify performance-sensitive configurations with minimal overhead. PerfSense
    employs LLM agents to simulate interactions between developers and performance
    engineers using advanced prompting techniques such as prompt chaining and retrieval-augmented
    generation (RAG). Our evaluation of seven open-source Java systems demonstrates
    that PerfSense achieves an average accuracy of 64.77% in classifying performance-sensitive
    configurations, outperforming both our LLM baseline (50.36%) and the previous
    state-of-the-art method (61.75%). Notably, our prompt chaining technique improves
    recall by 10% to 30% while maintaining similar precision levels. Additionally,
    a manual analysis of 362 misclassifications reveals common issues, including LLMs’
    misunderstandings of requirements (26.8%). In summary, PerfSense significantly
    reduces manual effort in classifying performance-sensitive configurations and
    offers valuable insights for future LLM-based code analysis research.
  prefs: []
  type: TYPE_NORMAL
- en: Large language model, Configuration, Performance, Multi-Agent
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Modern software systems feature numerous configuration options, enabling customization
    for diverse workloads and hardware platforms (Singh et al., [2016](#bib.bib44);
    Bao et al., [2018](#bib.bib3)). While these configurations provide flexibility,
    some configurations, known as performance-sensitive configurations, can impact
    system performance when their values change. Developers need to identify and understand
    the impact of such configurations to ensure they are set correctly, maintaining
    system performance and behavior. However, due to the large volume of configurations,
    pinpointing performance-sensitive configurations is time-consuming (Jin et al.,
    [2012](#bib.bib21); Han and Yu, [2016a](#bib.bib17)) and incorrect settings are
    a common source of system misbehavior and performance degradation (Ganapathi et al.,
    [2004](#bib.bib14); community, [[n.d.]](#bib.bib10)). Hence, automated approaches
    to quickly find performance-sensitive configurations that require special attention
    or further investigation are important to alleviate developers burden (Yonkovit,
    [[n.d.]](#bib.bib57); Tian et al., [2015](#bib.bib45)).
  prefs: []
  type: TYPE_NORMAL
- en: Performance experts have various tools at their disposal to assess performance-sensitive
    configurations. Alongside performance profiling tools (ej technologies, [[n.d.]](#bib.bib13);
    visualvm, [[n.d.]](#bib.bib50); Bornholt and Torlak, [2018](#bib.bib4)), they
    can identify inefficient code patterns (Chen et al., [2014](#bib.bib7); Liu et al.,
    [2014](#bib.bib30); Nistor et al., [2015](#bib.bib35)), and utilize data-flow
    and dynamic analysis to find performance-sensitive configurations (Li et al.,
    [2020](#bib.bib23); Lillack et al., [2014](#bib.bib27)). However, as highlighted
    by  Velez et al. ([2022a](#bib.bib48)), the adoption of these tools faces usability
    challenges for performance experts when analyzing the performance impact of configurations.
    These challenges arise from (1) a lack of comprehensive understanding of the codebase
    and its intricate interactions across multiple components, (2) difficulties in
    identifying the code affected by performance-sensitive configurations, and (3)
    the intricate cause-and-effect relationship between performance-sensitive configurations
    and the corresponding source code. Consequently, performance engineers may face
    challenges in accurately identifying the performance sensitivity of configurations.
    Effective collaboration between developers and performance engineers is crucial
    for overcoming these challenges and effectively identifying performance-sensitive
    configurations. Developers possess in-depth knowledge about the codebase and its
    functionality, while performance engineers specialize in analyzing performance-related
    issues. Leveraging their complementary expertise enables more thorough code analysis
    and more accurate classification of performance-sensitive configurations.
  prefs: []
  type: TYPE_NORMAL
- en: The rise of Large Language Models (LLMs) is revolutionizing programming and
    software engineering. Trained on vast code datasets, LLMs understand code deeply
    and excel in various code-related tasks. With tools like ChatGPT (OpenAI, [2023](#bib.bib36))
    and LLaMA (Touvron et al., [2023](#bib.bib46)), researchers showcase LLMs’ potential
    in tasks like generating commit messages (Zhang et al., [2024](#bib.bib60)), resolving
    merge conflicts (Shen et al., [2023](#bib.bib43)), creating tests (Xie et al.,
    [2023](#bib.bib53); Yuan et al., [2023](#bib.bib58); Schäfer et al., [2023](#bib.bib40)),
    renaming methods (AlOmar et al., [2024](#bib.bib2)), and aiding in log analytics (Ma
    et al., [2024b](#bib.bib32), [a](#bib.bib33)). Given the complexity of collaboration
    during software engineering tasks, using LLM agents stands out as a promising
    direction to replicate human workflows. Specifically, multi-agent systems have
    achieved significant progress in solving complex tasks by assigning agents to
    specific roles and emulating collaborative activities in software engineering
    practice (Hong et al., [2023](#bib.bib19); Dong et al., [2023](#bib.bib12); Qian
    et al., [2023](#bib.bib38)). For example, Dong et al. ([2023](#bib.bib12)) developed
    a self-collaboration framework, assigning LLM agents to work as distinct experts
    for sub-tasks in software development. Qian et al. ([2023](#bib.bib38)) proposed
    an end-to-end framework for software development through self-communication among
    the agents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inspired by multi-agent, we introduce PerfSense, a lightweight framework designed
    to effectively classify performance-sensitive configurations using Large Language
    Models (LLMs) as multi-agent systems. PerfSense leverages the collaborative capabilities
    of LLMs to mimic the interactions between developers and performance engineers,
    enabling a thorough analysis of the performance sensitivity of configurations.
    PerfSense employs two primary agents: DevAgent and PerfAgent. DevAgent focuses
    on retrieving relevant source code and documentation related to the configurations
    and conducting performance-aware code reviews. PerfAgent, on the other hand, utilizes
    the insights from DevAgent to classify configurations based on their performance
    sensitivity. This collaboration is facilitated through advanced prompting techniques
    such as prompt chaining and retrieval-augmented generation (RAG), which enhance
    the agents’ understanding and analytical capabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: To address the challenge of navigating a large codebase with limited LLM context
    size, PerfSense iteratively breaks down complex tasks into manageable subtasks.
    Specifically, PerfAgent iteratively communicates with DevAgent to gather and analyze
    relevant source code associated with the configurations under scrutiny. Through
    a series of prompt chains, PerfAgent refines its understanding by requesting specific
    details, clarifications, and performance-related insights from DevAgent. This
    iterative communication ensures that PerfAgent accumulates a comprehensive knowledge
    base without exceeding the context size limitations, enabling accurate classification
    of performance-sensitive configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Our evaluation of seven open-source systems demonstrates that PerfSense achieves
    64.77% accuracy in classifying performance-sensitive configurations, outperforming
    state-of-the-art technique (Chen et al., [2023a](#bib.bib9)) and our LLM baseline
    with an average accuracy of 61.75% and 50.36%, respectively. Compared to prior
    technique (Chen et al., [2023a](#bib.bib9)) that requires tens or hundreds of
    hours to collect performance data manually, PerfSense is lightweight and requires
    minimal human effort.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, we make the following contributions:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our evaluation of seven open-source systems demonstrates that PerfSense achieves
    an average accuracy of 64.77%, surpassing the state-of-the-art approaches with
    an average accuracy of 61.75%.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We proposed a new LLM-based code analysis technique that employs two primary
    agents, DevAgent and PerfAgent, to navigate large codebases with limited LLM context
    sizes through advanced prompting techniques such as prompt chaining and retrieval-augmented
    generation (RAG).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We analyzed the effect of different prompting components that we implemented
    in PerfSense. We found that our prompt chaining technique significantly improves
    the recall (10% to 30% improvement) while maintaining a similar level of precision.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We conducted a manual study of the 362 misclassified configurations, identifying
    key reasons for misclassification, including LLM’s misunderstanding of requirements
    (26.8%) and incorrect interpretation of performance impact (10.0%).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We provided a discussion on the implications of our findings and highlight future
    direction on LLM-based code analysis.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In conclusion, by leveraging multi-agent collaboration and advanced prompting
    techniques, PerfSense provides an efficient technique for classifying performance-sensitive
    configuration, one of the most important first steps in understanding system performance.
    PerfSense also presents a novel code navigation approach that may inspire future
    LLM-based research on code analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Paper Organization. Section [2](#S2 "2\. Background ‣ Identifying Performance-Sensitive
    Configurations in Software Systems through Code Analysis with LLM Agents") provides
    the background of the problem and technique. Section [3](#S3 "3\. Related Work
    ‣ Identifying Performance-Sensitive Configurations in Software Systems through
    Code Analysis with LLM Agents") discusses related work. Section [4](#S4 "4\. Design
    of PerfSense ‣ Identifying Performance-Sensitive Configurations in Software Systems
    through Code Analysis with LLM Agents") presents the details of PerfSense. Section [5](#S5
    "5\. Evaluation ‣ Identifying Performance-Sensitive Configurations in Software
    Systems through Code Analysis with LLM Agents") shows the evaluation results.
    Section [6](#S6 "6\. Discussion ‣ Identifying Performance-Sensitive Configurations
    in Software Systems through Code Analysis with LLM Agents") discusses the findings.
    Section [7](#S7 "7\. Threats to Validity ‣ Identifying Performance-Sensitive Configurations
    in Software Systems through Code Analysis with LLM Agents") discusses the threats
    to validity. Section [8](#S8 "8\. Conclusion ‣ Identifying Performance-Sensitive
    Configurations in Software Systems through Code Analysis with LLM Agents") concludes
    the paper.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we first discuss the definition and importance of performance-sensitive
    configuration. Then, we provide background on large language models (LLM) agents
    and retrieval-augmented generation (RAG).
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. Performance-Sensitive Configurations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Software systems often contain various configuration parameters to provide flexibility
    in deployment and execution (Singh et al., [2016](#bib.bib44); Bao et al., [2018](#bib.bib3)).
    Some configurations, known as performance-sensitive configurations, affect performance
    when their values change. For example, an application’s name is generally not
    performance-sensitive, whereas memory allocation settings can significantly impact
    performance (Yin et al., [2011](#bib.bib56); Chen et al., [2016](#bib.bib6)).
    Identifying these configurations is crucial, as their usage directly impacts system
    efficiency and stability. However, developers may not always be aware of the performance
    implications of configuration changes, leading to common misconfigurations, impacting
    overall system performance (Yin et al., [2011](#bib.bib56); Xu et al., [2013](#bib.bib54);
    Chen et al., [2016](#bib.bib6); Han and Yu, [2016b](#bib.bib18); Velez et al.,
    [2022b](#bib.bib49)).
  prefs: []
  type: TYPE_NORMAL
- en: Determining which configurations are performance-sensitive is challenging, given
    the high number of configurations and complex interactions among various system
    components (Zhang et al., [2015](#bib.bib59)), the absence of transparent documentation
    or feedback concerning the performance implications of each setting (Yin et al.,
    [2011](#bib.bib56)), and the complexity and time-intensive nature of performance
    testing (Yonkovit, [[n.d.]](#bib.bib57)). Performance engineers need to conduct
    load tests to evaluate the performance sensitivity and impacts of various configurations.
    These tests involve altering the values of configuration parameters and assessing
    their impacts on system performance (Zhang et al., [2015](#bib.bib59); Singh et al.,
    [2016](#bib.bib44); Wang et al., [2021](#bib.bib52); Vitui and Chen, [2021](#bib.bib51)).
    Therefore, an important step that can reduce the testing cost is to only conduct
    such tests on performance-sensitive configurations.
  prefs: []
  type: TYPE_NORMAL
- en: While developers implement code functionality with the best coding standards
    in mind, they may not always adhere to best-performance engineering practices.
    In collaborative efforts, developers and performance engineers work together to
    identify performance-sensitive configurations. Performance engineers leverage
    domain-specific knowledge to design and implement performance tests that uncover
    configuration sensitivities. However, performance engineers need the assistance
    of developers who have an in-depth understanding of the codebase to navigate across
    multiple source code components. Hence, to narrow down performance-sensitive configurations
    that impact overall system performance, there must be synergy in sharing knowledge
    between developers and performance engineers.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. LLM-based Multi-agent Framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Large language models (LLMs) are pre-trained using vast datasets comprising
    a wide range of texts, such as documentation and source code. The core of LLM
    agents consists of large language models (LLMs) designed to understand questions
    and generate human-like responses. These agents refine their responses based on
    feedback (Madaan et al., [2024](#bib.bib34)), use memory mechanisms to learn from
    historical experiences (Li et al., [2024b](#bib.bib25)), retrieve informative
    knowledge to improve prompting and generate better responses (Zhao et al., [2023](#bib.bib61)),
    and collaborate with other LLM agents to solve complex tasks in a multi-agent
    process (Guo et al., [2024](#bib.bib16)). By using prompting, agents can assume
    specific roles (e.g., developer or tester) and provide domain-specific responses (Deshpande
    et al., [2023](#bib.bib11)). In particular, a multi-agent system has been shown
    to improve the capabilities of individual LLM agents by enabling collaboration
    among agents, each with specialized abilities (Hong et al., [2023](#bib.bib19);
    Chan et al., [2023](#bib.bib5)). Multiple LLM agents can share domain expertise
    and make collective decisions. Effective communication patterns are crucial for
    optimizing the overall performance of a multi-agent framework, allowing them to
    tackle complex projects using a divide-and-conquer approach (Chen et al., [2023b](#bib.bib8)).
    Finally, with modern frameworks like LangChain (langchain, [2023](#bib.bib22)),
    one key characteristic of LLM agents is their ability to interact with external
    tools to perform tasks similarly to humans. For example, an LLM agent acting as
    a test engineer can generate test cases, use test automation tools to collect
    code coverage, and answer further queries based on the gathered information.
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we propose PerfSense, which leverages LLM agents to emulate the
    collaboration between developers and performance engineers. PerfSense analyzes
    the source code and classifies whether a configuration is performance-sensitive.
    PerfSense is zero-shot and unsupervised. It requires minimal input from developers
    and achieves better results than the state-of-the-art technique on classifying
    performance-sensitive configuration (Chen et al., [2023a](#bib.bib9)).
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we discuss existing research and literature on three topics:
    1) Performance Analysis of Configuration; 2) Using LLMs to Analyze Configuration;
    and 3) Multi-Agent-Based Code Analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Performance Analysis of Configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some previous research aims to analyze the performance of configuration to help
    developers understand the performance issue during the software configuration
    tuning. ConfigCrusher (Velez et al., [2020](#bib.bib47)) relies on static taint
    analysis to reveal the relationship between an option and the affected code regions,
    dynamically analyze the influence of configuration options on the regions’ performance,
    and build the performance-influence model through white-box performance analysis.
    DiagConfig (Chen et al., [2023a](#bib.bib9)) leverages static taint analysis to
    identify the dependencies between performance-related operations and options.
    Through manual performance experiments and labeling on training systems, they
    build a random forest model to classify the performance-sensitive configurations.
    Different from the above work, in our work, we employ the LLM agent alongside
    static code analysis, specifically the call graph analysis, to study the performance
    of configurations. Given LLMs’ promising performance in understanding code, call
    graph analysis for LLMs can provide more information and incur lower overhead
    compared to taint analysis. More importantly, our approach is zero-shot and reduces
    minimal human effort, which can help developers efficiently identical potential
    performance-sensitive configurations for further analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Using LLMs to Analyze Configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recently, large language models have shown promising performance on various
    software engineer tasks, such as code generation and summarization. Much research
    leveraged LLMs for tasks related to software configuration.  Lian et al. ([2024](#bib.bib26))
    proposed an LLM-based framework, Ciri, using few-shot learning and prompt engineering
    to validate the correctness of configuration files from the file level and parameter
    level. From the evaluation of real-world misconfigurations, comprising 64 configurations,
    and synthesized misconfigurations involving 1,582 parameters, Ciri achieves F1
    scores of 0.79 and 0.65 at the file level and parameter level, respectively. Liu
    et al. ([2024b](#bib.bib31)) introduced the LLM-CompDroid framework, which employs
    LLMs alongside the bug resolution tool to address configuration compatibility
    issues in Android applications. Their framework surpasses the state-of-the-art
    (SOTA) methods by at least 9.8% and 10.4% in the Correct and Correct@k metrics,
    respectively, respectively. Shan et al. ([2024](#bib.bib42)) came up with the
    framework, LogConfigLocalizer, which leverages Large Language Models and logs
    to localize root-cause configuration properties, achieving a high average accuracy
    of 99.91%. Different from these works, our work explores the potential of LLMs
    to analyze the performance sensitivity of configurations, which can assist developers
    in reducing performance testing costs.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Multi-Agent Based Code Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Agent-based code analysis emphasizes the importance of defining roles and facilitating
    communication among multiple LLM agents. Some approaches incorporate external
    tools as agents. For example, Huang et al. ([2023](#bib.bib20)) introduced a test
    executor agent that employs a Python interpreter to provide test logs for LLMs.
    Similarly, Zhong et al. ([2024](#bib.bib62)) presented a debugger agent that uses
    a static analysis tool to construct control flow graphs, aiding LLMs in locating
    bugs. Other studies (Hong et al., [2023](#bib.bib19); Qian et al., [2023](#bib.bib38);
    Dong et al., [2023](#bib.bib12); Lin et al., [2024](#bib.bib28)) assigned LLMs
    to emulate diverse human roles, such as analysts, engineers, testers, project
    managers, and chief technology officers (CTOs). These approaches use software
    process models (e.g., Waterfall) for inter-role communication, varying the prompts
    and roles to enhance code generation. Our technique leverages similar multi-agent
    systems to classify performance-sensitive configurations. By integrating prompt
    chaining and retrieval-augmented generation (RAG), PerfSense enhances the collaborative
    capabilities of LLM agents, leading to a lightweight technique that addresses
    the challenge of limited LLM context size when analyzing a large codebase.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4372bf77c7443de229d26254bbe72532.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1. Overview of PerfSense
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Design of PerfSense
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we introduce PerfSense, a lightweight framework designed for
    identifying performance-sensitive configurations. We begin by discussing various
    LLM agents and their communication and conclude with a detailed running example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [1](#S3.F1 "Figure 1 ‣ 3.3\. Multi-Agent Based Code Analysis ‣ 3\. Related
    Work ‣ Identifying Performance-Sensitive Configurations in Software Systems through
    Code Analysis with LLM Agents") illustrates the overview of PerfSense. To analyze
    the performance sensitivity of configurations, PerfSense comprises two different
    agents: the developer (DevAgent) and the performance expert (PerfAgent). At a
    high level, given a potential performance-sensitive configuration, PerfAgent utilizes
    iterative self-refinement and retrieval-augmented prompting techniques in a zero-shot
    setting, with the assistance of DevAgent, to iteratively build a knowledge base
    of the codebase and classify whether the configuration is performance-sensitive.
    In the following section, we elaborate on the roles of PerfAgent and DevAgent,
    and their communication pattern for determining performance-sensitive configurations.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Agent Roles and Definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/755a9b73546c63f2b64cf57096420fc1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2. An example of performance-sensitive configuration.
  prefs: []
  type: TYPE_NORMAL
- en: '4.1.1\. Developer Agent: Retrieving Configuration-Related Code'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The main role of a DevAgent is to retrieve source code and conduct performance-aware
    code review, upon PerfAgent’s request, and respond with the result so that PerfAgent
    has the necessary information to make the classification decision. Initially,
    PerfAgent receives the potential performance-sensitive configuration to analyze.
    However, multiple methods across various classes may have some dependencies with
    the configuration parameter, making it difficult for PerfAgent to assess the configuration’s
    performance sensitivity accurately. Providing additional summaries of the configuration-related
    code, such as related source code and documentation, can help improve PerfAgent’s
    output (Ye et al., [2020](#bib.bib55)). Hence, PerfAgent relies on DevAgent, which
    utilizes two tools: (1) traditional program analysis to extract source code that
    may be associated with the configuration through inter-procedural call graphs,
    and (2) document retrieval to extract official documentation associated with the
    configuration. In addition to retrieving the code, PerfAgent may also rely on
    DevAgent to provide feedback on the specific source code, since our intuition
    is that the developer should have a better understanding of the functionality
    of the code. Thus, DevAgent conducts performance-aware code reviews on the source
    code methods requested by PerfAgent, as indicated in Figure [3](#S4.F3 "Figure
    3 ‣ 4.1.1\. Developer Agent: Retrieving Configuration-Related Code ‣ 4.1\. Agent
    Roles and Definition ‣ 4\. Design of PerfSense ‣ Identifying Performance-Sensitive
    Configurations in Software Systems through Code Analysis with LLM Agents"). Below,
    we further discuss how we extract code context, official documentation, and the
    prompt design for DevAgent’s performance-aware code review.'
  prefs: []
  type: TYPE_NORMAL
- en: Extracting Configuration-Related Code. We define configuration-related code
    as the caller source code that invokes a method that directly accesses the configuration.
    For example, as indicated in Figure [2](#S4.F2 "Figure 2 ‣ 4.1\. Agent Roles and
    Definition ‣ 4\. Design of PerfSense ‣ Identifying Performance-Sensitive Configurations
    in Software Systems through Code Analysis with LLM Agents"), the configuration
    under analysis is key_cache_size_in_mb, and its related source code is initKeyCache,
    which is the caller source code that accesses the configuration. To extract configuration-related
    source code, we first utilize static code analysis to extract the inter-procedural
    call graph (Gousios, [[n.d.]](#bib.bib15)). We first identify the method that
    directly accesses the configuration, then we traverse the graph to retrieve all
    the methods that have either a direct or indirect caller-callee relationship.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting Configuration Documentation. The description of a configuration on
    the document may provide additional information that can help classify configurations.
    For example, in the studied system Batik, the documentation for the configuration
    called Width provides additional information that it is the “Output Image Width”,
    which may help the agents with the analysis. Therefore, we extract the configuration
    descriptions, if they are available, from the official project website. The description
    is passed to both DevAgent and PerfAgent as part of the prompts when analyzing
    the configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'DevAgent’s Performance Aware Code Review. Figure [3](#S4.F3 "Figure 3 ‣ 4.1.1\.
    Developer Agent: Retrieving Configuration-Related Code ‣ 4.1\. Agent Roles and
    Definition ‣ 4\. Design of PerfSense ‣ Identifying Performance-Sensitive Configurations
    in Software Systems through Code Analysis with LLM Agents") shows our prompt design
    for performance-aware code review that DevAgent carries out. Firstly, we give
    personification to the DevAgent, describing its role and goals, such as “You are
    a developer. Your job is to conduct performance-aware code review.”. Consequently,
    we provide context about the (1) source code and (2) configuration description
    to the DevAgent. Finally, we ask DevAgent to output the following requirements:
    (i) summarize the functionality of the code, (ii) how many times such source code
    may be triggered (estimation based on the provided textual information), and (iii)
    whether the code may have an impact on memory or execution time. It is important
    to note that performance-aware code review does not determine whether a configuration
    is performance-sensitive; this task falls to PerfAgent. However, DevAgent should
    be aware of common performance issues within the code they write, such as excessive
    memory usage and frequency of invocation, which may help PerfAgent with the analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="S4.F3.pic1" class="ltx_picture ltx_centering" height="1494.14" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,1494.14) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 12.82 1469.01)"><foreignobject width="574.35"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">Prompt
    Template for Performance-Aware Code Review</foreignobject></g> <g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 12.82 8.89)"><foreignobject width="574.35" height="1442.35"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Role: You
    are a developer. Your job is to conduct performance-aware code reviews on the
    given configuration-related code and official documentation for configuration
    to output the performance impact code that you wrote. Configuration-related code:
    
    1private  static  AutoSavingCache  initKeyCache()){ 2  ... 3  long  keyCacheInMemoryCapacity  =  DatabaseDescriptor.getKeyCacheSizeInMB()  *  1024  *  1024;
    4  kc  =  CaffeineCache.create(keyCacheInMemoryCapacity); 5  ... 6} Configuration
    description: Configuration Documentation after summarization. AutoSavingCache:
    “Specify the way Cassandra allocates and manages memtable memory.” Requirement:
    You must output three things below: 1\. Understand the functionality of the configuration
    in the code. 2\. Investigate triggering frequency of configuration-related operations.
    3\. Check the potential impact of configuration on the system.</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3. DevAgent’s Performance-Aware Code Review.
  prefs: []
  type: TYPE_NORMAL
- en: '4.1.2\. Performance Expert Agent: Analyzing the Performance Sensitivity of
    Configuration'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Given DevAgent’s feedback on a specific configuration-related operation, PerfAgent
    utilizes this feedback to classify performance-sensitive configurations. However,
    PerfAgent may require further clarification on the retrieved code. For example,
    as indicated in Figure [4](#S4.F4 "Figure 4 ‣ 4.1.2\. Performance Expert Agent:
    Analyzing the Performance Sensitivity of Configuration ‣ 4.1\. Agent Roles and
    Definition ‣ 4\. Design of PerfSense ‣ Identifying Performance-Sensitive Configurations
    in Software Systems through Code Analysis with LLM Agents"), it may reference
    other methods (e.g., create) about which PerfAgent may lack performance knowledge.
    Hence, PerfAgent may request additional information about these operations. In
    particular, we use the prompt template in Figure [4](#S4.F4 "Figure 4 ‣ 4.1.2\.
    Performance Expert Agent: Analyzing the Performance Sensitivity of Configuration
    ‣ 4.1\. Agent Roles and Definition ‣ 4\. Design of PerfSense ‣ Identifying Performance-Sensitive
    Configurations in Software Systems through Code Analysis with LLM Agents"), which
    starts by personifying PerfAgent with the introduction, “You are a performance
    expert… Check whether the provided configuration-related code is sufficient for
    performance analysis.” In the prompt, PerfAgent receives the source code, as well
    as feedback from DevAgent as indicated in the template from Figure [3](#S4.F3
    "Figure 3 ‣ 4.1.1\. Developer Agent: Retrieving Configuration-Related Code ‣ 4.1\.
    Agent Roles and Definition ‣ 4\. Design of PerfSense ‣ Identifying Performance-Sensitive
    Configurations in Software Systems through Code Analysis with LLM Agents"). Based
    on this context, PerfSense instructs PerfAgent to pinpoint unclear or ambiguous
    methods crucial for accurate performance analysis. Upon identifying the code that
    needs further analysis, PerfAgent requests DevAgent to retrieve and analyze it.
    By retrieving and clarifying the code when needed, PerfAgent can explore configuration-related
    code information, ensuring that all necessary code information is retrieved while
    minimizing the tokens and not exceeding the size limitation.'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="S4.F4.pic1" class="ltx_picture ltx_centering" height="1458.24" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,1458.24) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 12.82 1433.12)"><foreignobject width="574.35"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">Prompt
    Template for Code Understanding</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 12.82 8.89)"><foreignobject width="574.35" height="1406.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Role: You are a performance
    expert. Your job is to analyze the performance of the configuration. Check whether
    the provided configuration-related code is sufficient for performance analysis.
    Configuration-related code: 
    1private  static  AutoSavingCache  initKeyCache()){ 2  ... 3  long  keyCacheInMemoryCapacity  =  DatabaseDescriptor.getKeyCacheSizeInMB()  *  1024  *  1024;
    4  kc  =  CaffeineCache.create(keyCacheInMemoryCapacity); 5  ... 6} Code Context:
    Responses received from DevAgent. Requirement: If you need further code context
    to help understand the code, return the name of method name.</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4. PerfAgent’s Prompt for Code Understanding.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Multi-Agent Communications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Based on our definition of DevAgent and PerfAgent, below we discuss how the
    agents collaborate together to classify performance-sensitive configurations.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1\. Prompt Chaining to Iteratively Build Code Understanding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One effective technique for enhancing the reliability and performance of LLMs
    is to use a prompting paradigm called prompt chaining. Prompt chaining refers
    to breaking a complex task into simpler subtasks, prompting the LLM with each
    subtask sequentially, and using its responses as inputs for subsequent prompts (promptingguide,
    [[n.d.]](#bib.bib37)). In our performance chaining analysis, our goal is to retrieve
    all the necessary code for PerfAgent to assess the configuration’s sensitivity
    to performance. To achieve this, PerfAgent iteratively instructs DevAgent to retrieve
    source code methods sequentially. The DevAgent fetches a single method based on
    PerfAgent’s requests (include the source code, DevAgent’s description of the code,
    and DevAgent’s performance-aware code review result) until a termination condition
    is met, indicating that PerfAgent has gathered sufficient code and no longer requires
    assistance from DevAgent. PerfSense includes a memory mechanism that saves the
    DevAgent feedback at the end of each iteration of source code retrieval. This
    saved feedback can then be used as a code example in the next iteration of prompt
    chaining, allowing PerfAgent to clarify unclear contexts and request additional
    source code methods if needed.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2\. Retrieval Augmented Generation for Performance Classifier
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Based on the result of prompt chaining in prior steps, PerfAgent sequentially
    builds a memory of the knowledge base, which allows PerfAgent to classify performance-sensitive
    configurations more accurately. More precisely, we use the prompt template in
    Figure [5](#S4.F5 "Figure 5 ‣ 4.2.2\. Retrieval Augmented Generation for Performance
    Classifier ‣ 4.2\. Multi-Agent Communications ‣ 4\. Design of PerfSense ‣ Identifying
    Performance-Sensitive Configurations in Software Systems through Code Analysis
    with LLM Agents"). Like prior templates, our RAG starts by personifying PerfAgent
    with the introduction, “You are a performance expert. Given feedback from DevAgent,
    your job is to perform performance analysis of configurations.” We then provide
    the retrieved context from DevAgent: (1) configuration-related code, (2) performance-aware
    code reviews, and (3) other code contexts to resolve clarity issues related to
    the configuration. Finally, we require PerfAgent to classify whether or not the
    configuration is performance-sensitive.'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="S4.F5.pic1" class="ltx_picture ltx_centering" height="244.04" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,244.04) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 12.82 205)"><foreignobject width="574.35" height="26.21"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">Prompt
    Template for Retrieval Augmented Generation for Performance-Sensitive Configuration
    Classifier</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0
    0.0 1.0 12.82 8.89)"><foreignobject width="574.35" height="178.34" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Role: You are a performance
    expert. Your job is to analyze the performance of the configuration. You can check
    the provided code if code is clear and enough for performance analysis of configuration.
    Background knowledge about performance: Background information about performance-sensitive
    configuration and performance operations. RAG information: Access the memory,
    which includes the following retrieved configuration information: 1\. Configuration-related
    code 2\. Code understanding 3\. Analysis result of unclear context Requirement:
    Classify the configuration as performance sensitive or insensitive.</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5. Prompt Template 3: Retrieval Augmented Generation for Performance
    Classifier'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Implementation and Experiment Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Environment. We use GPT 3.5 (version gpt-3.5-turbo-0125) as our underlying LLM
    due to its popularity and wide usage. We leverage the OpenAI APIs and the LangGraph
    library (langchain, [2023](#bib.bib22)) to implement the LLM agents for recursive
    code analysis and performance configuration classification. Temperature is a parameter
    in LLMs that ranges from 0 to 1\. A low temperature makes the results more deterministic,
    and a higher value makes the results more diverse. To ensure the generated outputs
    are more stable across runs, We set the temperature to 0.3, which is a relatively
    low value but it still allows some diversity in the output. We also repeat our
    experiments five times and report the average. Note that although we use GPT 3.5
    as the underlying LLM, our approach is general and can be replaced with other
    LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmark Datasets. Table [1](#S4.T1 "Table 1 ‣ 4.3\. Implementation and Experiment
    Settings ‣ 4\. Design of PerfSense ‣ Identifying Performance-Sensitive Configurations
    in Software Systems through Code Analysis with LLM Agents") presents the studied
    systems in our experiment. These seven systems are real-world open-source Java
    applications that cover various domains, ranging from databases to rendering engines.
    The systems have various configurations, some of which are related to performance.
    Previous work (Chen et al., [2023a](#bib.bib9)) conducted manual performance testing
    and provided the ground truth about the performance-sensitive configurations for
    these seven systems. We leverage the ground truth provided by Chen et al. (Chen
    et al., [2023a](#bib.bib9)) with some adjustments based on manually examining
    the source code and official documents. The replication package is available online (SensitiveTeeth,
    [2024](#bib.bib41)).
  prefs: []
  type: TYPE_NORMAL
- en: Table 1. An overview of the systems, versions, the number of configurations,
    and the number of performance-sensitive configurations that we studied.
  prefs: []
  type: TYPE_NORMAL
- en: '| System | Domain | Version | Config. | Perf. |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  | Config. |'
  prefs: []
  type: TYPE_TB
- en: '| Cassandra | NoSQL Database | 4.0.5 | 133 | 76 |'
  prefs: []
  type: TYPE_TB
- en: '| DConverter | image Density Converter | bdf1535 | 23 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| Prevayler | Database | 2.6 | 12 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| BATIK | SVG rasterizer | 1.14 | 21 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| Catena | Password hashing | 1281e4b | 12 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| Sunflow | Rendering engine | 0.07.2 | 6 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| H2 | Database | 2.1.210 | 20 | 11 |'
  prefs: []
  type: TYPE_TB
- en: 5\. Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we evaluate PerfSense by answering three research questions
    (RQs).
  prefs: []
  type: TYPE_NORMAL
- en: 'RQ1: How effective is PerfSense in identifying performance-sensitive configurations?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this RQ, we evaluate the classification result of PerfSense in identifying
    the performance-sensitive configuration. We compare PerfSense with two baselines:
    DiagConfig and ChatGPT. DiagConfig (Chen et al., [2023a](#bib.bib9)) utilized
    the taint static analysis on several systems to extract the performance-related
    operations related to configurations. Through manual performance tests by altering
    the configuration values and evaluating the variation of throughput, the performance-sensitive
    configurations would be identified and labeled. Utilizing the labeled configurations
    and taint static analysis of configuration, DiagConfig is trained using a random
    forest model to classify performance-sensitive configurations. ChatGPT directly
    calls ChatGPT APIs to classify if a configuration is performance-sensitive. We
    provide the system name, the configuration name, and the definition of a performance-sensitive
    configuration to ChatGPT (the same version as PerfSense) for classification. It
    is important to note that for PerfSense, the system name is not provided to reduce
    potential data leakage issues (Sallou et al., [2024](#bib.bib39)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The classification result of PerfSense is assessed using three accuracy metrics:
    accuracy, precision, and recall. Specifically, we focus on the precision and recall
    of classifying the performance-sensitive configurations. True Positives (TP) happen
    when a performance-sensitive configuration is correctly classified. True Negatives
    (TN) happens when a non-performance-sensitive configuration is correctly classified
    as not performance-sensitive. False Positives (FP) happen when PerfSense incorrectly
    classifies a non-performance-sensitive configuration as performance-sensitive.
    False negatives (FN) happen when PerfSense misclassifies a performance-sensitive
    configuration as non-performance-sensitive. Given the TP, FP, and FN, we calculate
    precision as $\frac{TP}{TP+FP}$.'
  prefs: []
  type: TYPE_NORMAL
- en: Because of the generative nature of LLMs, the output may vary in each execution.
    Hence, we repeat each experiment five times and report the average precision,
    recall, and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2. The accuracy, precision, and recall of PerfSense and the baselines
    in classifying performance-sensitive configurations. Note that DiagConfig’s results
    are unavailable for all the systems, except Cassandra, because DiagConfig trained
    a classifier using other systems and applied it on Cassandra.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | PerfSense | ChatGPT | DiaConfig |'
  prefs: []
  type: TYPE_TB
- en: '|  | Accuracy | Precision | Recall | Accuracy | precision | recall | Accuracy
    | precision | recall |'
  prefs: []
  type: TYPE_TB
- en: '| Cassandra | 64.01% | 64.46% | 82.32% | 56.99% | 57.08% | 99.74% | 61.75%
    | 87.88% | 38.26% |'
  prefs: []
  type: TYPE_TB
- en: '| DConverter | 66.09% | 39.06% | 100.00% | 26.96% | 20.79% | 84.00% | – | –
    | – |'
  prefs: []
  type: TYPE_TB
- en: '| Prevayler | 75.00% | 75.51% | 95.50% | 66.70% | 66.70% | 100.00% | – | –
    | – |'
  prefs: []
  type: TYPE_TB
- en: '| BATIK | 72.38% | 63.41% | 65.00% | 34.29% | 35.35% | 87.50% | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| Catena | 46.67% | 48.15% | 86.67% | 50.00% | 50.00% | 83.00% | – | – | –
    |'
  prefs: []
  type: TYPE_TB
- en: '| Sunflow | 53.30% | 61.54% | 80.00% | 66.70% | 66.70% | 100.00% | – | – |
    – |'
  prefs: []
  type: TYPE_TB
- en: '| H2 | 76% | 78.18% | 78.18% | 50.91% | 50.46% | 100.00% | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| Average | 64.77% | 61.47% | 83.95% | 50.36% | 49.58% | 93.46% | – | – | –
    |'
  prefs: []
  type: TYPE_TB
- en: 'Results. PerfSense achieves a better accuracy (64.77% on average) compared
    to ChatGPT and DiagConfig (50.36% and 61.75%, respectively). Table [2](#S5.T2
    "Table 2 ‣ RQ1: How effective is PerfSense in identifying performance-sensitive
    configurations? ‣ 5\. Evaluation ‣ Identifying Performance-Sensitive Configurations
    in Software Systems through Code Analysis with LLM Agents") shows the classification
    result of PerfSense and the baselines. We find that the PerfSense provides a better
    balance of precision and recall, achieving better accuracy than the two baselines.
    ChatGPT achieves a higher recall (93.46%) than both PerfSense (83.95%) and DiagConfig
    (38.26%) but with a much lower precision (49.58% v.s. 61.47% and 87.88%). We find
    that the reason for a high recall and low precision is that ChatGPT misclassifies
    most configurations as performance-sensitive. In systems with less performance-sensitive
    configurations, ChatGPT achieves much worse results. For example, in DConverter,
    since 80% of the configurations are not performance-sensitive, ChatGPT achieves
    only a 27% accuracy rate. In contrast, the agents and prompting techniques implemented
    in PerfSense help improve the balance between precision and recall, resulting
    in much higher accuracy. We find that DiagConfig achieves a relatively high precision
    of 87.88% in Cassandra (it uses a classification model trained using data from
    all other systems, so the results are only available for Cassandra). However,
    DiagConfig has a very low recall (38.26% compared to PerfSense’s 82.32%) because
    DiagConfig misses many configurations where there were issues with obtaining the
    taint analysis result.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that, in theory, we can adjust PerfSense’s precision/recall by asking LLMs
    to estimate the performance impact (e.g., severe, medium, or low) and only classify
    the ones with a severe impact as performance sensitive to improve precision. In
    our pilot study, we achieve a much higher precision of 72.41% but a lower recall
    of 27.63%. PerfSense only classifies the configurations with severe performance
    impact as the performance-sensitive configurations. However, in this work, we
    aim to achieve higher recall and maintain good precision because the goal of PerfSense
    is to provide an initial list of configurations for performance engineers efficiently
    for further investigation.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to DiagConfig, PerfSense requires less running time and no manual effort.
    DiagConfig requires a taint analysis to identify all the code that is reachable
    from the configuration, which may require tens of hours of computation time for
    large systems like Cassandra. Moreover, DiagConfig built a random forest classification
    model by manually collecting test results from other systems. This manual-intensive
    process may need to be repeated if we want to apply the model to systems in other
    domains or if they are developed by a different development practice. In contrast,
    PerfSense’s running time is less than 50 minutes for Cassandra (the largest studied
    system with over 130 configuration parameters) and can be easily extended to any
    system because of its zero-shot and unsupervised nature.
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S5.SSx1.p7.pic1" class="ltx_picture" height="63.28" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,63.28) matrix(1 0 0 -1 0 0)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 12.82 8.89)"><foreignobject width="574.35" height="45.51" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Answers to RQ1. PerfSense provides
    a better balance of precision and recall, achieving the highest accuracy compared
    to the baselines. PerfSense is also lightweight and requires less than one hour
    to run for the largest studied system.</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'RQ2: How do different components in PerfSense affect the classification result?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'PerfSense contains various components, including the retrieval augmented generation
    (e.g., retrieving related code to help make classification decisions) and chain-of-thought
    (e.g., asking agents to generate a code summary and combine the generated summary
    with subsequent tasks). In this RQ, we aim to study the impact of each component.
    We remove each component separately, re-execute PerfSense, and re-evaluate the
    classification accuracy. In particular, we consider five combinations: 1) code:
    retrieve only the source code method that directly uses the configuration value;
    2) code + analysis: expands code by enabling the DevAgent to iteratively traverse
    the code to analyze the methods that the agent believes is relevant (i.e., through
    prompt chaining); 3) dev: the DevAgent generates a summary and description of
    the retrieved code; 4) code + dev: expands code by asking the DevAgent to provide
    a summary and description of the retrieved code for subsequent prompts (i.e.,
    chain-of-thought); and 5) code + dev + analysis: the full version of PerfSense.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3. Classification results of PerfSense with different components. The
    best precision and recall values in each system are marked in bold. The numbers
    in the parentheses show the percentage difference compared to the full version
    of PerfSense (code+dev+analysis).
  prefs: []
  type: TYPE_NORMAL
- en: '| System | Approach | Precision | Recall |'
  prefs: []
  type: TYPE_TB
- en: '| Cassandra | PerfSense${}_{\text{code+dev+analysis}}$ | 64.46% | 82.32% |'
  prefs: []
  type: TYPE_TB
- en: '| PerfSense${}_{\text{code}}$ | 67.55% (+4.79%) | 73.42% (-10.81%) |'
  prefs: []
  type: TYPE_TB
- en: '| PerfSense${}_{\text{code+analysis}}$ | 61.81% (-4.11%) | 77.11% (-6.32%)
    |'
  prefs: []
  type: TYPE_TB
- en: '| PerfSense${}_{\text{dev}}$ | 62.32% (-3.31%) | 74.47% (-9.53%)% |'
  prefs: []
  type: TYPE_TB
- en: '| PerfSense${}_{\text{code+dev}}$ | 64.12% (-0.53%) % | 73.09% (-11.23%) |'
  prefs: []
  type: TYPE_TB
- en: '| Dconverter | PerfSense${}_{\text{code+dev+analysis}}$ | 39.06% | 100.00%
    |'
  prefs: []
  type: TYPE_TB
- en: '| PerfSense${}_{\text{code}}$ | 30.49% (-21.94%) | 100.00% (-0.00%) |'
  prefs: []
  type: TYPE_TB
- en: '| PerfSense${}_{\text{code+analysis}}$ | 29.76% (-23.81%) | 100.00% (-0.00%)
    |'
  prefs: []
  type: TYPE_TB
- en: '| PerfSense${}_{\text{dev}}$ | 32.20% (-17.56%) | 100.00% (-0.00%) |'
  prefs: []
  type: TYPE_TB
- en: '| PerfSense${}_{\text{code+dev}}$ | 36.76% (-5.89%) | 100.00% (-0.00%) |'
  prefs: []
  type: TYPE_TB
- en: '| Prevayler | PerfSense${}_{\text{code+dev+analysis}}$ | 75.51% | 92.50% |'
  prefs: []
  type: TYPE_TB
- en: '| PerfSense${}_{\text{code}}$ | 80.56% (+6.70%) | 72.50% (-21.62%) |'
  prefs: []
  type: TYPE_TB
- en: '| PerfSense${}_{\text{code+analysis}}$ | 74.47% (-1.38%) | 87.50% (-5.41%)
    |'
  prefs: []
  type: TYPE_TB
- en: '| PerfSense${}_{\text{dev}}$ | 70.83% (-6.20%) | 85.00% (-8.11%) |'
  prefs: []
  type: TYPE_TB
- en: '| PerfSense${}_{\text{code+dev}}$ | 70.45% (-6.70%) | 79.49% (-14.06%) |'
  prefs: []
  type: TYPE_TB
- en: '| BATIK | PerfSense${}_{\text{code+dev+analysis}}$ | 63.41% | 65.00% |'
  prefs: []
  type: TYPE_TB
- en: '| PerfSense${}_{\text{code}}$ | 42.86% (-32.40%) | 45.00% (-30.76%) |'
  prefs: []
  type: TYPE_TB
- en: '| PerfSense${}_{\text{code+analysis}}$ | 47.92% (-24.42%) | 57.50% (-11.53%)
    |'
  prefs: []
  type: TYPE_TB
- en: '| PerfSense${}_{\text{dev}}$ | 59.09% (-6.86%) | 32.50% (-50.00%) |'
  prefs: []
  type: TYPE_TB
- en: '| PerfSense${}_{\text{code+dev}}$ | 51.28 (-19.12%) | 50.00% (-23.07%) |'
  prefs: []
  type: TYPE_TB
- en: '| Catena | PerfSense${}_{\text{code+dev+analysis}}$ | 48.15% | 86.67% |'
  prefs: []
  type: TYPE_TB
- en: '| PerfSense${}_{\text{code}}$ | 51.43% (+6.81%) | 60.0% (-30.77%) |'
  prefs: []
  type: TYPE_TB
- en: '| PerfSense${}_{\text{code+analysis}}$ | 48.89% (+1.53%) | 73.33% (-15.39%)
    |'
  prefs: []
  type: TYPE_TB
- en: '| PerfSense${}_{\text{dev}}$ | 48.15% (-0.00%) | 86.67% (-0.00%) |'
  prefs: []
  type: TYPE_TB
- en: '| PerfSense${}_{\text{code+dev}}$ | 50.00% (+3.84%) | 76.67% (-11.53%) |'
  prefs: []
  type: TYPE_TB
- en: '| Sunflow | PerfSense${}_{\text{code+dev+analysis}}$ | 61.54% | 80.00 % |'
  prefs: []
  type: TYPE_TB
- en: '| PerfSense${}_{\text{code}}$ | 83.00% (+34.87%) | 50.00% (-37.50%) |'
  prefs: []
  type: TYPE_TB
- en: '| PerfSense${}_{\text{code+analysis}}$ | 65.22% (+6.00%) | 75.00% (-6.25%)
    |'
  prefs: []
  type: TYPE_TB
- en: '| PerfSense${}_{\text{dev}}$ | 68.00% (+10.49%) | 100% (+25.00%) |'
  prefs: []
  type: TYPE_TB
- en: '| PerfSense${}_{\text{code+dev}}$ | 65.38% (+6.23%) | 89.47% (+11.83%) |'
  prefs: []
  type: TYPE_TB
- en: '| H2 | PerfSense${}_{\text{code+dev+analysis}}$ | 78.18% | 78.18% |'
  prefs: []
  type: TYPE_TB
- en: '| PerfSense${}_{\text{code}}$ | 58.70% (-24.91%) | 50.00% (-36.04%) |'
  prefs: []
  type: TYPE_TB
- en: '| PerfSense${}_{\text{code+analysis}}$ | 66.04% (-15.52%) | 63.64% (-18.59%)
    |'
  prefs: []
  type: TYPE_TB
- en: '| PerfSense${}_{\text{dev}}$ | 66.07% (-15.48%) | 67.27% (-13.95%) |'
  prefs: []
  type: TYPE_TB
- en: '| PerfSense${}_{\text{code+dev}}$ | 80.00% (+2.33%) | 72.73% (-6.97%) |'
  prefs: []
  type: TYPE_TB
- en: 'Results. Integrating code results in the highest precision in 4/7 studied systems,
    with the sacrifice of recall. Further adding dev improves recall significantly
    across most systems, achieving a more balanced trade-off between precision and
    recall while maintaining reasonably high precision. Table [3](#S5.T3 "Table 3
    ‣ RQ2: How do different components in PerfSense affect the classification result?
    ‣ 5\. Evaluation ‣ Identifying Performance-Sensitive Configurations in Software
    Systems through Code Analysis with LLM Agents") shows the precision and recall
    of the combinations of the components in PerfSense. We find that a simple RAG
    approach by retrieving only the method that directly uses the configuration parameter
    (PerfSense ${}_{\text{code}}$ achieves a precision of 67.55%, which is 4.79% higher
    than the full version, but the recall drops significantly by 10.81% to 73.42%.'
  prefs: []
  type: TYPE_NORMAL
- en: While code alone can achieve high precision, it often sacrifices recall significantly.
    On the other hand, code+dev tends to provide a better balance between precision
    and recall, with generally higher recall rates and more stable precision. This
    suggests that incorporating LLM-generated summaries and descriptions helps to
    enhance the overall performance of the tool by maintaining a more comprehensive
    approach. In comparison, integrating the LLM-generated summary/description of
    a given piece of code in the prompt (code v.s. code+dev) tends to provide a better
    balance between precision and recall, with generally higher recall rates and more
    stable precision. This suggests that incorporating LLM-generated summaries/descriptions
    helps to enhance the overall performance of PerfSense by maintaining a more comprehensive
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: Adding analysis to code further improves recall significantly across most systems
    while maintaining similar precision. For example, in Prevayler, the precision
    of code+analysis (74.47%) is slightly lower than code (80.56%), but the recall
    increases from 72.56% to 87.50%. Similarly, in BATIK, while code+analysis achieves
    a precision of 47.92% compared to 42.86% for code, the recall improves significantly
    from 45.00% to 57.50%. This finding suggests that the additional context and understanding
    provided by the analysis help PerfSense identify more relevant methods, thereby
    improving recall and maintaining precision.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating code+dev+analysis offers a holistic approach that leverages the
    strengths of individual components—code, developer insights, and analytical context—to
    achieve the best balanced performance. For instance, in the case of Cassandra,
    code+dev+analysis achieves a precision of 64.46% and a recall of 82.32%. The full
    version of PerfSense surpasses the recall of both code (73.42%) and code+dev (73.09%),
    while maintaining a competitive precision. Our findings show that each component
    has its own benefits to the result, and the integrated components enhance the
    overall effectiveness and reliability of PerfSense, providing a robust solution
    for identifying relevant methods and classifying performance-sensitive configurations
    within codebases.
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S5.SSx2.p6.pic1" class="ltx_picture" height="77.2" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,77.2) matrix(1 0 0 -1 0 0)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 12.82 8.89)"><foreignobject width="574.35" height="59.42" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Answers to RQ2. Adding dev
    improves recall with a balanced trade-off in precision, and incorporating analysis
    further enhances recall while maintaining competitive precision. The combined
    code+dev+analysis approach effectively leverages each component’s strengths for
    comprehensive method identification.</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'RQ3: What are the reasons for PerfSense’s misclassification?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Despite PerfSense achieving the highest accuracy and maintaining a balance between
    precision and recall, there remain instances of misclassification. Understanding
    the underlying causes of these misclassifications is crucial for understanding
    the limitations of PerfSense and providing insights for future performance analysis
    utilizing LLMs. Hence, in this RQ, we conduct a detailed manual study on the reasons
    for misclassification.
  prefs: []
  type: TYPE_NORMAL
- en: We collected and examined 362 configurations incorrectly classified by PerfSense.
    To systematically analyze the reasons for misclassifications by PerfSense, we
    began by selecting a 20% random sample from our dataset of 362 misclassified configurations.
    This subset was thoroughly reviewed to identify and categorize the various reasons
    for misclassification. In particular, we studied the communication history among
    the agents, the source code, and all related documents that we could find. With
    these categories established, we then manually examined the remaining 80% of the
    dataset, applying the derived categories to each configuration to understand the
    distribution of the different reasons for misclassification. We did not find any
    new categories during the process.
  prefs: []
  type: TYPE_NORMAL
- en: Table 4. Reasons and prevalence for the misclassification of performance sensitivity
    by PerfSense.
  prefs: []
  type: TYPE_NORMAL
- en: '| Reason | Description | Percentage |'
  prefs: []
  type: TYPE_TB
- en: '| No clear evidence of performance sensitivity | Through an examination of
    the related code and a careful review of available information (e.g., code and
    documentation), there is no clear evidence to indicate the performance sensitivity
    of the configuration. | 54.1% |'
  prefs: []
  type: TYPE_TB
- en: '| Misunderstood requirements | LLMs misunderstand the requirements for classification
    of performance-sensitive. | 26.8% |'
  prefs: []
  type: TYPE_TB
- en: '| Incorrect interpretation on the impact of performance-related operation |
    LLMs incorrectly interpreted the impact of performance-related operations. This
    misinterpretation led to the misclassification of the performance sensitivity
    of the configurations. | 10.0% |'
  prefs: []
  type: TYPE_TB
- en: '| Influenced by performance-related keywords | When LLMs classifies performance-sensitive
    configurations, keywords like “memory” and “scalability” can lead to misclassifications.
    These keywords are inherently associated with performance-related aspects, which
    may cause performance-insensitive configurations to be incorrectly identified
    as performance-sensitive. | 8.0% |'
  prefs: []
  type: TYPE_TB
- en: '| Hallucination | LLMs generate information that is not based on actual facts
    or truths. | 1.1% |'
  prefs: []
  type: TYPE_TB
- en: 'Results Table [4](#S5.T4 "Table 4 ‣ RQ3: What are the reasons for PerfSense’s
    misclassification? ‣ 5\. Evaluation ‣ Identifying Performance-Sensitive Configurations
    in Software Systems through Code Analysis with LLM Agents") shows the reasons
    for the misclassification of performance-sensitivity by PerfSense and their percentage.
    In total, we uncovered five reasons that cause the misclassification.'
  prefs: []
  type: TYPE_NORMAL
- en: Most misclassifications (54.1%) occur in configurations without clear evidence
    to support the performance sensitivity. For the configuration where the classification
    results by PerfSense differ from the ground truth (Chen et al., [2023a](#bib.bib9)),
    although we conducted a thorough examination of the related code and a careful
    review of available information (e.g., documentation and source code), there is
    no substantive evidence supporting the performance sensitivity of this configuration.
    For example, in Cassandra, the configuration column_index_cache_size_in_kb is
    not performance-sensitive (Chen et al., [2023a](#bib.bib9)). However, the LLM
    agents responded that this configuration can impact the amount of memory used
    for holding index entries in memory, which can cause performance variations. Setting
    a higher value may improve performance by reducing disk reads for index entries
    while setting a lower value may result in more disk reads and potentially slower
    performance. Based on reading the source code, we believe the explanation of PerfSense’s
    decision is valid, but there is no available evidence to support the performance
    sensitivity of the configuration. This misinterpretation can lead to inaccurate
    classification of performance sensitivity, highlighting the need for providing
    better requirements to LLMs for code understanding.
  prefs: []
  type: TYPE_NORMAL
- en: PerfSense may misunderstand requirements on performance sensitivities and classify
    other aspects as performance sensitive (26.8%). PerfSense may do some interpolation
    and infer that some non-performance-sensitive operations as performance sensitive.
    For example, the configuration _prevalenceDirectory in Prevayler specifies the
    directory used for reading and writing files. The configuration is related to
    file storage and is not performance-sensitive. However, PerfSense incorrectly
    assumes that the location of these read-write operations impacts system performance,
    whereas the configuration primarily pertains to system storage rather than performance.
  prefs: []
  type: TYPE_NORMAL
- en: 10.0% of the misclassifications are due to incorrect interpretations of performance
    impact. In some instances, PerfSense inaccurately assesses the performance impact
    of specific configurations that do not inherently influence system efficiency.
    For example, the configuration BACKGROUND_COLOR in Batik, which sets the background
    color, is performance-sensitive. The reason is different color settings can have
    an impact on the performance of graph rendering. However, PerfSense incorrectly
    classifies the code related to the configuration of the color as performance-insensitive.
  prefs: []
  type: TYPE_NORMAL
- en: 8.0% misclassification is related to having performance-related keywords for
    a performance non-sensitive configuration. LLMs are trained using natural language
    texts so they are sensitive to keywords in the prompts. If there are performance-related
    keywords in the prompt, PerfSense is more likely to classify a configuration as
    performance-sensitive. For example, the configuration gc_log_threshold_in_ms’
    is not performance-sensitive in the Cassandra project based on the ground truth (Chen
    et al., [2023a](#bib.bib9)). Enabling or disabling the configuration does not
    affect the system execution time. However, the keyword “gc” (garbage collection)
    is often considered to be performance-related in many situations, PerfSense incorrectly
    classified the configuration as performance-sensitive. However, the configuration
    is related to logging during gc, and setting a lower/higher threshold does not
    have a noticeable impact on the performance.
  prefs: []
  type: TYPE_NORMAL
- en: Hallucination is rare but can still cause misclassification (1.1%). Hallucination
    in LLMs can lead to incorrect or misleading results (Liu et al., [2024a](#bib.bib29);
    Li et al., [2024a](#bib.bib24)). For example, the configuration hinted_handoff_enabled
    in Cassandra is considered performance-sensitive (Chen et al., [2023a](#bib.bib9)).
    This configuration is to allow Cassandra to “continue performing the same number
    of writes even when the cluster is operating at reduced capacity”. However, due
    to hallucination, PerfSense erroneously states that this configuration is related
    to the name of applications, causing misclassification of the configuration.
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S5.SSx3.p9.pic1" class="ltx_picture" height="96.49" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,96.49) matrix(1 0 0 -1 0 0)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 12.82 8.89)"><foreignobject width="574.35" height="78.72" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Answers to RQ3. PerfSense’s
    misclassifications of performance-sensitive configurations are primarily due to
    a lack of clear evidence supporting performance sensitivity (54.1%) and misunderstanding
    of requirements leading to incorrect classifications (26.8%). Addressing these
    issues may require better requirements specification and enhanced understanding
    by LLMs to improve classification accuracy.</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Better requirements for analyzing the performance sensitivity of configuration
    are needed. During the reason analysis of the misclassification of configurations,
    we find that misunderstanding the requirement on performance sensitivities affects
    the precision in identifying the performance-sensitive configurations by PerfSense.
    The specificity and clarity of prompts used to interact with these agents can
    influence their ability to accurately identify performance-sensitive configurations.
    Incorporating more explicit performance-related criteria into the prompts can
    help reduce misclassifications by aligning the LLM’s analysis more closely with
    the actual performance impacts of the configurations. This adjustment could guide
    the LLM agents to distinguish between configurations that truly affect performance
    and those that do not, despite potentially misleading indicators such as performance-related
    keywords.
  prefs: []
  type: TYPE_NORMAL
- en: PerfSense efficiently narrows down the scope of investigation performance sensitivity
    of configurations. One of the strengths of PerfSense is its ability to efficiently
    narrow down the list of configurations that need deeper investigation. Performance
    sensitivity in software configurations is a complex domain where manual identification
    processes are time-consuming and prone to errors. By automating the identification
    process, PerfSense allows performance engineers and developers to focus their
    efforts and expertise on a refined subset of configurations, enhancing productivity
    and optimizing resource allocation.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt chaining and RAG technique enhance PerfSense understanding and analytical
    capabilities of the LLM on performance analysis. tool leverages advanced prompting
    techniques, such as prompt chaining and retrieval-augmented generation, to improve
    the interaction dynamics between the developer and performance expert agents.
    Prompt chaining breaks down complex tasks into simpler, sequential queries that
    build upon each other, which helps in constructing a comprehensive performance
    analysis for each performance-sensitive decision. RAG integrates the configuration-related
    information from external sources and reduces the context size, ensuring that
    the analysis from LLMs is both relevant and deeply informed for the performance
    assessment of configurations. The integration of various prompting techniques
    enhances PerfSense’s ability to accurately identify performance-sensitive configurations
    with balanced precision and recall, minimizing the risk of overlooking critical
    performance nuances in software behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Threats to Validity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 7.1\. Internal Validity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Due to the generative nature of LLM, the responses may change across runs. To
    mitigate the threat, we try to execute the LLMs five times and report the average
    for our evaluation. We set the temperature value to 0.3, which makes the result
    more consistent but still allows some diversity. We find that the results are
    similar across runs, which means the outputs are stable. However, future studies
    are needed to understand the impact of temperature on the results. Since LLMs
    are trained using open-source systems, there is the possibility of data leakage
    problems. To minimize the impact, we excluded system-specific information (e.g.,
    system and package names) when classifying configuration performance sensitivity
    to mitigate data(Sallou et al., [2024](#bib.bib39)).
  prefs: []
  type: TYPE_NORMAL
- en: 7.2\. External Validity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We conducted the study on open-source Java systems. Although we tried to choose
    matured and popular systems that are also used in prior studies, the results may
    not apply to systems implemented in other programming languages. Future research
    is needed to examine the results of other types of systems.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3\. Construct Validity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Classifying the performance sensitivity of a configuration parameter is a challenging
    task due to varying workloads (Vitui and Chen, [2021](#bib.bib51)). Hence, in
    this paper, we rely on the prior benchmark (Chen et al., [2023a](#bib.bib9)) and
    validate the result by an in-depth analysis and all the documents that we could
    find. To encourage replication and validation of our study, we made the dataset
    publicly online (SensitiveTeeth, [2024](#bib.bib41)).
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Configuration parameters are crucial for customizing software behavior, and
    some configurations can have a performance impact on systems. However, misconfigurations
    are common and can lead to significant performance degradation, making it essential
    to identify performance-sensitive configurations. In this paper, we introduced
    PerfSense, a novel framework leveraging LLM-based multi-agent systems to identify
    performance-sensitive configurations in software systems. By combining static
    code analysis and retrieval-augmented prompting techniques, PerfSense can identify
    performance-sensitive configurations with minimal manual work. Our evaluation
    of seven open-source systems demonstrated that PerfSense achieves a higher accuracy
    of 64.77% compared to existing the state-of-the-art method (61.75%). Furthermore,
    our evaluation of studying the effect of different prompting components revealed
    that the implementation of prompt chaining in PerfSense substantially enhances
    recall, with improvements ranging from 10% to 30%. To understand the limitations
    of PerfSense, we conduct a manual analysis of 362 misclassification configurations
    to analyze and summarize the reasons for the misclassification of performance
    sensitivity by PerfSense. LLM’s misunderstanding of requirements (26.8%) is the
    key reason for misclassification. Additionally, we provide a detailed discussion
    to offer insights for future research to enhance the robustness and accuracy of
    LLM-based configuration performance analysis.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AlOmar et al. (2024) Eman Abdullah AlOmar, Anushkrishna Venkatakrishnan, Mohamed Wiem
    Mkaouer, Christian D Newman, and Ali Ouni. 2024. How to Refactor this Code? An
    Exploratory Study on Developer-ChatGPT Refactoring Conversations. *arXiv preprint
    arXiv:2402.06013* (2024).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bao et al. (2018) Liang Bao, Xin Liu, Ziheng Xu, and Baoyin Fang. 2018. Autoconfig:
    Automatic configuration tuning for distributed message systems. In *Proceedings
    of the 33rd ACM/IEEE International Conference on Automated Software Engineering*.
    29–40.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bornholt and Torlak (2018) James Bornholt and Emina Torlak. 2018. Finding code
    that explodes under symbolic evaluation. *Proceedings of the ACM on Programming
    Languages* 2, OOPSLA (2018), 1–26.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chan et al. (2023) Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue,
    Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2023. Chateval: Towards better llm-based
    evaluators through multi-agent debate. *arXiv preprint arXiv:2308.07201* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2016) Tse-Hsun Chen, Weiyi Shang, Ahmed E. Hassan, Mohamed Nasser,
    and Parminder Flora. 2016. CacheOptimizer: helping developers configure caching
    frameworks for hibernate-based database-centric web applications. In *Proceedings
    of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software
    Engineering* (Seattle, WA, USA) *(FSE 2016)*. 666–677.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2014) Tse-Hsun Chen, Weiyi Shang, Zhen Ming Jiang, Ahmed E Hassan,
    Mohamed Nasser, and Parminder Flora. 2014. Detecting performance anti-patterns
    for applications developed using object-relational mapping. In *Proceedings of
    the 36th international conference on software engineering*. 1001–1012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2023b) Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei
    Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, et al. 2023b.
    Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors
    in agents. *arXiv preprint arXiv:2308.10848* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2023a) Zhiming Chen, Pengfei Chen, Peipei Wang, Guangba Yu, Zilong
    He, and Genting Mai. 2023a. DiagConfig: Configuration Diagnosis of Performance
    Violations in Configurable Software Systems. In *Proceedings of the 31st ACM Joint
    European Software Engineering Conference and Symposium on the Foundations of Software
    Engineering* *(ESEC/FSE 2023)*. 566–578.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: community ([n.d.]) LinkedIn community. [n.d.]. *What are the most common factors
    that slow down web application performance?* [https://www.linkedin.com/advice/0/what-most-common-factors-slow-down-web-application-lcsme](https://www.linkedin.com/advice/0/what-most-common-factors-slow-down-web-application-lcsme)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deshpande et al. (2023) Ameet Deshpande, Tanmay Rajpurohit, Karthik Narasimhan,
    and Ashwin Kalyan. 2023. Anthropomorphization of AI: opportunities and risks.
    *arXiv preprint arXiv:2305.14784* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2023) Yihong Dong, Xue Jiang, Zhi Jin, and Ge Li. 2023. Self-collaboration
    code generation via chatgpt. *arXiv preprint arXiv:2304.07590* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ej technologies ([n.d.]) ej technologies. [n.d.]. *Jprofiler*. [https://www.ej-technologies.com/products/jprofiler/overview.html](https://www.ej-technologies.com/products/jprofiler/overview.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ganapathi et al. (2004) Archana Ganapathi, Yi-Min Wang, Ni Lao, and Ji-Rong
    Wen. 2004. Why pcs are fragile and what we can do about it: A study of windows
    registry problems. In *International Conference on Dependable Systems and Networks,
    2004*. IEEE, 561–566.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gousios ([n.d.]) Georgios Gousios. [n.d.]. *java-callgraph: Java Call Graph
    Utilities*. [https://github.com/gousiosg/java-callgraph](https://github.com/gousiosg/java-callgraph)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2024) Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao
    Pei, Nitesh V Chawla, Olaf Wiest, and Xiangliang Zhang. 2024. Large language model
    based multi-agents: A survey of progress and challenges. *arXiv preprint arXiv:2402.01680*
    (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han and Yu (2016a) Xue Han and Tingting Yu. 2016a. An empirical study on performance
    bugs for highly configurable software systems. In *Proceedings of the 10th ACM/IEEE
    International Symposium on Empirical Software Engineering and Measurement*. 1–10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han and Yu (2016b) Xue Han and Tingting Yu. 2016b. An Empirical Study on Performance
    Bugs for Highly Configurable Software Systems. In *Proceedings of the 10th ACM/IEEE
    International Symposium on Empirical Software Engineering and Measurement* (Ciudad
    Real, Spain) *(ESEM ’16)*. Article 23, 10 pages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hong et al. (2023) Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin
    Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al.
    2023. Metagpt: Meta programming for multi-agent collaborative framework. *arXiv
    preprint arXiv:2308.00352* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2023) Dong Huang, Qingwen Bu, Jie M Zhang, Michael Luck, and
    Heming Cui. 2023. AgentCoder: Multi-Agent-based Code Generation with Iterative
    Testing and Optimisation. *arXiv preprint arXiv:2312.13010* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jin et al. (2012) Guoliang Jin, Linhai Song, Xiaoming Shi, Joel Scherpelz, and
    Shan Lu. 2012. Understanding and detecting real-world performance bugs. *ACM SIGPLAN
    Notices* 47, 6 (2012), 77–88.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: langchain (2023) langchain. 2023. LangGraph. [https://python.langchain.com/v0.1/docs/langgraph/](https://python.langchain.com/v0.1/docs/langgraph/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020) Chi Li, Shu Wang, Henry Hoffmann, and Shan Lu. 2020. Statically
    inferring performance properties of software configurations. In *Proceedings of
    the Fifteenth European Conference on Computer Systems*. 1–16.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2024a) Junyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Wayne Xin
    Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2024a. The Dawn After the Dark: An Empirical
    Study on Factuality Hallucination in Large Language Models. arXiv:cs.CL/2401.03205'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2024b) Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan,
    Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, et al. 2024b. Personal
    llm agents: Insights and survey about the capability, efficiency and security.
    *arXiv preprint arXiv:2401.05459* (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lian et al. (2024) Xinyu Lian, Yinfang Chen, Runxiang Cheng, Jie Huang, Parth
    Thakkar, Minjia Zhang, and Tianyin Xu. 2024. Configuration Validation with Large
    Language Models. arXiv:cs.SE/2310.09690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lillack et al. (2014) Max Lillack, Christian Kästner, and Eric Bodden. 2014.
    Tracking load-time configuration options. In *Proceedings of the 29th ACM/IEEE
    international conference on Automated software engineering*. 445–456.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2024) Feng Lin, Dong Jae Kim, et al. 2024. When llm-based code generation
    meets the software development process. *arXiv preprint arXiv:2403.15852* (2024).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2024a) Fang Liu, Yang Liu, Lin Shi, Houkun Huang, Ruifeng Wang,
    Zhen Yang, Li Zhang, Zhongqi Li, and Yuchi Ma. 2024a. Exploring and Evaluating
    Hallucinations in LLM-Powered Code Generation. arXiv:cs.SE/2404.00971
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2014) Yepang Liu, Chang Xu, and Shing-Chi Cheung. 2014. Characterizing
    and detecting performance bugs for smartphone applications. In *Proceedings of
    the 36th international conference on software engineering*. 1013–1024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2024b) Zhijie Liu, Yutian Tang, Meiyun Li, Xin Jin, Yunfei Long,
    Liang Feng Zhang, and Xiapu Luo. 2024b. LLM-CompDroid: Repairing Configuration
    Compatibility Bugs in Android Apps with Pre-trained Large Language Models. arXiv:cs.SE/2402.15078'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2024b) Lipeng Ma, Weidong Yang, Bo Xu, Sihang Jiang, Ben Fei, Jiaqing
    Liang, Mingjie Zhou, and Yanghua Xiao. 2024b. KnowLog: Knowledge Enhanced Pre-trained
    Language Model for Log Understanding. In *Proceedings of the 46th IEEE/ACM International
    Conference on Software Engineering*. 1–13.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2024a) Zeyang Ma, An Ran Chen, Dong Jae Kim, Tse-Hsun Chen, and
    Shaowei Wang. 2024a. LLMParser: An Exploratory Study on Using Large Language Models
    for Log Parsing. In *2024 IEEE/ACM 46th International Conference on Software Engineering
    (ICSE)*. IEEE Computer Society, 883–883.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Madaan et al. (2024) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    et al. 2024. Self-refine: Iterative refinement with self-feedback. *Advances in
    Neural Information Processing Systems* 36 (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nistor et al. (2015) Adrian Nistor, Po-Chun Chang, Cosmin Radoi, and Shan Lu.
    2015. Caramel: Detecting and fixing performance problems that have non-intrusive
    fixes. In 2015 IEEE/ACM 37th IEEE International Conference on Software Engineering.
    *IEEE, 902ś912* (2015).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. 2023. ChatGPT. [https://chat.openai.com/](https://chat.openai.com/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: promptingguide ([n.d.]) promptingguide. [n.d.]. *Prompt Chaining*. [https://www.promptingguide.ai/techniques/prompt_chaining](https://www.promptingguide.ai/techniques/prompt_chaining)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qian et al. (2023) Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su,
    Juyuan Xu, Zhiyuan Liu, and Maosong Sun. 2023. Communicative agents for software
    development. *arXiv preprint arXiv:2307.07924* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sallou et al. (2024) June Sallou, Thomas Durieux, and Annibale Panichella.
    2024. Breaking the Silence: the Threats of Using LLMs in Software Engineering.
    In *Proceedings of the 2024 ACM/IEEE 44th International Conference on Software
    Engineering: New Ideas and Emerging Results* (, Lisbon, Portugal,) *(ICSE-NIER’24)*.
    Association for Computing Machinery, New York, NY, USA, 102–106. [https://doi.org/10.1145/3639476.3639764](https://doi.org/10.1145/3639476.3639764)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schäfer et al. (2023) Max Schäfer, Sarah Nadi, Aryaz Eghbali, and Frank Tip.
    2023. An empirical evaluation of using large language models for automated unit
    test generation. *IEEE Transactions on Software Engineering* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SensitiveTeeth (2024) SensitiveTeeth 2024. Replication Package for SensitiveTeeth.
    [https://github.com/anonymous334455/SensitiveTeeth](https://github.com/anonymous334455/SensitiveTeeth).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shan et al. (2024) Shiwen Shan, Yintong Huo, Yuxin Su, Yichen Li, Dan Li, and
    Zibin Zheng. 2024. Face It Yourselves: An LLM-Based Two-Stage Strategy to Localize
    Configuration Errors via Logs. arXiv:cs.SE/2404.00640'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen et al. (2023) Chaochao Shen, Wenhua Yang, Minxue Pan, and Yu Zhou. 2023.
    Git Merge Conflict Resolution Leveraging Strategy Classification and LLM. In *2023
    IEEE 23rd International Conference on Software Quality, Reliability, and Security
    (QRS)*. IEEE, 228–239.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singh et al. (2016) Ravjot Singh, Cor-Paul Bezemer, Weiyi Shang, and Ahmed E
    Hassan. 2016. Optimizing the performance-related configurations of object-relational
    mapping frameworks using a multi-objective genetic algorithm. In *Proceedings
    of the 7th ACM/SPEC on International Conference on Performance Engineering*. 309–320.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tian et al. (2015) Xinhui Tian, Rui Han, Lei Wang, Gang Lu, and Jianfeng Zhan.
    2015. Latency critical big data computing in finance. *The Journal of Finance
    and Data Science* 1, 1 (2015), 33–41.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Velez et al. (2020) Miguel Velez, Pooyan Jamshidi, Florian Sattler, Norbert
    Siegmund, Sven Apel, and Christian Kästner. 2020. ConfigCrusher: towards white-box
    performance analysis for configurable systems. *Automated Software Engg.* 27,
    3–4 (dec 2020), 265–300. [https://doi.org/10.1007/s10515-020-00273-8](https://doi.org/10.1007/s10515-020-00273-8)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Velez et al. (2022a) Miguel Velez, Pooyan Jamshidi, Norbert Siegmund, Sven
    Apel, and Christian Kästner. 2022a. On debugging the performance of configurable
    software systems: Developer needs and tailored tool support. In *Proceedings of
    the 44th International Conference on Software Engineering*. 1571–1583.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Velez et al. (2022b) Miguel Velez, Pooyan Jamshidi, Norbert Siegmund, Sven
    Apel, and Christian Kästner. 2022b. On debugging the performance of configurable
    software systems: developer needs and tailored tool support. In *Proceedings of
    the 44th International Conference on Software Engineering* (Pittsburgh, Pennsylvania)
    *(ICSE ’22)*. 1571–1583.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: visualvm ([n.d.]) visualvm. [n.d.]. *VisualVM*. [https://visualvm.github.io/](https://visualvm.github.io/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vitui and Chen (2021) Arthur Vitui and Tse-Hsun Chen. 2021. MLASP: Machine
    learning assisted capacity planning: An industrial experience report. *Empirical
    Software Engineering* 26, 5 (2021), 87.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021) Zehao Wang, Haoxiang Zhang, Tse-Hsun Chen, and Shaowei Wang.
    2021. Would you like a quick peek? providing logging support to monitor data processing
    in big data applications. In *Proceedings of the 29th ACM Joint Meeting on European
    Software Engineering Conference and Symposium on the Foundations of Software Engineering*.
    516–526.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. (2023) Zhuokui Xie, Yinghao Chen, Chen Zhi, Shuiguang Deng, and
    Jianwei Yin. 2023. ChatUniTest: a ChatGPT-based automated unit test generation
    tool. *arXiv preprint arXiv:2305.04764* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2013) Tianyin Xu, Jiaqi Zhang, Peng Huang, Jing Zheng, Tianwei Sheng,
    Ding Yuan, Yuanyuan Zhou, and Shankar Pasupathy. 2013. Do not blame users for
    misconfigurations. In *Proceedings of the Twenty-Fourth ACM Symposium on Operating
    Systems Principles* (Farminton, Pennsylvania) *(SOSP ’13)*. New York, NY, USA,
    244–259.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ye et al. (2020) Wei Ye, Rui Xie, Jinglei Zhang, Tianxiang Hu, Xiaoyin Wang,
    and Shikun Zhang. 2020. Leveraging code generation to improve code retrieval and
    summarization via dual learning. In *Proceedings of The Web Conference 2020*.
    2309–2319.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yin et al. (2011) Zuoning Yin, Xiao Ma, Jing Zheng, Yuanyuan Zhou, Lakshmi N
    Bairavasundaram, and Shankar Pasupathy. 2011. An empirical study on configuration
    errors in commercial and open source systems. In *Proceedings of the Twenty-Third
    ACM Symposium on Operating Systems Principles*. 159–172.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yonkovit ([n.d.]) Matt Yonkovit. [n.d.]. *Netherlands ’will pay the price’ for
    blocking Turkish visit – Erdoğan*. [https://www.percona.com/blog/cost-not-properly-managing-databases/](https://www.percona.com/blog/cost-not-properly-managing-databases/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yuan et al. (2023) Zhiqiang Yuan, Yiling Lou, Mingwei Liu, Shiji Ding, Kaixin
    Wang, Yixuan Chen, and Xin Peng. 2023. No more manual tests? evaluating and improving
    ChatGPT for unit test generation. *arXiv preprint arXiv:2305.04207* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2015) Yi Zhang, Jianmei Guo, Eric Blais, and Krzysztof Czarnecki.
    2015. Performance prediction of configurable software systems by fourier learning
    (t). In *2015 30th IEEE/ACM International Conference on Automated Software Engineering
    (ASE)*. IEEE, 365–373.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2024) Yuxia Zhang, Zhiqing Qiu, Klaas-Jan Stol, Wenhui Zhu, Jiaxin
    Zhu, Yingchen Tian, and Hui Liu. 2024. Automatic Commit Message Generation: A
    Critical Review and Directions for Future Work. *IEEE Transactions on Software
    Engineering* (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2023) Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Xuan Long
    Do, Chengwei Qin, Bosheng Ding, Xiaobao Guo, Minzhi Li, Xingxuan Li, et al. 2023.
    Retrieving multimodal information for augmented generation: A survey. *arXiv preprint
    arXiv:2303.10868* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhong et al. (2024) Li Zhong, Zilong Wang, and Jingbo Shang. 2024. LDB: A Large
    Language Model Debugger via Verifying Runtime Execution Step-by-step. *arXiv preprint
    arXiv:2402.16906* (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
