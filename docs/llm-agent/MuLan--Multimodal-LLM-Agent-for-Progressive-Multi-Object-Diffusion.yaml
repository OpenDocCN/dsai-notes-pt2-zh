- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»åˆ«ï¼šæœªåˆ†ç±»
- en: 'date: 2024-09-08 18:51:55'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¥æœŸï¼š2024-09-08 18:51:55
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'MuLan: å¤šæ¨¡æ€-LLM ä»£ç†ç”¨äºŽæ¸è¿›å¼å¤šå¯¹è±¡æ‰©æ•£'
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2402.12741](https://ar5iv.labs.arxiv.org/html/2402.12741)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2402.12741](https://ar5iv.labs.arxiv.org/html/2402.12741)
- en: Sen Li â€ƒâ€ƒ Ruochen Wang â€ƒâ€ƒ Cho-Jui Hsieh â€ƒâ€ƒ Minhao Cheng â€ƒâ€ƒ Tianyi Zhou
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Sen Li â€ƒâ€ƒ Ruochen Wang â€ƒâ€ƒ Cho-Jui Hsieh â€ƒâ€ƒ Minhao Cheng â€ƒâ€ƒ Tianyi Zhou
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: Existing text-to-image models still struggle to generate images of multiple
    objects, especially in handling their spatial positions, relative sizes, overlapping,
    and attribute bindings. In this paper, we develop a training-free Multimodal-LLM
    agent (MuLan) to address these challenges by progressive multi-object generation
    with planning and feedback control, like a human painter. MuLan harnesses a large
    language model (LLM) to decompose a prompt to a sequence of sub-tasks, each generating
    only one object conditioned on previously generated objects by stable diffusion.
    Unlike existing LLM-grounded methods, MuLan only produces a high-level plan at
    the beginning while the exact size and location of each object are determined
    by an LLM and attention guidance upon each sub-task. Moreover, MuLan adopts a
    vision-language model (VLM) to provide feedback to the image generated in each
    sub-task and control the diffusion model to re-generate the image if it violates
    the original prompt. Hence, each model in every step of MuLan only needs to address
    an easy sub-task it is specialized for. We collect 200 prompts containing multi-objects
    with spatial relationships and attribute bindings from different benchmarks to
    evaluate MuLan. The results demonstrate the superiority of MuLan in generating
    multiple objects over baselines. The code is available on [https:github.com/measure-infinity/mulan-code](https:github.com/measure-infinity/mulan-code).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: çŽ°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡åž‹åœ¨ç”Ÿæˆå¤šä¸ªå¯¹è±¡çš„å›¾åƒæ—¶ä»ç„¶å­˜åœ¨å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å®ƒä»¬çš„ç©ºé—´ä½ç½®ã€ç›¸å¯¹å¤§å°ã€é‡å å’Œå±žæ€§ç»‘å®šæ–¹é¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ— è®­ç»ƒçš„å¤šæ¨¡æ€-LLM
    ä»£ç† (MuLan)ï¼Œé€šè¿‡æ¸è¿›å¼çš„å¤šå¯¹è±¡ç”Ÿæˆã€è§„åˆ’å’Œåé¦ˆæŽ§åˆ¶æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œç±»ä¼¼äºŽäººç±»ç”»å®¶ã€‚MuLan åˆ©ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹ (LLM) å°†æç¤ºåˆ†è§£ä¸ºä¸€ç³»åˆ—å­ä»»åŠ¡ï¼Œæ¯ä¸ªå­ä»»åŠ¡ä»…ç”Ÿæˆä¸€ä¸ªå¯¹è±¡ï¼Œå¹¶ä»¥ç¨³å®šæ‰©æ•£ç”Ÿæˆçš„å…ˆå‰å¯¹è±¡ä½œä¸ºæ¡ä»¶ã€‚ä¸ŽçŽ°æœ‰çš„
    LLM åŸºç¡€æ–¹æ³•ä¸åŒï¼ŒMuLan åªåœ¨å¼€å§‹æ—¶ç”Ÿæˆä¸€ä¸ªé«˜çº§è®¡åˆ’ï¼Œè€Œæ¯ä¸ªå¯¹è±¡çš„ç¡®åˆ‡å¤§å°å’Œä½ç½®ç”± LLM å’Œæ³¨æ„åŠ›å¼•å¯¼åœ¨æ¯ä¸ªå­ä»»åŠ¡ä¸­ç¡®å®šã€‚æ­¤å¤–ï¼ŒMuLan é‡‡ç”¨è§†è§‰-è¯­è¨€æ¨¡åž‹
    (VLM) å¯¹æ¯ä¸ªå­ä»»åŠ¡ç”Ÿæˆçš„å›¾åƒæä¾›åé¦ˆï¼Œå¹¶åœ¨å›¾åƒè¿ååŽŸå§‹æç¤ºæ—¶æŽ§åˆ¶æ‰©æ•£æ¨¡åž‹é‡æ–°ç”Ÿæˆå›¾åƒã€‚å› æ­¤ï¼ŒMuLan æ¯ä¸€æ­¥ä¸­çš„æ¯ä¸ªæ¨¡åž‹åªéœ€è¦è§£å†³å®ƒä¸“é—¨åŒ–çš„ç®€å•å­ä»»åŠ¡ã€‚æˆ‘ä»¬ä»Žä¸åŒçš„åŸºå‡†ä¸­æ”¶é›†äº†åŒ…å«ç©ºé—´å…³ç³»å’Œå±žæ€§ç»‘å®šçš„
    200 ä¸ªå¤šå¯¹è±¡æç¤ºï¼Œä»¥è¯„ä¼° MuLanã€‚ç»“æžœå±•ç¤ºäº† MuLan åœ¨ç”Ÿæˆå¤šä¸ªå¯¹è±¡æ–¹é¢ç›¸è¾ƒäºŽåŸºçº¿çš„ä¼˜è¶Šæ€§ã€‚ä»£ç å¯åœ¨ [https:github.com/measure-infinity/mulan-code](https:github.com/measure-infinity/mulan-code)
    èŽ·å–ã€‚
- en: Computer Vision, Machine Learning, ICML
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—æœºè§†è§‰ã€æœºå™¨å­¦ä¹ ã€ICML
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 ä»‹ç»
- en: 'Diffusion modelsÂ (Sohl-Dickstein etÂ al., [2015](#bib.bib19); Ho etÂ al., [2020](#bib.bib9);
    Song etÂ al., [2020](#bib.bib20)) have shown growing potential in generative AI
    tasks, especially in creating diverse and high-quality images with text promptsÂ (Saharia
    etÂ al., [2022](#bib.bib18); Rombach etÂ al., [2022](#bib.bib17)). However, current
    state-of-the-art text-to-image (T2I) models such as Stable DiffusionÂ (Rombach
    etÂ al., [2022](#bib.bib17)) and DALL-E 3Â (Betker etÂ al., [2023](#bib.bib2)) still
    struggle to deal with complicated prompts involving multiple objects and lack
    precise control of their spatial relations, potential occlusions, relative sizes,
    etc. As shown in FigureÂ [2](#S1.F2 "Figure 2 â€£ 1 Introduction â€£ MuLan: Multimodal-LLM
    Agent for Progressive Multi-Object Diffusion"), to generate a sketch of â€œThe orange
    pumpkin is on the right side of the black doorâ€, even the SOTA open-source T2I
    model, Stable Diffusion XLÂ (Podell etÂ al., [2023](#bib.bib16)), still generates
    wrong attribute-binding as well as incorrect spatial positions of several objects.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ‰©æ•£æ¨¡åž‹ (Sohl-Dickstein ç­‰, [2015](#bib.bib19); Ho ç­‰, [2020](#bib.bib9); Song ç­‰,
    [2020](#bib.bib20)) åœ¨ç”Ÿæˆæ€§ AI ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºè¶Šæ¥è¶Šå¤§çš„æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨æ–‡æœ¬æç¤ºç”Ÿæˆå¤šæ ·åŒ–å’Œé«˜è´¨é‡å›¾åƒæ–¹é¢ (Saharia ç­‰,
    [2022](#bib.bib18); Rombach ç­‰, [2022](#bib.bib17))ã€‚ç„¶è€Œï¼Œå½“å‰æœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒ (T2I) æ¨¡åž‹ï¼Œå¦‚ Stable
    Diffusion (Rombach ç­‰, [2022](#bib.bib17)) å’Œ DALL-E 3 (Betker ç­‰, [2023](#bib.bib2))ï¼Œä»ç„¶åœ¨å¤„ç†æ¶‰åŠå¤šä¸ªå¯¹è±¡çš„å¤æ‚æç¤ºæ—¶é‡åˆ°å›°éš¾ï¼Œå¹¶ä¸”å¯¹å…¶ç©ºé—´å…³ç³»ã€æ½œåœ¨é®æŒ¡ã€ç›¸å¯¹å°ºå¯¸ç­‰ç¼ºä¹ç²¾ç¡®æŽ§åˆ¶ã€‚å¦‚å›¾
    [2](#S1.F2 "Figure 2 â€£ 1 Introduction â€£ MuLan: Multimodal-LLM Agent for Progressive
    Multi-Object Diffusion") æ‰€ç¤ºï¼Œä¸ºäº†ç”Ÿæˆâ€œæ©™è‰²å—ç“œåœ¨é»‘è‰²é—¨çš„å³ä¾§â€çš„è‰å›¾ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„å¼€æº T2I æ¨¡åž‹ Stable Diffusion
    XL (Podell ç­‰, [2023](#bib.bib16))ï¼Œä»ç„¶ä¼šç”Ÿæˆé”™è¯¯çš„å±žæ€§ç»‘å®šä»¥åŠå¤šä¸ªå¯¹è±¡çš„ä½ç½®é”™è¯¯ã€‚'
- en: Among works that aim to improve the controllability of T2I models on complicated
    prompts, a recent promising line of research seeks to utilize large language models
    (LLMs), e.g., ChatGPT, GPT-4Â (Achiam etÂ al., [2023](#bib.bib1)), to guide the
    generation processÂ (Lian etÂ al., [2023](#bib.bib12); Feng etÂ al., [2023](#bib.bib6)).
    Specifically, an LLM is prompted to generate a layout for the given prompt, i.e.,
    a bounding box for each object in the image, given detailed instructions or demonstrations
    if necessary. However, due to the limited spatial reasoning capability of LLMs
    as well as their lack of alignment with the diffusion models, it is still challenging
    for LLMs to directly generate a complete and precise layout for multiple objects.
    Without a feedback loop interacting with the generative process, the layoutâ€™s
    possible mistakes cannot be effectively detected and corrected. Moreover, the
    layout is often applied as an extra condition in addition to the original prompt
    (e.g., bounding boxes combined with GLIGENÂ (Li etÂ al., [2023](#bib.bib11))), so
    the diffusion models may still generate an incorrect image due to its misunderstanding
    of the complicated prompt.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é‚£äº›æ—¨åœ¨æ”¹å–„T2Iæ¨¡åž‹å¯¹å¤æ‚æç¤ºçš„æŽ§åˆ¶èƒ½åŠ›çš„å·¥ä½œä¸­ï¼Œæœ€è¿‘ä¸€ä¸ªæœ‰å‰é€”çš„ç ”ç©¶æ–¹å‘æ˜¯åˆ©ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰ï¼Œä¾‹å¦‚ChatGPTã€GPT-4ï¼ˆAchiamç­‰ï¼Œ[2023](#bib.bib1)ï¼‰ï¼Œæ¥æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ï¼ˆLianç­‰ï¼Œ[2023](#bib.bib12)ï¼›Fengç­‰ï¼Œ[2023](#bib.bib6)ï¼‰ã€‚å…·ä½“è€Œè¨€ï¼ŒLLMè¢«æç¤ºç”Ÿæˆç»™å®šæç¤ºçš„å¸ƒå±€ï¼Œå³å›¾åƒä¸­æ¯ä¸ªå¯¹è±¡çš„è¾¹ç•Œæ¡†ï¼Œå¿…è¦æ—¶æä¾›è¯¦ç»†çš„æŒ‡ç¤ºæˆ–æ¼”ç¤ºã€‚ç„¶è€Œï¼Œç”±äºŽLLMçš„ç©ºé—´æŽ¨ç†èƒ½åŠ›æœ‰é™ï¼Œä»¥åŠå®ƒä»¬ä¸Žæ‰©æ•£æ¨¡åž‹çš„ä¸åŒ¹é…ï¼ŒLLMä»ç„¶éš¾ä»¥ç›´æŽ¥ç”Ÿæˆå¤šä¸ªå¯¹è±¡çš„å®Œæ•´ä¸”ç²¾ç¡®çš„å¸ƒå±€ã€‚æ²¡æœ‰ä¸Žç”Ÿæˆè¿‡ç¨‹äº¤äº’çš„åé¦ˆå¾ªçŽ¯ï¼Œå¸ƒå±€å¯èƒ½å‡ºçŽ°çš„é”™è¯¯æ— æ³•æœ‰æ•ˆæ£€æµ‹å’Œçº æ­£ã€‚æ­¤å¤–ï¼Œå¸ƒå±€é€šå¸¸ä½œä¸ºåŽŸå§‹æç¤ºçš„é™„åŠ æ¡ä»¶ï¼ˆä¾‹å¦‚ï¼Œå°†è¾¹ç•Œæ¡†ä¸ŽGLIGENï¼ˆLiç­‰ï¼Œ[2023](#bib.bib11)ï¼‰ç»“åˆï¼‰ï¼Œå› æ­¤ç”±äºŽå¯¹å¤æ‚æç¤ºçš„è¯¯è§£ï¼Œæ‰©æ•£æ¨¡åž‹ä»ç„¶å¯èƒ½ç”Ÿæˆä¸æ­£ç¡®çš„å›¾åƒã€‚
- en: '![Refer to caption](img/d7e07ee781b1b94e34befc2ca4f28179.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/d7e07ee781b1b94e34befc2ca4f28179.png)'
- en: 'Figure 1: The proposed training-free Multimodal-LLM Agent (MuLan) for Progressive
    Multi-Object Diffusion. MuLan consists of three main components: (1) LLM planning;
    (2) Single-object diffusion with attention guidance; and (3) VLM-feedback control.
    MuLan first decomposes a complicated prompt into a sequence of sub-prompts each
    for one object, and then generates one object per step conditioned on a sub-prompt
    and previously generated objects, where LLM plans the rough layout of the object
    and attention guidance provides an accurate mask for it. The VLM-feedback control
    allows MuLan to correct mistakes in each step by adjusting hyperparameters in
    (2).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1ï¼šæå‡ºçš„æ— è®­ç»ƒMultimodal-LLMä»£ç†ï¼ˆMuLanï¼‰ç”¨äºŽæ¸è¿›å¼å¤šå¯¹è±¡æ‰©æ•£ã€‚MuLanç”±ä¸‰ä¸ªä¸»è¦ç»„ä»¶ç»„æˆï¼šï¼ˆ1ï¼‰LLMè§„åˆ’ï¼›ï¼ˆ2ï¼‰å¸¦æ³¨æ„åŠ›æŒ‡å¯¼çš„å•å¯¹è±¡æ‰©æ•£ï¼›ï¼ˆ3ï¼‰VLMåé¦ˆæŽ§åˆ¶ã€‚MuLané¦–å…ˆå°†å¤æ‚æç¤ºåˆ†è§£ä¸ºæ¯ä¸ªå¯¹è±¡ä¸€ä¸ªå­æç¤ºçš„åºåˆ—ï¼Œç„¶åŽæ ¹æ®å­æç¤ºå’Œå…ˆå‰ç”Ÿæˆçš„å¯¹è±¡æ¯æ¬¡ç”Ÿæˆä¸€ä¸ªå¯¹è±¡ï¼Œå…¶ä¸­LLMè§„åˆ’å¯¹è±¡çš„å¤§è‡´å¸ƒå±€ï¼Œæ³¨æ„åŠ›æŒ‡å¯¼æä¾›å‡†ç¡®çš„æŽ©æ¨¡ã€‚VLMåé¦ˆæŽ§åˆ¶å…è®¸MuLané€šè¿‡è°ƒæ•´ï¼ˆ2ï¼‰ä¸­çš„è¶…å‚æ•°æ¥çº æ­£æ¯ä¸€æ­¥çš„é”™è¯¯ã€‚
- en: To address the limitations and challenges of previous methods, we develop a
    training-free and controllable T2I generation paradigm that does not require demonstrations
    but mainly focuses on improving the tool usage of existing models. Our paradigm
    is built upon a progressive multi-object generation by a Multimodal-LLM agent
    (MuLan), which generates only one object per stage, conditioned on generated objects
    in the image and attention masks of the most plausible positions to place the
    new object. Unlike previous methods that add conditions to each model and make
    the task even more challenging, MuLan uses an LLM as a planner decomposing the
    original T2I task into a sequence of easier subtasks. Each subtask generates one
    single object, which can be easily handled by diffusion models. To be noted, the
    LLM applied at the beginning of MuLan only focuses on high-level planning rather
    than a precise layout of bounding boxes, while the exact size and position of
    each object are determined later in each stage by LLM and attention guidance based
    on the generated objects in the image. Hence, we can avoid mistakes in the planning
    stage and find a better placement for each object adaptive to the generated content
    and adhering to the original prompt. In addition, MuLan builds a feedback loop
    monitoring the generation process, which assesses the generated image per stage
    using a vision-language model (VLM). When the generated image violates the prompt,
    the VLM will adjust the diffusion model to re-generate the image so any mistake
    can be corrected before moving to the next stage. Furthermore, we develop a strategy
    applied in each stage to handle the overlapping between objects, which is commonly
    ignored by previous workÂ (Lian etÂ al., [2023](#bib.bib12)).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è§£å†³å…ˆå‰æ–¹æ³•çš„å±€é™æ€§å’ŒæŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ— éœ€è®­ç»ƒä¸”å¯æŽ§çš„ T2I ç”ŸæˆèŒƒå¼ï¼Œè¯¥èŒƒå¼ä¸éœ€è¦æ¼”ç¤ºï¼Œè€Œä¸»è¦å…³æ³¨äºŽæé«˜çŽ°æœ‰æ¨¡åž‹çš„å·¥å…·ä½¿ç”¨æ•ˆæžœã€‚æˆ‘ä»¬çš„èŒƒå¼åŸºäºŽç”±å¤šæ¨¡æ€
    LLM ä»£ç† (MuLan) è¿›è¡Œçš„æ¸è¿›å¼å¤šç‰©ä½“ç”Ÿæˆï¼Œè¯¥ä»£ç†æ¯ä¸ªé˜¶æ®µåªç”Ÿæˆä¸€ä¸ªç‰©ä½“ï¼Œæ¡ä»¶æ˜¯å›¾åƒä¸­å·²ç”Ÿæˆç‰©ä½“å’Œæœ€æœ‰å¯èƒ½æ”¾ç½®æ–°ç‰©ä½“çš„ä½ç½®çš„æ³¨æ„åŠ›æŽ©ç ã€‚ä¸Žå…ˆå‰å°†æ¡ä»¶æ·»åŠ åˆ°æ¯ä¸ªæ¨¡åž‹å¹¶ä½¿ä»»åŠ¡æ›´åŠ å›°éš¾çš„æ–¹æ³•ä¸åŒï¼ŒMuLan
    ä½¿ç”¨ LLM ä½œä¸ºè§„åˆ’è€…ï¼Œå°†åŽŸå§‹ T2I ä»»åŠ¡åˆ†è§£ä¸ºä¸€ç³»åˆ—æ›´ç®€å•çš„å­ä»»åŠ¡ã€‚æ¯ä¸ªå­ä»»åŠ¡ç”Ÿæˆä¸€ä¸ªå•ç‹¬çš„ç‰©ä½“ï¼Œè¿™å¯ä»¥è¢«æ‰©æ•£æ¨¡åž‹è½»æ¾å¤„ç†ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒMuLan
    å¼€å§‹é˜¶æ®µåº”ç”¨çš„ LLM åªå…³æ³¨äºŽé«˜å±‚æ¬¡çš„è§„åˆ’ï¼Œè€Œä¸æ˜¯è¾¹ç•Œæ¡†çš„ç²¾ç¡®å¸ƒå±€ï¼Œè€Œæ¯ä¸ªç‰©ä½“çš„ç¡®åˆ‡å¤§å°å’Œä½ç½®åˆ™ç”± LLM å’ŒåŸºäºŽå›¾åƒä¸­ç”Ÿæˆç‰©ä½“çš„æ³¨æ„åŠ›æŒ‡å¯¼åœ¨æ¯ä¸ªé˜¶æ®µåŽç»­ç¡®å®šã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥é¿å…è§„åˆ’é˜¶æ®µçš„é”™è¯¯ï¼Œå¹¶æ‰¾åˆ°é€‚åº”ç”Ÿæˆå†…å®¹å¹¶ç¬¦åˆåŽŸå§‹æç¤ºçš„æ›´å¥½ç‰©ä½“æ”¾ç½®ã€‚æ­¤å¤–ï¼ŒMuLan
    å»ºç«‹äº†ä¸€ä¸ªåé¦ˆå¾ªçŽ¯æ¥ç›‘æŽ§ç”Ÿæˆè¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡åž‹ (VLM) è¯„ä¼°æ¯ä¸ªé˜¶æ®µç”Ÿæˆçš„å›¾åƒã€‚å½“ç”Ÿæˆçš„å›¾åƒè¿åæç¤ºæ—¶ï¼ŒVLM ä¼šè°ƒæ•´æ‰©æ•£æ¨¡åž‹ä»¥é‡æ–°ç”Ÿæˆå›¾åƒï¼Œä»¥ä¾¿åœ¨è¿›å…¥ä¸‹ä¸€é˜¶æ®µä¹‹å‰çº æ­£ä»»ä½•é”™è¯¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åº”ç”¨äºŽæ¯ä¸ªé˜¶æ®µçš„ç­–ç•¥ï¼Œä»¥å¤„ç†ç‰©ä½“ä¹‹é—´çš„é‡å ï¼Œè¿™æ˜¯å…ˆå‰å·¥ä½œä¸­å¸¸è¢«å¿½è§†çš„é—®é¢˜
    (Lian et al., [2023](#bib.bib12))ã€‚
- en: 'Therefore, MuLan obtains better controllability of the multi-object composition.
    An illustration of the progressive generation process is shown in FigureÂ [1](#S1.F1
    "Figure 1 â€£ 1 Introduction â€£ MuLan: Multimodal-LLM Agent for Progressive Multi-Object
    Diffusion"). To evaluate MuLan, we curate a dataset of intricate and challenging
    prompts from different benchmarks. To compare MuLan with existing approaches,
    we prompt GPT-4VÂ (OpenAI, [2023](#bib.bib15)) several questions based on the input
    texts to comprehensively evaluate the alignment of the generated images with the
    prompts from three aspects. We further conduct human evaluations of the generated
    images. Extensive experimental results show that MuLan can achieve better controllability
    over the generation process and generate high-quality images aligning better with
    the prompts than the baselines. Example images generated by different methods
    are shown in FigureÂ [2](#S1.F2 "Figure 2 â€£ 1 Introduction â€£ MuLan: Multimodal-LLM
    Agent for Progressive Multi-Object Diffusion"). Our main contributions are summarized
    as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 'å› æ­¤ï¼ŒMuLan å®žçŽ°äº†å¯¹å¤šç‰©ä½“ç»„åˆçš„æ›´å¥½æŽ§åˆ¶ã€‚æ¸è¿›ç”Ÿæˆè¿‡ç¨‹çš„ç¤ºæ„å›¾è§å›¾Â [1](#S1.F1 "Figure 1 â€£ 1 Introduction
    â€£ MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion")ã€‚ä¸ºäº†è¯„ä¼° MuLanï¼Œæˆ‘ä»¬ç­–åˆ’äº†ä¸€ä¸ªæ¥è‡ªä¸åŒåŸºå‡†çš„å¤æ‚ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„æç¤ºæ•°æ®é›†ã€‚ä¸ºäº†å°†
    MuLan ä¸ŽçŽ°æœ‰æ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬å‘ GPT-4V (OpenAI, [2023](#bib.bib15)) æå‡ºäº†å‡ ä¸ªåŸºäºŽè¾“å…¥æ–‡æœ¬çš„é—®é¢˜ï¼Œä»¥å…¨é¢è¯„ä¼°ç”Ÿæˆçš„å›¾åƒä¸Žæç¤ºçš„ä¸€è‡´æ€§ï¼Œä»Žä¸‰ä¸ªæ–¹é¢è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¿›è¡Œäº†ç”Ÿæˆå›¾åƒçš„äººç±»è¯„ä¼°ã€‚å¤§é‡å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒMuLan
    å¯ä»¥åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å®žçŽ°æ›´å¥½çš„æŽ§åˆ¶ï¼Œå¹¶ç”Ÿæˆä¸Žæç¤ºæ›´å¥½å¯¹é½çš„é«˜è´¨é‡å›¾åƒï¼Œä¼˜äºŽåŸºå‡†æ–¹æ³•ã€‚ä¸åŒæ–¹æ³•ç”Ÿæˆçš„ç¤ºä¾‹å›¾åƒè§å›¾Â [2](#S1.F2 "Figure 2 â€£ 1
    Introduction â€£ MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion")ã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®æ€»ç»“å¦‚ä¸‹ï¼š'
- en: '![Refer to caption](img/4277fec584bbedd7ba278d69e51fb0a9.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜Ž](img/4277fec584bbedd7ba278d69e51fb0a9.png)'
- en: 'Figure 2: Examples of MuLan-generated images, compared to the original SD-v1.4Â (Rombach
    etÂ al., [2022](#bib.bib17)), the original SDXLÂ (Podell etÂ al., [2023](#bib.bib16)),
    Structure diffusionÂ (Feng etÂ al., [2022](#bib.bib5)), PromptistÂ (Hao etÂ al., [2022](#bib.bib7)),
    and PixArt-$\alpha$Â (Chen etÂ al., [2023](#bib.bib3)).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2ï¼šMuLanç”Ÿæˆçš„å›¾åƒç¤ºä¾‹ï¼Œä¸ŽåŽŸå§‹SD-v1.4Â (Rombach etÂ al., [2022](#bib.bib17))ã€åŽŸå§‹SDXLÂ (Podell
    etÂ al., [2023](#bib.bib16))ã€ç»“æž„æ‰©æ•£Â (Feng etÂ al., [2022](#bib.bib5))ã€PromptistÂ (Hao
    etÂ al., [2022](#bib.bib7))å’ŒPixArt-$\alpha$Â (Chen etÂ al., [2023](#bib.bib3))ç›¸æ¯”ã€‚
- en: â€¢
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: We propose a novel training-free paradigm for text-to-image generation and a
    Multimodal-LLM agent. It achieves better control in generating images for complicated
    prompts consisting of multiple objects with specified spatial relationships and
    attribute bindings.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ— è®­ç»ƒèŒƒå¼ç”¨äºŽæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼Œä»¥åŠä¸€ä¸ªå¤šæ¨¡æ€LLMä»£ç†ã€‚å®ƒåœ¨ç”Ÿæˆå…·æœ‰æŒ‡å®šç©ºé—´å…³ç³»å’Œå±žæ€§ç»‘å®šçš„å¤šä¸ªå¯¹è±¡çš„å¤æ‚æç¤ºçš„å›¾åƒæ—¶ï¼Œå®žçŽ°äº†æ›´å¥½çš„æŽ§åˆ¶ã€‚
- en: â€¢
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: We propose an effective strategy to handle multi-object occlusion in T2I generation,
    which improves the image quality and makes them more realistic.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„ç­–ç•¥æ¥å¤„ç†T2Iç”Ÿæˆä¸­çš„å¤šå¯¹è±¡é®æŒ¡ï¼Œè¿™æé«˜äº†å›¾åƒè´¨é‡å¹¶ä½¿å…¶æ›´å…·çŽ°å®žæ„Ÿã€‚
- en: â€¢
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: We curate a dataset of prompts to evaluate multi-object composition with spatial
    relationships and attribute bindings in T2I tasks. The quantitative results and
    human evaluation results show that our method can achieve better results compared
    to different controllable generation methods and general T2I generation methods.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç­–åˆ’äº†ä¸€ä¸ªæç¤ºæ•°æ®é›†ï¼Œä»¥è¯„ä¼°T2Iä»»åŠ¡ä¸­å¤šå¯¹è±¡çš„ç©ºé—´å…³ç³»å’Œå±žæ€§ç»‘å®šçš„ç»„åˆã€‚å®šé‡ç»“æžœå’Œäººå·¥è¯„ä¼°ç»“æžœè¡¨æ˜Žï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå®žçŽ°æ¯”ä¸åŒçš„å¯æŽ§ç”Ÿæˆæ–¹æ³•å’Œé€šç”¨T2Iç”Ÿæˆæ–¹æ³•æ›´å¥½çš„ç»“æžœã€‚
- en: 2 Related Work
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 ç›¸å…³å·¥ä½œ
- en: Diffusion models
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ‰©æ•£æ¨¡åž‹
- en: As a new family of generative models, diffusion models have attracting more
    and more attention due to its powerful creative capability. Text-to-image generation,
    which aims to generate the high-quality image aligning with given text prompts,
    is one of the most popular applicationsÂ (Nichol etÂ al., [2021](#bib.bib14); Saharia
    etÂ al., [2022](#bib.bib18); Rombach etÂ al., [2022](#bib.bib17); Betker etÂ al.,
    [2023](#bib.bib2)). Among different powerful diffusion models, the latent diffusion
    modelÂ (Rombach etÂ al., [2022](#bib.bib17)) has shown amazing capability and has
    been widely used in practice due to the efficiency and superior performance, which
    is also the backbone of the current SOTA stable diffusion models. Different from
    the typical diffusion models which directly perform the diffusion and denoising
    process in the pixel space, the latent diffusion model perform the whole process
    in the encoded latent spaceÂ (Rombach etÂ al., [2022](#bib.bib17)), which can greatly
    reduce the training and inference time. Recently, empowered by a significantly
    expanded model capacity, Stable Diffusion XL has demonstrated performance levels
    approaching commercial application standardsÂ (Podell etÂ al., [2023](#bib.bib16)).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºæ–°ä¸€ä»£ç”Ÿæˆæ¨¡åž‹ï¼Œæ‰©æ•£æ¨¡åž‹å› å…¶å¼ºå¤§çš„åˆ›ä½œèƒ½åŠ›è€Œå—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ—¨åœ¨ç”Ÿæˆä¸Žç»™å®šæ–‡æœ¬æç¤ºå¯¹é½çš„é«˜è´¨é‡å›¾åƒï¼Œæ˜¯æœ€å—æ¬¢è¿Žçš„åº”ç”¨ä¹‹ä¸€Â (Nichol
    etÂ al., [2021](#bib.bib14); Saharia etÂ al., [2022](#bib.bib18); Rombach etÂ al.,
    [2022](#bib.bib17); Betker etÂ al., [2023](#bib.bib2))ã€‚åœ¨ä¸åŒçš„å¼ºå¤§æ‰©æ•£æ¨¡åž‹ä¸­ï¼Œæ½œåœ¨æ‰©æ•£æ¨¡åž‹Â (Rombach
    etÂ al., [2022](#bib.bib17))å±•çŽ°äº†æƒŠäººçš„èƒ½åŠ›ï¼Œå¹¶å› å…¶æ•ˆçŽ‡å’Œä¼˜è¶Šçš„æ€§èƒ½è¢«å¹¿æ³›åº”ç”¨ï¼Œäº¦æ˜¯å½“å‰SOTAç¨³å®šæ‰©æ•£æ¨¡åž‹çš„æ ¸å¿ƒã€‚ä¸Žç›´æŽ¥åœ¨åƒç´ ç©ºé—´è¿›è¡Œæ‰©æ•£å’ŒåŽ»å™ªå¤„ç†çš„å…¸åž‹æ‰©æ•£æ¨¡åž‹ä¸åŒï¼Œæ½œåœ¨æ‰©æ•£æ¨¡åž‹åœ¨ç¼–ç çš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ•´ä¸ªè¿‡ç¨‹Â (Rombach
    etÂ al., [2022](#bib.bib17))ï¼Œè¿™å¯ä»¥å¤§å¤§å‡å°‘è®­ç»ƒå’ŒæŽ¨ç†æ—¶é—´ã€‚æœ€è¿‘ï¼Œå¾—ç›ŠäºŽæ˜¾è‘—æ‰©å±•çš„æ¨¡åž‹å®¹é‡ï¼Œç¨³å®šæ‰©æ•£XLå±•ç¤ºäº†æŽ¥è¿‘å•†ä¸šåº”ç”¨æ ‡å‡†çš„æ€§èƒ½æ°´å¹³Â (Podell
    etÂ al., [2023](#bib.bib16))ã€‚
- en: Composed generation in diffusion models
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ‰©æ•£æ¨¡åž‹ä¸­çš„å¤åˆç”Ÿæˆ
- en: Although Stable Diffusion model has shown unprecedented performance on the T2I
    generation task, it still struggles with text prompts with multi-object, especially
    when there are several spatial relationships and attribute bindings in the prompts.
    To achieve more controllable and accurate image compositions, many compositional
    generation methods have been proposed. StructureDiffusionÂ (Feng etÂ al., [2022](#bib.bib5))
    proposed a training-free method to parse the input prompt and combine it with
    the cross-attention to achieve better control over attribute bindings and compositional
    generation. On the other hand, PromptistÂ (Hao etÂ al., [2022](#bib.bib7)) aimed
    to train a language model with the objective of optimizing input prompts, rendering
    them more comprehensible and facilitative for diffusion models. Recently, several
    works utilize the large language model to directly generate the whole layout for
    the input prompt with in-context learning, and then generate the image conditioned
    on the layoutÂ (Lian etÂ al., [2023](#bib.bib12); Feng etÂ al., [2023](#bib.bib6)).
    While all the previous take the whole input prompt, we propose to turn the original
    complicated task into several easier sub-tasks. A training-free multimodal-LLM
    agent is utilized to progressively generate objects with feedback control so that
    the whole generation process would be better controlled.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ç¨³å®šæ‰©æ•£æ¨¡åž‹åœ¨T2Iç”Ÿæˆä»»åŠ¡ä¸Šè¡¨çŽ°å‡ºå‰æ‰€æœªæœ‰çš„æ€§èƒ½ï¼Œä½†åœ¨å¤„ç†å…·æœ‰å¤šä¸ªå¯¹è±¡çš„æ–‡æœ¬æç¤ºæ—¶ä»é¢ä¸´å›°éš¾ï¼Œå°¤å…¶æ˜¯å½“æç¤ºä¸­å­˜åœ¨å¤šä¸ªç©ºé—´å…³ç³»å’Œå±žæ€§ç»‘å®šæ—¶ã€‚ä¸ºäº†å®žçŽ°æ›´å¯æŽ§å’Œå‡†ç¡®çš„å›¾åƒæž„å›¾ï¼Œå·²ç»æå‡ºäº†è®¸å¤šç»„åˆç”Ÿæˆæ–¹æ³•ã€‚StructureDiffusionï¼ˆFengç­‰ï¼Œ[2022](#bib.bib5)ï¼‰æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œä»¥è§£æžè¾“å…¥æç¤ºå¹¶ç»“åˆäº¤å‰æ³¨æ„åŠ›ï¼Œä»¥å®žçŽ°å¯¹å±žæ€§ç»‘å®šå’Œç»„åˆç”Ÿæˆçš„æ›´å¥½æŽ§åˆ¶ã€‚å¦ä¸€æ–¹é¢ï¼ŒPromptistï¼ˆHaoç­‰ï¼Œ[2022](#bib.bib7)ï¼‰æ—¨åœ¨è®­ç»ƒä¸€ä¸ªè¯­è¨€æ¨¡åž‹ï¼Œä»¥ä¼˜åŒ–è¾“å…¥æç¤ºï¼Œä½¿å…¶å¯¹æ‰©æ•£æ¨¡åž‹æ›´å…·å¯ç†è§£æ€§å’Œè¾…åŠ©æ€§ã€‚æœ€è¿‘ï¼Œä¸€äº›å·¥ä½œåˆ©ç”¨å¤§è¯­è¨€æ¨¡åž‹ç›´æŽ¥ç”Ÿæˆè¾“å…¥æç¤ºçš„æ•´ä¸ªå¸ƒå±€ï¼Œå¹¶åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ çš„åŸºç¡€ä¸Šç”Ÿæˆå›¾åƒï¼ˆLianç­‰ï¼Œ[2023](#bib.bib12)ï¼›Fengç­‰ï¼Œ[2023](#bib.bib6)ï¼‰ã€‚è™½ç„¶ä¹‹å‰çš„æ–¹æ³•éƒ½å¤„ç†æ•´ä¸ªè¾“å…¥æç¤ºï¼Œæˆ‘ä»¬æå‡ºå°†åŽŸå§‹å¤æ‚ä»»åŠ¡è½¬åŒ–ä¸ºå‡ ä¸ªæ›´ç®€å•çš„å­ä»»åŠ¡ã€‚é‡‡ç”¨æ— è®­ç»ƒçš„å¤šæ¨¡æ€-LLMä»£ç†é€æ­¥ç”Ÿæˆå¯¹è±¡ï¼Œå¹¶é€šè¿‡åé¦ˆæŽ§åˆ¶æ¥æ›´å¥½åœ°æŽ§åˆ¶æ•´ä¸ªç”Ÿæˆè¿‡ç¨‹ã€‚
- en: 3 Multimodal-LLM Agent (MuLan)
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 å¤šæ¨¡æ€-LLMä»£ç†ï¼ˆMuLanï¼‰
- en: Existing diffusion models often struggle with complicated prompts but can handle
    simpler ones. Recent approaches train a model or apply in-context learning given
    similar examples to produce a detailed layout for the prompt in advance and the
    diffusion model can generate each part of the layout with a simpler prompt separately.
    Rather than generating all objects at once or in parallel, MuLan is inspired by
    many human painters, who start by making a high-level plan, painting objects one
    after another as planned, and correcting mistakes after each step if needed. Thereby,
    the constraints between objects can be naturally taken into account.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: çŽ°æœ‰çš„æ‰©æ•£æ¨¡åž‹é€šå¸¸åœ¨å¤„ç†å¤æ‚æç¤ºæ—¶ä¼šé‡åˆ°å›°éš¾ï¼Œä½†å¯ä»¥å¤„ç†è¾ƒç®€å•çš„æç¤ºã€‚è¿‘æœŸçš„æ–¹æ³•è®­ç»ƒä¸€ä¸ªæ¨¡åž‹æˆ–åº”ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œç»™å‡ºç±»ä¼¼çš„ç¤ºä¾‹ï¼Œä»¥æå‰ç”Ÿæˆè¯¦ç»†çš„å¸ƒå±€ï¼Œç„¶åŽæ‰©æ•£æ¨¡åž‹å¯ä»¥ç”¨æ›´ç®€å•çš„æç¤ºå•ç‹¬ç”Ÿæˆå¸ƒå±€çš„æ¯ä¸€éƒ¨åˆ†ã€‚ä¸Žå…¶ä¸€æ¬¡æ€§æˆ–å¹¶è¡Œç”Ÿæˆæ‰€æœ‰å¯¹è±¡ï¼ŒMuLanå—åˆ°è®¸å¤šäººç±»ç”»å®¶çš„å¯å‘ï¼Œä»–ä»¬å¼€å§‹æ—¶åˆ¶å®šé«˜å±‚æ¬¡è®¡åˆ’ï¼ŒæŒ‰ç…§è®¡åˆ’ä¸€ä¸ªä¸€ä¸ªåœ°ç»˜åˆ¶å¯¹è±¡ï¼Œå¹¶åœ¨æ¯ä¸€æ­¥åŽå¦‚æœ‰éœ€è¦è¿›è¡Œçº æ­£ã€‚è¿™æ ·ï¼Œå¯¹è±¡ä¹‹é—´çš„çº¦æŸå¯ä»¥è‡ªç„¶åœ°è€ƒè™‘è¿›åŽ»ã€‚
- en: 3.1 Problem Formulation
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 é—®é¢˜è¡¨è¿°
- en: 'Likewise, MuLan begins by strategically planning and decomposing an intricate
    input prompt into a manageable sequence of sub-prompts, each focusing on an easier
    sub-task generating one single object. MuLan then adopts a progressive strategy
    that generates one object in each stage conditioned on previously generated objects
    using a diffusion model. Simultaneously, a VLM offers insightful feedback and
    adaptively adjusts the generation process to guarantee precision in accomplishing
    each subtask. Compared to previous methods, MuLan is entirely training-free and
    does not require any examples. As illustrated in Fig.Â [1](#S1.F1 "Figure 1 â€£ 1
    Introduction â€£ MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion"),
    MuLan is composed of three components:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 'åŒæ ·ï¼ŒMuLanå¼€å§‹æ—¶ä¼šæˆ˜ç•¥æ€§åœ°è§„åˆ’å’Œåˆ†è§£å¤æ‚çš„è¾“å…¥æç¤ºï¼Œå°†å…¶æ‹†åˆ†ä¸ºä¸€ä¸ªå¯ç®¡ç†çš„å­æç¤ºåºåˆ—ï¼Œæ¯ä¸ªå­æç¤ºä¸“æ³¨äºŽç”Ÿæˆä¸€ä¸ªå•ä¸€å¯¹è±¡ã€‚ç„¶åŽï¼ŒMuLané‡‡ç”¨é€æ­¥ç”Ÿæˆç­–ç•¥ï¼Œåœ¨æ¯ä¸ªé˜¶æ®µç”Ÿæˆä¸€ä¸ªå¯¹è±¡ï¼Œè¿™ä¸ªè¿‡ç¨‹æ˜¯åŸºäºŽä¹‹å‰ç”Ÿæˆçš„å¯¹è±¡ï¼Œå¹¶ä½¿ç”¨æ‰©æ•£æ¨¡åž‹ã€‚åŒæ—¶ï¼ŒVLMæä¾›æœ‰ä»·å€¼çš„åé¦ˆï¼Œå¹¶è‡ªé€‚åº”åœ°è°ƒæ•´ç”Ÿæˆè¿‡ç¨‹ï¼Œä»¥ç¡®ä¿æ¯ä¸ªå­ä»»åŠ¡çš„ç²¾ç¡®å®Œæˆã€‚ä¸Žä»¥å‰çš„æ–¹æ³•ç›¸æ¯”ï¼ŒMuLanå®Œå…¨æ— è®­ç»ƒè¦æ±‚ï¼Œä¸éœ€è¦ä»»ä½•ç¤ºä¾‹ã€‚å¦‚å›¾[1](#S1.F1
    "Figure 1 â€£ 1 Introduction â€£ MuLan: Multimodal-LLM Agent for Progressive Multi-Object
    Diffusion")æ‰€ç¤ºï¼ŒMuLanç”±ä¸‰ä¸ªç»„ä»¶ç»„æˆï¼š'
- en: â€¢
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Prompt decomposition by LLM planning, which produces a sequence of sub-prompts,
    each focusing on generating one object in the prompt.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç”±LLMè§„åˆ’è¿›è¡Œçš„æç¤ºåˆ†è§£ï¼Œç”Ÿæˆä¸€ä¸ªå­æç¤ºåºåˆ—ï¼Œæ¯ä¸ªå­æç¤ºé›†ä¸­äºŽç”Ÿæˆæç¤ºä¸­çš„ä¸€ä¸ªå¯¹è±¡ã€‚
- en: â€¢
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Conditional single-object diffusion with LLM planning and attention guidance,
    which generates a new object conditioned on the previous stepâ€™s image using a
    stable diffusion model. While a sub-prompt from LLM planning provides text guidance,
    the objectâ€™s size and position are controlled by an attention mask.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å…·æœ‰ LLM è§„åˆ’å’Œæ³¨æ„åŠ›æŒ‡å¯¼çš„æ¡ä»¶å•å¯¹è±¡æ‰©æ•£ï¼Œå®ƒä½¿ç”¨ç¨³å®šæ‰©æ•£æ¨¡åž‹ç”Ÿæˆä¸€ä¸ªæ–°çš„å¯¹è±¡ï¼Œè¯¥å¯¹è±¡ä»¥å…ˆå‰æ­¥éª¤çš„å›¾åƒä¸ºæ¡ä»¶ã€‚è™½ç„¶ LLM è§„åˆ’ä¸­çš„å­æç¤ºæä¾›äº†æ–‡æœ¬æŒ‡å¯¼ï¼Œä½†å¯¹è±¡çš„å¤§å°å’Œä½ç½®ç”±æ³¨æ„åŠ›æŽ©ç æŽ§åˆ¶ã€‚
- en: â€¢
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: VLM-feedback control, which inspects the image generated per stage and adjusts
    hyperparameters to re-generate the image if it violates the original prompt.
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: VLM åé¦ˆæŽ§åˆ¶ï¼Œæ£€æŸ¥æ¯ä¸ªé˜¶æ®µç”Ÿæˆçš„å›¾åƒï¼Œå¹¶è°ƒæ•´è¶…å‚æ•°ä»¥é‡æ–°ç”Ÿæˆå›¾åƒï¼Œå¦‚æžœå®ƒè¿åäº†åŽŸå§‹æç¤ºã€‚
- en: 3.2 Background on (Latent) Diffusion Models
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 (æ½œåœ¨) æ‰©æ•£æ¨¡åž‹èƒŒæ™¯
- en: Consisting of the diffusion process and the reverse process, diffusion models
    have shown impressive capability for high-quality image generation by iteratively
    adding noise and denoisingÂ (Ho etÂ al., [2020](#bib.bib9)). Let $\bm{x}_{0}\sim
    q(\bm{x}_{0})$, the training loss of the model can be simplified as
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±æ‰©æ•£è¿‡ç¨‹å’Œåå‘è¿‡ç¨‹ç»„æˆï¼Œæ‰©æ•£æ¨¡åž‹é€šè¿‡åå¤æ·»åŠ å™ªå£°å’ŒåŽ»å™ªæ˜¾ç¤ºäº†åœ¨é«˜è´¨é‡å›¾åƒç”Ÿæˆæ–¹é¢çš„æ˜¾è‘—èƒ½åŠ› (Ho ç­‰ï¼Œ [2020](#bib.bib9))ã€‚è®© $\bm{x}_{0}\sim
    q(\bm{x}_{0})$ï¼Œæ¨¡åž‹çš„è®­ç»ƒæŸå¤±å¯ä»¥ç®€åŒ–ä¸º
- en: '|  | $1$2 |  | (1) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: Latent diffusion modelsÂ (Rombach etÂ al., [2022](#bib.bib17)) have recently attracted
    growing attention due to their efficiency and superior performance. Instead of
    performing diffusion and its reverse process in the pixel space, they add noise
    and denoise in a latent space of $\bm{z}$. Accordingly, the training loss becomes
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æ½œåœ¨æ‰©æ•£æ¨¡åž‹ (Rombach ç­‰ï¼Œ [2022](#bib.bib17)) æœ€è¿‘å› å…¶é«˜æ•ˆæ€§å’Œä¼˜è¶Šçš„æ€§èƒ½è€Œå—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚å®ƒä»¬ä¸æ˜¯åœ¨åƒç´ ç©ºé—´ä¸­æ‰§è¡Œæ‰©æ•£åŠå…¶åå‘è¿‡ç¨‹ï¼Œè€Œæ˜¯åœ¨
    $\bm{z}$ çš„æ½œåœ¨ç©ºé—´ä¸­æ·»åŠ å™ªå£°å’ŒåŽ»å™ªã€‚å› æ­¤ï¼Œè®­ç»ƒæŸå¤±å˜ä¸º
- en: '|  | $\displaystyle L_{LDM}=\mathbb{E}_{\bm{z}_{0},\bm{\epsilon},t}\&#124;\bm{\epsilon}-\bm{\epsilon}_{\theta}(\bm{z}_{t},t)\&#124;^{2}.$
    |  | (2) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L_{LDM}=\mathbb{E}_{\bm{z}_{0},\bm{\epsilon},t}\&#124;\bm{\epsilon}-\bm{\epsilon}_{\theta}(\bm{z}_{t},t)\&#124;^{2}.$
    |  | (2) |'
- en: 3.3 Prompt Decomposition by LLM Planning
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 LLM è§„åˆ’çš„æç¤ºåˆ†è§£
- en: 'Given a complicated prompt p, MuLan first uses an LLM to automatically decompose
    p into $N$â€. MuLan conducts the above global planning by an LLM at the very beginning
    before generating any image. The detailed prompts and template for LLM planning
    can be found in AppendixÂ [A](#A1 "Appendix A Detailed prompt template of the global
    planning by the LLM â€£ MuLan: Multimodal-LLM Agent for Progressive Multi-Object
    Diffusion").'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç»™å®šä¸€ä¸ªå¤æ‚çš„æç¤º pï¼ŒMuLan é¦–å…ˆä½¿ç”¨ LLM è‡ªåŠ¨å°† p åˆ†è§£ä¸º $N$â€ã€‚MuLan åœ¨ç”Ÿæˆä»»ä½•å›¾åƒä¹‹å‰ï¼Œé¦–å…ˆç”± LLM è¿›è¡Œä¸Šè¿°å…¨å±€è§„åˆ’ã€‚LLM
    è§„åˆ’çš„è¯¦ç»†æç¤ºå’Œæ¨¡æ¿å¯ä»¥åœ¨é™„å½• [A](#A1 "é™„å½• A LLM å…¨å±€è§„åˆ’è¯¦ç»†æç¤ºæ¨¡æ¿ â€£ MuLan: å¤šæ¨¡æ€-LLM ä»£ç†ç”¨äºŽæ¸è¿›å¼å¤šå¯¹è±¡æ‰©æ•£") ä¸­æ‰¾åˆ°ã€‚'
- en: 'When generating each object in SectionÂ [3.4](#S3.SS4 "3.4 Conditional Single-Object
    Diffusion with LLM Planning and Attention Guidance â€£ 3 Multimodal-LLM Agent (MuLan)
    â€£ MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion"), we will
    use the LLM again as a local planner of the objectâ€™s position and size, i.e.,
    by generating a mask in the image and coordinating its overlap with previous objects.
    Then a diffusion model is used to generate the object under the attention guidance
    of the mask. These will be further elaborated in SectionÂ [3.4](#S3.SS4 "3.4 Conditional
    Single-Object Diffusion with LLM Planning and Attention Guidance â€£ 3 Multimodal-LLM
    Agent (MuLan) â€£ MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion").'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨ç”Ÿæˆç¬¬ [3.4](#S3.SS4 "3.4 å…·æœ‰ LLM è§„åˆ’å’Œæ³¨æ„åŠ›æŒ‡å¯¼çš„æ¡ä»¶å•å¯¹è±¡æ‰©æ•£ â€£ 3 å¤šæ¨¡æ€-LLM ä»£ç† (MuLan) â€£ MuLan:
    å¤šæ¨¡æ€-LLM ä»£ç†ç”¨äºŽæ¸è¿›å¼å¤šå¯¹è±¡æ‰©æ•£") èŠ‚ä¸­çš„æ¯ä¸ªå¯¹è±¡æ—¶ï¼Œæˆ‘ä»¬å°†å†æ¬¡ä½¿ç”¨ LLM ä½œä¸ºå¯¹è±¡ä½ç½®å’Œå¤§å°çš„å±€éƒ¨è§„åˆ’å™¨ï¼Œå³é€šè¿‡åœ¨å›¾åƒä¸­ç”ŸæˆæŽ©ç å¹¶åè°ƒå…¶ä¸Žå…ˆå‰å¯¹è±¡çš„é‡å ã€‚ç„¶åŽï¼Œä½¿ç”¨æ‰©æ•£æ¨¡åž‹åœ¨æŽ©ç çš„æ³¨æ„åŠ›æŒ‡å¯¼ä¸‹ç”Ÿæˆå¯¹è±¡ã€‚è¿™äº›å°†åœ¨ç¬¬
    [3.4](#S3.SS4 "3.4 å…·æœ‰ LLM è§„åˆ’å’Œæ³¨æ„åŠ›æŒ‡å¯¼çš„æ¡ä»¶å•å¯¹è±¡æ‰©æ•£ â€£ 3 å¤šæ¨¡æ€-LLM ä»£ç† (MuLan) â€£ MuLan: å¤šæ¨¡æ€-LLM
    ä»£ç†ç”¨äºŽæ¸è¿›å¼å¤šå¯¹è±¡æ‰©æ•£") èŠ‚ä¸­è¿›ä¸€æ­¥é˜è¿°ã€‚'
- en: 3.4 Conditional Single-Object Diffusion with LLM Planning and Attention Guidance
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 å…·æœ‰ LLM è§„åˆ’å’Œæ³¨æ„åŠ›æŒ‡å¯¼çš„æ¡ä»¶å•å¯¹è±¡æ‰©æ•£
- en: '![Refer to caption](img/a677a4ab6ce43fd95c5fea4526fd850f.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜Ž](img/a677a4ab6ce43fd95c5fea4526fd850f.png)'
- en: 'Figure 3: Single object diffusion with LLM planning and attention guidance
    for $\texttt{obj}_{n}$ (detailed procedure in AlgorithmÂ [1](#alg1 "Algorithm 1
    â€£ 3.4 Conditional Single-Object Diffusion with LLM Planning and Attention Guidance
    â€£ 3 Multimodal-LLM Agent (MuLan) â€£ MuLan: Multimodal-LLM Agent for Progressive
    Multi-Object Diffusion")).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 3ï¼šä½¿ç”¨ LLM è§„åˆ’å’Œæ³¨æ„åŠ›å¼•å¯¼çš„å•å¯¹è±¡æ‰©æ•£ï¼Œé’ˆå¯¹ $\texttt{obj}_{n}$ï¼ˆè¯¦ç»†è¿‡ç¨‹è§ç®—æ³• [1](#alg1 "ç®—æ³• 1 â€£ 3.4
    æ¡ä»¶å•å¯¹è±¡æ‰©æ•£ä¸Ž LLM è§„åˆ’å’Œæ³¨æ„åŠ›å¼•å¯¼ â€£ 3 å¤šæ¨¡æ€-LLM ä»£ç† (MuLan) â€£ MuLan: å¤šæ¨¡æ€-LLM ä»£ç†ç”¨äºŽæ¸è¿›å¼å¤šå¯¹è±¡æ‰©æ•£")ï¼‰ã€‚'
- en: Algorithm 1 Single Object Diffusion in MuLan
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ç®—æ³• 1 MuLan ä¸­çš„å•å¯¹è±¡æ‰©æ•£
- en: '1:Â Â Input: Object number $n$}'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '1: è¾“å…¥ï¼šå¯¹è±¡ç¼–å· $n$}'
- en: 'At stage-$n$-1). The pipeline is given in FigureÂ [3](#S3.F3 "Figure 3 â€£ 3.4
    Conditional Single-Object Diffusion with LLM Planning and Attention Guidance â€£
    3 Multimodal-LLM Agent (MuLan) â€£ MuLan: Multimodal-LLM Agent for Progressive Multi-Object
    Diffusion") with the complete procedure listed in AlgorithmÂ [1](#alg1 "Algorithm
    1 â€£ 3.4 Conditional Single-Object Diffusion with LLM Planning and Attention Guidance
    â€£ 3 Multimodal-LLM Agent (MuLan) â€£ MuLan: Multimodal-LLM Agent for Progressive
    Multi-Object Diffusion"). We will introduce it step by step in the following.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨é˜¶æ®µ-$n$-1ï¼‰ã€‚ç®¡é“è§å›¾ [3](#S3.F3 "å›¾ 3 â€£ 3.4 æ¡ä»¶å•å¯¹è±¡æ‰©æ•£ä¸Ž LLM è§„åˆ’å’Œæ³¨æ„åŠ›å¼•å¯¼ â€£ 3 å¤šæ¨¡æ€-LLM ä»£ç†
    (MuLan) â€£ MuLan: å¤šæ¨¡æ€-LLM ä»£ç†ç”¨äºŽæ¸è¿›å¼å¤šå¯¹è±¡æ‰©æ•£")ï¼Œå®Œæ•´è¿‡ç¨‹åˆ—äºŽç®—æ³• [1](#alg1 "ç®—æ³• 1 â€£ 3.4 æ¡ä»¶å•å¯¹è±¡æ‰©æ•£ä¸Ž
    LLM è§„åˆ’å’Œæ³¨æ„åŠ›å¼•å¯¼ â€£ 3 å¤šæ¨¡æ€-LLM ä»£ç† (MuLan) â€£ MuLan: å¤šæ¨¡æ€-LLM ä»£ç†ç”¨äºŽæ¸è¿›å¼å¤šå¯¹è±¡æ‰©æ•£")ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹é¢é€æ­¥ä»‹ç»ã€‚'
- en: LLM Planning of a Rough Mask for $\texttt{obj}_{n}$.
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: é’ˆå¯¹ $\texttt{obj}_{n}$ çš„ç²—ç•¥æŽ©è†œçš„ LLM è§„åˆ’ã€‚
- en: At stage-$n$.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é˜¶æ®µ-$n$ã€‚
- en: 'When $n=1$, which leads to the following bounding box:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ $n=1$ æ—¶ï¼Œå¯¼è‡´ä»¥ä¸‹è¾¹ç•Œæ¡†ï¼š
- en: '|  | $1$2 |  | (3) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: '![Refer to caption](img/eaf6680d2961f1b44158eed3637777d6.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/eaf6680d2961f1b44158eed3637777d6.png)'
- en: 'Figure 4: The rough mask $\bm{M}_{n}$.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4ï¼šç²—ç•¥æŽ©è†œ $\bm{M}_{n}$ã€‚
- en: When $$n> as followings.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ $$n> å¦‚ä¸‹ã€‚
- en: '|  | <math id="S3.E4.m1.4" class="ltx_Math" alttext="\displaystyle\bm{M}_{n}=\begin{cases}(\tilde{x}_{n-1}+\tilde{w}_{n-1},0,\frac{W-\tilde{x}_{n-1}+\tilde{w}_{n-1}}{\texttt{Num}_{n}},H),\\
    \text{if }\texttt{opt}_{n}=\texttt{right},\\'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math id="S3.E4.m1.4" class="ltx_Math" alttext="\displaystyle\bm{M}_{n}=\begin{cases}(\tilde{x}_{n-1}+\tilde{w}_{n-1},0,\frac{W-\tilde{x}_{n-1}+\tilde{w}_{n-1}}{\texttt{Num}_{n}},H),\\
    \text{if }\texttt{opt}_{n}=\texttt{right},\\'
- en: \\
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: \\
- en: (0,\frac{\tilde{y}_{n-1}\cdot(\texttt{Num}_{n}-1)}{\texttt{Num}_{n}},W,\frac{\tilde{y}_{n-1}}{\texttt{Num}_{n}}),\\
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: (0,\frac{\tilde{y}_{n-1}\cdot(\texttt{Num}_{n}-1)}{\texttt{Num}_{n}},W,\frac{\tilde{y}_{n-1}}{\texttt{Num}_{n}}),\\
- en: \text{if }\texttt{opt}^{n}=\texttt{top}.\end{cases}$$ |  | (4) |
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: \text{if }\texttt{opt}^{n}=\texttt{top}.\end{cases}$$ |  | (4) |
- en: 'FigureÂ [4](#S3.F4 "Figure 4 â€£ LLM Planning of a Rough Mask for "obj"_ð‘›. â€£ 3.4
    Conditional Single-Object Diffusion with LLM Planning and Attention Guidance â€£
    3 Multimodal-LLM Agent (MuLan) â€£ MuLan: Multimodal-LLM Agent for Progressive Multi-Object
    Diffusion") illustrates how the rough mask can be computed based on the precise
    mask of previous objects.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ [4](#S3.F4 "å›¾ 4 â€£ é’ˆå¯¹â€œobjâ€_ð‘› çš„ç²—ç•¥æŽ©è†œçš„ LLM è§„åˆ’ â€£ 3.4 æ¡ä»¶å•å¯¹è±¡æ‰©æ•£ä¸Ž LLM è§„åˆ’å’Œæ³¨æ„åŠ›å¼•å¯¼ â€£ 3
    å¤šæ¨¡æ€-LLM ä»£ç† (MuLan) â€£ MuLan: å¤šæ¨¡æ€-LLM ä»£ç†ç”¨äºŽæ¸è¿›å¼å¤šå¯¹è±¡æ‰©æ•£") è¯´æ˜Žäº†å¦‚ä½•åŸºäºŽå…ˆå‰å¯¹è±¡çš„ç²¾ç¡®æŽ©è†œè®¡ç®—ç²—ç•¥æŽ©è†œã€‚'
- en: '![Refer to caption](img/f5bebbdfc8f48f560f2b5bfdd1892c6f.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/f5bebbdfc8f48f560f2b5bfdd1892c6f.png)'
- en: 'Figure 5: Illustration of the rough mask $\bm{M}_{1}$ for the mask since the
    LLM is prompted to plan the object order from left to right, bottom to top.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5ï¼šç²—ç•¥æŽ©è†œ $\bm{M}_{1}$ çš„ç¤ºæ„å›¾ï¼Œå› ä¸º LLM è¢«æç¤ºæŒ‰ç…§ä»Žå·¦åˆ°å³ã€ä»Žä¸‹åˆ°ä¸Šçš„é¡ºåºè§„åˆ’å¯¹è±¡ã€‚
- en: Single-Object Diffusion with Attention Guidance.
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å¸¦æœ‰æ³¨æ„åŠ›å¼•å¯¼çš„å•å¯¹è±¡æ‰©æ•£ã€‚
- en: 'Given the rough mask $\bm{M}_{n}$) to have a larger value inside the mask:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šç²—ç•¥æŽ©è†œ $\bm{M}_{n}$ ä»¥åœ¨æŽ©è†œå†…å…·æœ‰æ›´å¤§çš„å€¼ï¼š
- en: '|  | $1$2 |  | (5) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (5) |'
- en: Then, in every step-$t$ by
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åŽï¼Œåœ¨æ¯ä¸€æ­¥ $t$ ä¸­ç”±
- en: '|  | $\displaystyle\bm{z}_{n,t}=\bm{z}_{n,t}-\eta\cdot\nabla_{\bm{z}_{n,t}}\sum_{j\in
    B}E(\bm{A}^{(j)},\bm{M}_{n},k),$ |  | (6) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{z}_{n,t}=\bm{z}_{n,t}-\eta\cdot\nabla_{\bm{z}_{n,t}}\sum_{j\in
    B}E(\bm{A}^{(j)},\bm{M}_{n},k),$ |  | (6) |'
- en: where $\eta$ by
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\eta$ ç”±
- en: '|  | $\displaystyle\bm{z}_{n,(t-1)}=\bm{M}^{\prime}_{n}\odot\bm{z}_{n,(t-1)}+(1-\bm{M}^{\prime}_{n})\odot\bm{z}_{(n-1),(t-1)},$
    |  | (7) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{z}_{n,(t-1)}=\bm{M}^{\prime}_{n}\odot\bm{z}_{n,(t-1)}+(1-\bm{M}^{\prime}_{n})\odot\bm{z}_{(n-1),(t-1)},$
    |  | (7) |'
- en: where $\odot$.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\odot$ã€‚
- en: MuLan applies the above single-object diffusion to each object one after another
    from $\texttt{obj}_{1}$-defined bounding box via attention guidance.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: MuLan å°†ä¸Šè¿°å•å¯¹è±¡æ‰©æ•£åº”ç”¨äºŽæ¯ä¸ªå¯¹è±¡ï¼Œä»Ž $\texttt{obj}_{1}$ å®šä¹‰çš„è¾¹ç•Œæ¡†å¼€å§‹ï¼Œé€šè¿‡æ³¨æ„åŠ›å¼•å¯¼ä¾æ¬¡å¤„ç†ã€‚
- en: Overlapping between Objects
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å¯¹è±¡é—´çš„é‡å 
- en: 'Overlapping between objects is a key challenge when generating one object conditioned
    on the previous one(s). However, it lacks attention in previous methodsÂ (Lian
    etÂ al., [2023](#bib.bib12); Feng etÂ al., [2023](#bib.bib6)). Instead, we propose
    an effective strategy that can be merged into the procedure above. Specifically,
    at the generation of object $\texttt{obj}_{n}$. An illustration is given in FigureÂ [7](#A3.F7
    "Figure 7 â€£ Appendix C More details on the overlapping processing â€£ MuLan: Multimodal-LLM
    Agent for Progressive Multi-Object Diffusion") with more details of candidate
    masks in AppendixÂ [C](#A3 "Appendix C More details on the overlapping processing
    â€£ MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion").'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç”Ÿæˆä¸€ä¸ªå¯¹è±¡æ—¶ï¼Œå¦‚ä½•å¤„ç†å¯¹è±¡ä¹‹é—´çš„é‡å æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œä¹‹å‰çš„æ–¹æ³•ä¸­å¯¹æ­¤å…³æ³¨ä¸è¶³ (Lian et al., [2023](#bib.bib12);
    Feng et al., [2023](#bib.bib6))ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„ç­–ç•¥ï¼Œå¯ä»¥ä¸Žä¸Šè¿°è¿‡ç¨‹åˆå¹¶ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨ç”Ÿæˆå¯¹è±¡ $\texttt{obj}_{n}$
    æ—¶ï¼Œå›¾ç¤ºå¦‚å›¾ [7](#A3.F7 "å›¾ 7 â€£ é™„å½• C æ›´å¤šé‡å å¤„ç†ç»†èŠ‚ â€£ MuLan: ç”¨äºŽæ¸è¿›å¼å¤šå¯¹è±¡æ‰©æ•£çš„å¤šæ¨¡æ€-LLM ä»£ç†")ï¼Œæ›´å¤šå€™é€‰æŽ©ç çš„ç»†èŠ‚è§é™„å½•
    [C](#A3 "é™„å½• C æ›´å¤šé‡å å¤„ç†ç»†èŠ‚ â€£ MuLan: ç”¨äºŽæ¸è¿›å¼å¤šå¯¹è±¡æ‰©æ•£çš„å¤šæ¨¡æ€-LLM ä»£ç†")ã€‚'
- en: Given the three masks $\bm{M}_{n,i}$.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šä¸‰ä¸ªæŽ©ç  $\bm{M}_{n,i}$ã€‚
- en: 3.5 Adaptive Feedback Control by VLM
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 VLM çš„è‡ªé€‚åº”åé¦ˆæŽ§åˆ¶
- en: To correct the possible mistakes made in the sequential generation process,
    MuLan builds a feedback-loop control by a vision-language model (VLM). After each
    generation stage, MuLan queries the VLM to inspect the generated object(s)and
    its consistency with the input prompt. If they do not align well, MuLan will adjust
    the backward guidance and $T^{*}$ of the current stage to re-generate the object.
    Such a close-loop control involves LLM, diffusion, and VLM and significantly automates
    the T2I generation for complicated prompts, leading to a more accurate generation
    in practice.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†çº æ­£é¡ºåºç”Ÿæˆè¿‡ç¨‹ä¸­çš„å¯èƒ½é”™è¯¯ï¼ŒMuLan é€šè¿‡è§†è§‰-è¯­è¨€æ¨¡åž‹ï¼ˆVLMï¼‰å»ºç«‹äº†ä¸€ä¸ªåé¦ˆå¾ªçŽ¯æŽ§åˆ¶ã€‚åœ¨æ¯ä¸ªç”Ÿæˆé˜¶æ®µä¹‹åŽï¼ŒMuLan æŸ¥è¯¢ VLM æ¥æ£€æŸ¥ç”Ÿæˆçš„å¯¹è±¡åŠå…¶ä¸Žè¾“å…¥æç¤ºçš„ä¸€è‡´æ€§ã€‚å¦‚æžœä¸ä¸€è‡´ï¼ŒMuLan
    å°†è°ƒæ•´å½“å‰é˜¶æ®µçš„åå‘æŒ‡å¯¼å’Œ $T^{*}$ ä»¥é‡æ–°ç”Ÿæˆå¯¹è±¡ã€‚è¿™ç§é—­çŽ¯æŽ§åˆ¶æ¶‰åŠ LLMã€æ‰©æ•£å’Œ VLMï¼Œå¤§å¤§è‡ªåŠ¨åŒ–äº†å¤æ‚æç¤ºçš„ T2I ç”Ÿæˆï¼Œä»Žè€Œåœ¨å®žè·µä¸­å®žçŽ°äº†æ›´å‡†ç¡®çš„ç”Ÿæˆã€‚
- en: 4 Experiments
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 å®žéªŒ
- en: 'Table 1: GPT-4V evaluation/human evaluation of images generated by different
    methods for complicated prompts.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¡¨ 1: GPT-4V å¯¹ä¸åŒæ–¹æ³•ç”Ÿæˆçš„å¤æ‚æç¤ºå›¾åƒçš„è¯„ä¼°/äººå·¥è¯„ä¼°ã€‚'
- en: '| Method | Object completeness | Attribute bindings | Spatial relationships
    | Overall |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| æ–¹æ³• | å¯¹è±¡å®Œæ•´æ€§ | å±žæ€§ç»‘å®š | ç©ºé—´å…³ç³» | æ€»ä½“ |'
- en: '| Structure DiffusionÂ (Feng etÂ al., [2022](#bib.bib5)) | 88.97%/87.37% | 54.62%/62.63%
    | 34.36%/24.24% | 64.31%/64.85% |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| ç»“æž„æ‰©æ•£ (Feng et al., [2022](#bib.bib5)) | 88.97%/87.37% | 54.62%/62.63% | 34.36%/24.24%
    | 64.31%/64.85% |'
- en: '| Promptist-SD v1.4Â (Hao etÂ al., [2022](#bib.bib7)) | 80.36%/70.71% | 49.23%/52.02%
    | 24.49%/13.13% | 56.73%/51.72% |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Promptist-SD v1.4 (Hao et al., [2022](#bib.bib7)) | 80.36%/70.71% | 49.23%/52.02%
    | 24.49%/13.13% | 56.73%/51.72% |'
- en: '| Promptist-SDXLÂ (Hao etÂ al., [2022](#bib.bib7)) | 94.36%/93.94% | 70.00%/78.28%
    | 35.89%/33.33% | 72.92%/75.56% |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Promptist-SDXL (Hao et al., [2022](#bib.bib7)) | 94.36%/93.94% | 70.00%/78.28%
    | 35.89%/33.33% | 72.92%/75.56% |'
- en: '| SD v1.4Â (Rombach etÂ al., [2022](#bib.bib17)) | 90.31%/74.49% | 57.14%/51.02%
    | 37.24%/32.65% | 66.43%/56.73% |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| SD v1.4 (Rombach et al., [2022](#bib.bib17)) | 90.31%/74.49% | 57.14%/51.02%
    | 37.24%/32.65% | 66.43%/56.73% |'
- en: '| SDXLÂ (Podell etÂ al., [2023](#bib.bib16)) | 94.64%/78.57% | 66.07%/53.06%
    | 41.14%/24.49% | 72.34%/57.55% |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| SDXL (Podell et al., [2023](#bib.bib16)) | 94.64%/78.57% | 66.07%/53.06%
    | 41.14%/24.49% | 72.34%/57.55% |'
- en: '| PixArt-$\alpha$Â (Chen etÂ al., [2023](#bib.bib3)) | 92.09%/76.53% | 66.58%/61.22%
    | 34.69%/32.65% | 70.41%/61.63% |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| PixArt-$\alpha$ (Chen et al., [2023](#bib.bib3)) | 92.09%/76.53% | 66.58%/61.22%
    | 34.69%/32.65% | 70.41%/61.63% |'
- en: '| MuLan-SD v1.4 (Ours) | 93.11%/86.36% | 74.23%/74.24% | 51.53%/54.54% | 77.24%/75.15%
    |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| MuLan-SD v1.4 (æˆ‘ä»¬çš„) | 93.11%/86.36% | 74.23%/74.24% | 51.53%/54.54% | 77.24%/75.15%
    |'
- en: '| MuLan-SDXL (Ours) | 96.17%/90.40% | 75.00%/79.29% | 39.29%/49.49% | 76.33%/77.78%
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| MuLan-SDXL (æˆ‘ä»¬çš„) | 96.17%/90.40% | 75.00%/79.29% | 39.29%/49.49% | 76.33%/77.78%
    |'
- en: Dataset
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ•°æ®é›†
- en: To evaluate our framework, we construct a prompt dataset from different benchmarks.
    Specifically, since our focus is to achieve better generation for complex prompts
    containing multi-objects with both spatial relationships and attribute bindings,
    we first collect all complex spatial prompts from T2I-CompBenchÂ (Huang etÂ al.,
    [2023](#bib.bib10)). To make the experiments more comprehensive, we let ChatGPT
    generate about 400 prompts with different objects, spatial relationships, and
    attribute bindings so that the prompt sets consists of about 600 prompts. To further
    evaluate the capability of our framework on extremely complex and hard prompts,
    we manually add prompts that SDXL fails to generate, leading to a hard prompt
    dataset containing 200 prompts. Similar to the complex spatial prompts in T2I-CompBenchÂ (Huang
    etÂ al., [2023](#bib.bib10)), each prompt in our curated dataset typically contains
    two objects with various spatial relationships, with each object containing attribute
    bindings randomly selected from {color,shape,texture}.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ¡†æž¶ï¼Œæˆ‘ä»¬ä»Žä¸åŒçš„åŸºå‡†ä¸­æž„å»ºäº†ä¸€ä¸ªæç¤ºæ•°æ®é›†ã€‚å…·ä½“æ¥è¯´ï¼Œç”±äºŽæˆ‘ä»¬çš„é‡ç‚¹æ˜¯å®žçŽ°å¯¹åŒ…å«å¤šå¯¹è±¡çš„å¤æ‚æç¤ºçš„æ›´å¥½ç”Ÿæˆï¼Œè¿™äº›æç¤ºå…·æœ‰ç©ºé—´å…³ç³»å’Œå±žæ€§ç»‘å®šï¼Œæˆ‘ä»¬é¦–å…ˆä»ŽT2I-CompBenchï¼ˆHuangç­‰ï¼Œ[2023](#bib.bib10)ï¼‰æ”¶é›†æ‰€æœ‰å¤æ‚çš„ç©ºé—´æç¤ºã€‚ä¸ºäº†ä½¿å®žéªŒæ›´å…¨é¢ï¼Œæˆ‘ä»¬è®©ChatGPTç”Ÿæˆçº¦400ä¸ªå…·æœ‰ä¸åŒå¯¹è±¡ã€ç©ºé—´å…³ç³»å’Œå±žæ€§ç»‘å®šçš„æç¤ºï¼Œä»Žè€Œä½¿æç¤ºé›†åŒ…å«çº¦600ä¸ªæç¤ºã€‚ä¸ºäº†è¿›ä¸€æ­¥è¯„ä¼°æˆ‘ä»¬æ¡†æž¶åœ¨æžå…¶å¤æ‚å’Œå›°éš¾æç¤ºä¸Šçš„èƒ½åŠ›ï¼Œæˆ‘ä»¬æ‰‹åŠ¨æ·»åŠ äº†SDXLæœªèƒ½ç”Ÿæˆçš„æç¤ºï¼Œå½¢æˆäº†ä¸€ä¸ªåŒ…å«200ä¸ªæç¤ºçš„å›°éš¾æç¤ºæ•°æ®é›†ã€‚ä¸ŽT2I-CompBenchï¼ˆHuangç­‰ï¼Œ[2023](#bib.bib10)ï¼‰ä¸­çš„å¤æ‚ç©ºé—´æç¤ºç±»ä¼¼ï¼Œæˆ‘ä»¬ç­–åˆ’çš„æ•°æ®é›†ä¸­çš„æ¯ä¸ªæç¤ºé€šå¸¸åŒ…å«ä¸¤ä¸ªå¯¹è±¡ï¼Œè¿™äº›å¯¹è±¡å…·æœ‰å„ç§ç©ºé—´å…³ç³»ï¼Œæ¯ä¸ªå¯¹è±¡çš„å±žæ€§ç»‘å®šä»Ž{é¢œè‰²ã€å½¢çŠ¶ã€çº¹ç†}ä¸­éšæœºé€‰æ‹©ã€‚
- en: Models & Baseline
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ¨¡åž‹ä¸ŽåŸºçº¿
- en: As a training-free framework, MuLan can be incorporated into any existing diffusion
    models. We evaluate two stable diffusion models with our framework, Stable Diffusion
    v1.4Â (Rombach etÂ al., [2022](#bib.bib17)) and the SOTA Stable Diffusion XLÂ (Podell
    etÂ al., [2023](#bib.bib16)). To verify the superiority of MuLan, we compare it
    with previous controllable generation methods and general T2I generation methods.
    Specifically, we evaluate Structure DiffusionÂ (Feng etÂ al., [2022](#bib.bib5)),
    PromptistÂ (Hao etÂ al., [2022](#bib.bib7)), the original Stable Diffusion v1.4,
    the original SDXL, and the recent SOTA diffusion model PixArt-$\alpha$Â (Chen etÂ al.,
    [2023](#bib.bib3)).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æž¶ï¼ŒMuLanå¯ä»¥è¢«æ•´åˆåˆ°ä»»ä½•çŽ°æœ‰çš„æ‰©æ•£æ¨¡åž‹ä¸­ã€‚æˆ‘ä»¬ç”¨æˆ‘ä»¬çš„æ¡†æž¶è¯„ä¼°äº†ä¸¤ä¸ªç¨³å®šæ‰©æ•£æ¨¡åž‹ï¼Œå³Stable Diffusion v1.4ï¼ˆRombachç­‰ï¼Œ[2022](#bib.bib17)ï¼‰å’ŒSOTA
    Stable Diffusion XLï¼ˆPodellç­‰ï¼Œ[2023](#bib.bib16)ï¼‰ã€‚ä¸ºäº†éªŒè¯MuLançš„ä¼˜è¶Šæ€§ï¼Œæˆ‘ä»¬å°†å…¶ä¸Žå…ˆå‰çš„å¯æŽ§ç”Ÿæˆæ–¹æ³•å’Œé€šç”¨T2Iç”Ÿæˆæ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è¯„ä¼°äº†Structure
    Diffusionï¼ˆFengç­‰ï¼Œ[2022](#bib.bib5)ï¼‰ã€Promptistï¼ˆHaoç­‰ï¼Œ[2022](#bib.bib7)ï¼‰ã€åŽŸå§‹çš„Stable
    Diffusion v1.4ã€åŽŸå§‹çš„SDXLä»¥åŠæœ€è¿‘çš„SOTAæ‰©æ•£æ¨¡åž‹PixArt-$\alpha$ï¼ˆChenç­‰ï¼Œ[2023](#bib.bib3)ï¼‰ã€‚
- en: Implementation Details
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å®žçŽ°ç»†èŠ‚
- en: MuLan use GPT-4Â (Achiam etÂ al., [2023](#bib.bib1)) as the LLM planner, and LLaVA-1.5Â (Liu
    etÂ al., [2023](#bib.bib13)) as the VLM checker to provide the feedback. We also
    conducted an ablation study to show the importance of the feedback control provided
    by the VLM and the effect of different VLMs. Moreover, we found the attention
    blocks utilized during the attention guidance are vital, which can be classified
    as near-input blocks, near-middle blocks, and near-output blocks. We utilize the
    near-middle blocks in our main experiments and also show the ablation results
    of different blocks.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: MuLanä½¿ç”¨GPT-4ï¼ˆAchiamç­‰ï¼Œ[2023](#bib.bib1)ï¼‰ä½œä¸ºLLMè§„åˆ’å™¨ï¼Œå¹¶ä½¿ç”¨LLaVA-1.5ï¼ˆLiuç­‰ï¼Œ[2023](#bib.bib13)ï¼‰ä½œä¸ºVLMæ£€æŸ¥å™¨æä¾›åé¦ˆã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†æ¶ˆèžç ”ç©¶ï¼Œä»¥å±•ç¤ºVLMæä¾›çš„åé¦ˆæŽ§åˆ¶çš„é‡è¦æ€§ä»¥åŠä¸åŒVLMçš„æ•ˆæžœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘çŽ°ï¼Œåœ¨æ³¨æ„åŠ›å¼•å¯¼è¿‡ç¨‹ä¸­ä½¿ç”¨çš„æ³¨æ„åŠ›å—æ˜¯è‡³å…³é‡è¦çš„ï¼Œè¿™äº›å—å¯ä»¥åˆ†ä¸ºè¿‘è¾“å…¥å—ã€è¿‘ä¸­é—´å—å’Œè¿‘è¾“å‡ºå—ã€‚æˆ‘ä»¬åœ¨ä¸»è¦å®žéªŒä¸­ä½¿ç”¨äº†è¿‘ä¸­é—´å—ï¼Œå¹¶å±•ç¤ºäº†ä¸åŒå—çš„æ¶ˆèžç»“æžœã€‚
- en: Evaluation
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è¯„ä¼°
- en: 'Since the prompt dataset contains texts with complex compositions, we design
    a questionnaire to comprehensively investigate the alignment between the generated
    image and the corresponding input text. The questionnaire is composed of three
    aspects - object completeness, correctness of attribute bindings, and correctness
    of spatial relationships. We only set two options for each question (Yes or No),
    without any ambiguity. For detailed questions and examples of the evaluation,
    please refer to AppendixÂ [D](#A4 "Appendix D More details on the evaluation questionnaire
    â€£ MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion"). For each
    aspect of the evaluation, we compute the percentage of answers with â€œYesâ€. Given
    the generated image, we assess the imageâ€™s quality using a questionnaire asking
    both the state-of-the-art multi-modal large language model (GPT-4VÂ (OpenAI, [2023](#bib.bib15)))
    and the human evaluator.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç”±äºŽæç¤ºæ•°æ®é›†åŒ…å«å¤æ‚æž„æˆçš„æ–‡æœ¬ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé—®å·æ¥å…¨é¢è°ƒæŸ¥ç”Ÿæˆå›¾åƒä¸Žç›¸åº”è¾“å…¥æ–‡æœ¬ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚é—®å·ç”±ä¸‰ä¸ªæ–¹é¢ç»„æˆâ€”â€”å¯¹è±¡å®Œæ•´æ€§ã€å±žæ€§ç»‘å®šçš„æ­£ç¡®æ€§å’Œç©ºé—´å…³ç³»çš„æ­£ç¡®æ€§ã€‚æ¯ä¸ªé—®é¢˜ä»…è®¾ç½®äº†ä¸¤ä¸ªé€‰é¡¹ï¼ˆæ˜¯æˆ–å¦ï¼‰ï¼Œæ²¡æœ‰ä»»ä½•æ¨¡ç³Šæ€§ã€‚æœ‰å…³è¯„ä¼°çš„è¯¦ç»†é—®é¢˜å’Œç¤ºä¾‹ï¼Œè¯·å‚è§é™„å½•[D](#A4
    "Appendix D More details on the evaluation questionnaire â€£ MuLan: Multimodal-LLM
    Agent for Progressive Multi-Object Diffusion")ã€‚å¯¹äºŽè¯„ä¼°çš„æ¯ä¸ªæ–¹é¢ï¼Œæˆ‘ä»¬è®¡ç®—äº†â€œæ˜¯â€ç­”æ¡ˆçš„ç™¾åˆ†æ¯”ã€‚ç»™å®šç”Ÿæˆçš„å›¾åƒï¼Œæˆ‘ä»¬ä½¿ç”¨é—®å·è¯„ä¼°å›¾åƒçš„è´¨é‡ï¼Œé—®å·ç”±æœ€æ–°çš„å¤šæ¨¡æ€å¤§åž‹è¯­è¨€æ¨¡åž‹
    (GPT-4V [OpenAI, 2023](#bib.bib15)) å’Œäººå·¥è¯„ä¼°è€…å®Œæˆã€‚'
- en: 4.1 Main Results and Analysis
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 ä¸»è¦ç»“æžœä¸Žåˆ†æž
- en: Results with GPT Evaluation
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: GPT è¯„ä¼°ç»“æžœ
- en: 'Given the generated image, we prompt GPT-4V to answer the questions about the
    image in the questionnaire, where each only focuses on one of the three aspects.
    The results for different methods and different base models are shown in TableÂ [1](#S4.T1
    "Table 1 â€£ 4 Experiments â€£ MuLan: Multimodal-LLM Agent for Progressive Multi-Object
    Diffusion"). The results show that our framework can achieve the best performance
    compared to different controllable generation methods and T2I generation methods.
    In particular, in the two â€˜harderâ€™ aspects - attribute bindings and spatial relationships,
    MuLan can surpass other methods by a large margin. More qualitative results can
    be found in FigureÂ [6](#S4.F6 "Figure 6 â€£ Results with GPT Evaluation â€£ 4.1 Main
    Results and Analysis â€£ 4 Experiments â€£ MuLan: Multimodal-LLM Agent for Progressive
    Multi-Object Diffusion") and AppendixÂ [E](#A5 "Appendix E More qualitative results
    â€£ MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion").'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç»™å®šç”Ÿæˆçš„å›¾åƒï¼Œæˆ‘ä»¬è®© GPT-4V åœ¨é—®å·ä¸­å›žç­”å…³äºŽå›¾åƒçš„é—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜åªå…³æ³¨ä¸‰ä¸ªæ–¹é¢ä¸­çš„ä¸€ä¸ªã€‚ä¸åŒæ–¹æ³•å’Œä¸åŒåŸºç¡€æ¨¡åž‹çš„ç»“æžœè§è¡¨æ ¼[1](#S4.T1
    "Table 1 â€£ 4 Experiments â€£ MuLan: Multimodal-LLM Agent for Progressive Multi-Object
    Diffusion")ã€‚ç»“æžœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ¡†æž¶åœ¨ä¸åŒçš„å¯æŽ§ç”Ÿæˆæ–¹æ³•å’Œ T2I ç”Ÿæˆæ–¹æ³•ä¸­è¡¨çŽ°æœ€ä½³ã€‚ç‰¹åˆ«æ˜¯åœ¨ä¸¤ä¸ªâ€œæ›´éš¾â€çš„æ–¹é¢â€”â€”å±žæ€§ç»‘å®šå’Œç©ºé—´å…³ç³»ï¼ŒMuLan
    å¯ä»¥å¤§å¹…è¶…è¶Šå…¶ä»–æ–¹æ³•ã€‚æ›´å¤šå®šæ€§ç»“æžœè§å›¾è¡¨[6](#S4.F6 "Figure 6 â€£ Results with GPT Evaluation â€£ 4.1 Main
    Results and Analysis â€£ 4 Experiments â€£ MuLan: Multimodal-LLM Agent for Progressive
    Multi-Object Diffusion")å’Œé™„å½•[E](#A5 "Appendix E More qualitative results â€£ MuLan:
    Multimodal-LLM Agent for Progressive Multi-Object Diffusion")ã€‚'
- en: '![Refer to caption](img/86f64cf6bc764564666a32e3e0e87822.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒæ ‡é¢˜](img/86f64cf6bc764564666a32e3e0e87822.png)'
- en: 'Figure 6: More qualitative examples of images generated by different methods
    on intricate prompts.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 6ï¼šä¸åŒæ–¹æ³•åœ¨å¤æ‚æç¤ºä¸‹ç”Ÿæˆçš„å›¾åƒçš„æ›´å¤šå®šæ€§ç¤ºä¾‹ã€‚
- en: Results with Human evaluation
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: äººå·¥è¯„ä¼°ç»“æžœ
- en: 'To further accurately evaluate the generated images about the alignments with
    human preferences, we further conduct a human evaluation by randomly sampling
    100 prompts from the prompt dataset. Similarly, we ask human evaluators to finish
    the questionnaire used in GPT evaluation. The results are shown in TableÂ [1](#S4.T1
    "Table 1 â€£ 4 Experiments â€£ MuLan: Multimodal-LLM Agent for Progressive Multi-Object
    Diffusion"), which indicates that our method can still achieve the best performance
    and is consistent with the GPT-4V evaluation results.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¸ºäº†æ›´å‡†ç¡®åœ°è¯„ä¼°ç”Ÿæˆå›¾åƒä¸Žäººç±»åå¥½çš„å¯¹é½æƒ…å†µï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡éšæœºæŠ½å– 100 ä¸ªæç¤ºä»Žæç¤ºæ•°æ®é›†ä¸­è¿›è¡Œäººå·¥è¯„ä¼°ã€‚ç±»ä¼¼åœ°ï¼Œæˆ‘ä»¬è®©äººå·¥è¯„ä¼°è€…å®Œæˆ GPT
    è¯„ä¼°ä¸­ä½¿ç”¨çš„é—®å·ã€‚ç»“æžœè§è¡¨æ ¼[1](#S4.T1 "Table 1 â€£ 4 Experiments â€£ MuLan: Multimodal-LLM Agent
    for Progressive Multi-Object Diffusion")ï¼Œè¡¨æ˜Žæˆ‘ä»¬çš„æ–¹æ³•ä»èƒ½å–å¾—æœ€ä½³æ€§èƒ½ï¼Œå¹¶ä¸”ä¸Ž GPT-4V çš„è¯„ä¼°ç»“æžœä¸€è‡´ã€‚'
- en: 4.2 Ablation study
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 æ¶ˆèžç ”ç©¶
- en: In this section, we show ablation results on the effect of the attention blocks
    during diffusion generation and the importance of the VLM feedback control in
    the proposed framework. 50 prompts are randomly sampled from the prompt dataset
    for all experiments in the ablation study.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚å±•ç¤ºäº†æ³¨æ„åŠ›å—åœ¨æ‰©æ•£ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ•ˆæžœæ¶ˆèžç»“æžœä»¥åŠåœ¨æè®®æ¡†æž¶ä¸­VLMåé¦ˆæŽ§åˆ¶çš„é‡è¦æ€§ã€‚æ‰€æœ‰æ¶ˆèžå®žéªŒä¸­çš„50ä¸ªæç¤ºä»Žæç¤ºæ•°æ®é›†ä¸­éšæœºæŠ½å–ã€‚
- en: Ablation on the attention blocks
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å¯¹æ³¨æ„åŠ›å—çš„æ¶ˆèžå®žéªŒ
- en: 'As we mentioned at the beginning of SectionÂ [4](#S4 "4 Experiments â€£ MuLan:
    Multimodal-LLM Agent for Progressive Multi-Object Diffusion"), there are three
    options for the attention blocks used for backward guidance, i.e., near-input
    blocks, near-middle blocks, and near-output blocks. We empirically found the near-middle
    blocks can achieve the best control and performance for the generation, which
    generally contains the richest semantics. Hence here we show the ablation results
    on different choices of the attention blocks. We utilize SD-v1.4 as the base model,
    and evaluate the performance of different attention blocks under our framework
    by GPT-4V. The results are shown in TableÂ [2](#S4.T2 "Table 2 â€£ Ablation on the
    attention blocks â€£ 4.2 Ablation study â€£ 4 Experiments â€£ MuLan: Multimodal-LLM
    Agent for Progressive Multi-Object Diffusion"), which indicates the diffusion
    generation with near-middle blocks can achieve much better results compared to
    the other two options.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ­£å¦‚æˆ‘ä»¬åœ¨ç¬¬[4](#S4 "4 Experiments â€£ MuLan: Multimodal-LLM Agent for Progressive
    Multi-Object Diffusion")èŠ‚å¼€å§‹æ—¶æåˆ°çš„ï¼Œå¯¹äºŽç”¨äºŽå‘åŽæŒ‡å¯¼çš„æ³¨æ„åŠ›å—æœ‰ä¸‰ç§é€‰æ‹©ï¼Œå³near-inputå—ã€near-middleå—å’Œnear-outputå—ã€‚æˆ‘ä»¬ç»éªŒæ€§åœ°å‘çŽ°ï¼Œnear-middleå—å¯ä»¥å®žçŽ°æœ€ä½³çš„æŽ§åˆ¶å’Œç”Ÿæˆæ€§èƒ½ï¼Œè¿™é€šå¸¸åŒ…å«æœ€ä¸°å¯Œçš„è¯­ä¹‰ã€‚å› æ­¤ï¼Œè¿™é‡Œå±•ç¤ºäº†ä¸åŒé€‰æ‹©çš„æ³¨æ„åŠ›å—çš„æ¶ˆèžç»“æžœã€‚æˆ‘ä»¬ä½¿ç”¨SD-v1.4ä½œä¸ºåŸºç¡€æ¨¡åž‹ï¼Œå¹¶é€šè¿‡GPT-4Vè¯„ä¼°åœ¨æˆ‘ä»¬æ¡†æž¶ä¸‹ä¸åŒæ³¨æ„åŠ›å—çš„æ€§èƒ½ã€‚ç»“æžœè§è¡¨[2](#S4.T2
    "Table 2 â€£ Ablation on the attention blocks â€£ 4.2 Ablation study â€£ 4 Experiments
    â€£ MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion")ï¼Œç»“æžœè¡¨æ˜Žï¼Œä½¿ç”¨near-middleå—çš„æ‰©æ•£ç”Ÿæˆç›¸æ¯”å…¶ä»–ä¸¤ä¸ªé€‰é¡¹å¯ä»¥å–å¾—æ›´å¥½çš„æ•ˆæžœã€‚'
- en: 'Table 2: Ablation study on attention blocks with SD-v1.4 as the base model.
    â€œObjectsâ€, â€œAttributesâ€, and â€œSpatialâ€ denote Object completeness, Attribute bindings,
    and Spatial relationships. The results (evaluated by GPT-4VÂ (OpenAI, [2023](#bib.bib15)))
    show that near-middle attention blocks perform the best for attention guidance.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨2ï¼šä»¥SD-v1.4ä½œä¸ºåŸºç¡€æ¨¡åž‹çš„æ³¨æ„åŠ›å—æ¶ˆèžç ”ç©¶ã€‚â€œObjectsâ€ã€â€œAttributesâ€å’Œâ€œSpatialâ€åˆ†åˆ«è¡¨ç¤ºå¯¹è±¡å®Œæ•´æ€§ã€å±žæ€§ç»‘å®šå’Œç©ºé—´å…³ç³»ã€‚ç»“æžœï¼ˆç”±GPT-4Vè¯„ä¼°ï¼ˆOpenAIï¼Œ[2023](#bib.bib15)ï¼‰ï¼‰æ˜¾ç¤ºï¼Œnear-middleæ³¨æ„åŠ›å—åœ¨æ³¨æ„åŠ›æŒ‡å¯¼æ–¹é¢è¡¨çŽ°æœ€ä½³ã€‚
- en: '| Guidance | Objects | Attributes | Spatial | Overall |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Guidance | Objects | Attributes | Spatial | Overall |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| near-input | 83.67% | 55.10% | 14.29% | 58.37% |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| near-input | 83.67% | 55.10% | 14.29% | 58.37% |'
- en: '| near-middle | 97.96% | 80.61% | 30.61% | 77.55% |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| near-middle | 97.96% | 80.61% | 30.61% | 77.55% |'
- en: '| near-output | 72.45% | 45.92% | 22.45% | 51.84% |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| near-output | 72.45% | 45.92% | 22.45% | 51.84% |'
- en: Ablation on the VLM feedback control
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å¯¹VLMåé¦ˆæŽ§åˆ¶çš„æ¶ˆèžå®žéªŒ
- en: 'The VLM feedback control is a key componenet in MuLan to provide feedback and
    adjust the generation process to ensure the every stageâ€™s correct generation.
    Here, we show the importance of the feedback by removing feedback control from
    the whole framework. As shown in TableÂ [3](#S4.T3 "Table 3 â€£ Ablation on the VLM
    feedback control â€£ 4.2 Ablation study â€£ 4 Experiments â€£ MuLan: Multimodal-LLM
    Agent for Progressive Multi-Object Diffusion"), after removing the VLM, the results
    would be much worse. It is because there is no guarantee or adaptive adjustment
    for each generation stage, which verifies that the feedback control provided by
    the VLM is essential to handle complex prompts. Moreover, we also test MuLanâ€™s
    compatibility with different VLMs. As shown in TableÂ [4](#S4.T4 "Table 4 â€£ Ablation
    on the VLM feedback control â€£ 4.2 Ablation study â€£ 4 Experiments â€£ MuLan: Multimodal-LLM
    Agent for Progressive Multi-Object Diffusion"), we compare the Mulanâ€™s performance
    using different VLMs including LLaVA-1.5Â (Liu etÂ al., [2023](#bib.bib13)), GPT-4VÂ (OpenAI,
    [2023](#bib.bib15)), and Gemini-ProÂ (Team etÂ al., [2023](#bib.bib21)). The results
    show that MuLan could still maintain a good performance with different choices
    of the VLM and achieve good compatibility.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 'VLMåé¦ˆæŽ§åˆ¶æ˜¯MuLanä¸­çš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼Œç”¨äºŽæä¾›åé¦ˆå¹¶è°ƒæ•´ç”Ÿæˆè¿‡ç¨‹ï¼Œä»¥ç¡®ä¿æ¯ä¸ªé˜¶æ®µçš„æ­£ç¡®ç”Ÿæˆã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬é€šè¿‡ä»Žæ•´ä¸ªæ¡†æž¶ä¸­ç§»é™¤åé¦ˆæŽ§åˆ¶æ¥å±•ç¤ºåé¦ˆçš„é‡è¦æ€§ã€‚å¦‚è¡¨[3](#S4.T3
    "Table 3 â€£ Ablation on the VLM feedback control â€£ 4.2 Ablation study â€£ 4 Experiments
    â€£ MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion")æ‰€ç¤ºï¼Œç§»é™¤VLMåŽï¼Œç»“æžœä¼šå˜å¾—æ›´å·®ã€‚è¿™æ˜¯å› ä¸ºæ²¡æœ‰å¯¹æ¯ä¸ªç”Ÿæˆé˜¶æ®µè¿›è¡Œä¿è¯æˆ–è‡ªé€‚åº”è°ƒæ•´ï¼Œè¿™éªŒè¯äº†VLMæä¾›çš„åé¦ˆæŽ§åˆ¶å¯¹äºŽå¤„ç†å¤æ‚æç¤ºçš„é‡è¦æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æµ‹è¯•äº†MuLanä¸Žä¸åŒVLMçš„å…¼å®¹æ€§ã€‚å¦‚è¡¨[4](#S4.T4
    "Table 4 â€£ Ablation on the VLM feedback control â€£ 4.2 Ablation study â€£ 4 Experiments
    â€£ MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion")æ‰€ç¤ºï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†MuLanä½¿ç”¨ä¸åŒVLMçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬LLaVA-1.5Â (åˆ˜ç­‰ï¼Œ[2023](#bib.bib13))ã€GPT-4VÂ (OpenAIï¼Œ[2023](#bib.bib15))å’ŒGemini-ProÂ (å›¢é˜Ÿç­‰ï¼Œ[2023](#bib.bib21))ã€‚ç»“æžœè¡¨æ˜Žï¼ŒMuLanåœ¨ä¸åŒVLMçš„é€‰æ‹©ä¸‹ä»èƒ½ä¿æŒè‰¯å¥½çš„æ€§èƒ½ï¼Œå¹¶å®žçŽ°è‰¯å¥½çš„å…¼å®¹æ€§ã€‚'
- en: 'Table 3: Ablation study comparing MuLan with vs. without VLM feedback control,
    using SD-v1.4 as the diffusion model and GPT-4 as the judge in evaluations. It
    indicates that feedback control can significantly improve the performance.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨3ï¼šæ¯”è¾ƒMuLanæœ‰æ— VLMåé¦ˆæŽ§åˆ¶çš„æ¶ˆèžç ”ç©¶ï¼Œä½¿ç”¨SD-v1.4ä½œä¸ºæ‰©æ•£æ¨¡åž‹ï¼ŒGPT-4ä½œä¸ºè¯„ä¼°åˆ¤æ–­ã€‚ç»“æžœè¡¨æ˜Žï¼Œåé¦ˆæŽ§åˆ¶å¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚
- en: '| MuLan | Objects | Attributes | Spatial | Overall |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| MuLan | å¯¹è±¡ | å±žæ€§ | ç©ºé—´ | æ€»ä½“ |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| w/ Feedback | 97.96% | 80.61% | 30.61% | 77.55% |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| æœ‰åé¦ˆ | 97.96% | 80.61% | 30.61% | 77.55% |'
- en: '| w/o Feedback | 81.63% | 59.18% | 18.37% | 60.00% |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| æ— åé¦ˆ | 81.63% | 59.18% | 18.37% | 60.00% |'
- en: 'Table 4: Ablation study of the VLM used in MuLan, using SD-v1.4 as the diffusion
    model and GPT-4 as the judge in evaluations. The results show that the choice
    of the VLM would not affect the overall performance too much.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨4ï¼šMuLanä¸­ä½¿ç”¨çš„VLMçš„æ¶ˆèžç ”ç©¶ï¼Œä½¿ç”¨SD-v1.4ä½œä¸ºæ‰©æ•£æ¨¡åž‹ï¼ŒGPT-4ä½œä¸ºè¯„ä¼°åˆ¤æ–­ã€‚ç»“æžœæ˜¾ç¤ºï¼ŒVLMçš„é€‰æ‹©ä¸ä¼šå¯¹æ€»ä½“æ€§èƒ½äº§ç”Ÿå¤ªå¤§å½±å“ã€‚
- en: '| VLM in MuLan | Objects | Attributes | Spatial | Overall |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| MuLanä¸­çš„VLM | å¯¹è±¡ | å±žæ€§ | ç©ºé—´ | æ€»ä½“ |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| LLaVA-1.5Â (Liu etÂ al., [2023](#bib.bib13)) | 97.96% | 80.61% | 30.61% | 77.55%
    |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| LLaVA-1.5Â (åˆ˜ç­‰ï¼Œ[2023](#bib.bib13)) | 97.96% | 80.61% | 30.61% | 77.55% |'
- en: '| GPT-4VÂ (OpenAI, [2023](#bib.bib15)) | 95.92% | 80.61% | 28.57% | 76.33% |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4VÂ (OpenAIï¼Œ[2023](#bib.bib15)) | 95.92% | 80.61% | 28.57% | 76.33% |'
- en: '| Gemini-ProÂ (Team etÂ al., [2023](#bib.bib21)) | 95.92% | 83.67% | 38.78% |
    79.59% |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-ProÂ (å›¢é˜Ÿç­‰ï¼Œ[2023](#bib.bib21)) | 95.92% | 83.67% | 38.78% | 79.59% |'
- en: 5 Conclusions and Limitations
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 ç»“è®ºä¸Žå±€é™æ€§
- en: In this paper, we propose a training-free multimodal-LLM agent (MuLan) to progressively
    generate objects contained in the complicated input prompt with closed-loop feedback
    control, achieving better and more precise control on the whole generation process.
    By first decomposing the complicated prompt into easier sub-tasks, our method
    takes turns to deal with each object, conditioned on the previous one. The VLM
    checker further provides a guarantee with feedback control and adaptive adjustment
    for correct generation at each stage. Extensive experiments demonstrate the superiority
    of MuLan over previous methods, showing the potential of MuLan as a new paradigm
    of controllable diffusion generation. However, there are still limitations to
    be further addressed in the future work. Since the whole generation contains multiple
    stages, depending on the number of objects, it will take a longer time than a
    one-stage generation approach. On the other hand, the LLM planner may mistakenly
    parse the input prompt which results in incorrect decomposition. This could be
    addressed by first re-writing the input prompt by the LLM to facilitate later
    processing.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„å¤šæ¨¡æ€LLMä»£ç†ï¼ˆMuLanï¼‰ï¼Œä»¥é—­çŽ¯åé¦ˆæŽ§åˆ¶é€æ­¥ç”Ÿæˆå¤æ‚è¾“å…¥æç¤ºä¸­åŒ…å«çš„å¯¹è±¡ï¼Œå®žçŽ°å¯¹æ•´ä¸ªç”Ÿæˆè¿‡ç¨‹çš„æ›´å¥½ã€æ›´ç²¾ç¡®çš„æŽ§åˆ¶ã€‚é€šè¿‡é¦–å…ˆå°†å¤æ‚æç¤ºåˆ†è§£ä¸ºæ›´ç®€å•çš„å­ä»»åŠ¡ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è½®æµå¤„ç†æ¯ä¸ªå¯¹è±¡ï¼Œä¾èµ–äºŽä¹‹å‰çš„å¯¹è±¡ã€‚VLMæ£€æŸ¥å™¨è¿›ä¸€æ­¥æä¾›åé¦ˆæŽ§åˆ¶å’Œè‡ªé€‚åº”è°ƒæ•´çš„ä¿éšœï¼Œä»¥ç¡®ä¿æ¯ä¸ªé˜¶æ®µçš„æ­£ç¡®ç”Ÿæˆã€‚å¤§é‡å®žéªŒè¡¨æ˜Žï¼ŒMuLanç›¸è¾ƒäºŽä»¥å¾€çš„æ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§ï¼Œå±•ç¤ºäº†MuLanä½œä¸ºä¸€ç§æ–°åž‹å¯æŽ§æ‰©æ•£ç”ŸæˆèŒƒå¼çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œä»ç„¶å­˜åœ¨æœªæ¥å·¥ä½œä¸­éœ€è¿›ä¸€æ­¥è§£å†³çš„å±€é™æ€§ã€‚ç”±äºŽæ•´ä¸ªç”Ÿæˆè¿‡ç¨‹åŒ…å«å¤šä¸ªé˜¶æ®µï¼Œä¾èµ–äºŽå¯¹è±¡æ•°é‡ï¼Œå®ƒå°†æ¯”å•é˜¶æ®µç”Ÿæˆæ–¹æ³•èŠ±è´¹æ›´é•¿çš„æ—¶é—´ã€‚å¦ä¸€æ–¹é¢ï¼ŒLLMè§„åˆ’å™¨å¯èƒ½ä¼šé”™è¯¯è§£æžè¾“å…¥æç¤ºï¼Œå¯¼è‡´ä¸æ­£ç¡®çš„åˆ†è§£ã€‚è¿™å¯ä»¥é€šè¿‡é¦–å…ˆç”±LLMé‡å†™è¾“å…¥æç¤ºä»¥ä¿ƒè¿›åŽç»­å¤„ç†æ¥è§£å†³ã€‚
- en: Broader Impact
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ›´å¹¿æ³›çš„å½±å“
- en: Our work will bring significant advantages to both the research community focused
    on diffusion models and the practical application of T2I generation.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„å·¥ä½œå°†ä¸ºä¸“æ³¨äºŽæ‰©æ•£æ¨¡åž‹çš„ç ”ç©¶ç¤¾åŒºå’ŒT2Iç”Ÿæˆçš„å®žé™…åº”ç”¨å¸¦æ¥æ˜¾è‘—ä¼˜åŠ¿ã€‚
- en: In terms of the research community, we present a new and novel controllable
    image generation paradigm that demonstrates exceptional controllability and produces
    remarkable results even when tackling challenging tasks. This pioneering approach
    can offer valuable insights for future investigations into diffusion models.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: å°±ç ”ç©¶ç¤¾åŒºè€Œè¨€ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…¨æ–°ä¸”å…·æœ‰å¯æŽ§æ€§çš„å›¾åƒç”ŸæˆèŒƒå¼ï¼Œè¿™ä¸€æ–¹æ³•å±•çŽ°äº†å“è¶Šçš„å¯æŽ§æ€§ï¼Œå³ä½¿åœ¨é¢å¯¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡æ—¶ä¹Ÿèƒ½äº§ç”Ÿæ˜¾è‘—çš„æˆæžœã€‚è¿™ä¸€å¼€åˆ›æ€§çš„æ–¹æ³•èƒ½å¤Ÿä¸ºæœªæ¥å¯¹æ‰©æ•£æ¨¡åž‹çš„ç ”ç©¶æä¾›å®è´µçš„è§è§£ã€‚
- en: Regarding industrial applications, our method can be readily employed by T2I
    generation service providers to enhance the performance of their models. Moreover,
    the diffusion models operating within our framework are less likely to generate
    harmful content due to the meticulous control exerted at each generation stage.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºŽå·¥ä¸šåº”ç”¨ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥è¢«T2Iç”ŸæˆæœåŠ¡æä¾›å•†è½»æ¾é‡‡ç”¨ï¼Œä»¥æå‡ä»–ä»¬æ¨¡åž‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç”±äºŽåœ¨æ¯ä¸ªç”Ÿæˆé˜¶æ®µæ–½åŠ äº†ç»†è‡´çš„æŽ§åˆ¶ï¼Œæˆ‘ä»¬æ¡†æž¶å†…è¿è¡Œçš„æ‰©æ•£æ¨¡åž‹ç”Ÿæˆæœ‰å®³å†…å®¹çš„å¯èƒ½æ€§è¾ƒå°ã€‚
- en: References
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: Achiam etÂ al. (2023) Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya,
    I., Aleman, F.Â L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., etÂ al.
    Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*, 2023.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam ç­‰ï¼ˆ2023ï¼‰Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman,
    F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., ç­‰. Gpt-4æŠ€æœ¯æŠ¥å‘Šã€‚*arXiv
    é¢„å°æœ¬ arXiv:2303.08774*ï¼Œ2023ã€‚
- en: Betker etÂ al. (2023) Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li,
    L., Ouyang, L., Zhuang, J., Lee, J., Guo, Y., etÂ al. Improving image generation
    with better captions. *Computer Science. https://cdn. openai. com/papers/dall-e-3\.
    pdf*, 2(3), 2023.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Betker ç­‰ï¼ˆ2023ï¼‰Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang,
    L., Zhuang, J., Lee, J., Guo, Y., ç­‰. é€šè¿‡æ›´å¥½çš„æ ‡é¢˜æ”¹è¿›å›¾åƒç”Ÿæˆã€‚*è®¡ç®—æœºç§‘å­¦ã€‚ https://cdn.openai.com/papers/dall-e-3.pdf*ï¼Œ2(3)ï¼Œ2023ã€‚
- en: 'Chen etÂ al. (2023) Chen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang,
    Z., Kwok, J., Luo, P., Lu, H., etÂ al. Pixart-$alpha$: Fast training of diffusion
    transformer for photorealistic text-to-image synthesis. *arXiv preprint arXiv:2310.00426*,
    2023.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen ç­‰ï¼ˆ2023ï¼‰Chen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang, Z., Kwok,
    J., Luo, P., Lu, H., ç­‰. Pixart-$alpha$: å¿«é€Ÿè®­ç»ƒæ‰©æ•£å˜æ¢å™¨ä»¥å®žçŽ° photorealistic æ–‡æœ¬åˆ°å›¾åƒåˆæˆã€‚*arXiv
    é¢„å°æœ¬ arXiv:2310.00426*ï¼Œ2023ã€‚'
- en: Chen etÂ al. (2024) Chen, M., Laina, I., and Vedaldi, A. Training-free layout
    control with cross-attention guidance. In *Proceedings of the IEEE/CVF Winter
    Conference on Applications of Computer Vision*, pp.Â  5343â€“5353, 2024.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen ç­‰ï¼ˆ2024ï¼‰Chen, M., Laina, I., å’Œ Vedaldi, A. æ— éœ€è®­ç»ƒçš„å¸ƒå±€æŽ§åˆ¶ä¸Žäº¤å‰æ³¨æ„åŠ›å¼•å¯¼ã€‚åœ¨*IEEE/CVFè®¡ç®—æœºè§†è§‰åº”ç”¨å†¬å­£ä¼šè®®è®ºæ–‡é›†*ä¸­ï¼Œé¡µç 5343â€“5353ï¼Œ2024ã€‚
- en: Feng etÂ al. (2022) Feng, W., He, X., Fu, T.-J., Jampani, V., Akula, A., Narayana,
    P., Basu, S., Wang, X.Â E., and Wang, W.Â Y. Training-free structured diffusion
    guidance for compositional text-to-image synthesis. *arXiv preprint arXiv:2212.05032*,
    2022.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fengç­‰ï¼ˆ2022ï¼‰Feng, W., He, X., Fu, T.-J., Jampani, V., Akula, A., Narayana, P.,
    Basu, S., Wang, X. E., å’Œ Wang, W. Y. æ— éœ€è®­ç»ƒçš„ç»“æž„åŒ–æ‰©æ•£æŒ‡å¯¼ç”¨äºŽç»„åˆæ–‡æœ¬åˆ°å›¾åƒåˆæˆã€‚*arXivé¢„å°æœ¬ arXiv:2212.05032*ï¼Œ2022å¹´ã€‚
- en: 'Feng etÂ al. (2023) Feng, W., Zhu, W., Fu, T.-j., Jampani, V., Akula, A., He,
    X., Basu, S., Wang, X.Â E., and Wang, W.Â Y. Layoutgpt: Compositional visual planning
    and generation with large language models. *arXiv preprint arXiv:2305.15393*,
    2023.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fengç­‰ï¼ˆ2023ï¼‰Feng, W., Zhu, W., Fu, T.-j., Jampani, V., Akula, A., He, X., Basu,
    S., Wang, X. E., å’Œ Wang, W. Y. Layoutgpt: å¤§åž‹è¯­è¨€æ¨¡åž‹çš„ç»„åˆè§†è§‰è§„åˆ’ä¸Žç”Ÿæˆã€‚*arXivé¢„å°æœ¬ arXiv:2305.15393*ï¼Œ2023å¹´ã€‚'
- en: Hao etÂ al. (2022) Hao, Y., Chi, Z., Dong, L., and Wei, F. Optimizing prompts
    for text-to-image generation. *arXiv preprint arXiv:2212.09611*, 2022.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haoç­‰ï¼ˆ2022ï¼‰Hao, Y., Chi, Z., Dong, L., å’Œ Wei, F. ä¼˜åŒ–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æç¤ºã€‚*arXivé¢„å°æœ¬ arXiv:2212.09611*ï¼Œ2022å¹´ã€‚
- en: 'Hessel etÂ al. (2021) Hessel, J., Holtzman, A., Forbes, M., Bras, R.Â L., and
    Choi, Y. Clipscore: A reference-free evaluation metric for image captioning. *arXiv
    preprint arXiv:2104.08718*, 2021.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hesselç­‰ï¼ˆ2021ï¼‰Hessel, J., Holtzman, A., Forbes, M., Bras, R. L., å’Œ Choi, Y.
    Clipscore: ä¸€ç§æ— å‚è€ƒçš„å›¾åƒå­—å¹•è¯„ä»·æŒ‡æ ‡ã€‚*arXivé¢„å°æœ¬ arXiv:2104.08718*ï¼Œ2021å¹´ã€‚'
- en: Ho etÂ al. (2020) Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic
    models. *Advances in neural information processing systems*, 33:6840â€“6851, 2020.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoç­‰ï¼ˆ2020ï¼‰Ho, J., Jain, A., å’Œ Abbeel, P. åŽ»å™ªæ‰©æ•£æ¦‚çŽ‡æ¨¡åž‹ã€‚*ç¥žç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•*ï¼Œ33:6840â€“6851ï¼Œ2020å¹´ã€‚
- en: 'Huang etÂ al. (2023) Huang, K., Sun, K., Xie, E., Li, Z., and Liu, X. T2i-compbench:
    A comprehensive benchmark for open-world compositional text-to-image generation.
    *arXiv preprint arXiv:2307.06350*, 2023.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huangç­‰ï¼ˆ2023ï¼‰Huang, K., Sun, K., Xie, E., Li, Z., å’Œ Liu, X. T2i-compbench: å¼€æ”¾ä¸–ç•Œç»„åˆæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„å…¨é¢åŸºå‡†æµ‹è¯•ã€‚*arXivé¢„å°æœ¬
    arXiv:2307.06350*ï¼Œ2023å¹´ã€‚'
- en: 'Li etÂ al. (2023) Li, Y., Liu, H., Wu, Q., Mu, F., Yang, J., Gao, J., Li, C.,
    and Lee, Y.Â J. Gligen: Open-set grounded text-to-image generation. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp.Â  22511â€“22521,
    2023.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liç­‰ï¼ˆ2023ï¼‰Li, Y., Liu, H., Wu, Q., Mu, F., Yang, J., Gao, J., Li, C., å’Œ Lee,
    Y. J. Gligen: å¼€æ”¾é›†åŸºç¡€æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚è§*IEEE/CVFè®¡ç®—æœºè§†è§‰ä¸Žæ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†*ï¼Œç¬¬22511â€“22521é¡µï¼Œ2023å¹´ã€‚'
- en: 'Lian etÂ al. (2023) Lian, L., Li, B., Yala, A., and Darrell, T. Llm-grounded
    diffusion: Enhancing prompt understanding of text-to-image diffusion models with
    large language models. *arXiv preprint arXiv:2305.13655*, 2023.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lianç­‰ï¼ˆ2023ï¼‰Lian, L., Li, B., Yala, A., å’Œ Darrell, T. Llm-grounded diffusion:
    ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹å¢žå¼ºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡åž‹çš„æç¤ºç†è§£ã€‚*arXivé¢„å°æœ¬ arXiv:2305.13655*ï¼Œ2023å¹´ã€‚'
- en: Liu etÂ al. (2023) Liu, H., Li, C., Li, Y., and Lee, Y.Â J. Improved baselines
    with visual instruction tuning. *arXiv preprint arXiv:2310.03744*, 2023.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liuç­‰ï¼ˆ2023ï¼‰Liu, H., Li, C., Li, Y., å’Œ Lee, Y. J. é€šè¿‡è§†è§‰æŒ‡ä»¤è°ƒä¼˜æ”¹è¿›åŸºçº¿ã€‚*arXivé¢„å°æœ¬ arXiv:2310.03744*ï¼Œ2023å¹´ã€‚
- en: 'Nichol etÂ al. (2021) Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin,
    P., McGrew, B., Sutskever, I., and Chen, M. Glide: Towards photorealistic image
    generation and editing with text-guided diffusion models. *arXiv preprint arXiv:2112.10741*,
    2021.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nicholç­‰ï¼ˆ2021ï¼‰Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P.,
    McGrew, B., Sutskever, I., å’Œ Chen, M. Glide: é€šè¿‡æ–‡æœ¬å¼•å¯¼æ‰©æ•£æ¨¡åž‹è¿ˆå‘çœŸå®žæ„Ÿå›¾åƒç”Ÿæˆä¸Žç¼–è¾‘ã€‚*arXivé¢„å°æœ¬
    arXiv:2112.10741*ï¼Œ2021å¹´ã€‚'
- en: OpenAI (2023) OpenAI. Gpt-4v(ision) system card. 2023.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAIï¼ˆ2023ï¼‰OpenAI. Gpt-4v(ision)ç³»ç»Ÿå¡ã€‚2023å¹´ã€‚
- en: 'Podell etÂ al. (2023) Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn,
    T., MÃ¼ller, J., Penna, J., and Rombach, R. Sdxl: Improving latent diffusion models
    for high-resolution image synthesis. *arXiv preprint arXiv:2307.01952*, 2023.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Podellç­‰ï¼ˆ2023ï¼‰Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T.,
    MÃ¼ller, J., Penna, J., å’Œ Rombach, R. Sdxl: æ”¹è¿›æ½œåœ¨æ‰©æ•£æ¨¡åž‹ä»¥è¿›è¡Œé«˜åˆ†è¾¨çŽ‡å›¾åƒåˆæˆã€‚*arXivé¢„å°æœ¬ arXiv:2307.01952*ï¼Œ2023å¹´ã€‚'
- en: Rombach etÂ al. (2022) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and
    Ommer, B. High-resolution image synthesis with latent diffusion models. In *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, pp.Â  10684â€“10695,
    2022.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rombachç­‰ï¼ˆ2022ï¼‰Rombach, R., Blattmann, A., Lorenz, D., Esser, P., å’Œ Ommer, B.
    é«˜åˆ†è¾¨çŽ‡å›¾åƒåˆæˆä¸Žæ½œåœ¨æ‰©æ•£æ¨¡åž‹ã€‚è§*IEEE/CVFè®¡ç®—æœºè§†è§‰ä¸Žæ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†*ï¼Œç¬¬10684â€“10695é¡µï¼Œ2022å¹´ã€‚
- en: Saharia etÂ al. (2022) Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J.,
    Denton, E.Â L., Ghasemipour, K., GontijoÂ Lopes, R., KaragolÂ Ayan, B., Salimans,
    T., etÂ al. Photorealistic text-to-image diffusion models with deep language understanding.
    *Advances in Neural Information Processing Systems*, 35:36479â€“36494, 2022.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sahariaç­‰ï¼ˆ2022ï¼‰Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton,
    E. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., ç­‰.
    å…·æœ‰æ·±åº¦è¯­è¨€ç†è§£çš„çœŸå®žæ„Ÿæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡åž‹ã€‚*ç¥žç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•*ï¼Œ35:36479â€“36494ï¼Œ2022å¹´ã€‚
- en: Sohl-Dickstein etÂ al. (2015) Sohl-Dickstein, J., Weiss, E., Maheswaranathan,
    N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics.
    In *International conference on machine learning*, pp.Â  2256â€“2265\. PMLR, 2015.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sohl-Dickstein ç­‰ï¼ˆ2015ï¼‰Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., å’Œ
    Ganguli, S. ä½¿ç”¨éžå¹³è¡¡çƒ­åŠ›å­¦çš„æ·±åº¦æ— ç›‘ç£å­¦ä¹ ã€‚åœ¨*å›½é™…æœºå™¨å­¦ä¹ ä¼šè®®*ï¼Œç¬¬2256â€“2265é¡µã€‚PMLRï¼Œ2015ã€‚
- en: Song etÂ al. (2020) Song, Y., Sohl-Dickstein, J., Kingma, D.Â P., Kumar, A., Ermon,
    S., and Poole, B. Score-based generative modeling through stochastic differential
    equations. *arXiv preprint arXiv:2011.13456*, 2020.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song ç­‰ï¼ˆ2020ï¼‰Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S.,
    å’Œ Poole, B. åŸºäºŽåˆ†æ•°çš„ç”Ÿæˆå»ºæ¨¡é€šè¿‡éšæœºå¾®åˆ†æ–¹ç¨‹ã€‚*arXiv é¢„å°æœ¬ arXiv:2011.13456*ï¼Œ2020ã€‚
- en: 'Team etÂ al. (2023) Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B.,
    Yu, J., Soricut, R., Schalkwyk, J., Dai, A.Â M., Hauth, A., etÂ al. Gemini: a family
    of highly capable multimodal models. *arXiv preprint arXiv:2312.11805*, 2023.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Team ç­‰ï¼ˆ2023ï¼‰Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J.,
    Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., ç­‰ã€‚Gemini: ä¸€ç³»åˆ—é«˜èƒ½åŠ›çš„å¤šæ¨¡æ€æ¨¡åž‹ã€‚*arXiv
    é¢„å°æœ¬ arXiv:2312.11805*ï¼Œ2023ã€‚'
- en: Appendix A Detailed prompt template of the global planning by the LLM
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é™„å½• A LLM å…¨çƒè§„åˆ’çš„è¯¦ç»†æç¤ºæ¨¡æ¿
- en: 'As stated in SectionÂ [3.3](#S3.SS3 "3.3 Prompt Decomposition by LLM Planning
    â€£ 3 Multimodal-LLM Agent (MuLan) â€£ MuLan: Multimodal-LLM Agent for Progressive
    Multi-Object Diffusion"), MuLan first conduct the global planning to decompose
    the input prompts into $N$ objects before the whole generation process. To this
    end, given the input prompt p, we prompt the LLM using the following template:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¦‚[3.3èŠ‚](#S3.SS3 "3.3 Prompt Decomposition by LLM Planning â€£ 3 Multimodal-LLM
    Agent (MuLan) â€£ MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion")æ‰€è¿°ï¼ŒMuLan
    é¦–å…ˆè¿›è¡Œå…¨çƒè§„åˆ’ï¼Œå°†è¾“å…¥æç¤ºåˆ†è§£ä¸º $N$ ä¸ªå¯¹è±¡ï¼Œç„¶åŽå†è¿›è¡Œæ•´ä¸ªç”Ÿæˆè¿‡ç¨‹ã€‚ä¸ºæ­¤ï¼Œç»™å®šè¾“å…¥æç¤º pï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹æ¨¡æ¿æç¤º LLMï¼š'
- en: 'You are an excellent painter. I will give you some descriptions. Your task
    is to turn the description into a painting. You only need to list the objects
    in the description by painting order, from left to right, from down to top. Do
    not list additional information other than the objects mentioned in the description.
    Description: {p}.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ æ˜¯ä¸€ä¸ªå‡ºè‰²çš„ç”»å®¶ã€‚æˆ‘ä¼šç»™ä½ ä¸€äº›æè¿°ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†æè¿°è½¬åŒ–ä¸ºä¸€å¹…ç”»ã€‚ä½ åªéœ€æŒ‰ç»˜ç”»é¡ºåºåˆ—å‡ºæè¿°ä¸­çš„å¯¹è±¡ï¼Œä»Žå·¦åˆ°å³ï¼Œä»Žä¸‹åˆ°ä¸Šã€‚ä¸è¦åˆ—å‡ºæè¿°ä¸­æœªæåŠçš„å…¶ä»–ä¿¡æ¯ã€‚æè¿°ï¼š{p}ã€‚
- en: In this way, the LLM will decompose the input prompt p following the pre-defined
    order.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒLLM å°†æŒ‰ç…§é¢„å®šä¹‰çš„é¡ºåºåˆ†è§£è¾“å…¥æç¤º pã€‚
- en: Appendix B Detailed prompt template of the local planning by the LLM
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é™„å½• B LLM å±€éƒ¨è§„åˆ’çš„è¯¦ç»†æç¤ºæ¨¡æ¿
- en: 'As stated in SectionÂ [3.4](#S3.SS4 "3.4 Conditional Single-Object Diffusion
    with LLM Planning and Attention Guidance â€£ 3 Multimodal-LLM Agent (MuLan) â€£ MuLan:
    Multimodal-LLM Agent for Progressive Multi-Object Diffusion"), the LLM is also
    utilized during the generation stage for local planning of the objectâ€™s rough
    position and the object counting.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¦‚[3.4èŠ‚](#S3.SS4 "3.4 Conditional Single-Object Diffusion with LLM Planning
    and Attention Guidance â€£ 3 Multimodal-LLM Agent (MuLan) â€£ MuLan: Multimodal-LLM
    Agent for Progressive Multi-Object Diffusion")æ‰€è¿°ï¼ŒLLM è¿˜åœ¨ç”Ÿæˆé˜¶æ®µç”¨äºŽå¯¹è±¡ç²—ç•¥ä½ç½®çš„å±€éƒ¨è§„åˆ’å’Œå¯¹è±¡è®¡æ•°ã€‚'
- en: 'For the rough position $\texttt{opt}_{1}$ planning of the first object, we
    utilize the following template:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºŽç¬¬ä¸€ä¸ªå¯¹è±¡çš„ç²—ç•¥ä½ç½®$\texttt{opt}_{1}$è§„åˆ’ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹æ¨¡æ¿ï¼š
- en: 'You are an excellent painter. I will give you some descriptions. Your task
    is to turn the description into a painting. Now given the description: {p}. If
    I want to paint the {$\texttt{obj}_{1}$}? Choose from left, right, top, and bottom.
    You can make reasonable guesses. Give one answer.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ æ˜¯ä¸€ä¸ªå‡ºè‰²çš„ç”»å®¶ã€‚æˆ‘ä¼šç»™ä½ ä¸€äº›æè¿°ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†æè¿°è½¬åŒ–ä¸ºä¸€å¹…ç”»ã€‚çŽ°åœ¨ç»™å‡ºçš„æè¿°æ˜¯ï¼š{p}ã€‚å¦‚æžœæˆ‘æƒ³ç”»{$\texttt{obj}_{1}$}ï¼Ÿä»Žå·¦ã€å³ã€ä¸Šå’Œä¸‹ä¸­é€‰æ‹©ã€‚ä½ å¯ä»¥åšå‡ºåˆç†çš„çŒœæµ‹ã€‚ç»™å‡ºä¸€ä¸ªç­”æ¡ˆã€‚
- en: Then the LLM is prompted to figure out the object number based on $\texttt{opt}_{1}$.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆ LLM å°†æ ¹æ®$\texttt{opt}_{1}$æ¥ç¡®å®šå¯¹è±¡æ•°é‡ã€‚
- en: 'If $\texttt{opt}_{1}=\texttt{left}$ is:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æžœ$\texttt{opt}_{1}=\texttt{left}$æ˜¯ï¼š
- en: 'You are an excellent painter. I will give you some descriptions. Your task
    is to turn the description into a painting. Now given the description: {p}. How
    many non-overlapping objects are there in the horizontal direction? ONLY give
    the final number.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ æ˜¯ä¸€ä¸ªå‡ºè‰²çš„ç”»å®¶ã€‚æˆ‘ä¼šç»™ä½ ä¸€äº›æè¿°ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†æè¿°è½¬åŒ–ä¸ºä¸€å¹…ç”»ã€‚çŽ°åœ¨ç»™å‡ºçš„æè¿°æ˜¯ï¼š{p}ã€‚åœ¨æ°´å¹³æ–¹å‘ä¸Šæœ‰å¤šå°‘ä¸ªä¸é‡å çš„å¯¹è±¡ï¼Ÿä»…ç»™å‡ºæœ€ç»ˆçš„æ•°å­—ã€‚
- en: 'If $\texttt{opt}_{1}=\texttt{bottom}$, the prompt template would be:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æžœ$\texttt{opt}_{1}=\texttt{bottom}$ï¼Œæç¤ºæ¨¡æ¿å°†æ˜¯ï¼š
- en: 'You are an excellent painter. I will give you some descriptions. Your task
    is to turn the description into a painting. Now given the description: {p}. How
    many non-overlapping objects are there in the vertical direction? ONLY give the
    final number.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ æ˜¯ä¸€ä½å‡ºè‰²çš„ç”»å®¶ã€‚æˆ‘å°†ç»™ä½ ä¸€äº›æè¿°ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†æè¿°è½¬åŒ–ä¸ºä¸€å¹…ç”»ã€‚çŽ°åœ¨ç»™å®šæè¿°ï¼š{p}ã€‚åœ¨åž‚ç›´æ–¹å‘ä¸Šæœ‰å¤šå°‘ä¸ªä¸é‡å çš„ç‰©ä½“ï¼Ÿåªç»™å‡ºæœ€ç»ˆçš„æ•°å­—ã€‚
- en: 'For the rough position $\texttt{opt}_{n}(n\geq 2)$, we utilize the following
    template:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºŽç²—ç•¥ä½ç½®$\texttt{opt}_{n}(n\geq 2)$ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹æ¨¡æ¿ï¼š
- en: 'You are an excellent painter. I will give you some descriptions. Your task
    is to turn the description into a painting. Now given the description: {p}. If
    I already have a painting that contains {$\{\texttt{obj}_{i}\}_{i=1}^{n-1}$}?
    Choose from left, right, above, bottom, and none of above. You can make reasonable
    guesses. Give one answer.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ æ˜¯ä¸€ä½å‡ºè‰²çš„ç”»å®¶ã€‚æˆ‘å°†ç»™ä½ ä¸€äº›æè¿°ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†æè¿°è½¬åŒ–ä¸ºä¸€å¹…ç”»ã€‚çŽ°åœ¨ç»™å®šæè¿°ï¼š{p}ã€‚å¦‚æžœæˆ‘å·²ç»æœ‰ä¸€å¹…åŒ…å«{$\{\texttt{obj}_{i}\}_{i=1}^{n-1}$}çš„ç”»ï¼Ÿé€‰æ‹©å·¦è¾¹ã€å³è¾¹ã€ä¸Šæ–¹ã€ä¸‹æ–¹æˆ–ä»¥ä¸Šéƒ½ä¸æ˜¯ã€‚ä½ å¯ä»¥åšå‡ºåˆç†çš„çŒœæµ‹ã€‚ç»™å‡ºä¸€ä¸ªç­”æ¡ˆã€‚
- en: 'Then we prompt the LLM to figure out the object number by:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åŽæˆ‘ä»¬æç¤ºLLMé€šè¿‡ä»¥ä¸‹æ–¹å¼ç¡®å®šç‰©ä½“æ•°é‡ï¼š
- en: 'You are an excellent painter. I will give you some descriptions. Your task
    is to turn the description into a painting. Now given the description: {p}. If
    I already have a painting that contains {$\{\texttt{obj}_{i}\}_{i=1}^{n-1}$}?
    Only give the final number.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ æ˜¯ä¸€ä½å‡ºè‰²çš„ç”»å®¶ã€‚æˆ‘å°†ç»™ä½ ä¸€äº›æè¿°ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†æè¿°è½¬åŒ–ä¸ºä¸€å¹…ç”»ã€‚çŽ°åœ¨ç»™å®šæè¿°ï¼š{p}ã€‚å¦‚æžœæˆ‘å·²ç»æœ‰ä¸€å¹…åŒ…å«{$\{\texttt{obj}_{i}\}_{i=1}^{n-1}$}çš„ç”»ï¼Ÿåªç»™å‡ºæœ€ç»ˆçš„æ•°å­—ã€‚
- en: Appendix C More details on the overlapping processing
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é™„å½•C é‡å å¤„ç†çš„æ›´å¤šç»†èŠ‚
- en: Given $\texttt{opt}_{n}$ can be computed as
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®š$\texttt{opt}_{n}$å¯ä»¥è®¡ç®—ä¸º
- en: '|  | $$\displaystyle\bm{M}_{n,i}=\begin{cases}\Big{(}\tilde{x}_{n-1}\cdot r_{i}+(\tilde{x}_{n-1}+\tilde{w}_{n-1})\cdot(1-r_{i}),\tilde{y}_{n-1},\tilde{w}_{n-1}\cdot
    r_{i}+\frac{W-\tilde{x}_{n-1}-\tilde{w}_{n-1}}{\texttt{Num}^{n}},\tilde{h}_{n-1}\Big{)},\text{if
    }\texttt{opt}^{n}=\texttt{right},\\ \\'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\displaystyle\bm{M}_{n,i}=\begin{cases}\Big{(}\tilde{x}_{n-1}\cdot r_{i}+(\tilde{x}_{n-1}+\tilde{w}_{n-1})\cdot(1-r_{i}),\tilde{y}_{n-1},\tilde{w}_{n-1}\cdot
    r_{i}+\frac{W-\tilde{x}_{n-1}-\tilde{w}_{n-1}}{\texttt{Num}^{n}},\tilde{h}_{n-1}\Big{)},\text{if
    }\texttt{opt}^{n}=\texttt{right},\\ \\'
- en: \Big{(}\tilde{x}_{n-1},\frac{(\texttt{Num}^{n}-1)\cdot\tilde{y}_{n-1}}{\texttt{Num}^{n}},\tilde{w}_{n-1},\tilde{h}_{n-1}\cdot
    r_{i}+\frac{\tilde{y}_{n-1}}{\texttt{Num}^{n}}\Big{)},\text{if }\texttt{opt}^{n}=\texttt{top}.\end{cases}$$
    |  | (8) |
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: \Big{(}\tilde{x}_{n-1},\frac{(\texttt{Num}^{n}-1)\cdot\tilde{y}_{n-1}}{\texttt{Num}^{n}},\tilde{w}_{n-1},\tilde{h}_{n-1}\cdot
    r_{i}+\frac{\tilde{y}_{n-1}}{\texttt{Num}^{n}}\Big{)},\text{if }\texttt{opt}^{n}=\texttt{top}.\end{cases}$$
    |  | (8) |
- en: 'The illustration for different overlapping ratios is shown in FigureÂ [7](#A3.F7
    "Figure 7 â€£ Appendix C More details on the overlapping processing â€£ MuLan: Multimodal-LLM
    Agent for Progressive Multi-Object Diffusion").'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¸åŒé‡å æ¯”çš„ç¤ºæ„å›¾è§å›¾[7](#A3.F7 "Figure 7 â€£ Appendix C More details on the overlapping
    processing â€£ MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion")ã€‚'
- en: '![Refer to caption](img/2e78c0fd2cf02630e7c5408947de4ee8.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜Ž](img/2e78c0fd2cf02630e7c5408947de4ee8.png)'
- en: 'Figure 7: Three candidate masks $\bm{M}_{n,i}$.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾7ï¼šä¸‰ä¸ªå€™é€‰æŽ©è†œ$\bm{M}_{n,i}$ã€‚
- en: Appendix D More details on the evaluation questionnaire
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é™„å½•D è¯„ä¼°é—®å·çš„æ›´å¤šç»†èŠ‚
- en: 'As shown in SectionÂ [4](#S4 "4 Experiments â€£ MuLan: Multimodal-LLM Agent for
    Progressive Multi-Object Diffusion"), we design a questionnaire to comprehensively
    evaluate the alignment between the generated image and the text by GPT-4VÂ (OpenAI,
    [2023](#bib.bib15)) and human, from three aspects - object completeness, correctness
    of attribute bindings, and correctness of spatial relationships. Specifically,
    given an image and a text prompt, for object completeness, we will evaluate if
    the image contains each single object in the prompt. If the object appears in
    the image, we will then judge if the attribute bindings of the object in the image
    align with the corresponding attribute bindings in the text prompt, to evaluate
    the correctness of attribute bindings. We will also ask GPT-4V or human to judge
    if the spatial relationships are correct and match the text, as the evaluation
    of the spatial relationships.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¦‚ç¬¬[4](#S4 "4 Experiments â€£ MuLan: Multimodal-LLM Agent for Progressive Multi-Object
    Diffusion")èŠ‚æ‰€ç¤ºï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä»½é—®å·æ¥å…¨é¢è¯„ä¼°ç”Ÿæˆçš„å›¾åƒä¸ŽGPT-4Vï¼ˆOpenAIï¼Œ[2023](#bib.bib15)ï¼‰å’Œäººç±»æ–‡æœ¬ä¹‹é—´çš„å¯¹é½æƒ…å†µï¼Œè¯„ä¼°å†…å®¹åŒ…æ‹¬
    - ç‰©ä½“å®Œæ•´æ€§ã€å±žæ€§ç»‘å®šçš„æ­£ç¡®æ€§å’Œç©ºé—´å…³ç³»çš„æ­£ç¡®æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œç»™å®šä¸€å¹…å›¾åƒå’Œä¸€ä¸ªæ–‡æœ¬æç¤ºï¼Œå¯¹äºŽç‰©ä½“å®Œæ•´æ€§ï¼Œæˆ‘ä»¬å°†è¯„ä¼°å›¾åƒæ˜¯å¦åŒ…å«æç¤ºä¸­çš„æ¯ä¸ªå•ç‹¬ç‰©ä½“ã€‚å¦‚æžœç‰©ä½“å‡ºçŽ°åœ¨å›¾åƒä¸­ï¼Œæˆ‘ä»¬å°†åˆ¤æ–­å›¾åƒä¸­ç‰©ä½“çš„å±žæ€§ç»‘å®šæ˜¯å¦ä¸Žæ–‡æœ¬æç¤ºä¸­çš„ç›¸åº”å±žæ€§ç»‘å®šä¸€è‡´ï¼Œä»¥è¯„ä¼°å±žæ€§ç»‘å®šçš„æ­£ç¡®æ€§ã€‚æˆ‘ä»¬è¿˜å°†è¦æ±‚GPT-4Væˆ–äººç±»åˆ¤æ–­ç©ºé—´å…³ç³»æ˜¯å¦æ­£ç¡®å¹¶ä¸Žæ–‡æœ¬åŒ¹é…ï¼Œä»¥è¯„ä¼°ç©ºé—´å…³ç³»ã€‚'
- en: 'Examples of the questionnaire for different images and text prompts are shown
    in FigureÂ [8](#A4.F8 "Figure 8 â€£ Appendix D More details on the evaluation questionnaire
    â€£ MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion").'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¸åŒå›¾åƒå’Œæ–‡æœ¬æç¤ºçš„é—®å·ç¤ºä¾‹å¦‚å›¾[8](#A4.F8 "å›¾ 8 â€£ é™„å½• D æ›´å¤šå…³äºŽè¯„ä¼°é—®å·çš„ç»†èŠ‚ â€£ MuLan: å¤šæ¨¡æ€LLMä»£ç†ç”¨äºŽæ¸è¿›å¼å¤šç›®æ ‡æ‰©æ•£")æ‰€ç¤ºã€‚'
- en: '![Refer to caption](img/d52a88ce7f6619b631c7a83f32248c86.png)![Refer to caption](img/f1cbcc625be0cab74a23aca5fcb67c04.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜Ž](img/d52a88ce7f6619b631c7a83f32248c86.png)![å‚è€ƒè¯´æ˜Ž](img/f1cbcc625be0cab74a23aca5fcb67c04.png)'
- en: 'Figure 8: Illustration of the questionnaire for the evaluation of generated
    images'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 8: ç”Ÿæˆå›¾åƒè¯„ä¼°é—®å·çš„ç¤ºä¾‹'
- en: Appendix E More qualitative results
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é™„å½• E æ›´å¤šå®šæ€§ç»“æžœ
- en: 'We show more examples of different methods in FigureÂ [9](#A5.F9 "Figure 9 â€£
    Appendix E More qualitative results â€£ MuLan: Multimodal-LLM Agent for Progressive
    Multi-Object Diffusion").'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 'æˆ‘ä»¬åœ¨å›¾[9](#A5.F9 "å›¾ 9 â€£ é™„å½• E æ›´å¤šå®šæ€§ç»“æžœ â€£ MuLan: å¤šæ¨¡æ€LLMä»£ç†ç”¨äºŽæ¸è¿›å¼å¤šç›®æ ‡æ‰©æ•£")ä¸­å±•ç¤ºäº†ä¸åŒæ–¹æ³•çš„æ›´å¤šç¤ºä¾‹ã€‚'
- en: '![Refer to caption](img/b1b08ddef8a8425379e74b9c4a58958d.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜Ž](img/b1b08ddef8a8425379e74b9c4a58958d.png)'
- en: 'Figure 9: More qualitative examples of images generated by different methods
    on intricate prompts.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 9: ä½¿ç”¨ä¸åŒæ–¹æ³•ç”Ÿæˆçš„å¤æ‚æç¤ºçš„æ›´å¤šå®šæ€§å›¾åƒç¤ºä¾‹ã€‚'
