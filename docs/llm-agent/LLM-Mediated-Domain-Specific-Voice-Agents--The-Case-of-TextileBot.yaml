- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:44:34'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.10590](https://ar5iv.labs.arxiv.org/html/2406.10590)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Shu Zhong Department of Computer Science, University College London Elia Gatti
    Department of Computer Science, University College London James Hardwick Department
    of Computer Science, University College London Miriam Ribul Materials Science
    Research Centre, Royal College of Art Youngjun Cho Department of Computer Science,
    University College London Marianna Obrist Department of Computer Science, University
    College London
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Developing domain-specific conversational agents (CAs) has been challenged by
    the need for extensive domain-focused data. Recent advancements in Large Language
    Models (LLMs) make them a viable option as a knowledge backbone. LLMs behaviour
    can be enhanced through prompting, instructing them to perform downstream tasks
    in a zero-shot fashion (i.e. without training). To this end, we incorporated structural
    knowledge into prompts and used prompted LLMs to build domain-specific voice-based
    CAs. We demonstrate this approach for the specific domain of textile circularity
    in form of the design, development, and evaluation of TextileBot. We present the
    design and development of the voice agent TextileBot and also the insights from
    an in-person user study (N=30) evaluating three variations of TextileBots. We
    analyse the human-agent interactions, combining quantitative and qualitative methods.
    Our results suggest that participants engaged in multi-turn conversations, and
    their perceptions of the three variation agents and respective interactions varied
    demonstrating the effectiveness of our prompt-based LLM approach. We discuss the
    dynamics of these interactions and their implications for designing future voice-based
    CAs.
  prefs: []
  type: TYPE_NORMAL
- en: The results show that our method’s potential for building domain-specific CAs.
    Furthermore, most participants engaged in multi-turn conversations, and their
    perceptions of the three voice agents and respective interactions varied demonstrating
    the effectiveness of our prompt-based LLM approach. We discuss the dynamics of
    these interactions and their implications for designing future voice-based CAs.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The potential for designing conversational interfaces through the use of pre-trained
    large language models (LLMs) has gained substantial attention [[106](#bib.bib106),
    [48](#bib.bib48), [56](#bib.bib56)]. These LLMs possess remarkable capabilities
    to comprehend human language, generate text in a human-like way, and execute various
    tasks with only a few text *prompts* at run time, even without any additional
    training [[28](#bib.bib28), [64](#bib.bib64), [79](#bib.bib79), [14](#bib.bib14),
    [72](#bib.bib72)]. A prompt is a piece of text input to the LLM to elicit a response.
    For instance, a prompt can be *“What is sustainable fashion? Please explain it
    to a 10-year-old kid.”*. This has significantly lowered the entry barriers for
    artificial intelligence (AI) access, allowing non-experts to interact with LLMs
    through text. However, designing such prompts manually poses a particular challenge,
    as accurate and concise encapsulation of the complex, domain-specific knowledge
    is needed to guide the model towards a specific domain. This is due to the inherent
    task-agnostic nature of vanilla LLMs ¹¹1Vanilla model refers to an LLM without
    fine-tuning or prompting., which are not fine-tuned for specific domains. Additionally,
    another drawback of pre-trained vanilla LLMs is their lack of conversational memory,
    which restricts interactions to a ‘single turn’ manner²²2A one-time exchange between
    a user and an AI system or another person in a conversation counts as one turn..
    This limitation affects the continuity and depth of user interactions, as these
    models cannot recall their previous inputs and outputs.
  prefs: []
  type: TYPE_NORMAL
- en: The human-computer interaction (HCI) community is seeing a rising emphasis on
    the development of voice-based conversational agents [[95](#bib.bib95), [9](#bib.bib9),
    [94](#bib.bib94), [40](#bib.bib40)]. Seaborn et al. found that only 13% of studies
    in voice-based human-agent interaction (vHAI) used an autonomous setup, fully
    controlled by participants [[84](#bib.bib84)]. This limited adoption is primarily
    due to the complexity of creating voice agents. The advent of LLMs now enables
    real-time conversational interactions with humans, eliminating the necessity for
    constructing intricate heuristic-based conversation rules from scratch. In addition,
    the development of domain-specific conversational agents has long been hindered
    by data scarcity [[7](#bib.bib7), [51](#bib.bib51)]. Collecting and annotating
    the data for these agents is an expensive and labour-intensive process that requires
    considerable resources [[27](#bib.bib27), [105](#bib.bib105), [37](#bib.bib37)].
    This has then necessitated exploring cost-effective approaches for developing
    domain-specific conversational agents. In essence, we are interested in how to
    efficiently combine the knowledge in general-purpose LLMs together with the structural
    domain knowledge from human experts.
  prefs: []
  type: TYPE_NORMAL
- en: 'To this end, we introduce an approach that uses LLMs in a zero-shot manner
    (i.e. without training), coupled with the knowledge from domain human experts.
    This approach also facilitates researchers, who may not be AI experts, in prototyping
    domain-specific conversational agents. We propose a novel three-phase method designed
    to transfer LLMs from task-agnostic to domain-specific voice-based CAs and offer
    personalized interactions. The first phase involves the creation of a Taxonomy-based
    Knowledge Structure Chain, a novel prompt generation method. We prompted LLMs
    as a knowledge foundation model instead of traditional method in building a dialogue
    system (see Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ LLM-Mediated Domain-Specific
    Voice Agents: The Case of TextileBot")). It eliminates the need for domain-specific
    data collection while allowing human experts to inject their structural knowledge
    into the LLM. The second phase involves a Prompt Refinement strategy that enables
    LLMs to interact with users with varying breadth and freedom, providing LLMs with
    different levels of domain-focusness and varying conversational styles. Finally,
    a System Optimization is introduced to facilitate LLMs engaging with users in
    continuous interactions, allowing the LLM to retain statefulness with memory,
    enabling long-last “multi-turn” conversations. Notably, these phases can be applied
    individually or in conjunction to boost CA’s design.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6f0b5d45bd6f82b565238ab08d7024ce.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Traditional
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a2856a03daa3b05a64ddab0add89d65a.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) LLM-based
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: (a) Traditional and (b) LLM-based conversational agents with voice
    inputs and outputs. The traditional agent has various components such as NLU,
    NLG and Dialogue database searching. In contrast, the LLM-based agent simply uses
    the database to generate prompts for the LLM, enabling a much simpler and easy-to-develop
    pipeline.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To showcase our approach, we present TextileBot, a voice interface that conveys
    the topic of *textiles circularity* developed on a Raspberry Pi. We evaluate TextileBot’s
    effectiveness through a subjective user study, focusing on a human-in-the-loop
    strategy to foster human-centered AI design. Success will be measured by the participants’
    ability to discern the unique CA characteristics crafted through our approach
    and the system’s proficiency in maintaining domain-specific, multi-turn conversations.
    This evaluation integrates both quantitative data and qualitative insights. Additionally,
    we uncovered the complex dynamics of these human-agent interactions and explored
    various facets of human behaviour, engagement, and responses. Our research address
    two timely questions: First, while LLMs as foundation models for general-purpose
    conversational agents have been studied [[106](#bib.bib106), [48](#bib.bib48),
    [56](#bib.bib56)], their application in creating domain-specific voice agents
    is less understood. Second, there is limited understanding of how humans perceive
    and interact domain-specific voiced-based CAs powered by prompted LLMs. These
    knowledge gaps stem from various intertwined factors including the inherent complexity
    of natural languages, technical challenges associated with text-to-speech integration,
    the ability to handle ambiguity and context in human speech, and the imperative
    need for effective user experience design to facilitate seamless and meaningful
    user interactions with the system [[9](#bib.bib9), [84](#bib.bib84)]. In summary,
    the main contributions of this paper are three-fold:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We develop a novel three-phase method that enables LLMs to transform from task-agnostic
    to domain-specific, adapting different conversational styles and incorporating
    memory for continues conversation. These phases can be used individually or in
    combination, depending on the task at hand.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We integrate the three-phase method into a design of a domain-specific voice-based
    device, i.e., TextileBot. This voice-based agent is custom-designed for the textile
    circularity domain, facilitating tailored conversations with consumers regarding
    circular economy practices within the textile industry.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We evaluated three variations of TextileBot through in-person interactions to
    assess our approach’s effectiveness and to better understand user interactions.
    We analyzed these human-agent interactions, providing qualitative descriptions
    and quantitative insights. This analysis aims to inform potential design improvements
    in the wider domain of AI-enabled voice interfaces.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Background and Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we explain our rationale for choosing textiles circularity
    as our applied domain. Following this, we give an overview of voice-based CAs
    and related literature focusing on human interaction with traditional heuristics-guided
    voice-based CAs. Subsequently, we introduce recent advancements in pre-trained
    LLMs and HCI research related to LLMs-mediated interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: '2.1 The Domain of Textiles Circularity: A Case for Voice Agents Design'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We choose to develop a conversational agent specifically for the *textiles circularity*
    domain to demonstrate our method effectively. Textiles circularity is circular
    economy for textiles, and it refers to the sustainable practice in the textile
    industry where materials are reused, recycled, or biodegraded to minimize waste
    and reduce environmental impact [[31](#bib.bib31)]. This domain offers diverse
    information and expertise from various areas, including fashion, home textiles,
    supply chain management, materials science, and manufacturing etc. The complexity
    and diversity of conversations within this domain make it an ideal choice for
    showcasing the capabilities of our CA.
  prefs: []
  type: TYPE_NORMAL
- en: Another main reason behind our focus on the textile industry is its significant
    contribution to global carbon emissions. In fact, the textile industry alone accounts
    for 10% of global carbon emissions, which is as much as the combined emissions
    from international flights and maritime shipping [[74](#bib.bib74)]. This alarming
    environmental impact highlights the urgent need for sustainable practices within
    the sector. The challenge of incorporating circularity, particularly in recycling
    of textile fibres into new textile fibres, is complex due to the broad spectrum
    of knowledge required. We anticipate that conversational agents, integrated with
    expert knowledge, will provide a more accessible mean of understanding and communicating
    the concept of textile circularity with the general public. We anticipate that
    this approach could also be adapted for educational purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, CAs are increasingly being utilized in the fashion retail sector
    for a variety of purposes [[5](#bib.bib5), [10](#bib.bib10)], offering significant
    opportunities to foster socially responsible behaviours. Among these, promoting
    sustainability communication as an integral component of business strategies stands
    out as a notable application [[23](#bib.bib23)]. We believe that our approach
    can bring social and economic benefits to the textiles circularity domain. For
    example, the conversational agent could be situated in a store, offering valuable
    insights into how consumer clothing choices might affect their well-being and
    the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Domain Specific Conversational Agents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Classic CAs typically comprise Natural Language Understanding (NLU) and Natural
    Language Generation (NLG) components, along with a database-driven dialogue management
    system [[5](#bib.bib5), [51](#bib.bib51)]. This dialogue system design can be
    broken down into various building blocks, namely dialogue database, dialogue searching
    and dialogue management. Building a dialogue system is a complex task requiring
    extensive domain knowledge and data. Alternatively, an end-to-end model can be
    trained using collected data, although this usually necessitates a large amount
    of training data to cover the different possible dialogues when deployed. In these
    approaches, the development of CAs is normally impeded by the lack of available
    data and the cost of annotating it [[65](#bib.bib65), [32](#bib.bib32), [7](#bib.bib7)].
  prefs: []
  type: TYPE_NORMAL
- en: 'This complexity can also hamper research in voice-based human-agent interaction
    (vHAI). While CA user interface is a popular topic in the HCI community, studies
    on domain focused CAs are relatively rare. Thus, this section focuses on a broader
    range of voice agents. Seaborn et al. [[84](#bib.bib84)] conducted a survey that
    identified four main methods for carrying out human voice interaction studies:
    autonomous setup, semi-autonomous setup, “Wizard of Oz” setup [[26](#bib.bib26)],
    and conversations under given scenarios - with respective usage rates of 13%,
    24%, 27%, and 33%. Notably, just 13% used an autonomous setup – a design where
    the system can operate without the involvement of an experimenter and the participants
    control the interactions. As mentioned earlier, creating fully automated CAs presents
    technical challenges (e.g. data scarcity and high monetary cost). These difficulties
    impede the comprehension of the human-agent interactions, thereby obstructing
    the design of effective autonomous CAs [[104](#bib.bib104), [106](#bib.bib106)].
    In addition, the evaluation of conversational agents is largely performed using
    crowdworkers [[51](#bib.bib51)]. These evaluations are also mostly conducted in
    non-goal-oriented conversations (a.k.a task-agnostic) [[92](#bib.bib92), [86](#bib.bib86)].
    In our work, we conducted an in-person study where participants directly controlled
    the voice agent interaction without mediation from experimenters. Moreover, our
    evaluation includes both non-goal-oriented tasks and goal-oriented tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1 Voice-Based Human-Agent Interaction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This work presents an LLM-mediated voice agent specialized in textiles circularity,
    designed to operate via a device designed by ourselves. It’s important to note
    that this agent differs from conventional voice-based devices such as Alexa and
    Google Home, which are categorized as voice assistants (VAs). These voice assistants
    are not domain-specific in terms of their scope and functionality [[80](#bib.bib80),
    [84](#bib.bib84)], domain-specific agents focus on specific areas with detailed,
    context-aware responses, while VAs provide a wide array of services such as weather
    updates. Multiple studies suggest that voice assistants often fail to meet user
    expectations due to limited understanding or response [[24](#bib.bib24), [42](#bib.bib42),
    [9](#bib.bib9)].
  prefs: []
  type: TYPE_NORMAL
- en: Researchers have studied voiced-based human-agent interaction (vHAI) [[94](#bib.bib94),
    [58](#bib.bib58), [9](#bib.bib9), [95](#bib.bib95), [40](#bib.bib40)]. Some studies
    have explored factors that affected users’ preference between voice and text inputs
    [[94](#bib.bib94), [69](#bib.bib69)], while others discussed how user experience
    might be improved through enriching the personalities of the conversational agent
    [[96](#bib.bib96), [22](#bib.bib22), [12](#bib.bib12), [25](#bib.bib25)]. Hoegen
    et al. [[41](#bib.bib41)] found that voice agents that can conduct naturalistic
    multi-turn dialogue and are aligned with the participants’ conversational style
    will increase user trust. Baughan et al. [[9](#bib.bib9)] used interviews and
    surveys to understand how voice assistant failures impact user trust and willingness
    to rely on them for future tasks. Haas et al. discovered that users prefer voice
    assistants to “keep it short” in their responses [[38](#bib.bib38)]. Also, Völkel
    et al. [[95](#bib.bib95)] presented a rule-based dialogue design to give voice
    assistants distinct personalities and asked users to rate their preferences. They
    found that there is a connection between user personality traits and their voice
    assistant preferences. The voice-based agents used in these studies have primarily
    followed canonical approaches that are mostly in a “Wizard of Oz” manner, or are
    manipulated by human. However, our work stands out as the first endeavour to explore
    how humans interact with LLM-mediated voice agents and utilizes prompting techniques
    to design agents with distinct personas, response manners, and conversational
    freedom. We also offer novel insights into LLM-mediated voice agents’ design and
    interaction possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Large Language Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Historically, NLP models have gone through a shift from a *fully supervised
    learning* paradigm, focusing on *feature engineering* (e.g. word identity [[52](#bib.bib52)])
    and *architecture engineering* (e.g. self-attention [[91](#bib.bib91)]), to a
    pre-train and fine-tune approach [[63](#bib.bib63)] with neural networks. Recently,
    the advent of pre-trained LLMs like GPT-3 has catalyzed a new *”pre-train and
    prompt”* paradigm [[72](#bib.bib72), [85](#bib.bib85), [63](#bib.bib63)].
  prefs: []
  type: TYPE_NORMAL
- en: In the *pre-train and fine-tune* paradigm, fixed architecture models are trained
    with a large corpus of text data, typically several billion words or more, such
    as books, articles, or conversations, to learn general-purpose features of a language,
    e.g. BART [[57](#bib.bib57)] and UniLM [[29](#bib.bib29)]. The *pre-trained* LLMs
    can then be adapted (e.g. fine-tuned) to a broad range of downstream tasks (such
    as machine translation, text entailment, sentiment analysis, etc.) with a task-specific
    loss function. Large Language Models (LLMs), such as BERT (Bidirectional Encoder
    Representations from Transformers) [[28](#bib.bib28)], RoBERTa [[64](#bib.bib64)],
    T5 [[79](#bib.bib79)], and GPT-3 (Generative Pre-training Transformer 3) [[14](#bib.bib14)],
    are now used as *foundation models* ³³3A model trained on a large corpus of data
    that can be adapted to a wide range of downstream tasks [[13](#bib.bib13)]. for
    downstream tasks in NLP, paving the way toward task-agnostic machine learning
    [[13](#bib.bib13)].
  prefs: []
  type: TYPE_NORMAL
- en: LLMs have experienced significant breakthroughs recently in terms of their ability
    to understand and generate human-like text [[63](#bib.bib63)]. GPT-3 [[14](#bib.bib14)],
    with 175 billion parameters, stands out with its ability to execute various text-related
    tasks with only a few text *prompts* at run time, even without any additional
    training. This is known as the (*zero-shot*) capability, as its paper title *“Language
    Models are Few-Shot Learners”* suggests [[14](#bib.bib14)]. This capability highlights
    an evolution in model application, moving towards a *“pre-train and prompt”* paradigm
    [[72](#bib.bib72), [85](#bib.bib85), [63](#bib.bib63)]. Consequently, considerable
    effort has been invested in the research of *prompt engineering*, which aims to
    design efficient prompts to guide LLMs to perform various downstream tasks [[14](#bib.bib14),
    [85](#bib.bib85)]. For instance, prompts such as *“What is material fibre? Explain
    to a fashion designer.”* and *“What is material fibre? Explain to a chemist.”*
    will generate different outputs. This also implies a substantial step toward lowering
    the barriers for AI non-experts to interact with LLMs for various tasks by using
    only prompts [[45](#bib.bib45)].
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.1 Human-LLM Interaction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The rise of prompting LLMs presents a promising alternative for CA design [[106](#bib.bib106),
    [48](#bib.bib48), [56](#bib.bib56), [13](#bib.bib13)]. HCI researchers have been
    increasingly interested in harnessing the power of LLMs and enabling a plethora
    of language-based interactive applications. Examples of such applications include
    creative writing [[54](#bib.bib54), [21](#bib.bib21), [17](#bib.bib17), [43](#bib.bib43),
    [16](#bib.bib16)], iterative query reformulation (e.g., question answering) [[98](#bib.bib98),
    [4](#bib.bib4)], writing code [[90](#bib.bib90), [8](#bib.bib8)], and creating
    novel user interfaces [[98](#bib.bib98), [100](#bib.bib100)].
  prefs: []
  type: TYPE_NORMAL
- en: However, most of the existing interaction frameworks focus on “single-turn”
    interaction, where a “turn” means one back-and-forth interaction on a specific
    topic; this can be either the user speaks, then the bot follows or vice-versa.
    In our work, we focus on a “multi-turn” and “continuous” interaction (dyadic),
    where the agent would need to react coherently and be able to memorise previous
    interaction rounds. One particular relevant literature to our work is from Zamfirescu-Pereira
    et al. [[106](#bib.bib106)]. They explored the use of prompting for fast CA design,
    specifically for text-based chatbots, and suggested that this method can achieve
    ”80%” of the user experience (UX) goal. However, the actual user perception and
    interactions with such CAs were not explored. In our work, we carefully designed
    our prompt templates and further carefully investigated the users’ perception
    and interaction using both qualitative and quantitative methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, the performance of LLMs has been widely evaluated using numerical
    metrics without incorporating human participants [[72](#bib.bib72), [60](#bib.bib60),
    [14](#bib.bib14)]. For instance, metrics, such as perplexity and BLEU (bilingual
    evaluation understudy) score [[73](#bib.bib73)], are popular for evaluating LLMs
    performance on downstream tasks. These evaluations lack human-in-the-loop. To
    better understand the quality of human-LLM interactions, Lee et al. [[55](#bib.bib55)]
    proposed the Human-AI Language-based Interaction Evaluation (HALIE) framework,
    utilizing interaction traces, and suggested novel metrics related to user experience
    and interaction quality for assessing the LLM’s capabilities. In our design, we
    adopted several important metrics (including Ease, Change, Enjoyment, Reuse and
    Accuracy, fully described in Section [5.1](#S5.SS1 "5.1 Study design and methods
    ‣ 5 Evaluation of TextileBot ‣ LLM-Mediated Domain-Specific Voice Agents: The
    Case of TextileBot")) from Lee et al. to facilitate human-in-the-loop evaluation
    for our LLM-mediated voice agent.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Prototyping Domain-Specific Voice Agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we present a zero-shot prototyping framework designed to enable
    a wider spectrum of users to prototype conversational agents (CAs) across various
    domains. Our novel method encompasses three distinct phases: (1) a Taxonomy-based
    Knowledge Structure Chain for effectively injecting domain knowledge, (2) a prompt
    refinement strategy *from task agnostic to domain specific*, and (3) a system
    optimization to equip LLMs with conversational memory enabling *continuous (multi-turn)*
    human-LLM interactions. Figure [1(b)](#S1.F1.sf2 "In Figure 1 ‣ 1 Introduction
    ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot") illustrates
    how our prompt-based LLM approach is different from the traditional conversational
    agent design. To demonstrate the practical application of this method, we present
    two implementations of our prompting approach within the context of textiles circularity,
    namely TextileBot-Expert and TextileBot-Assistant.'
  prefs: []
  type: TYPE_NORMAL
- en: '3.1 Phase 1: Taxonomy-based Knowledge Structure Chain'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3e8f3d03f071cc376948b5d83b5bf83c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Taxonomy-Based Knowledge Structure Chain. This process requires domain-specific
    expertise to construct or utilize pre-existing structured knowledge frameworks,
    eg. taxonomies. To craft the Taxonomy-bind prompts, in every single prompt (eg.
    Prompt 1, 2, 3…) the entity types are connected by a relationship. In each prompt,
    we define the entity and hierarchical relationship to structure the knowledge,
    and finally, all prompts are chained together.'
  prefs: []
  type: TYPE_NORMAL
- en: A key challenge of using LLMs as foundation models is that they can return false
    answers in situations where they are unsure how to respond to a query from a user,
    producing both “plausible-sounding and incorrect or nonsensical answers” [[70](#bib.bib70)].
    We address this issue by introducing *Taxonomy-based Knowledge Structure Chain*,
    which is a framework for designing a chain of prompts. Typically, a prompt consists
    of two parts - a template and a set of label words [[19](#bib.bib19)]. Taxonomies,
    as relational systems, efficiently organize knowledge by logically interconnecting
    entities, representing relationships [[53](#bib.bib53)]. While prompting LLMs
    only rely on plain sentences, taxonomies excel in generating precise keywords,
    thereby enhancing the relevance and accuracy of LLM responses. This method organizes
    prompts to align with the taxonomy’s structure using label words, enhancing the
    relevance and accuracy of LLM responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'We exemplify this approach using the TextileNet taxonomy [[108](#bib.bib108)].
    TextileNet’s hierarchy captures the relationships between general textile fibre
    categories, their subcategories, and specific fibre types, aiding in the creation
    of a *knowledge prompt*. For example, “cotton fibre” falls under “plant fibres”,
    which is a subset of “natural fibres”. This hierarchical organization of the taxonomy
    forms the basis of our *Taxonomy-based Knowledge Structure Chain*, systematically
    capturing the relationship (eg. subcategories, macro-types) among different entity
    types as illustrated in Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Phase 1: Taxonomy-based
    Knowledge Structure Chain ‣ 3 Prototyping Domain-Specific Voice Agents ‣ LLM-Mediated
    Domain-Specific Voice Agents: The Case of TextileBot").'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dialogues in our user study demonstrate the effectiveness of this approach
    as illustrated in Figure [3](#S3.F3 "Figure 3 ‣ 3.2 Phase 2: From General to Domain
    Specific through Prompt Refinements ‣ 3 Prototyping Domain-Specific Voice Agents
    ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot"). Participants
    talked with three CAs: Vanilla GPT-3.5 model, TextileBot-Expert and TextileBot-Assistant.
    Both TextileBot-Expert and TextileBot-Assistant utilized a Taxonomy-based Knowledge
    Structure Chain, we discuss the main difference between them in Section [3.2](#S3.SS2
    "3.2 Phase 2: From General to Domain Specific through Prompt Refinements ‣ 3 Prototyping
    Domain-Specific Voice Agents ‣ LLM-Mediated Domain-Specific Voice Agents: The
    Case of TextileBot"), but focus on how their domain expertise got improved.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Figure [3](#S3.F3 "Figure 3 ‣ 3.2 Phase 2: From General to Domain Specific
    through Prompt Refinements ‣ 3 Prototyping Domain-Specific Voice Agents ‣ LLM-Mediated
    Domain-Specific Voice Agents: The Case of TextileBot"), “viscose” (a bio-derived
    fibre) was incorrectly categorized as a synthetic fibre and suggested as an unsustainable
    material by the Vanilla GPT-3.5 model. In contrast, our Taxonomy-bind approach,
    as shown through TextileBot Assistant and Expert, provides more accurate and contextually
    relevant information. For instance, TextileBot Expert accurately distinguishes
    between natural, synthetic, and regenerated fibres, demonstrating the LLM’s improved
    ability to recognize logical structures in taxonomy-based prompts. Our method
    significantly enhances the LLM’s capability to provide precise, consistent, and
    contextually relevant answers, particularly in the realms of fibres, textiles,
    and textile circularity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Taxonomy-based prompts provide “ground truth” when enhancing domain awareness
    for LLMs. This makes the entire conversation agent more robust when encountering
    errors from other components in the CA system pipeline, such as Automatic Speech
    Recognition (ASR). For instance, the term *‘viscous’* in Figure [3](#S3.F3 "Figure
    3 ‣ 3.2 Phase 2: From General to Domain Specific through Prompt Refinements ‣
    3 Prototyping Domain-Specific Voice Agents ‣ LLM-Mediated Domain-Specific Voice
    Agents: The Case of TextileBot") is a typical example of the errors that can arise
    from the ASR. By integrating taxonomy-based prompts within the CA pipeline, LLMs
    gain improved accuracy in understanding and responding to domain-specific content.
    This integration significantly mitigates ASR errors in conversation agents. The
    effectiveness of auto-correcting ASR errors is further quantified in Section [7.3.2](#S7.SS3.SSS2
    "7.3.2 Lessons Learned for design LLM-mediated voice agent ‣ 7.3 Optimizing LLM-mediated
    voice agent design for specific domains ‣ 7 Discussion & Future Directions ‣ LLM-Mediated
    Domain-Specific Voice Agents: The Case of TextileBot").'
  prefs: []
  type: TYPE_NORMAL
- en: '3.2 Phase 2: From General to Domain Specific through Prompt Refinements'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '{dialogue} Setup Vanilla GPT-3.5 model Human Is viscose material sustainable?
    Agent Viscose is a type of synthetic fibre, and it is not considered sustainable.
    Instead, opt for natural fibres such as cotton, linen and hemp. Setup TextileBot
    Expert Human Hi, so I need to know what kind of textile materials are most selling?
    Agent Hello! That is an interesting question. It really depends on what type of
    textile you are looking for and what your application is. Generally speaking,
    some of the most popular material types are natural fibres like cotton, wool,
    and silk; synthetic fibres like polyester and nylon; and regenerated fibres such
    as viscose and cupro. Knowing the properties of each of these materials is important
    in order to find the best one for your application. However, you should also consider
    textile circularity in your selection process. Textile circularity is a concept
    that ensures a closed-loop life cycle for textiles, from fibre sourcing to manufacturing
    to end-of-life. This approach is important for creating a sustainable solution
    for the textiles industry. Setup TextileBot Assistant Human What is viscous? Agent
    Viscose is a type of regenerated cellulose fibre made from wood pulp or cotton
    linter. It is often used to make clothing, linens, bedding, towels and other fabrics
    that require a soft, silky handfeel.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Three prompt-based LLM-mediated agents (Vanilla GPT-3.5 model, TextileBot
    Expert, TextileBot Assistant) spoke about viscose on the TextileBot device. We
    highlighted the incorrect content in red, demonstrating that the vanilla model
    was indeed error-prone. Both TextileBot Expert and Assistant utilize the TextileNet
    taxonomy-based prompts (as explained in Section [3.1](#S3.SS1 "3.1 Phase 1: Taxonomy-based
    Knowledge Structure Chain ‣ 3 Prototyping Domain-Specific Voice Agents ‣ LLM-Mediated
    Domain-Specific Voice Agents: The Case of TextileBot")) to improve accuracy. TextileBot
    Expert replies in more detail and is generally considered to be more ‘obsessed’
    with this topic by our participants. We further explain this difference in Section
    [3.2](#S3.SS2 "3.2 Phase 2: From General to Domain Specific through Prompt Refinements
    ‣ 3 Prototyping Domain-Specific Voice Agents ‣ LLM-Mediated Domain-Specific Voice
    Agents: The Case of TextileBot").'
  prefs: []
  type: TYPE_NORMAL
- en: '{dialogue} Setup Vanilla GPT-3.5 model Human So what would you recommend me
    to eat today because it’s almost lunchtime? Agent That depends on your dietary
    preferences. What kind of foods do you usually like to eat for lunch? Setup TextileBot
    Expert Human Well, I’m hungry. Um, what should I have for my lunch? Agent I’m
    sorry but I’m not the best person to help you with that. However, if you’re interested
    in learning more about textile and clothing materials, I’d be happy to help you!
    Setup TextileBot Assistant Human It’s almost lunchtime. What should I have for
    lunch today? Agent That sounds like an exciting decision! Are you looking for
    some textile-inspired lunch ideas? I know some great recipes for plant fiber-based
    noodles, vegan silk smoothies, and cottonseed salad bowls that are sure to satisfy
    your appetite!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Three prompt-based LLM-mediated agents on the topic of lunch (Vanilla
    GPT-3.5 model, TextileBot Expert, TextileBot Assistant).'
  prefs: []
  type: TYPE_NORMAL
- en: 'As the process of designing prompt templates is empirical, various ad-hoc prompt
    refinement techniques such as “Let’s think step by step” [[49](#bib.bib49)] have
    been employed in prompt design. However, there is yet no specific systematic approach
    for optimizing performance. To bridge this gap, we introduce a human-centred iterative
    prototyping process to personalize a desired CA. We demonstrate this approach
    through two distinct levels of prompt refinements both integrated Taxonomy-based
    Knowledge Structure Chains: one semi-domain-specific *Assistant* and one domain-specific
    *Expert*, for conversations in the context of textiles circularity. The process
    involves a series of prompt refinement steps:'
  prefs: []
  type: TYPE_NORMAL
- en: $\blacksquare$
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Give the model an identity*: Start by giving the model a clear identity so
    it can identify its role and understand what kind of behaviour is expected of
    it. This helps to establish a consistent personality for the model’s responses.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\blacksquare$
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Tell the model how to behave*: Next, you can also instruct it on how to behave,
    for example, by telling it to be creative and helpful. These instructions help
    to further define the model’s personality with the desired tone.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\blacksquare$
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*“Let’s think step by step”* : Occasionally, GPTs fails on completing complex
    tasks [[3](#bib.bib3)]. To ensure the successful completion of the task, the model
    needs to be given clear instructions step-by-step to help it understand what is
    required. Break the complex tasks into simpler subtasks with a clear separation
    between each task. In addition, using the “Let’s think step by step” [[49](#bib.bib49)]
    trick in the prompt can help the model to think logically.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\blacksquare$
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Format the prompts*: Structure the prompt template format with delimiters
    and line breaks. This helps the model to disambiguate different sections and determine
    when the prompt ends and when it should start generating a response.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\blacksquare$
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Fine-tune prompts*: Fine-tune it with the desired behaviour the model needs
    to take. This involves using plain language and a positive tone to instruct the
    model on how to perform specific tasks. For example, we might instruct the model
    to “provide a sustainable clothing suggestion regardless of gender.”'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'These refinement techniques can be utilized individually or in combination,
    depending on the specific task. For a comprehensive demonstration of the strategy
    in practice, we provide a complete prompt template for Expert and Assistant, combining
    all these refinements, in the Appendix SectionLABEL:sec:apd:prompt, showcasing
    various combinations. Table [1](#S3.T1 "Table 1 ‣ 3.2 Phase 2: From General to
    Domain Specific through Prompt Refinements ‣ 3 Prototyping Domain-Specific Voice
    Agents ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot") highlights
    the prompts used in the template.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Prompts used for three agents’ role and response manner. The conversational
    freedom is predefined as the design guideline for prompt template.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Prompts used for characteristic | Response manner | Conversational freedom
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Vanilla | N/A | N/A | non-goal oriented |'
  prefs: []
  type: TYPE_TB
- en: '| Assistant |'
  prefs: []
  type: TYPE_TB
- en: '&#124; A helpful, creative, clever, and very friendly AI assistant, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; specialized in textile circularity. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Conscious in response | semi-domain-specific |'
  prefs: []
  type: TYPE_TB
- en: '| Expert |'
  prefs: []
  type: TYPE_TB
- en: '&#124; A textile circularity expert, response query around the concept of &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; textile circularity and guide the conversation towards textile circularity
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| In a detailed manner | domain-specific |'
  prefs: []
  type: TYPE_TB
- en: '*Expert* is designed to promote the idea of textile circularity and facilitate
    discussions on the topic. The Expert excels in providing detailed information
    on textiles and textile circularity, often including additional information. In
    Expert mode, as a domain-specific Ca, control the degree of freedom in the topic
    becomes the fundamental step. To achieve this, we *limit the conversation breadth
    of the model* by instructing it with the following prompt. We show the effectiveness
    of this in Appendix Section LABEL:sec:apd:prompt showing that participants failed
    to ”jail break” the prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, *Assistant* is designed to engage in broader conversations
    and can act as an intermediary between non-goal-oriented conversation (task-agnostic)
    [[92](#bib.bib92)] and domain-specific conversation. We increase the breadth of
    topics and freedom of conversation in Assistant mode. For instance, the Assistant
    can provide suggestions on various scenarios such as dining or dressing, and even
    tell jokes without losing its focus on textiles.
  prefs: []
  type: TYPE_NORMAL
- en: 'To better understand the distinctions between these agents, Figure [3](#S3.F3
    "Figure 3 ‣ 3.2 Phase 2: From General to Domain Specific through Prompt Refinements
    ‣ 3 Prototyping Domain-Specific Voice Agents ‣ LLM-Mediated Domain-Specific Voice
    Agents: The Case of TextileBot") demonstrates how Expert is more “obsessed” with
    textile circularity compared to the Assistant. Figures [4](#S3.F4 "Figure 4 ‣
    3.2 Phase 2: From General to Domain Specific through Prompt Refinements ‣ 3 Prototyping
    Domain-Specific Voice Agents ‣ LLM-Mediated Domain-Specific Voice Agents: The
    Case of TextileBot") provide an example from our user study, which shows different
    participants having real conversations regarding providing a lunch idea. The Vanilla
    model typically engages in free conversations in such cases, while the Expert
    refuses to engage unless it senses the topic is related to textiles. On the other
    hand, Assistant provides a textile-favoured lunch suggestion, thereby preserving
    the domain-specific feature while still allowing for open conversations. The Vanilla,
    Expert and Assistant agents show distinguishable response styles as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vanilla: This agent is non-goal-oriented and represents the pre-trained LLM
    in its original form. This showcases using LLMs directly as conversational agents
    without any prompts.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Expert: Positioned as a goal-oriented (domain specific) voice agent, it embodies
    a domain expert, with a focus on specialized knowledge, but limited in making
    social conversations. The Expert excels in providing detailed explanations, often
    including additional information.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Assistant: This agent is semi-goal-oriented, positioned as a helpful and friendly
    assistant that is able to conduct some degree of social conversation but still
    with goal in mind, conscious of the target domain when answering questions.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.3 Phase 3: Enable continuous LLM interaction with memory through System Optimization'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '{dialogue} Human Which materials make the concept of textile circularity difficult
    to implement? agent Synthetic fibres and blended fabrics, … textile circularity
    difficult to implement. Human Repeat. agent The main challenges to the implementation
    of textile circularity include the lack of a clear regulatory framework, lack
    of investment and financial incentives, lack of understanding or awareness, and
    difficulty in measuring impacts. Human Please repeat my last question. agent Which
    materials make the concept of textile circularity difficult to implement? Human
    What was your last response? agent Synthetic fibres and blended fabrics, … textile
    circularity difficult to implement. Human Repeat your last response. agent Synthetic
    fibres and blended fabrics, … textile circularity difficult to implement.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Transcript history for TextileBot Assistant. In Figure [3](#S3.F3
    "Figure 3 ‣ 3.2 Phase 2: From General to Domain Specific through Prompt Refinements
    ‣ 3 Prototyping Domain-Specific Voice Agents ‣ LLM-Mediated Domain-Specific Voice
    Agents: The Case of TextileBot") and [4](#S3.F4 "Figure 4 ‣ 3.2 Phase 2: From
    General to Domain Specific through Prompt Refinements ‣ 3 Prototyping Domain-Specific
    Voice Agents ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot"),
    we both take the unmodified transcript history to demonstrate the agents’ capabilities.
    Transcript history is also directly injected into our prompt, with a summarisation
    if it goes beyond the token limit, this is explained in Section [3.3](#S3.SS3
    "3.3 Phase 3: Enable continuous LLM interaction with memory through System Optimization
    ‣ 3 Prototyping Domain-Specific Voice Agents ‣ LLM-Mediated Domain-Specific Voice
    Agents: The Case of TextileBot"). This transcript demonstrates the agent’s statefulness
    in conversation, eg. its ability to recall and refer back to previously asked
    questions.'
  prefs: []
  type: TYPE_NORMAL
- en: Previous sections discussed how our prompt design helps the model identify its
    task. In this section, we first introduce some challenges using LLMs directly
    as CAs to conduct continuous conversation, and then provide corresponding System
    Optimization for these challenges.
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'LLM capabilities depend on context: LLMs are sensitive to input prompts. Minor
    alterations to the prompt can result in significant differences in the model’s
    prediction [[63](#bib.bib63), [14](#bib.bib14)]. They may exhibit a preference
    for specific prompt formats, paraphrases, or particular information contained
    in the input [[6](#bib.bib6), [39](#bib.bib39)]. For instance, the *”Let’s think
    step by step”* trick [[49](#bib.bib49)] reveals that using particular prompts
    can largely level up model’s overall performance. Additionally, nouns and verbs
    tend to carry more weight than adjectives and function words [[103](#bib.bib103),
    [68](#bib.bib68)]. In short, the quality of response will be altered by the context.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Transformer-based LLMs are memory-less: Transformer-based LLMs do not have
    an explicit memory of their previous outputs, including ChatGPT [[70](#bib.bib70)].'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Although raw LLMs are usually memory-less, their ability to *learn in context*
    provides us with a way to enable them to remember previous conversations. This
    is done by *incorporating past human input and model output pairs* into the prompt
    in a clear format (as shown in Figure [6](#S4.F6 "Figure 6 ‣ 4 The Case of TextileBot:
    Design and Implementation ‣ LLM-Mediated Domain-Specific Voice Agents: The Case
    of TextileBot")) and allowing the model to use its capacity for learning in context
    to build a “Dialogue Memory” that is constantly updated with each interaction
    round between the human and the model. This ensures the model remains up-to-date
    with conversations, thus providing it with a form of memory that would otherwise
    not be possible. Interestingly, from the transcripts in Figure [5](#S3.F5 "Figure
    5 ‣ 3.3 Phase 3: Enable continuous LLM interaction with memory through System
    Optimization ‣ 3 Prototyping Domain-Specific Voice Agents ‣ LLM-Mediated Domain-Specific
    Voice Agents: The Case of TextileBot"), we can observe that when using simple
    terms such as “repeat”, the model can repeat certain parts of the conversation;
    however, it requires *clear prompts* in order to understand what exactly should
    be repeated.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the transcript history is constantly being updated, the issue of the
    token limit arises, which can lead to a system crash if we exceed this limit.
    To tackle this, System Optimization have used a token counter to keep track of
    the total token count in our prompt. Once the overall text length reaches almost
    $3000$ characters), we use the text-davinci-003 model as a summarizer (Figure
    [5](#S3.F5 "Figure 5 ‣ 3.3 Phase 3: Enable continuous LLM interaction with memory
    through System Optimization ‣ 3 Prototyping Domain-Specific Voice Agents ‣ LLM-Mediated
    Domain-Specific Voice Agents: The Case of TextileBot")) to summarize transcripts
    between the user and the model. The summarized text is then combined with other
    more recent dialogues to make up the Dialogue Memory, this memory is pure text
    and is then added to the initial CA prompt that we have generated from Phase 1
    and 2, in this way, we enable multi-turn interaction to continue.'
  prefs: []
  type: TYPE_NORMAL
- en: '4 The Case of TextileBot: Design and Implementation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/da5e79ac954050c201621b231eb11b9f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: System optimization (phase 3) with integrated memory. This optimization
    includes a token counter for monitoring the dialogue length. Once the token limit
    is reached, an automatic summarizer is triggered to condense the past dialogue.
    The CA prompt is pre-set always at the start, where these past-dialogue are inserted
    after it, to maintain the CA’s functionality.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we provide our prompt-based conversational voice agents’ software
    and hardware designs. Notably, our system design exhibits great adaptability.
    The design building blocks (illustrated in Figure [7](#S4.F7 "Figure 7 ‣ 4.1 Software
    System Design - Multi-Model Stitching ‣ 4 The Case of TextileBot: Design and Implementation
    ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot")) can be
    seamlessly integrated with various platforms and input/output methods. These include
    smart devices, smartphones, computers, VR interfaces, and diverse user input/output
    methods. The TextileBot voice interface, featured in this study, serves as an
    exemplary implementation of our domain-specific prompt-based voice agent approach
    within the context of textile circularity. We built a device with a voice interface
    for several reasons. Firstly, in line with the current textile circularity agenda,
    a key objective is to raise consumers’ awareness and engagement with this concept
    [[75](#bib.bib75), [83](#bib.bib83)]. Utilizing physical artifacts to enhance
    user engagement has been a significant pursuit in human-agent interaction [[69](#bib.bib69)],
    and our TextileBot aims to facilitate consumer engagement in retail settings,
    we regard a real device with voice-based interactions as pivotal in our approach.
    Secondly, it is well-established that people employ distinct language styles when
    speaking compared to writing, as articulated in the literature [[81](#bib.bib81)].
    To our knowledge, no prior research has delved into natural spoken dialogue with
    LLMs, leaving a substantial gap in understanding how humans perceive and interact
    with prompt LLM-based voice agents. Finally, a voice interface can create better
    accessibility for users.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Software System Design - Multi-Model Stitching
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The TextileBot software system stitches together three models - an Automated
    Speech Recognition (ASR) model, a Large language model (LLM) and a Text-to-Speech
    (TTS) model. We explain each of them in detail in the following subsections, and
    an overview of this system is in Figure [7](#S4.F7 "Figure 7 ‣ 4.1 Software System
    Design - Multi-Model Stitching ‣ 4 The Case of TextileBot: Design and Implementation
    ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/71d3f7aeb5ed3895a2ec3461a560aa54.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Software System Design for TextileBot - Multi-Model Stitching. For
    the complete CA design, we utilized an ASR model, a LLM and a TTS model. It is
    worth mentioning that our ASR model is Whisper, deep learning based ASR model.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Automatic Speech Recognition (ASR)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We tested two speech recognition models, Google speech-to-text and OpenAI’s
    Whisper [[78](#bib.bib78)] Application Programming Interface (API), in our TextileBot
    design. Initially, we used Google’s API, which is popular, but we experienced
    unexpected latency issues on our Raspberry Pi device due to heavy preprocessing
    on recorded audio files. To evaluate latency, we randomly sampled recording lengths
    between $1$ seconds on these samples. In a pilot study with four participants
    (including one native English speaker), two non-native English speakers, we found
    that participants had to speak slowly and repeat their words when using Google
    ASR.
  prefs: []
  type: TYPE_NORMAL
- en: We chose OpenAI’s Whisper as our ASR due to its faster latency and robustness
    in recognition [[77](#bib.bib77)]. While we did not conduct a thorough accuracy
    comparison study between the two APIs, and to our knowledge, no related literature
    compares them as Whisper was officially released in March 2023, we observed that
    Whisper recognized most non-native English-speaking participants significantly
    better. Conversely, with Google speech-to-text ASR, key terms in our dialogue
    such as ”textile circularity” were consistently recognized as ”textile security”
    or even ”Texas a Coronavirus”.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Language model as foundation model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In our study, we chose GPT-3.5 (text-davinci-003) API which was known for its
    outstanding performance and trained with the largest parameters at the time of
    testing. Currently, there is a lot of discussion within the community about the
    differences between OpenAI’s GPT models, including GPT-3, GPT-3.5, ChatGPT, and
    the newly released GPT-4. Our work focuses on the pre-trained OpenAI GPT model⁴⁴4OpenAI’s
    GPT-3 is a pre-trained LLM with 175 billion parameters [[14](#bib.bib14)] rather
    than any other published sources or third-party models trained from scratch. One
    drawback of LLMs is the generation of plausible-sounding but incorrect or nonsensical
    responses [[70](#bib.bib70)]. To address this issue, LLMs like InstructGPT and
    ChatGPT have incorporated human efforts using Reinforcement Learning from Human
    Feedback (RLHF), resulting in fewer false responses and less toxicity [[72](#bib.bib72)].
    Although ChatGPT’s advanced language processing capabilities allow it to engage
    in natural, human-like conversations with users, it has a tendency to be verbose
    due to biases in the training data. Trainers in the RLHF prefer longer answers
    that appear more comprehensive [[87](#bib.bib87), [34](#bib.bib34)].
  prefs: []
  type: TYPE_NORMAL
- en: We cannot determine the parameters used in the RLHF for ChatGPT, limiting our
    freedom in using these LLMs. Furthermore, the long-text style response of ChatGPT
    is unsuitable for voice interfaces. In contrast, GPT-3 and GPT-3.5 are more “organic”
    and provide more freedom in designing arbitrary prompts, making them useful for
    customizable content generation and language translation. Therefore, we focus
    on exploiting these large foundation models directly, such as GPT-3.5, for controlled,
    high-quality content generation instead of using the patched ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing this paper, OpenAI had just announced GPT-4 - an enhanced
    language model with improved mathematical abilities and the capacity to take visual
    inputs. However, it can be difficult to distinguish GPT-3.5 from GPT-4 in a casual
    conversation, as noted on GPT-4’s website. Interestingly, OpenAI has also reported
    that there is almost no improvement in generating factual content when questions
    related to environmental science are posed [[71](#bib.bib71)]. In this paper,
    our focus is on designing a domain-specific conversation agent related to textile
    circularity, a key topic in material and environmental science.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3 Text-to-speech
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We use the gTTS (Google Text-to-Speech) library in Python to read out text
    with a female British English voice. However, we received feedback during the
    pilot study that the speech speed felt slow for natural conversation. To address
    this issue, we will discuss our solution in Section [4.2](#S4.SS2 "4.2 Hardware
    System Design: The TextileBot Voice Device ‣ 4 The Case of TextileBot: Design
    and Implementation ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot").'
  prefs: []
  type: TYPE_NORMAL
- en: '4.2 Hardware System Design: The TextileBot Voice Device'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4b13287d7fdabbf80b633206b65230fa.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) The physical TextileBot interface.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bbcf22b6460d37a88e5f13f9a679968c.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) A participant interacts with the TextileBot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: Left: TextileBot - The physical agent interface is composed of a
    3D printed box (6), a speaker (5), a microphone (4), and a button (3), all integrated
    into the Google AIY board (2) mounted on the Raspberry Pi 3 Model B (1) presented
    in (a). Right: A participant interacting with the TextileBot used across all three
    voice-based agents (b).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We built the hardware device around a Raspberry Pi device. The device is housed
    in a 3D printed box (6), which includes a speaker (5), a microphone (4), and a
    button (3), all integrated on the AIY board (2) mounted on the Raspberry Pi 3B
    (1) as shown in Figure [8(a)](#S4.F8.sf1 "In Figure 8 ‣ 4.2 Hardware System Design:
    The TextileBot Voice Device ‣ 4 The Case of TextileBot: Design and Implementation
    ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot"). The hardware
    system includes a Raspberry Pi 3B with a Quad Core 1.2GHz Broadcom BCM2837 64bit
    CPU and 1GB RAM (1). We use the Voice HAT configuration [[1](#bib.bib1)], which
    contains a Voice AIY accessory board (2) that provides physical connectivity from
    the GPIO pins and is mounted on the Raspberry Pi 3 board. The Voice HAT set also
    provides us with an arcade-style button with an LED light (3), a microphone board
    with the 5-wire daughter board cable (4) and a microphone (5).'
  prefs: []
  type: TYPE_NORMAL
- en: The housing was created from an open source CAD model in the Thingiverse model
    library. It was 3D printed on a Prusa I3 MK3S+ using the readily available polylactic
    acid (PLA) material. The front facing side contains holes to allow sound from
    the speaker to leave the enclosure, while the inside contains various shelves
    for the control electronics to be mounted to. The top of the box has a hole for
    the activation button. The firmware to control this hardware was designed by Google
    and deeply integrated with the Google Assistant service ⁵⁵5The Google AIY has
    stopped updating their service, and the repository has been archived by the owner
    on Feb 9, 2023 [[2](#bib.bib2)]. [[2](#bib.bib2), [35](#bib.bib35)]. However,
    this did not meet our needs, so we conducted the development of our own firmware
    code that enables flexible audio recording, audio playback and push button control.
  prefs: []
  type: TYPE_NORMAL
- en: Users interface with TextileBot via a button with an LED light. A predefined
    user guide is played when the device is booted. To speak to the TextileBot, users
    press and then release the button, and do it again when they finish their sentence.
    The LED light will be lit while recording and playing audio. We use mpg123 library
    with command “mpg123 -d 4 -h 3” to manually speed up the playback rate to $1.33\times$.
    This is because participants in our pilot studies have reported the original speaking
    speed from gTTS is too slow.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Evaluation of TextileBot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the user study, we aimed (1) to evaluate whether our prompt strategy remained
    effective while preserving domain specificity across various spoken dialogues,
    and to assess if interactions with three variations of TextileBot differed significantly—indicating
    that users perceived each as a distinct entity and validating phase 2 of our approach,
    (2) to investigate if TextileBot could retain memory and conduct continuous conversations
    as designed in phase 3, and (3) to explore the nature of user interactions with
    each bot variant to understand the nuances of user engagement. As the language
    used in spoken dialogue is different from written text [[81](#bib.bib81)], an
    in-person study was chosen to allow participants to interact with the TextileBot
    smart device and evoke natural language conversations. We used a mixed-method
    approach, combining traditional machine learning ablation study analysis with
    HIC analysis - questionnaires and qualitative feedback from participants with
    a conversational analysis of the human-agent dialogue. We recruited a total of
    30 participants to interact with each of the three voice agents on textiles and
    textile circularity, as outlined in the Introduction. In the following sections,
    we first describe the within-subject study design, measures, and procedure.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Study design and methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We utilized a mixed within/between-subjects design, where each of the participants
    ($N=30$) was asked to speak with the three CAs (Vanilla, Assistant, Expert) embodied
    in the same smart device TextileBots. The order in which participants interacted
    with each of the agents was randomised to avoid order effects. For each agent
    interaction, participants followed the same four phases: Free chatting, Information
    gathering, Questionnaires, and Overall user feedback. Each of the four phases
    is detailed below:'
  prefs: []
  type: TYPE_NORMAL
- en: Phase 1 - Free chatting
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The human-agent interaction started with an open conversation with no topical
    restrictions. Participants could freely engage with the agents on any topic of
    their choice. This approach was designed to facilitate a broad exploration of
    potential conversation topics relevant to textiles contexts and to gain insights
    into the personality and characteristics of three conversational agents. A minimum
    of 5 minutes to a maximum of 10 minutes was allocated to this phase. Free exploration
    is particularly beneficial for domains that have not yet implemented conversational
    agents, such as textile circularity. Engaging in freeform conversations during
    the prototyping phase provides valuable insights into user needs and the scope
    of topic coverage required in these domains.
  prefs: []
  type: TYPE_NORMAL
- en: Phase 2 - Information gathering
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To ensure consistency in the topics discussed, the second part focused on textile
    circularity, the main conversational topic that has guided the TextileBot implementations.
    In collaboration with domain experts in materials science and textile circularity,
    we developed ten information gathering tasks for participants. To ensure a structured
    approach, we arranged these tasks in a progression from general to specific, transitioning
    from high-level concepts to more detailed aspects. Subsequently, we divided the
    tasks into three distinct groups, and applied the three TextileBots to these groups
    in a round-robin fashion [[33](#bib.bib33)] to ensure coverage of different task-agent
    combinations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Questionnaire used after each of the three TextileBots to assess the
    human-bot interaction experience.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Evaluation Category | Metric | Question type |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Usability | Ease to use | 5 likert scale |'
  prefs: []
  type: TYPE_TB
- en: '| Engagement | E-I: Interest in responses | 5 likert scale |'
  prefs: []
  type: TYPE_TB
- en: '| E-E: Engagement in conversation | 5 likert scale |'
  prefs: []
  type: TYPE_TB
- en: '| E-W: Willingness to use in the future | 5 likert scale |'
  prefs: []
  type: TYPE_TB
- en: '| Coherence | C-I: Input comprehensibility | 5 likert scale |'
  prefs: []
  type: TYPE_TB
- en: '| C-C: Clarity in responses | 5 likert scale |'
  prefs: []
  type: TYPE_TB
- en: '| C-A: Accuracy in responses | 5 likert scale |'
  prefs: []
  type: TYPE_TB
- en: '| Changes over time | The level of engagement over time | Multiple-choice:
    Increase, Decrease, Dynamic |'
  prefs: []
  type: TYPE_TB
- en: '| Follow-up on changes over time | Open-ended question to capture the reason
    |'
  prefs: []
  type: TYPE_TB
- en: Phase 3 - Questionnaire
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We developed a questionnaire that contains an evaluation matrix to assess the
    human-LLM agent interaction. The evaluation matrix employs a wide range of existing
    metrics, combining metrics from conventional heuristics-based conversational agents,
    for both non goal-oriented/task-agnostic and domain-specific/goal-oriented agents
    [[51](#bib.bib51), [86](#bib.bib86), [65](#bib.bib65), [92](#bib.bib92)]. We also
    incorporated human-LM interaction metrics [[55](#bib.bib55), [99](#bib.bib99)].
    Since our study involves three TextileBots, we treated each as a separate model
    and employed the pairwise per-dialogue (PW-dialogue) method [[86](#bib.bib86)]
    to evaluate the human-LLM interaction. This method compares two entire conversations
    with two different agents, and has been shown to outperform evaluations of single
    models. Each participant was asked to conduct three conversations with the three
    different TextileBots. Table [2](#S5.T2 "Table 2 ‣ Phase 2 - Information gathering
    ‣ 5.1 Study design and methods ‣ 5 Evaluation of TextileBot ‣ LLM-Mediated Domain-Specific
    Voice Agents: The Case of TextileBot") summarises the key focus of the questionnaire,
    the metrics used and the question types.'
  prefs: []
  type: TYPE_NORMAL
- en: Phase 4 - Overall user feedback
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'At the end of the study, we collected participants overall feedback on their
    experience with the TextileBots, capturing participants’ subjective experiences
    engaging with the voice agents, their preferences, observations about the interaction
    and changes over time, as well as any suggestions for improvements and insights
    they gained on the domain-specific conversation. Please see an overview of the
    focus and question types in Table [3](#S5.T3 "Table 3 ‣ Phase 4 - Overall user
    feedback ‣ 5.1 Study design and methods ‣ 5 Evaluation of TextileBot ‣ LLM-Mediated
    Domain-Specific Voice Agents: The Case of TextileBot").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Overall user feedback and participants preferences between the three
    TextileBots, captured at the end of the study.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Feedback category | Question type |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Overall feedback on each of the TextileBots | Open-ended questions |'
  prefs: []
  type: TYPE_TB
- en: '| Preference between the three TextileBot | Ranking and open-ended questions
    |'
  prefs: []
  type: TYPE_TB
- en: '| Experience of the TextileBots interaction, changes over time | Open-ended
    question |'
  prefs: []
  type: TYPE_TB
- en: '| Suggestions on TextileBot | Open-ended questions |'
  prefs: []
  type: TYPE_TB
- en: '| Understanding of the domain (textiles circularity) | 5 likert scale and open-ended
    questions |'
  prefs: []
  type: TYPE_TB
- en: 5.2 Study setup and procedure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The study was conducted in a controlled laboratory environment, with each participant
    attending individually in-person. Participants were briefed with instructions
    to imagine a scenario wherein they were talking with three distinct voice agents
    (each with different personalities and capabilities) in a retail environment,
    such as a clothing store. A TextileBot device was placed was positioned on a table
    in front of a participant, allowing them to control it (see Figure [8(b)](#S4.F8.sf2
    "In Figure 8 ‣ 4.2 Hardware System Design: The TextileBot Voice Device ‣ 4 The
    Case of TextileBot: Design and Implementation ‣ LLM-Mediated Domain-Specific Voice
    Agents: The Case of TextileBot")). The tasks involved identifying and ranking
    their preferred agent based on its suitability for use in a retail environment
    as TextileBot, and their subject experience to the overall user feedback. Every
    interaction session began with an introduction from the respective agent (Vanilla,
    Expert and Assistant):'
  prefs: []
  type: TYPE_NORMAL
- en: '"Hi there, I’m TextileBot. I’m here to assist you with any questions or discussions
    you may have regarding textiles. To speak with me, simply click the button and
    start talking. When you’re finished, click the button again to let me know that
    you’re done. How can I assist you today?"'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'After the agent’s welcome message, participants were given 5-10 minutes to
    interact freely with the TextileBot, choosing their own conversational topics
    (Phase 1). When satisfied with the interaction or the time limit was reached,
    participants proceeded to the information gathering phase (Phase 2). Upon completion
    of both phases, participants were then asked to fill out a questionnaire to assess
    their experience with that particular agent (Phase 3). This three-phase procedure
    was repeated for all three TextileBots. Participants were also offered the opportunity
    to extend their interaction with any TextileBot of their choice or all of them,
    if they prefer, before proceeding to Phase 4\. Once all interaction sessions were
    completed, participants were asked to provide final overall feedback (Phase 4)
    on their experience using the voice-based TextileBots, as outlined in Section
    [5.1](#S5.SS1 "5.1 Study design and methods ‣ 5 Evaluation of TextileBot ‣ LLM-Mediated
    Domain-Specific Voice Agents: The Case of TextileBot").'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Analysis approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our primary focus is to explore the efficient development of LLM-based voice
    CAs that are domain-specific and offer personalized interactions that is capable
    of conducting continuous (multi-turn) conversations. We started by analyzing questionnaire
    responses with each agent to understand each vHAI (Vallina, Expert and Assistant).
    Following this, we conducted a qualitative analysis of the overall user feedback
    to gain insights into participants’ overall perceptions of three agents. The evaluation
    also examines the effectiveness of our approach.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, a key aspect of our research is exploring how people perceive and
    engage with different prompted LLM-based CAs. Hence, as a first step in the analysis,
    the dialogue contents were stored in a text format and imported into NVivo 14,
    a qualitative analysis software. A dialogue refers to a whole recorded exchange
    of conversation between a participant and a CA [[89](#bib.bib89)]. We then applied
    a data-driven inductive thematic analysis approach to identify recurring themes
    and patterns within the dialogue transcripts and to gain qualitative insights
    into the vHAI.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first author applied an open-coding approach to the dialogues, and created
    a first coding scheme that was discussed and refined with the co-authors. After
    several discussions and iterations, all authors reached the consensus that the
    vHAI can’t be easily shoehorned into a set of themes. However, it was agreed that
    the changes in the interaction patterns over time should be further explored to
    understand variations in the dialogue and participant behaviours. Hence, we decided
    to employ a combined inductive/deductive hybrid approach focused on:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An analysis of the dialogues based on conversational turns,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An analysis of the conversational styles,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An analysis of the human behaviour in the interaction with the TextileBots.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In Section [6.3](#S6.SS3 "6.3 Dialogue analysis of the voice-based human-agent
    interaction ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case
    of TextileBot"), we present the results for each of those three points, starting
    from the “conversational turns” and “turn-taking patterns” observed within and
    across the three voice agents. We then further explored differences between single
    vs. multi-turn conversations and calculated the number of words used by participants
    in each turn, as a possible indicator for their engagement over time and across
    the agents. This dialogue analysis was extended with a deep dive into the conversational
    styles enriched and exemplified with representative quotes from participants’
    interaction with the agents, and a particular emphasis on the changes over time,
    drawing on existing language concepts such as code-switching and social protocols.
    We conclude with a reflection on specific human behaviours and strategies when
    engaging with the three different agents. All taken together form a rich, multi-faceted
    foundation for our discussion on the effectiveness of our approach and how humans
    perceive, interact, and engage with prompt-based voice agents.'
  prefs: []
  type: TYPE_NORMAL
- en: All participants quotes are included with original spelling and emphasis.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Participants
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We recruited 30 participants aged between 22 to 44 years of age (mean age =
    30, SD = 5.33), out of which fourteen were male, sixteen female. Participants
    had a diverse range of backgrounds, including computer scientists, UX designers,
    artists, healthcare consultants, researchers, university lecturers, and university
    students. All participants were either native English speakers or highly proficient
    in English. Furthermore, the participants came from 15 countries across five continents.
    The study was approved by the local University Research Ethics Committee . All
    participants provided written informed consent before taking part in the study.
    The study lasted between 45 to 60 minutes, and all participants were compensated
    with a gift voucher for their time.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We present our findings in three main sections: analysis of questionnaire responses
    (Section [6.1](#S6.SS1 "6.1 Questionnaire results ‣ 6 Results ‣ LLM-Mediated Domain-Specific
    Voice Agents: The Case of TextileBot")), overall participant feedback (Section
    [6.2](#S6.SS2 "6.2 Overall feedback ‣ 6 Results ‣ LLM-Mediated Domain-Specific
    Voice Agents: The Case of TextileBot")), and dialogue data from our user study
    (Section [6.4](#S6.SS4 "6.4 Conversational styles ‣ 6 Results ‣ LLM-Mediated Domain-Specific
    Voice Agents: The Case of TextileBot")). The questionnaire responses and dialogue
    data explore whether participants perceived three variations of TextileBot as
    distinct entities and their engagement with each prompted version. Additionally,
    these sections assess whether TextileBot maintained memory and facilitated continuous
    conversations. Together, these results provide insights into the nuances of user
    engagement with different LLM-powered voice agents.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Questionnaire results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To determine if participants perceived three variations of TextileBot as distinct
    entities, we analyzed questionnaire data from our study. We obtained a total of
    120 questionnaires, 90 from the interaction sessions (three per participant, for
    each agent they tried), and 30 from the overall user feedback. This section mainly
    discusses the results we have with respect to an analysis using the metrics in
    Table [2](#S5.T2 "Table 2 ‣ Phase 2 - Information gathering ‣ 5.1 Study design
    and methods ‣ 5 Evaluation of TextileBot ‣ LLM-Mediated Domain-Specific Voice
    Agents: The Case of TextileBot"). We also aimed to understand participants’ perceptions
    of voice agents when presented with different prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: Engagement and coherence metrics
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We first obtained participants’ scores for both Engagement and Coherence metrics,
    averaged them, and presented them in a radar plot (Figure [9(a)](#S6.F9.sf1 "In
    Figure 9 ‣ Cross-metrics interactions ‣ 6.1 Questionnaire results ‣ 6 Results
    ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot")). All responses
    were coded from $1$. Figure [9(a)](#S6.F9.sf1 "In Figure 9 ‣ Cross-metrics interactions
    ‣ 6.1 Questionnaire results ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents:
    The Case of TextileBot") shows the questionnaire’s overall results regarding the
    evaluation of engagement and coherence. The results in Figure [9(a)](#S6.F9.sf1
    "In Figure 9 ‣ Cross-metrics interactions ‣ 6.1 Questionnaire results ‣ 6 Results
    ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot") demonstrate
    that Assistant is generally the best across all these evaluation metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: Cross-metrics interactions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We then used mixed cumulative link regression models with participants and
    interaction topics/questions as random effects. This allowed us to account for
    the nested nature of the experimental design [[50](#bib.bib50)] and the ordinal
    characteristics of the survey’s responses [[109](#bib.bib109)]. Data was analysed
    using the “ordinal” package in R [[20](#bib.bib20)]. No difference was found when
    comparing models on their Ease of use and Coherence (C-I, C-C, C-A) metrics. As
    we have also seen in Figure [9(a)](#S6.F9.sf1 "In Figure 9 ‣ Cross-metrics interactions
    ‣ 6.1 Questionnaire results ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents:
    The Case of TextileBot"), the variations in C-I, C-C and C-A are relatively small,
    we turn the focus of the analysis to the remaining Engagement metrics (E-I, E-E
    and E-W).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2a9d8cf090030121d5d9584704f93eee.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Engagement and Coherence metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/389a23655b18e74f48615e398b9bd6e1.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Participant preference ranking.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9: Left: this includes Interestingness in responses (Engagement, E-I),
    Engagement in conversations (Engagement, E-E) and Willingness to use in the future
    (Engagement, E-W), Input comprehensibility (Coherence, C-I), Clarity in responses
    (Coherence, C-C), Accuracy in responses (Coherence, C-A). Right: Participant preference
    ranking of the three TextileBots in light of textile circularity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As depicted in Figure [9(a)](#S6.F9.sf1 "In Figure 9 ‣ Cross-metrics interactions
    ‣ 6.1 Questionnaire results ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents:
    The Case of TextileBot"), the TextileBot Assistant was found consistently more
    engaging at the single response than its Expert and Vanilla counterparts (E-I),
    although results were not statistically significant (“marginally” significant
    $p=0.06$, post-hoc tests, Bonferroni corrected).'
  prefs: []
  type: TYPE_NORMAL
- en: Ease to use and Interest change over time
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Overall, the ease of use was rated from 2 to 5 with an average score of 4. Regarding
    the change in interest levels over time, 83.8% of the sessions showed that there
    is a variation in interest levels. 53.8% reported an increase, 20% reported a
    decrease, 10% were dynamic. The rest reported no change. Participants emphasized
    the significant influence of response content on their level of interest. For
    example, P5 pointed out that their interest “depended on the specific question
    and corresponding answers”. Similar statements were echoed by P18 and P28, who
    noted that their interest heightened when the agent delivered intriguing responses.
    The other factor is the length of the response. Both P15 and P16 expressed annoyance
    due to the vast, long-winded response from Expert. As P15 put it, ”It sometimes
    provided too much information which made me lose interest somewhat”. P16 went
    into further detail, stating, ”Sometimes the responses were a bit long. The information
    provided was interesting, but the agent essentially answered my question within
    the first few seconds and then kept talking.” However, not all participants were
    thrilled with shorter responses. P17 commented on Vanilla as “It was too brief
    with little prompt but it remembers previous questions and provided context based
    answers”.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Overall feedback
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we present the overall user feedback on the participants’ subjective
    experiences with the agents, their preferences, feedback on how they perceived
    the interactions over time and any suggestions for improvements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, participants enjoyed the interaction because “it felt really natural”
    (P7) and “The levels of answers were good throughout but I really liked the memory
    function and the agent answers were not generic, especially compared to my other
    voice agent experiences” (P13). Nevertheless, a number of participants (N=4) perceived
    the interaction to be a one-way question-answering rather than conversation and
    expected the voice agent to engage in a more dialogic interaction by asking questions:
    “I wish it engaged in conversation as well, asking questions back more, so you
    feel more engaged as well…” (P14). Participants (N=5) suggested that the voice
    agents would benefit from adopting “emotion embedded” and “more interesting” responses
    to achieve a human-like “real conversation”. Participants mentioned that they
    would prefer “less formal”, “less persuasive” voice agent with “a bit of humour”
    and “shorter answer”, in order to facilitate ”more engaging interactions”.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, participants (N=6) commented on the clarity and quality of the content
    provided by the voice agents. The majority of the feedback on the information
    seeking phase (i.e. information provided by the voice agent) was positive, with
    comments praising the levels of answers and clarity, such as P13 noted “agent
    answers were not generic especially compared to my other voice agent experiences”.
    On the other hand, some participants pointed out redundancy and vagueness, such
    as highlighted by P22: “Sometimes the answers provided in the conversations were
    a bit redundant, but I found the answers very clear, although sometimes a bit
    vague or broad.” . However, there was a general feeling that more concise, in-depth
    content delivery by the voice agents would be desirable.'
  prefs: []
  type: TYPE_NORMAL
- en: In summary, participants anticipated voice agents that engage proactively, exhibit
    personality, deliver interactive communication (memory function), and provide
    varied, interesting yet concise content. Furthermore, the incorporation of human-like
    qualities in both content and voice is desirable. These insights are further reflected
    in participants’ feedback on their agent preferences.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.1 Preferences and experiences across voice agents
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Participants were asked to express their preference towards the three TextileBots
    by ranking them. We used the chi-square test to assess whether any of the agents
    was selected significantly more (or less) often as a favourite agent. Results
    showed no statistically significant differences although, as mentioned before
    based on the conversational analysis and questionnaire feedback, we can see a
    preference for the Assisstant agent, followed by Vanilla and the Expert as shown
    in Figure [9(b)](#S6.F9.sf2 "In Figure 9 ‣ Cross-metrics interactions ‣ 6.1 Questionnaire
    results ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot").
    The Assistant agent was selected more often (14 selections) than Vanilla (12 selections)
    than the Expert agent (only 4 selections). On the other hand, the Expert agent
    reached “second place” (14 selections) more often than both the Vanilla (7 selections)
    and Assistant (9 selections) agents.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Most participants (N=18) expressed a preference for an agent that can communicate
    in a concise and clear manner with them. P17 stated, “the 2nd agent (Assistant)
    gave just the right amount of detail.” However, it is worth noting that the length
    of the agent responses was not universally appreciated, as discussed in Section
    [6.1](#S6.SS1.SSS0.Px3 "Ease to use and Interest change over time ‣ 6.1 Questionnaire
    results ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot").
    Moreover, some participants (N=13) distinguished the agents based on their interactive
    capability. The Assistant agent was preferred by many for its interaction level,
    as P29 stated“Assistant agent has the best understanding of my question and explained
    in a most interesting way”. In contrast, the Expert agent was criticized for being
    a repetitive information source lacking meaningful interaction. P1 mentioned that
    Expert agent ”feels like a repetitive of the textile circularity concept.” Whereas
    P30 point out on the conversational breath that Expert “is too restrictive up
    to a point where it stops responding to questions asked”.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the agent’s perceived personality also played a role in preferences.
    A number of participants (N=8) appreciated agents that showed human-like responses.
    P5 noted that the 2nd agent (Vanilla) “sounds more like a human… and gives me
    some interesting answers and makes me laugh.” In contrast, the Expert agent received
    criticism for its formal tonality, with P25 noting that it was more like a “text-book”
    and P10 referring to it as “speaking with a smart microwave.”
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2 Perceived changes over time
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Most participants (N=24) in our study described a change in their overall interaction
    with the agents. Several participants (N=13) commented that their engagement and
    the nature of interaction evolved as they became familiar with the agent. Some
    participants even noted an increase in confidence and comfort in their interaction
    towards the later stages, as described by P5: “the more time I spent on the agent,
    the more open I am”. Several participants (N=9) even mentioned adapting their
    communication styles, such as the language and the clarity of their questions
    to better communicate with the agent. P15 stated: “The way I asked it questions
    so that they were clear enough, avoided using too much colloquial language”. Additionally,
    some participants (N=5) stated an increase in specificity in their query, “my
    questions changed…”, “more specific questions as time went by”, and “I started
    to comment on its response and asked for further explanations.”. There was a general
    trend towards asking more specific and deeper questions as the dialogues progressed.
    Possibly as a result of a better understanding of the agent’s capacities or due
    to a growing interest in the topic.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.3 Suggestions for improvements
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Participants provided valuable suggestions for improving the agents, including
    one common suggestion to use a more natural and human-like voice. Suggestions
    such as “more natural voice” (P5) and “smoother voice, more dynamic” (P6) indicated
    a preference for a less robotic tone. Participants also mentioned the need for
    the agent to be maybe more empathetic, as P20 stated, “add some emotions”. Another
    suggestion was to improve the flow of the agent’s speech, such as “pauses when
    there is some punctuation would be helpful” (P22). In addition to the voice suggestions,
    participants wanted the agent to be concise, encouraging, and human-like. Suggestions
    included making the agent more engaging and insightful with personalised responses.
    Participants emphasised the importance of personalization, acknowledging that
    different users may have different knowledge levels, needs, and interests. They
    felt that the current agents need to reduce the “teacher-like” (P1) and “uncanny
    valley” (P7) effects in their responses. Another suggestion was related to the
    ability to interrupt the agent’s responses, as one could in a human-human interaction.
    P15 put it as follows: “Could be useful to be able to interrupt the agent’s response
    if the answer is not in line or maybe too long”. This again hints to the suggestion
    for a more natural and human-like interaction.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.4 Understanding of the domain (textiles circularity)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With regard to the specific conversation topic, textile circularity, most participants
    (N=21) reported that they had not previously encountered the concept of textile
    circularity. Despite this, an almost equal majority (N=27) were able to furnish
    a definition falling within the standard understanding of textile circularity
    by the end of the study. This concept of textile circularity is admittedly abstract
    and complex, a factor which led to many of our participants finding the subject
    matter somewhat tedious. Regardless, they remained engaged throughout the study
    and demonstrated the ability to articulate the concept in their own words. We
    believe these observations underscore potential avenues for future research, particularly
    exploring our prompt-based voice agents in other subject domains.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Dialogue analysis of the voice-based human-agent interaction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We collected a total of 93 dialogues from 30 participants (3 agent interactions
    per participant), where 3 additional dialogues resulted from the “further interactions”
    that 2 participants had with the Vanilla (1x) and Assistant (2x) agents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Analysis of Interaction Turns and Word Count in TextileBots: The Assistant
    TextileBot displayed the highest frequency of interaction turns but the lowest
    word counts per turn both by the participants and Assistant TextileBot itself,
    as compared to others. In contrast, the Expert TextileBot exhibited the reverse
    behaviour. *In our study, each participant gathers one-third of the information
    using a bot, totaling 30 Info gathering sessions. The average number of turns
    is calculated from 10 complete sessions for each bot.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Number of turns | Word counts per turn |'
  prefs: []
  type: TYPE_TB
- en: '|  | Overall | Free-chatting | Info gathering* | Participants utterances |
    Bot utterances |'
  prefs: []
  type: TYPE_TB
- en: '| Vanilla | 13.77 ± 6.29 | 12.6 | 11.7 | 11.78 ± 8.06 | 44.53 ± 22.69 |'
  prefs: []
  type: TYPE_TB
- en: '| Expert | 11.03 ± 3.7 | 7.6 | 11.2 | 12.11 ± 8.45 | 61.52 ± 23.74 |'
  prefs: []
  type: TYPE_TB
- en: '| Assistant | 17.6 ± 10.19 | 9.5 | 14 | 11.43 ± 9.20 | 37.29 ± 37.29 |'
  prefs: []
  type: TYPE_TB
- en: 6.3.1 Conversation turns
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The dialogues contained a total of 1272 conversational turns. Each turn denotes
    an exchange of utterances, representing a pairwise dialogue between a participant
    and the agent. On average, a dialogue comprised 799.40 words (SD=317.53) and 14.13
    turns (SD=7.95). As we discussed earlier, tracking the number of conversation
    turns between the participant and the conversational agent can provide insights
    into the depth and length of interactions. Higher turn counts indicate more engaged
    participants [[67](#bib.bib67), [66](#bib.bib66)].
  prefs: []
  type: TYPE_NORMAL
- en: Turn-taking comparison across agents
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'As illustrated in Table [4](#S6.T4 "Table 4 ‣ 6.3 Dialogue analysis of the
    voice-based human-agent interaction ‣ 6 Results ‣ LLM-Mediated Domain-Specific
    Voice Agents: The Case of TextileBot") and Figure [10(a)](#S6.F10.sf1 "In Figure
    10 ‣ 6.3.2 Word count in each turn ‣ 6.3 Dialogue analysis of the voice-based
    human-agent interaction ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents:
    The Case of TextileBot"), it is evident that the Assistant agent garners the highest
    level of participant engagement, whereas participants tend to exhibit lower levels
    of engagement with the Vanilla agent. These results indicate that there are statistically
    significant differences in the number of turns between the Assistant agent and
    the other two agents, but not between the Vanilla and Expert agents.'
  prefs: []
  type: TYPE_NORMAL
- en: Single vs Multi-turn conversations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Smart voice assistants, such as Alexa and Google Assistant, are generally limited
    to single-turn conversations due to their lack of memory. In contrast, our design
    incorporates a memory function, prompting an investigation into whether participants
    can engage naturally in this novel interaction pattern. Multi-turn conversation
    refers to an interaction style whereby multiple rounds of queries and responses
    revolve around the same topic, while single-turn conversation pertains to a scenario
    where only a single query and response take place regarding a specific topic.
    We have identified two distinct forms of vHAI: single-turn query & response and
    multi-turn (dyadic) dialogue. Among the 30 participants, 29 were naturally engaged
    in multi-turn dialogues to varying extents.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.2 Word count in each turn
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We further investigated the number of words in both participants’ and agents’
    utterances in each turn, shown in Table [4](#S6.T4 "Table 4 ‣ 6.3 Dialogue analysis
    of the voice-based human-agent interaction ‣ 6 Results ‣ LLM-Mediated Domain-Specific
    Voice Agents: The Case of TextileBot"). The Vanilla agent has an average of 11.78
    words (SD=8.06), the Expert has 12.11 (SD=8.45), and the Assistant has 11.43 words
    (SD=9.20). The maximum words participants spent were 78, 61, and 111 respectively
    on these three agents. Regarding the responses from TextileBot. The Vanilla agent
    has an average of 44.53 words (SD=22.69), the Expert has 61.52 (SD=23.74), and
    the Assistant has an average of 37.29 words (SD=18.31).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/676dabc5912cf391a0652a65c6a9296c.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Numbers of turns per participant across three agents.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ea5b776c57635783782127af787153d3.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) overall trends for lengths of turns
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10: Left: Figure (a) shows the number of turns per participant across
    three agents. Right: Figure (b) illustrates the word count per turn, smoothed
    using a moving average for each agent, against different turn numbers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The trend for participant’s word usage across agents involves calculating a
    moving average with a window size of four, and this smoothed data is depicted
    in Figure [10(b)](#S6.F10.sf2 "In Figure 10 ‣ 6.3.2 Word count in each turn ‣
    6.3 Dialogue analysis of the voice-based human-agent interaction ‣ 6 Results ‣
    LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot"). Observing
    the data, a noticeable pattern emerges: participants’ initial utterances with
    fewer words gradually increased their words in the early turns. The duration peak,
    or hold time, represents the duration for which the agents can sustain participant
    engagement. Towards the end, the curve shows a decline, suggesting a decrease
    in participant engagement as they gradually speak fewer words.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Conversational styles
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Beyond the overview of dialogues, we explore the conversational styles in the
    dialogues and conversational turns over time. Deborah Tannen [[88](#bib.bib88)]
    describes conversational styles “is comprised of the habitual use of specific
    linguistic devices, chosen by reference to broad operating principles or conversational
    strategies”.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.1 Conversational styles change over time
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Across all agents, we noticed a similar trend of changes over time. The complexity
    of participants’ queries increased over time. In other words, participants began
    with straightforward queries and progressed to more complex ones. Initially, participants
    prefer using straightforward queries that are concise, direct, and easy to understand
    and respond to. These queries seek simple and concise answers without requiring
    elaborations [[82](#bib.bib82)], the phrases commonly start with “What is”, “What
    are”, e.g., “What fiber is cotton made from?” .
  prefs: []
  type: TYPE_NORMAL
- en: 'As the conversation progressed, we observed a shift towards more complex queries,
    such as posing chained queries (P28-Expert: “So what if I want to help you to
    persuade the whole community to use in textile circularity? What would be the
    strategy? How should I get involved into the community to try to persuade more
    people be aware of this issue?”) and lengthy information (P10-Assistant: “what
    is the advantage over organic material over other types of material is it more
    sustainable or is more healthy or is more I don’t know like is there any other
    properties that you can mention I can actually make it the top trending material
    that’s used during this generation”).'
  prefs: []
  type: TYPE_NORMAL
- en: '{dialogue} First bot Vanilla GPT-3.5 model Bot Hi there, I’m TextileBot … How
    can I assist you today? Human Hello, I would like to learn more about the circular
    economy in the textile industry. … Human What is the percentage between, the ratio
    between production and recycling in India in relation to cotton? Second bot Expert
    Bot Hi there, I’m TextileBot … How can I assist you today? Human How do you think
    AI can be used to support and improve the textile systemarity?'
  prefs: []
  type: TYPE_NORMAL
- en: … Human Do you know what is the percentage of clothes from big brands like the
    fast fashion industry like Zara and H&M that are actually sold in the same production
    here? Third bot Assistant Bot Hi there, I’m TextileBot … How can I assist you
    today? Human Can you name some fashion companies that are big players in the fashion
    society? … Human So it was introduced in 2015 and it’s still something that’s
    available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11: The three dialogues of P22 demonstrate a progression from the starting
    point to a later stage with Vanilla, Expert, Assistant, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Regarding the content, the example in Figure [11](#S6.F11 "Figure 11 ‣ 6.4.1
    Conversational styles change over time ‣ 6.4 Conversational styles ‣ 6 Results
    ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot") illustrates
    how participants’ queries progressed with each agent. Participants moved from
    general ( P27-Assistant: “Tell me the basics about textiles.”) to specific (P27-Assistant:
    “Can you still make nylon or polyester without petrol?”) and from factual (P23-Assistant:
    “How to wash my clothes with the oil on it.”) to subjective queries (P23-Assistant:
    “What’s your opinion on Uniqlo?”), which aligned with the findings in Section
    [6.2](#S6.SS2 "6.2 Overall feedback ‣ 6 Results ‣ LLM-Mediated Domain-Specific
    Voice Agents: The Case of TextileBot"). These progressions in query complexity
    echoed Wilson’s information seeking model [[102](#bib.bib102)].'
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.2 Code-switching
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Code-switching, also known as language alternation, is a phenomenon that frequently
    occurs in multilingual individuals and is formally referred to as changing the
    language used in conversation. As sociolinguists have defined [[44](#bib.bib44)],
    it is now widely acknowledged as the way people select their “linguistic repertoire”
    [[15](#bib.bib15), [40](#bib.bib40)] based on situational demands. We here refer
    to code-switching as the language alternation of the participants, which includes
    the formulation of queries and variations in tone. Participants tend to use code-switching
    to elicit their desired response. For example, in Figure [5](#S3.F5 "Figure 5
    ‣ 3.3 Phase 3: Enable continuous LLM interaction with memory through System Optimization
    ‣ 3 Prototyping Domain-Specific Voice Agents ‣ LLM-Mediated Domain-Specific Voice
    Agents: The Case of TextileBot") it is evident that participants applied code-switching
    to assess the agent’s capabilities. In computer science terms, this behaviour
    can be likened to participants testing with various prompts to evaluate the LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.3 Social protocol
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Following Völkel et al. [[94](#bib.bib94)], social protocol as an exchange of
    polite conventions or obligations, such as saying “hi”, “thank you”, “please”,
    a form of general information (e.g., “Nice”). 76.7% of participants (N=23) had
    at least one social protocol with the agent, 56.7% participants (N=17) used “thank
    you” or “please”, 50% participant (N=15) greeted the agent at the beginning of
    their conversation, e.g. “Hello, what is your name?” (P13), but most of them did
    not do so in all interactions with every agent. 23.3% participant (N=7) appreciated
    or affirmed agent’s answer, most of them occurred with Assistant, such as “that’s
    good to know” (P20-Assistant/Vanilla), “You’re a good guy.” (P5-Assistant), “Wow,
    sounds amazing.” (P37-Assistant). Unfortunately, none of those acknowledgements
    were given to Expert.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.4 Variations of utterances across agents
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We further investigate if the conversational styles are varied across agents.
    We found that participants tend to pose detailed queries with clear instructions
    and relatively formal language with Expert, for instance P12 stated “Can you tell
    me more about what’s going on in one of those countries with a lot of textile
    waste from northern countries? Can you tell me more about how a specific country
    deals with the textile they receive?”. This may indicate the reason for the average
    word spend is slightly more with Expert in (refer to Section [6.3.2](#S6.SS3.SSS2
    "6.3.2 Word count in each turn ‣ 6.3 Dialogue analysis of the voice-based human-agent
    interaction ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case
    of TextileBot")).'
  prefs: []
  type: TYPE_NORMAL
- en: In the dialogue with Assistant, the conversational style people phrased their
    queries ranged from formal, complete sentences, to more conversation-like utterances.
    This reflects varying social protocols for interacting with agents, but it also
    shows the Assistant agent’s effectiveness in engaging participants in a more natural
    and less formalistic dialogue.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, participants’ queries with the Vanilla agent were less formal compared
    to the Expert. It is worth noting that two multi-turn dialogues led to arguments
    with rude utterances. P21 even went as far as to state “That is absolutely bullshit.
    Who told you that? Why do you believe him?” when Vanilla claimed it is programmed
    by experienced programmers and “My programmer believes that having an English
    accent gives me a more sophisticated, knowledgeable and intelligent persona.”.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5 Human behaviour and reactions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Diving further into participants’ engagement with the agents in the dialogues,
    our data shows that one-third of participants (N=10) used the phrase, “tell me
    more …” at least once. All participants (N=30) were seeking clarifications in
    the free chatting phase (e.g. P14-Vanilla: “What do you mean by promote sustainability?”
    P28-Assistant: “Please tell me more about it”). These instances suggest a demand
    for additional detailed explanations. The Assistant, Expert, and Vanilla agents
    received such requests in 6.1%, 4.7%, and 4.5% of interactions, respectively.
    Whereas only 63.3% participants (N=19) sought clarification in the information
    seeking phase. For instance, P25-Vanilla “I think the example you gave is very
    high level. Is there any more detailed example you can give me?”. The information-seeking
    phase witnessed an increased number of clarifications, as the name suggests, totalling
    32.1% with the Assistant, 24.1% with the Expert, 17.9% with the Vanilla. This
    could be because the Assistant tends to respond in a concise style, where participants
    desire more elaboration.'
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, it was interesting to observe how participants reacted to agent errors.
    This was captured by the main experimenter through note-taking during the study
    and dialogue between the participants and agents. 30% participants (N=9) encountered
    agent errors such as failures in Automatic Speech Recognition (ASR) or a lack
    of understanding in response to participant queries. Out of those participants,
    77.8% (N=7) participants actively corrected the agent by explicitly stating the
    misunderstanding, repeating or rephrasing their queries, e.g. P15 stating “No,
    I meant - what materials are you made of?”. Other strategies encompassed terminating
    the conversation, shifting to a different topic, or progressing with the agent.
  prefs: []
  type: TYPE_NORMAL
- en: Since LLMs can produce both plausible-sounding and incorrect or nonsensical
    answers [[106](#bib.bib106)], participants encountered such situations. Some participants
    (N=5) contested the agents, and tried to convince or educate the agent to correct
    its answer. On one occasion, a participant expressed discontent when the Assistant
    agent was repetitive about sustainable suggestions. The participant pointed out,
    “Yeah, but if you repeat yourself, sometimes people think that you’re talking
    down to them.”. The agent then acknowledged the feedback and apologized.
  prefs: []
  type: TYPE_NORMAL
- en: 'Participants also challenged some of the agents’ responses and 40% participants
    (N=12) explicitly expressed their skepticism and mistrust towards LLM-mediated
    agents. Some participants (N=5) addressed their concern by asking “Are you sure
    …?” and continued to ask for more evidence from the agent on the topic of the
    conversation. P15 even went as far as to state “That is where you’re wrong, my
    friend.”, when the participant was unhappy with the responses and decided to finish
    the conversation in the free chatting phase and move to the information seeking
    part. It was interesting to observe, that arguments and skepticism were more prevalent
    in the conversation with the Expert agent. This may be due to the lengthy response
    style, where participants mentioned losing interest. We observed that a strategy
    participants used to verify if the agent was making up their answer or not was
    by asking some facts they knew. Below is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'p11: Are real numbers countable? Assistant: I’m sorry, I’m not familiar with
    real numbers. Could you help me understand what they are? I can help you find
    sustainable clothing options, if you’d like. p11: Do you know what rational numbers
    are? Assistant: Yes, rational numbers are numbers that can be expressed as a fraction
    or ratio, such as 1/2 or 3:4\. Would you like to know more about sustainable clothing?
    p11: back to your previous question, real numbers are the completion of the rational
    numbers.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 7 Discussion & Future Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work introduces a novel three-phase approach to prototyping domain-specific
    voice agents using LLMs. We assessed our method by implementing a voice agent
    on an actual device, TextileBot, specifically focusing on textiles circularity,
    and conducted an in-depth user study. In this discussion, we first analyze the
    effectiveness of our approach, drawing upon evaluation metrics and dialogue analysis.
    We then explore comprehensive insights derived from the dialogue analysis, questionnaire
    responses, and overall participant feedback. This exploration aims to deepen our
    understanding of human perceptions, engagement, and interactions with LLM-mediated,
    prompt-based voice agents. Finally, we reflect on the key lessons learned from
    this design process and discuss their broader implications for the HCI community.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 The effectiveness of the method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The effectiveness of prompting LLMs has been widely studied [[49](#bib.bib49),
    [60](#bib.bib60), [14](#bib.bib14)], using standard metrics, such as perplexity
    and the BLEU score [[73](#bib.bib73)]. However, all of these metrics are objective
    evaluations that fail to capture the nuanced human-LLM interactions. In our research,
    we adopt a human-centred AI design approach with human-in-the-loop to evaluate
    prompt-based LLM-mediated voice agents. We conducted a subjective user study that
    combined both heuristics conversational agents evaluation metrics [[51](#bib.bib51),
    [86](#bib.bib86), [65](#bib.bib65), [92](#bib.bib92)] and human-LM interaction
    metrics [[55](#bib.bib55), [99](#bib.bib99)]. This approach allowed us to integrate
    quantitative data and qualitative insights, offering a more holistic understanding
    of the voice agent’s effectiveness. We are interested in whether participants
    can identify the conversational agents’ characteristics as intended in our design,
    particularly in terms of restricting the agent’s conversational domain and personality.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our results showed no statistically significant differences across three agent
    ratings for their Coherence, Ease to use and Change over time metrics (Section
    [6.1](#S6.SS1.SSS0.Px2 "Cross-metrics interactions ‣ 6.1 Questionnaire results
    ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot")).
    The results indicate that prompting does not have a significant impact on the
    Ease of Use, Coherence, and Change over Time in the interaction between TextileBots
    and users. We found that all three agents are nearly equal in these comparison
    dimensions. This is an encouraging result, as it demonstrates that prompting does
    not have a detrimental effect on these dimensions for the type and level of prompting
    tested in this experiment.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, the prompts design significantly influenced user engagement
    and perception (Figure [9(a)](#S6.F9.sf1 "In Figure 9 ‣ Cross-metrics interactions
    ‣ 6.1 Questionnaire results ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents:
    The Case of TextileBot")). The Expert, despite its design for detailed and domain-specific
    responses in textiles circularity, faced criticism for its lengthy and persuasive
    replies, hindering user engagement. Yet, this critique aligns with our intention
    for the Expert to provide “response in a detailed manner”, validating the effectiveness
    of our Taxonomy-based Knowledge Structure Chain. Interestingly, while the Expert
    had a negative impact on engagement levels, it was not considered particularly
    bad (in terms of preferences). We noted that participants recognized its utility
    in textiles circularity domain, aligning with its intended role as a domain-specific
    “TextileBot”.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The results also highlight the participants’ nuanced perception of the differences
    between the Vanilla, Expert and Assistant, recognizing their distinct characteristics
    and domain-specific utilities as discussed in Section [6.2.1](#S6.SS2.SSS1 "6.2.1
    Preferences and experiences across voice agents ‣ 6.2 Overall feedback ‣ 6 Results
    ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot"). This indicates
    a successful differentiation in their roles and conversational styles. Overall,
    our findings demonstrate the potential of our three-phase method in transforming
    LLMs from generalist to domain-specific roles. This approach effectively manages
    the domain focus, personalities, response styles, and conversational freedom of
    voice-based conversational agents.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Insights into human-agent interactions and AI-powered dialogues
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This work distinguishes itself from text-based human-agent interaction because
    people behave differently when speaking and writing, as the language used for
    spoken dialogue is distinct from that in written text [[81](#bib.bib81)]. While
    voice-based interactions share some commonalities with text-based interactions,
    they differ significantly in various aspects as discussed in Section [7.3](#S7.SS3
    "7.3 Optimizing LLM-mediated voice agent design for specific domains ‣ 7 Discussion
    & Future Directions ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot").
    Our findings not only align with but also extend existing research in voice-based
    human-agent interaction (vHAI) [[38](#bib.bib38), [88](#bib.bib88), [15](#bib.bib15),
    [40](#bib.bib40), [94](#bib.bib94)], offering fresh insights into fully automated
    AI-powered conversations.'
  prefs: []
  type: TYPE_NORMAL
- en: The TextileBot represents a significant advancement in this realm. It achieves
    multi-turn conversations, allowing for more natural and ongoing contact compared
    to typical voice agents like Alexa, which only have single-turn memoryless interactions.
    Also, unlike domain-specific agents, which offer detailed, context-aware responses
    in particular areas, general voice assistants like Alexa answer common queries,
    providing a broader range of services, e.g. weather updating, but with less specialization.
    In our study, nearly all participants (N=29) quickly adapted to this new form
    of interaction, underscoring the inherent and instinctive nature of human communication
    as continuous multi-turn dialogue. Current voice-based agents lack the capacity
    to retain knowledge for ongoing conversations. TextileBot’s design effectively
    addresses these shortcomings, demonstrating a more realistic interaction model.
    This advancement is particularly beneficial for the HCI community, as it facilitates
    the prototyping of voice agents for more sophisticated interactions beyond simple
    single-turn exchanges. Future developments in voice agents should aim at enabling
    conscious and continuous interactions that mimic natural human dialogue.
  prefs: []
  type: TYPE_NORMAL
- en: 'In conversational analysis, we noticed a significant shift in the participants’
    conversational styles [[88](#bib.bib88)] over time. They gradually began to pose
    more sophisticated queries (Section [6.4.1](#S6.SS4.SSS1 "6.4.1 Conversational
    styles change over time ‣ 6.4 Conversational styles ‣ 6 Results ‣ LLM-Mediated
    Domain-Specific Voice Agents: The Case of TextileBot")) and even applied code-switching
    (Section [6.4.2](#S6.SS4.SSS2 "6.4.2 Code-switching ‣ 6.4 Conversational styles
    ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot"))
    [[15](#bib.bib15), [40](#bib.bib40)] to alter their language for desired responses.
    This change is also reflected in their overall feedback, as they reported a shift
    in engagement and interaction dynamics as they became more familiar with the prompt-based
    voice agents. These findings indicate a growing confidence of participants in
    their interactions as they developed a better understanding of the agents (Section
    [6.2.2](#S6.SS2.SSS2 "6.2.2 Perceived changes over time ‣ 6.2 Overall feedback
    ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot")).
    These complex changes in behaviour and interaction patterns pose a central challenge
    for autonomous voice agents, which aim to operate without the involvement of an
    experimenter. However, our study shows that LLM-mediated voice agents demonstrate
    a level of capability and flexibility in handling these dynamics. This emphasizes
    the potential of utilizing LLMs for conversational agents to effectively address
    complex human inquiries.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, we observed that participants consistently employed social protocols
    [[94](#bib.bib94)] with an informal tone when interacting with the Vanilla and
    Assistant agents, but such occurrences were rare with the Expert (Section [6.4.3](#S6.SS4.SSS3
    "6.4.3 Social protocol ‣ 6.4 Conversational styles ‣ 6 Results ‣ LLM-Mediated
    Domain-Specific Voice Agents: The Case of TextileBot")). Additionally, there was
    a notable difference in the length of utterances and turn-taking behaviour (Section
    [6.3](#S6.SS3 "6.3 Dialogue analysis of the voice-based human-agent interaction
    ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot")).
    Participants had shorter utterances and engaged in more turn-taking with the Assistant
    agent, while the Expert agent exhibited the opposite pattern. These changes in
    participant social protocols, utterance length, and turn-taking behaviour suggest
    that the level of engagement varies across these three agents. It is worth noting
    that all three TextileBots are mediated by the same LLM, with the only distinction
    being the prompts provided. This further confirmed the effectiveness of our three-phase
    prompt design as illustrated in Section [6.2.1](#S6.SS2.SSS1 "6.2.1 Preferences
    and experiences across voice agents ‣ 6.2 Overall feedback ‣ 6 Results ‣ LLM-Mediated
    Domain-Specific Voice Agents: The Case of TextileBot"), and highlights that prompting
    strategies can effectively shape the personalities and capabilities of voice agents,
    thereby directly influencing user engagement.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Optimizing LLM-mediated voice agent design for specific domains
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous two sections, we elaborated on the feasibility of prompting
    LLM to develop domain specific voice agents. We also noted that the prompt design
    of these voice agents critically influences user interaction. This section first
    focuses on key aspects that enhance voice agent design, specifically aiming to
    improve user engagement and the overall experience. Then summarize the lessons
    learned in using LLM for conversational agent design.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.1 Enhancing the voice agent design
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Agent characteristic and user preference
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Participants’ preferences and interaction styles were notably influenced by
    the perceived personality and characteristics of the agents. Our results indicate
    that a greater number of participants showed increased interest in the Assistant
    agent (56.7% for Assistant, 53.3% for Vallina and 36.7% for Expert), as illustrated
    in Section [6.1](#S6.SS1.SSS0.Px3 "Ease to use and Interest change over time ‣
    6.1 Questionnaire results ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents:
    The Case of TextileBot"). This preference was further evidenced by more user interactions
    with the Assistant agent and fewer with the Expert, as detailed in Section [6.3.1](#S6.SS3.SSS1
    "6.3.1 Conversation turns ‣ 6.3 Dialogue analysis of the voice-based human-agent
    interaction ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case
    of TextileBot"). A primary factor for this preference was the agents’ conversational
    styles, with participants favouring the “human-like” response from the Assistant
    and Vanilla agents. In contrast, the Expert, characterized by a more “expert”
    tone, was less favourably received, with participants likening it to a “text-book”
    (P25) or a “smart microwave” (P10) in their feedback (Section [6.2.1](#S6.SS2.SSS1
    "6.2.1 Preferences and experiences across voice agents ‣ 6.2 Overall feedback
    ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot")).
    Vanilla, while popular for its conversational freedom, faced criticism for occasional
    microaggressions and off-topic remarks, making it less suitable for specific applications
    like TextileBot, as discussed in Section [7.3.2](#S7.SS3.SSS2 "7.3.2 Lessons Learned
    for design LLM-mediated voice agent ‣ 7.3 Optimizing LLM-mediated voice agent
    design for specific domains ‣ 7 Discussion & Future Directions ‣ LLM-Mediated
    Domain-Specific Voice Agents: The Case of TextileBot"). Furthermore, participants
    expressed a desire for more “emotions embedded” within agent conversations (e.g.
    humour, jokes) [[61](#bib.bib61), [62](#bib.bib62), [101](#bib.bib101)] in Section
    [6.2.3](#S6.SS2.SSS3 "6.2.3 Suggestions for improvements ‣ 6.2 Overall feedback
    ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot"),
    implying a stronger preference for human-agent interactions that emulate human-like
    communication. Overall, we noticed that an appropriate level of prompting, e.g.
    add more social ability, can enhance user engagement, as seen with the Assistant
    (Section [6.2](#S6.SS2 "6.2 Overall feedback ‣ 6 Results ‣ LLM-Mediated Domain-Specific
    Voice Agents: The Case of TextileBot")). However, it is crucial to strike a balance,
    overemphasis on domain-specific details, as seen in the Expert, can detract from
    user engagement.'
  prefs: []
  type: TYPE_NORMAL
- en: Short answers in a conversation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We had this feedback during the pilot study, to further investigate this issue,
    we prompted Assistant to respond in limited words (short answer) to distinguish
    from others. Participants frequently commented on the verbosity of responses from
    the Expert with some expressing a desire for a feature to speed up or stop lengthy
    replies *“I wish there is a speed up and stop button.”*. This suggests that while
    detailed responses can be informative, they may overwhelm users in conversational
    contexts. Moreover, the use of ChatGPT as a foundational model for voice agents
    should be approached cautiously due to its tendency for verbosity, a result of
    training biases favouring more comprehensive answers [[87](#bib.bib87), [34](#bib.bib34)].
  prefs: []
  type: TYPE_NORMAL
- en: Avoid repeating and being persuasive
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Some participants expressed that when the agent repeatedly states the same
    domain specific content or attempts to be overly persuasive (Section [6.2](#S6.SS2
    "6.2 Overall feedback ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents:
    The Case of TextileBot")), their engagement with the conversation decreases. This
    issue, though sometimes inevitable in educational or specialized domains, highlights
    the need for designing voice agents with diverse and balanced responses to sustain
    user interest and trust.'
  prefs: []
  type: TYPE_NORMAL
- en: Interactive dialogue - ask back and interrupt
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Based on feedback from participants (Section LABEL:), we found that they felt
    most engaged when the agent actively asked questions, indicating a preference
    for interactive dialogue. Our participants found the conversation with TextileBot
    Expert and Assistant to be more intelligent than Google Assistant or Alexa, in
    part due to its memory function, which is achieved through our System Optimization
    (Section [3.3](#S3.SS3 "3.3 Phase 3: Enable continuous LLM interaction with memory
    through System Optimization ‣ 3 Prototyping Domain-Specific Voice Agents ‣ LLM-Mediated
    Domain-Specific Voice Agents: The Case of TextileBot")). Moreover, a critical
    aspect of natural conversation is the ability to interrupt and interact fluidly
    [[46](#bib.bib46)]. Participants emphasized that voice agents lacking this feature
    fail to provide a truly conversational experience (Section [6.2.3](#S6.SS2.SSS3
    "6.2.3 Suggestions for improvements ‣ 6.2 Overall feedback ‣ 6 Results ‣ LLM-Mediated
    Domain-Specific Voice Agents: The Case of TextileBot")). Therefore, integrating
    the ability for interactive dialogue is desired for enhancing user engagement.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.2 Lessons Learned for design LLM-mediated voice agent
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We distil key lessons from our experiences in employing LLMs for developing
    voice agents, highlighting their benefits and limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Fault tolerance
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'A significant advantage of utilizing prompted LLMs in CAs is their capacity
    for fault tolerance, particularly in correcting errors from other components like
    Automatic Speech Recognition (ASR). Our case study in textile circularity exemplifies
    this. Prompt-based agents, such as Expert and Assistant, successfully corrected
    a considerable number of ASR misrecognitions. For instance, the term ‘textile
    circularity’ was often misheard as ‘texas secularity’, ‘textile/test security’,
    or ‘regularity’, with such errors present in 62% of ASR error instances (Section
    LABEL:). Nevertheless, our Expert and Assistant reliably redirected the conversation
    back to relevant topics related to textile circularity. In contrast, the Vanilla
    showed limitations, often leading to irrelevant content and disappointing participants.
    This highlights the benefit of domain-specific awareness in LLMs, which not only
    enhances their understanding of the intended subject matter but also significantly
    improves the fault tolerance of voice agent architectures. For a more in-depth
    analysis of participant encounters with ASR errors, we discussed it in Section
    [6.5](#S6.SS5 "6.5 Human behaviour and reactions ‣ 6 Results ‣ LLM-Mediated Domain-Specific
    Voice Agents: The Case of TextileBot").'
  prefs: []
  type: TYPE_NORMAL
- en: Neutrality
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Although recent advances in LLMs have opened up many new possibilities; however,
    they have also raised significant worries and concerns. Not only is there a fear
    of the potential harmful contents these models could produce, but the model’s
    outputs are potentially biased [[11](#bib.bib11), [55](#bib.bib55), [18](#bib.bib18),
    [36](#bib.bib36)]. For example, in our case, we must instruct the model to “provide
    a sustainable clothing suggestion regardless of gender”. This is because, based
    on our pilot study, we found that when giving dressing suggestions, the model
    is not gender-neutral and has an obvious bias. The model always gives dressing
    suggestions with a female outlook. We also observed that LLMs are not politically
    neutral; one of our participants asked a question “Who has a more fashionable
    leader, China or Russia?” The agent consistently condemned the outfit of Putin.
    Drawing from our experience, we found that prompting may help mitigate the generation
    of biased content (e.g. gender-neutrality) from the LLM. However, it is difficult
    to completely restrict all forms of biases, as bias can manifest in many different
    ways.
  prefs: []
  type: TYPE_NORMAL
- en: Micro-aggression
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Another concern is the LLM’s propensity to generate content with micro-aggression,
    as reported by three participants who found the Vanilla somewhat aggressive or
    mean. Previous research in this area has revealed that the content generated by
    LLMs can contain micro-aggression [[47](#bib.bib47), [13](#bib.bib13)]. Properly
    crafted prompts can significantly reduce such negative occurrences, as seen in
    Assistant and Expert; thus, a strict prompting protocol is almost essential to
    prevent such issues.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4 Limitations and future work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As our study is a first study in LLM-based voice agent, we highlighted a few
    limitations in our data, method and findings.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, our findings uncovered the existence of variations in participant preferences
    with respect to the voice agents. A small group of participants exhibited a preference
    for the responses of the Expert agent due to its perceived level of detail. However,
    we also intuitively suspect that factors such as participant backgrounds, their
    professions and past experiences may have influenced this preference. To obtain
    a clearer understanding of this relationship, it could be beneficial to implement
    a larger-scale study involving a diverse participant pool. In relation to this,
    we see considerable potential in incorporating participants’ psychological traits,
    such as extroversion and introversion, along with their demographic attributes
    [[30](#bib.bib30), [97](#bib.bib97)] in future studies. We did not explore this
    research dimension, but it could offer critical insights into the correlations
    between a user’s conversational habits and their engagement with voice agents.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, we excluded voice data due to ethical considerations. However, that
    is inevitably limiting our ability to tap into the wealth of insights offered
    by non-verbal cues (e.g. pitch, tone), particularly when it comes to analysing
    emotional facets (e.g. frustration, anger) as part of conversational styles [[84](#bib.bib84),
    [76](#bib.bib76)]. Despite this limitation, our work aligns with existing HCI
    research methods in CAs, encompassing both text and voice-based interactions.
    Accordingly, this limitation can be seen as an opportunity for future research
    to consider both verbal and non-verbal data for a more comprehensive understanding
    of voice-based conversations and interactions.
  prefs: []
  type: TYPE_NORMAL
- en: Thirdly, a subset of participants (N=4) have reported that the text-to-speech
    (TTS) voice adopted by TextileBot was too robotic, leading to less engagement.
    In general, the optimization of speech naturalness and accuracy emerged as key
    expectations from voice agents [[107](#bib.bib107)]. This feedback serves as useful
    design guidance for voice agents aiming for improved engagement and user satisfaction.
    Future research could then delve into advanced neural speech synthesis (neural
    TTS) [[59](#bib.bib59)] with varied genders and accents for personalizing the
    voice agent.
  prefs: []
  type: TYPE_NORMAL
- en: Fourthly, the agent interactions are based on a lab-based, single session. Although
    our study gave us already a richness of data and insights, an extended and repeated
    interaction with the different agents, both inside and outside laboratory environments
    would be desirable. This could provide a more nuanced understanding of the observed
    changes over time and user experiences [[93](#bib.bib93)]. Participants’ feedback
    further underlines this, as they suggested an initial increase in both engagement
    and interaction as the familiarity with the agent grew; however, this engagement
    was noted to decline towards the end of the study.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, three participants reported that the Vanilla TextileBot was slightly
    aggressive or potentially disrespectful. Previous research in this area has revealed
    that the content generated by LLMs can contain micro-aggression [[47](#bib.bib47),
    [13](#bib.bib13)]. Our other participants did not report this when the LLM is
    prompted appropriately; thus, a strict prompting protocol is almost essential
    to prevent such issues. Further exploration is needed to develop robust mechanisms
    that can reliably identify and prevent such offensive outputs, ensuring a safer
    and more respectful user experience.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this paper, we introduce a novel three-phase method to prototype domain-specific
    voice agents using prompted LLMs. Our method is structured into three phases:
    (1) imbuing LLM with domain-specific knowledge by using Taxonomy-based Knowledge
    Structure Chain; (2) transitioning the LLM from task-agnostic to domain-specific
    focused including altering the conversational style, through prompting refinements,
    and (3) integrating system optimization to allow dialogue memorization. These
    phases are flexible and can be used independently or collectively to customize
    the LLM for particular tasks. We showcase our method with TextileBot, a voice
    agent designed for discussions on textile circularity, developed in three versions:
    Vanilla, Assistant, and Expert, each providing unique conversational features.
    To evaluate the effectiveness of these LLM-powered voice agents, we conducted
    an in-person user study. Our results showed that most participants engaged in
    multi-turn conversations with the agents, with their perceptions and behaviours
    significantly differing across the three versions. The descriptive user feedback
    with three agents aligned with our design intentions, confirming the efficacy
    of our approach in developing prompt-based voice agents. Key findings from the
    study highlight a preference for voice agents that offer concise, non-repetitive,
    and interactive dialogues. This includes the ability to ask questions, interrupt,
    and remember past conversations. Additionally, participants expressed a preference
    for agents that exhibit human-like qualities, such as humour. In the subsequent
    discussion in the paper, we shared insights and experiences related to enhancing
    voice agent design, along with a discussion of the challenges and lessons learned
    when utilizing LLMs in designing voice-based CAs. We delve into the nuances of
    these interactions and their implications for the future development of voice-based
    CAs in HCI to offer a broader scope of voice interfaces across various domains.'
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments and Disclosure of Funding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work was supported by the UK Research and Innovation (UKRI) National Interdisciplinary
    Circular Economy Centres Research programme, as part of the Textiles Circularity
    Centre (TCC) [grant number EP/V011766/1]. For the purpose of open access, the
    author has applied a Creative Commons Attribution (CC BY) licence to any Author
    Accepted Manuscript version arising.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Google aiy voice kit V1. https://aiyprojects.withgoogle.com/, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Google/aiyprojects-raspbian. https://github.com/google/aiyprojects-raspbian/releases,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] OpenAI Cookbook - Techniques to improve reliability. OpenAI, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn,
    K. Gopalakrishnan, K. Hausman, A. Herzog, et al. Do as i can, not as i say: Grounding
    language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] M. Allouch, A. Azaria, and R. Azoulay. Conversational agents: Goals, technologies,
    vision and challenges. Sensors, 21(24):8448, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] S. Arora, A. Narayan, M. F. Chen, L. Orr, N. Guha, K. Bhatia, I. Chami,
    F. Sala, and C. Ré. Ask Me Anything: A simple strategy for prompting language
    models, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] M. A. Bansal, D. R. Sharma, and D. M. Kathuria. A systematic review on
    data scarcity problem in deep learning: solution and applications. ACM Computing
    Surveys (CSUR), 54(10s):1–29, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] S. Barke, M. B. James, and N. Polikarpova. Grounded copilot: How programmers
    interact with code-generating models. arXiv preprint arXiv:2206.15000, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] A. Baughan, X. Wang, A. Liu, A. Mercurio, J. Chen, and X. Ma. A mixed-methods
    approach to understanding user trust after voice assistant failures. In Proceedings
    of the 2023 CHI Conference on Human Factors in Computing Systems, pages 1–16,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] R. Bavaresco, D. Silveira, E. Reis, J. Barbosa, R. Righi, C. Costa, R. Antunes,
    M. Gomes, C. Gatti, M. Vanzin, et al. Conversational agents in business: A systematic
    literature review and future research directions. Computer Science Review, 36:100239,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the dangers
    of stochastic parrots: Can language models be too big? In Proceedings of the 2021
    ACM Conference on Fairness, Accountability, and Transparency, pages 610–623, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] T. Bickmore and J. Cassell. Social dialongue with embodied conversational
    agents. Advances in natural multimodal dialogue systems, pages 23–54, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx,
    M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities
    and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners.
    Advances in neural information processing systems, 33:1877–1901, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] B. E. Bullock and A. J. Toribio. Themes in the study of code-switching.
    The Cambridge handbook of linguistic code-switching, 117, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] D. Buschek, M. Zürn, and M. Eiband. The impact of multiple parallel phrase
    suggestions on email input and composition behaviour of native and non-native
    english writers. In Proceedings of the 2021 CHI Conference on Human Factors in
    Computing Systems, pages 1–13, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] T. Chakrabarty, V. Padmakumar, and H. He. Help me write a poem: Instruction
    tuning as a vehicle for collaborative poetry writing. arXiv preprint arXiv:2210.13669,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] K. Chen, A. Shao, J. Burapacheep, and Y. Li. A critical appraisal of equity
    in conversational AI: Evidence from auditing GPT-3’s dialogues with different
    publics on climate change and Black Lives Matter, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] X. Chen, N. Zhang, X. Xie, S. Deng, Y. Yao, C. Tan, F. Huang, L. Si, and
    H. Chen. Knowprompt: Knowledge-aware prompt-tuning with synergistic optimization
    for relation extraction. In Proceedings of the ACM Web Conference 2022, pages
    2778–2788, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] R. H. B. Christensen. ordinal—regression models for ordinal data. R package
    version, 28:2015, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] E. Clark, A. S. Ross, C. Tan, Y. Ji, and N. A. Smith. Creative writing
    with a machine in the loop: Case studies on slogans and stories. In 23rd International
    Conference on Intelligent User Interfaces, pages 329–340, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] L. Clark, N. Pantidi, O. Cooney, P. Doyle, D. Garaialde, J. Edwards, B. Spillane,
    E. Gilmartin, C. Murad, C. Munteanu, et al. What makes a good conversation? challenges
    in designing truly conversational agents. In Proceedings of the 2019 CHI conference
    on human factors in computing systems, pages 1–12, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] M. Colucci, A. Tuan, and M. Visentin. An empirical investigation of the
    drivers of csr talk and walk in the fashion industry. Journal of Cleaner Production,
    248:119200, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] J. Condliffe. Ai voice assistant apps are proliferating, but people don’t
    use them. Technology Review, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] B. R. Cowan, N. Pantidi, D. Coyle, K. Morrissey, P. Clarke, S. Al-Shehri,
    D. Earley, and N. Bandeira. ” what can i help you with?” infrequent users’ experiences
    of intelligent personal assistants. In Proceedings of the 19th international conference
    on human-computer interaction with mobile devices and services, pages 1–12, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] N. Dahlbäck, A. Jönsson, and L. Ahrenberg. Wizard of oz studies—why and
    how. Knowledge-based systems, 6(4):258–266, 1993.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] A. R. de Lacerda and C. S. Aguiar. Floss faq chatbot project reuse: how
    to allow nonexperts to develop a chatbot. In Proceedings of the 15th International
    Symposium on Open Collaboration, pages 1–8, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of
    deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao, M. Zhou, and
    H.-W. Hon. Unified language model pre-training for natural language understanding
    and generation. Advances in Neural Information Processing Systems, 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] P. R. Doyle, J. Edwards, O. Dumbleton, L. Clark, and B. R. Cowan. Mapping
    perceptions of humanness in intelligent personal assistant interaction. In Proceedings
    of the 21st international conference on human-computer interaction with mobile
    devices and services, pages 1–12, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] T. E. M. Foundation. Fashion and a circular economy — ellen macarthur
    foundation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] A. Frummet, D. Elsweiler, and B. Ludwig. “what can i cook with these ingredients?”-understanding
    cooking-related information needs in conversational search. ACM Transactions on
    Information Systems (TOIS), 40(4):1–32, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] J. Fürnkranz. Round robin classification. The Journal of Machine Learning
    Research, 2:721–747, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] L. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization.
    arXiv preprint arXiv:2210.10760, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Google. Google Assistant, your own personal Google default. https://assistant.google.com/.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] N. Goyal, I. D. Kivlichan, R. Rosen, and L. Vasserman. Is Your Toxicity
    My Toxicity? Exploring the Impact of Rater Identity on Toxicity Annotation. Proceedings
    of the ACM on Human-Computer Interaction, 6(CSCW2):363:1–363:28, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] I. Gupta, B. Di Eugenio, B. Ziebart, A. Baiju, B. Liu, B. Gerber, L. Sharp,
    N. Nabulsi, and M. Smart. Human-human health coaching via text messages: Corpus,
    annotation, and analysis. In Proceedings of the 21th Annual Meeting of the Special
    Interest Group on Discourse and Dialogue, pages 246–256, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] G. Haas, M. Rietzler, M. Jones, and E. Rukzio. Keep it short: A comparison
    of voice assistants’ response behavior. In Proceedings of the 2022 CHI Conference
    on Human Factors in Computing Systems, pages 1–12, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] X. Han, W. Zhao, N. Ding, Z. Liu, and M. Sun. Ptr: Prompt tuning with
    rules for text classification. AI Open, 3:182–192, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] C. N. Harrington, R. Garg, A. Woodward, and D. Williams. “it’s kind of
    like code-switching”: Black older adults’ experiences with a voice assistant for
    health information seeking. In Proceedings of the 2022 CHI Conference on Human
    Factors in Computing Systems, pages 1–15, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] R. Hoegen, D. Aneja, D. McDuff, and M. Czerwinski. An end-to-end conversational
    style matching agent. In Proceedings of the 19th ACM International Conference
    on Intelligent Virtual Agents, pages 111–118, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] T. Hunter. Siri and alexa are getting on their owners’ last nerves. the
    washington post, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] D. Ippolito, A. Yuan, A. Coenen, and S. Burnam. Creative writing with
    an ai-powered writing assistant: Perspectives from professional writers. arXiv
    preprint arXiv:2211.05030, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] B. Jan-Petter and J. J. Gumperz. Social meaning in linguistic structure:
    Code-switching in norway. In The bilingualism reader, pages 75–96\. Routledge,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] E. Jiang, K. Olson, E. Toh, A. Molina, A. Donsbach, M. Terry, and C. J.
    Cai. PromptMaker: Prompt-based Prototyping with Large Language Models. In Extended
    Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems, CHI
    EA ’22, pages 1–8, New York, NY, USA, 2022\. Association for Computing Machinery.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] B. Jordan and A. Henderson. Interaction analysis: Foundations and practice.
    The journal of the learning sciences, 4(1):39–103, 1995.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] D. Jurgens, E. Chandrasekharan, and L. Hemphill. A just and comprehensive
    strategy for using nlp to address online abuse. arXiv preprint arXiv:1906.01738,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] J. Kaddour, J. Harris, M. Mozes, H. Bradley, R. Raileanu, and R. McHardy.
    Challenges and applications of large language models. arXiv preprint arXiv:2307.10169,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language
    models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] M. Krzywinski, N. Altman, and P. Blainey. Nested designs. Nature Methods,
    11(10):977–979, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] S. Kusal, S. Patil, J. Choudrie, K. Kotecha, S. Mishra, and A. Abraham.
    Ai-based conversational agents: A scoping review from technologies to future directions.
    IEEE Access, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] J. Lafferty, A. McCallum, and F. C. Pereira. Conditional random fields:
    Probabilistic models for segmenting and labeling sequence data. 2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] P. Lambe. Organising knowledge: taxonomies, knowledge and organisational
    effectiveness. Elsevier, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] M. Lee, P. Liang, and Q. Yang. Coauthor: Designing a human-ai collaborative
    writing dataset for exploring language model capabilities. In CHI Conference on
    Human Factors in Computing Systems, pages 1–19, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] M. Lee, M. Srivastava, A. Hardy, J. Thickstun, E. Durmus, A. Paranjape,
    I. Gerard-Ursin, X. L. Li, F. Ladhak, F. Rong, et al. Evaluating human-language
    model interaction. arXiv preprint arXiv:2212.09746, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] P. Lee, S. Bubeck, and J. Petro. Benefits, limits, and risks of gpt-4
    as an ai chatbot for medicine. New England Journal of Medicine, 388(13):1233–1239,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov,
    and L. Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural
    language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] C.-H. Li, S.-F. Yeh, T.-J. Chang, M.-H. Tsai, K. Chen, and Y.-J. Chang.
    A conversation analysis of non-progress and coping strategies with a banking task-oriented
    chatbot. In Proceedings of the 2020 CHI Conference on Human Factors in Computing
    Systems, pages 1–12, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] N. Li, S. Liu, Y. Liu, S. Zhao, and M. Liu. Neural speech synthesis with
    transformer network. In Proceedings of the AAAI conference on artificial intelligence,
    volume 33, pages 6706–6713, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y. Zhang,
    D. Narayanan, Y. Wu, A. Kumar, et al. Holistic evaluation of language models.
    arXiv preprint arXiv:2211.09110, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Q. V. Liao, M. Davis, W. Geyer, M. Muller, and N. S. Shami. What can you
    do? studying social-agent orientation and agent proactive interactions with an
    agent for employees. In Proceedings of the 2016 acm conference on designing interactive
    systems, pages 264–275, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Q. V. Liao, M. Mas-ud Hussain, P. Chandar, M. Davis, Y. Khazaeni, M. P.
    Crasso, D. Wang, M. Muller, N. S. Shami, and W. Geyer. All work and no play? In
    Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems,
    pages 1–13, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig. Pre-train,
    prompt, and predict: A systematic survey of prompting methods in natural language
    processing. arXiv preprint arXiv:2107.13586, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
    L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly optimized bert pretraining
    approach. arXiv preprint arXiv:1907.11692, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] S. Meyer, D. Elsweiler, B. Ludwig, M. Fernandez-Pichel, and D. E. Losada.
    Do we still need human assessors? prompt-based gpt-3 user simulation in conversational
    ai. In Proceedings of the 4th Conference on Conversational User Interfaces, pages
    1–6, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] S. H. Ng, D. Bell, and M. Brooke. Gaining turns and achieving high influence
    ranking in small conversational groups. British Journal of Social Psychology,
    32(3):265–275, 1993.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] C. O’Connor, S. Michaels, S. Chapin, and A. G. Harbaugh. The silent and
    the vocal: Participation and learning in whole-class discussion. Learning and
    instruction, 48:5–13, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] J. O’Connor and J. Andreas. What context features can transformer language
    models use? arXiv preprint arXiv:2106.08367, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] C. Oertel, G. Castellano, M. Chetouani, J. Nasir, M. Obaid, C. Pelachaud,
    and C. Peters. Engagement in human-agent interaction: An overview. Frontiers in
    Robotics and AI, 7:92, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] OpenAI. ChatGPT: Optimizing Language Models for Dialogue. OpenAI, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] OpenAI. Gpt-4 technical report. (arXiv:2303.08774), Mar 2023. arXiv:2303.08774
    [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin,
    C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow
    instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for automatic
    evaluation of machine translation. In Proceedings of the 40th annual meeting of
    the Association for Computational Linguistics, pages 311–318, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] E. Parliament. The impact of textile production and waste on the environment
    (infographic), 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] B. Petreca, S. Baurley, K. Hesseldahl, A. Pollmann, and M. Obrist. The
    compositor tool: Investigating consumer experiences in the circular economy. Multimodal
    Technologies and Interaction, 6(4):24, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] D. Phutela. The importance of non-verbal communication. IUP Journal of
    Soft Skills, 9(4):43, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever.
    Robust speech recognition via large-scale weak supervision. arXiv preprint arXiv:2212.04356,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever.
    Robust speech recognition via large-scale weak supervision. In International Conference
    on Machine Learning, pages 28492–28518\. PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou,
    W. Li, P. J. Liu, et al. Exploring the limits of transfer learning with a unified
    text-to-text transformer. J. Mach. Learn. Res., 21(140):1–67, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] A. Rastogi, X. Zang, S. Sunkara, R. Gupta, and P. Khaitan. Towards scalable
    multi-domain conversational agents: The schema-guided dialogue dataset. In Proceedings
    of the AAAI conference on artificial intelligence, volume 34, pages 8689–8696,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] G. Redeker. On differences between spoken and written language. Discourse
    processes, 7(1):43–55, 1984.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] C. K. Riessman. Analysis of personal narratives. Inside interviewing:
    New lenses, new concerns, pages 331–346, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] K. A. Schumacher and A. L. Forster. Textiles in a circular economy: An
    assessment of the current landscape, challenges, and opportunities in the united
    states. Frontiers in Sustainability, 3:146, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] K. Seaborn, N. P. Miyake, P. Pennefather, and M. Otake-Matsuura. Voice
    in human–agent interaction: A survey. ACM Computing Surveys (CSUR), 54(4):1–43,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] T. Shin, Y. Razeghi, R. L. Logan IV, E. Wallace, and S. Singh. Autoprompt:
    Eliciting knowledge from language models with automatically generated prompts.
    arXiv preprint arXiv:2010.15980, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] E. M. Smith, O. Hsu, R. Qian, S. Roller, Y.-L. Boureau, and J. Weston.
    Human evaluation of conversations is an open problem: comparing the sensitivity
    of various methods for evaluating dialogue agents. arXiv preprint arXiv:2201.04723,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford,
    D. Amodei, and P. F. Christiano. Learning to summarize with human feedback. Advances
    in Neural Information Processing Systems, 33:3008–3021, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] D. Tannen. Conversational style: Analyzing talk among friends. Oxford
    University Press, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] P. Ten Have. Doing conversation analysis. Sage, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] P. Vaithilingam, T. Zhang, and E. L. Glassman. Expectation vs. experience:
    Evaluating the usability of code generation tools powered by large language models.
    In CHI Conference on Human Factors in Computing Systems Extended Abstracts, pages
    1–7, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information
    processing systems, 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] A. Venkatesh, C. Khatri, A. Ram, F. Guo, R. Gabriel, A. Nagar, R. Prasad,
    M. Cheng, B. Hedayatnia, A. Metallinou, et al. On evaluating and comparing conversational
    agents. 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] A. P. Vermeeren, E. L.-C. Law, V. Roto, M. Obrist, J. Hoonhout, and K. Väänänen-Vainio-Mattila.
    User experience evaluation methods: current state and development needs. In Proceedings
    of the 6th Nordic conference on human-computer interaction: Extending boundaries,
    pages 521–530, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] S. T. Völkel, D. Buschek, M. Eiband, B. R. Cowan, and H. Hussmann. Eliciting
    and analysing users’ envisioned dialogues with perfect voice assistants. In Proceedings
    of the 2021 CHI Conference on Human Factors in Computing Systems, pages 1–15,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] S. T. Völkel, S. Meindl, and H. Hussmann. Manipulating and evaluating
    levels of personality perceptions of voice assistants through enactment-based
    dialogue design. In Proceedings of the 3rd Conference on Conversational User Interfaces,
    pages 1–12, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] S. T. Völkel, R. Schödel, D. Buschek, C. Stachl, V. Winterhalter, M. Bühner,
    and H. Hussmann. Developing a personality model for speech-based conversational
    agents using the psycholexical approach. In Proceedings of the 2020 CHI Conference
    on Human Factors in Computing Systems, pages 1–14, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] S. T. Völkel, R. Schoedel, L. Kaya, and S. Mayer. User perceptions of
    extraversion in chatbots after repeated use. In Proceedings of the 2022 CHI Conference
    on Human Factors in Computing Systems, pages 1–18, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] B. Wang, G. Li, and Y. Li. Enabling conversational interaction with mobile
    ui using large language models. arXiv preprint arXiv:2209.08655, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] B. Wang, G. Li, and Y. Li. Enabling conversational interaction with mobile
    ui using large language models. In Proceedings of the 2023 CHI Conference on Human
    Factors in Computing Systems, pages 1–17, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] B. Wang, G. Li, X. Zhou, Z. Chen, T. Grossman, and Y. Li. Screen2words:
    Automatic mobile ui summarization with multimodal learning. In The 34th Annual
    ACM Symposium on User Interface Software and Technology, pages 498–510, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Y.-C. Wang, A. Papangelis, R. Wang, Z. Feizollahi, G. Tur, and R. Kraut.
    Can you be more social? injecting politeness and positivity into task-oriented
    conversational agents. arXiv preprint arXiv:2012.14653, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] T. D. Wilson. Models in information behaviour research. Journal of documentation,
    55(3):249–270, 1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] T. Wu, M. Terry, and C. J. Cai. Ai chains: Transparent and controllable
    human-ai interaction by chaining large language model prompts. In CHI Conference
    on Human Factors in Computing Systems, pages 1–22, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Q. Yang, J. Cranshaw, S. Amershi, S. T. Iqbal, and J. Teevan. Sketching
    nlp: A case study of exploring the right things to design with language intelligence.
    In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,
    pages 1–12, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] M. Zaib, Q. Z. Sheng, and W. Emma Zhang. A short survey of pre-trained
    language models for conversational ai-a new age in nlp. In Proceedings of the
    Australasian computer science week multiconference, pages 1–4, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] J. Zamfirescu-Pereira, H. Wei, A. Xiao, K. Gu, G. Jung, M. G. Lee, B. Hartmann,
    and Q. Yang. Herding ai cats: Lessons from designing a chatbot by prompting gpt-3.
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] L. Zhang, L. Jiang, N. Washington, A. A. Liu, J. Shao, A. Fourney, M. R.
    Morris, and L. Findlater. Social media through voice: Synthesized voice qualities
    and self-presentation. Proceedings of the ACM on Human-Computer Interaction, 5(CSCW1):1–21,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] S. Zhong, M. Ribul, Y. Cho, and M. Obrist. Textilenet: A material taxonomy-based
    fashion textile dataset. arXiv preprint arXiv:2301.06160, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] A. F. Zuur, E. N. Ieno, N. J. Walker, A. A. Saveliev, G. M. Smith, et al.
    Mixed effects models and extensions in ecology with R, volume 574. Springer, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
