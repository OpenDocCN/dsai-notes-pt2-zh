- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»åˆ«ï¼šæœªåˆ†ç±»
- en: 'date: 2024-09-08 18:42:02'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¥æœŸï¼š2024-09-08 18:42:02
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Richelieu: è‡ªæˆ‘è¿›åŒ–çš„åŸºäºLLMçš„äººå·¥æ™ºèƒ½å¤–äº¤ä»£ç†'
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2407.06813](https://ar5iv.labs.arxiv.org/html/2407.06813)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2407.06813](https://ar5iv.labs.arxiv.org/html/2407.06813)
- en: Zhenyu Guan
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Zhenyu Guan
- en: Peking University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: åŒ—äº¬å¤§å­¦
- en: gzyxxn@stu.pku.edu.cn
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: gzyxxn@stu.pku.edu.cn
- en: '&Xiangyu Kong^(ğŸ–‚)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '&Xiangyu Kong^(ğŸ–‚)'
- en: Beijing Information Science and Technology University
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åŒ—äº¬ä¿¡æ¯ç§‘æŠ€å¤§å­¦
- en: xykong@bistu.edu.cn
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: xykong@bistu.edu.cn
- en: Fangwei Zhong^(ğŸ–‚)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Fangwei Zhong^(ğŸ–‚)
- en: Peking University
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åŒ—äº¬å¤§å­¦
- en: zfw@pku.edu.cn
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: zfw@pku.edu.cn
- en: '&Yizhou Wang'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '&Yizhou Wang'
- en: Peking University
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: åŒ—äº¬å¤§å­¦
- en: yizhou.wang@pku.edu.cn
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: yizhou.wang@pku.edu.cn
- en: Abstract
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: 'Diplomacy is one of the most sophisticated activities in human society. The
    complex interactions among multiple parties/ agents involve various abilities
    like social reasoning, negotiation arts, and long-term strategy planning. Previous
    AI agents surely have proved their capability of handling multi-step games and
    larger action spaces on tasks involving multiple agents. However, diplomacy involves
    a staggering magnitude of decision spaces, especially considering the negotiation
    stage required. Recently, LLM agents have shown their potential for extending
    the boundary of previous agents on a couple of applications, however, it is still
    not enough to handle a very long planning period in a complex multi-agent environment.
    Empowered with cutting-edge LLM technology, we make the first stab to explore
    AIâ€™s upper bound towards a human-like agent for such a highly comprehensive multi-agent
    mission by combining three core and essential capabilities for stronger LLM-based
    societal agents: 1) strategic planner with memory and reflection; 2) goal-oriented
    negotiate with social reasoning; 3) augmenting memory by self-play games to self-evolving
    without any human in the loop.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å¤–äº¤æ˜¯äººç±»ç¤¾ä¼šä¸­æœ€å¤æ‚çš„æ´»åŠ¨ä¹‹ä¸€ã€‚å¤šä¸ªæ–¹/ä»£ç†ä¹‹é—´çš„å¤æ‚äº’åŠ¨æ¶‰åŠç¤¾äº¤æ¨ç†ã€è°ˆåˆ¤è‰ºæœ¯å’Œé•¿æœŸæˆ˜ç•¥è§„åˆ’ç­‰å¤šç§èƒ½åŠ›ã€‚ä¹‹å‰çš„AIä»£ç†ç¡®å®è¯æ˜äº†å®ƒä»¬åœ¨å¤„ç†å¤šæ­¥éª¤æ¸¸æˆå’Œæ¶‰åŠå¤šä¸ªä»£ç†çš„ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¤–äº¤æ¶‰åŠåˆ°æƒŠäººçš„å†³ç­–ç©ºé—´è§„æ¨¡ï¼Œå°¤å…¶æ˜¯è€ƒè™‘åˆ°æ‰€éœ€çš„è°ˆåˆ¤é˜¶æ®µã€‚æœ€è¿‘ï¼ŒLLMä»£ç†åœ¨å‡ ä¸ªåº”ç”¨ä¸­å±•ç¤ºäº†å…¶æ‰©å±•å‰ä»£ç†è¾¹ç•Œçš„æ½œåŠ›ï¼Œä½†åœ¨å¤æ‚çš„å¤šä»£ç†ç¯å¢ƒä¸­å¤„ç†éå¸¸é•¿çš„è§„åˆ’å‘¨æœŸä»ç„¶ä¸å¤Ÿã€‚å€ŸåŠ©å°–ç«¯çš„LLMæŠ€æœ¯ï¼Œæˆ‘ä»¬é¦–æ¬¡å°è¯•é€šè¿‡ç»“åˆä¸‰ç§æ ¸å¿ƒå’ŒåŸºæœ¬èƒ½åŠ›æ¥æ¢ç´¢äººå·¥æ™ºèƒ½åœ¨è¿™ç§é«˜åº¦ç»¼åˆçš„å¤šä»£ç†ä»»åŠ¡ä¸­çš„ä¸Šé™ï¼š1ï¼‰å…·æœ‰è®°å¿†å’Œåæ€çš„æˆ˜ç•¥è§„åˆ’è€…ï¼›2ï¼‰å…·æœ‰ç¤¾äº¤æ¨ç†çš„ç›®æ ‡å¯¼å‘è°ˆåˆ¤è€…ï¼›3ï¼‰é€šè¿‡è‡ªæˆ‘æ¸¸æˆå¢å¼ºè®°å¿†ï¼Œå®ç°è‡ªæˆ‘è¿›åŒ–è€Œæ— éœ€ä»»ä½•äººå·¥å¹²é¢„ã€‚
- en: 1 Introduction
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 å¼•è¨€
- en: Diplomacy, as a cornerstone of international relations, is an intricate and
    multifaceted activity that lies at the heart of human societyâ€™s most complex interactions.
    It encompasses a wide array of skills and strategies, including social reasoning,
    negotiation, and long-term planning, to navigate the intricate web of relationships
    and alliances between multiple parties. Mirroring this complexity, the board game
    Diplomacy[[59](#bib.bib59)] involves seven players to control European powers,
    presenting a complex strategic challenge that requires both sophisticated negotiation
    and strategic planning to triumph.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å¤–äº¤ä½œä¸ºå›½é™…å…³ç³»çš„åŸºçŸ³ï¼Œæ˜¯ä¸€é¡¹é”™ç»¼å¤æ‚çš„æ´»åŠ¨ï¼Œå¤„äºäººç±»ç¤¾ä¼šæœ€å¤æ‚äº’åŠ¨çš„æ ¸å¿ƒã€‚å®ƒæ¶µç›–äº†å¹¿æ³›çš„æŠ€èƒ½å’Œç­–ç•¥ï¼ŒåŒ…æ‹¬ç¤¾äº¤æ¨ç†ã€è°ˆåˆ¤å’Œé•¿æœŸè§„åˆ’ï¼Œä»¥é©¾é©­å¤šä¸ªæ–¹ä¹‹é—´é”™ç»¼å¤æ‚çš„å…³ç³»å’Œè”ç›Ÿã€‚ä¸æ­¤å¤æ‚æ€§ç›¸æ˜ ï¼Œæ¡Œæ¸¸ã€Šå¤–äº¤ã€‹[[59](#bib.bib59)]
    æ¶‰åŠä¸ƒåç©å®¶æ§åˆ¶æ¬§æ´²å¤§å›½ï¼Œæå‡ºäº†ä¸€ä¸ªå¤æ‚çš„æˆ˜ç•¥æŒ‘æˆ˜ï¼Œéœ€è¦ç²¾å·§çš„è°ˆåˆ¤å’Œæˆ˜ç•¥è§„åˆ’æ‰èƒ½æˆåŠŸã€‚
- en: 'The AI community has shown an increasing interest in the deployment of AI agents
    to master such gamesÂ [[45](#bib.bib45), [26](#bib.bib26), [29](#bib.bib29), [15](#bib.bib15),
    [36](#bib.bib36), [28](#bib.bib28)]. The recent breakthroughÂ [[6](#bib.bib6)]
    has turned into press diplomacy, which allows communication between players. However,
    the previous methodsÂ [[6](#bib.bib6)] heavily rely on domain-specific human data,
    leading to its poor generalization to other scenarios/ applications. The question
    then arises: Can we build an AI agent that excels in the art of diplomacy without
    relying on domain-specific human data?'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: äººå·¥æ™ºèƒ½ç¤¾åŒºå¯¹éƒ¨ç½²AIä»£ç†ä»¥æŒæ¡æ­¤ç±»æ¸¸æˆè¡¨ç°å‡ºè¶Šæ¥è¶Šå¤§çš„å…´è¶£[[45](#bib.bib45), [26](#bib.bib26), [29](#bib.bib29),
    [15](#bib.bib15), [36](#bib.bib36), [28](#bib.bib28)]ã€‚æœ€è¿‘çš„çªç ´[[6](#bib.bib6)]è½¬å˜ä¸ºæ–°é—»å¤–äº¤ï¼Œå…è®¸ç©å®¶ä¹‹é—´çš„æ²Ÿé€šã€‚ç„¶è€Œï¼Œä»¥å‰çš„æ–¹æ³•[[6](#bib.bib6)]ä¸¥é‡ä¾èµ–ç‰¹å®šé¢†åŸŸçš„äººç±»æ•°æ®ï¼Œå¯¼è‡´å…¶åœ¨å…¶ä»–åœºæ™¯/åº”ç”¨ä¸­çš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚é‚£ä¹ˆé—®é¢˜æ¥äº†ï¼šæˆ‘ä»¬èƒ½å¦æ„å»ºä¸€ä¸ªåœ¨å¤–äº¤è‰ºæœ¯ä¸Šè¡¨ç°å‡ºè‰²çš„AIä»£ç†ï¼Œè€Œä¸ä¾èµ–äºç‰¹å®šé¢†åŸŸçš„äººç±»æ•°æ®ï¼Ÿ
- en: Recently, agents based on the Large Language Model(LLM) have emerged as a promising
    development for AI agents. The previous applications on personal assistantsÂ [[32](#bib.bib32)],
    roboticsÂ [[10](#bib.bib10), [64](#bib.bib64)], and video gamesÂ [[48](#bib.bib48)]
    have shown the surprising ability of LLM-based agents in communication and planning,
    benefiting from the emergent ability of common sense reasoning, in-context/ few-shot
    learning, and sophisticated natural language processing on LLMs. However, diplomacy
    presents a unique set of challenges. It not only requires planning long-horizon
    strategicÂ [[40](#bib.bib40)] and communicating with natural language, but also
    reasoning and adopting the complex social dynamics with partial observations,
    including gaining trust and reputation, building rapport, detecting deception,
    and assessing the reliability of other players.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†å·²ç»æˆä¸ºAIä»£ç†çš„ä¸€ä¸ªæœ‰å‰æ™¯çš„å‘å±•æ–¹å‘ã€‚ä¹‹å‰åœ¨ä¸ªäººåŠ©ç†[[32](#bib.bib32)]ã€æœºå™¨äºº[[10](#bib.bib10),
    [64](#bib.bib64)]å’Œè§†é¢‘æ¸¸æˆ[[48](#bib.bib48)]ä¸Šçš„åº”ç”¨æ˜¾ç¤ºäº†LLMåŸºäºä»£ç†åœ¨æ²Ÿé€šå’Œè§„åˆ’æ–¹é¢çš„æƒŠäººèƒ½åŠ›ï¼Œå¾—ç›ŠäºLLMçš„å¸¸è¯†æ¨ç†ã€ä¸Šä¸‹æ–‡/å°‘é‡å­¦ä¹ å’Œå¤æ‚è‡ªç„¶è¯­è¨€å¤„ç†çš„æ¶Œç°èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¤–äº¤å¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚å®ƒä¸ä»…éœ€è¦è§„åˆ’é•¿æœŸæˆ˜ç•¥[[40](#bib.bib40)]å’Œç”¨è‡ªç„¶è¯­è¨€è¿›è¡Œæ²Ÿé€šï¼Œè¿˜éœ€è¦æ¨ç†å’Œé‡‡ç”¨å¤æ‚çš„ç¤¾ä¼šåŠ¨æ€ï¼ŒåŒ…æ‹¬è·å¾—ä¿¡ä»»å’Œå£°èª‰ã€å»ºç«‹å…³ç³»ã€æ£€æµ‹æ¬ºéª—å’Œè¯„ä¼°å…¶ä»–å‚ä¸è€…çš„å¯é æ€§ã€‚
- en: 'In this work, we aim to make the first attempt to explore LLMsâ€™ potential to
    develop a human-like AI diplomacy agent. We name the agent Richelieu in memorizing
    a pivotal figure in European history who had enduring impacts on French politics,
    foreign affairs, and state building. To achieve this goal, we have identified
    three core and essential capabilities that are crucial for building an LLM-based
    societal agent: 1) Social reasoning This is the basic function for a social agent
    to interact with others, particularly for adapting to the dynamic changes in the
    nationâ€™s intentions and relationships. 2) Balance long- and short-term planning
    Diplomacy often requires a delicate balance between short-term tactics and long-term
    strategies. An effective AI agent must be able to recognize and weigh the immediate
    consequences of its actions against the potential long-term outcomes. 3) Powerful
    memory A robust memory system is a critical component of learning and improvement.
    The AI agent must be able to recall and integrate information from past negotiations
    and actions to inform its current and future decision-making processes. This endows
    the agent with the ability to evolve. 4) Profound reflection An AI agent capable
    of profound reflection can analyze its own decisions, learn from its memory experience,
    and adapt its strategies accordingly. By integrating these three capabilities,
    the agent can operate at the highest level of diplomatic sophistication, outperforming
    the state-of-the-art AI diplomatsÂ [[6](#bib.bib6)].'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨é¦–æ¬¡æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å‘å±•ç±»äººAIå¤–äº¤ä»£ç†æ–¹é¢çš„æ½œåŠ›ã€‚æˆ‘ä»¬å°†è¯¥ä»£ç†å‘½åä¸º**é‡Œè°¢ç•™**ï¼Œä»¥çºªå¿µè¿™ä½åœ¨æ¬§æ´²å†å²ä¸Šå…·æœ‰é‡è¦å½±å“çš„å…³é”®äººç‰©ï¼Œä»–å¯¹æ³•å›½æ”¿æ²»ã€å¤–äº¤äº‹åŠ¡å’Œå›½å®¶å»ºè®¾äº§ç”Ÿäº†æ·±è¿œå½±å“ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬ç¡®å®šäº†ä¸‰ä¸ªæ ¸å¿ƒä¸”è‡³å…³é‡è¦çš„èƒ½åŠ›ï¼Œè¿™äº›èƒ½åŠ›å¯¹äºæ„å»ºåŸºäºLLMçš„ç¤¾ä¼šä»£ç†è‡³å…³é‡è¦ï¼š1)
    ç¤¾ä¼šæ¨ç† è¿™æ˜¯ç¤¾ä¼šä»£ç†ä¸ä»–äººäº’åŠ¨çš„åŸºæœ¬åŠŸèƒ½ï¼Œç‰¹åˆ«æ˜¯é€‚åº”å›½å®¶æ„å›¾å’Œå…³ç³»çš„åŠ¨æ€å˜åŒ–ã€‚2) å¹³è¡¡é•¿æœŸå’ŒçŸ­æœŸè§„åˆ’ å¤–äº¤å¾€å¾€éœ€è¦åœ¨çŸ­æœŸæˆ˜æœ¯å’Œé•¿æœŸæˆ˜ç•¥ä¹‹é—´å–å¾—å¾®å¦™çš„å¹³è¡¡ã€‚ä¸€ä¸ªæœ‰æ•ˆçš„AIä»£ç†å¿…é¡»èƒ½å¤Ÿè®¤è¯†å¹¶æƒè¡¡å…¶è¡ŒåŠ¨çš„å³æ—¶åæœä¸æ½œåœ¨çš„é•¿æœŸç»“æœã€‚3)
    å¼ºå¤§çš„è®°å¿†ç³»ç»Ÿ è¿™æ˜¯å­¦ä¹ å’Œæ”¹è¿›çš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚AIä»£ç†å¿…é¡»èƒ½å¤Ÿå›å¿†å’Œæ•´åˆè¿‡å»è°ˆåˆ¤å’Œè¡ŒåŠ¨ä¸­çš„ä¿¡æ¯ï¼Œä»¥æŒ‡å¯¼å½“å‰å’Œæœªæ¥çš„å†³ç­–è¿‡ç¨‹ã€‚è¿™ä½¿å¾—ä»£ç†å…·å¤‡äº†è¿›åŒ–çš„èƒ½åŠ›ã€‚4)
    æ·±åˆ»åæ€ ä¸€ä¸ªèƒ½å¤Ÿæ·±åˆ»åæ€çš„AIä»£ç†å¯ä»¥åˆ†æè‡ªèº«çš„å†³ç­–ï¼Œä»è®°å¿†ç»éªŒä¸­å­¦ä¹ ï¼Œå¹¶ç›¸åº”åœ°è°ƒæ•´å…¶ç­–ç•¥ã€‚é€šè¿‡æ•´åˆè¿™ä¸‰ç§èƒ½åŠ›ï¼Œè¯¥ä»£ç†å¯ä»¥åœ¨å¤–äº¤å¤æ‚æ€§æ–¹é¢è¾¾åˆ°æœ€é«˜æ°´å¹³ï¼Œè¶…è¶Šæœ€å…ˆè¿›çš„AIå¤–äº¤å®˜[[6](#bib.bib6)]ã€‚
- en: 'Our contributions can be summarized in three-fold: 1) We introduced a new paradigm
    for building AI diplomacy agents, compared to previous work (Fig.Â [1](#S1.F1 "Figure
    1 â€£ 1 Introduction â€£ Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy")).
    The agent can self-evolve by generating experience via self-play games, without
    any task-specific human data. 2) We demonstrate the superior performance of our
    agent playing against the SOTA method, e.g., CiceroÂ [[6](#bib.bib6)], that relies
    on a large-scale human demonstration for training. 3) We further analyze the effectiveness
    of each module in our agent and the generalization of our agent in adopting different
    LLMs, such as [GPT4.0](https://openai.com/index/gpt-4/) and [Llamma 3](https://llama.meta.com/llama3).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 'æˆ‘ä»¬çš„è´¡çŒ®å¯ä»¥æ€»ç»“ä¸ºä¸‰ç‚¹ï¼š1) æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„AIå¤–äº¤ä»£ç†äººæ„å»ºèŒƒå¼ï¼Œç›¸è¾ƒäºä¹‹å‰çš„å·¥ä½œï¼ˆå›¾ [1](#S1.F1 "å›¾ 1 â€£ 1 ä»‹ç» â€£ Richelieu:
    è‡ªæˆ‘è¿›åŒ–çš„åŸºäºLLMçš„AIå¤–äº¤ä»£ç†äºº")ï¼‰ã€‚è¯¥ä»£ç†äººå¯ä»¥é€šè¿‡è‡ªæˆ‘å¯¹å¼ˆç”Ÿæˆç»éªŒï¼Œè‡ªæˆ‘è¿›åŒ–ï¼Œæ— éœ€ä»»ä½•ä»»åŠ¡ç‰¹å®šçš„äººç±»æ•°æ®ã€‚2) æˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„ä»£ç†äººåœ¨å¯¹æŠ—SOTAæ–¹æ³•ï¼ˆä¾‹å¦‚
    Cicero [[6](#bib.bib6)]ï¼‰æ—¶çš„ä¼˜è¶Šæ€§èƒ½ï¼Œè¯¥æ–¹æ³•ä¾èµ–äºå¤§è§„æ¨¡çš„äººç±»ç¤ºèŒƒè¿›è¡Œè®­ç»ƒã€‚3) æˆ‘ä»¬è¿›ä¸€æ­¥åˆ†æäº†ä»£ç†äººæ¯ä¸ªæ¨¡å—çš„æœ‰æ•ˆæ€§ä»¥åŠä»£ç†äººåœ¨é‡‡ç”¨ä¸åŒLLMï¼ˆå¦‚
    [GPT4.0](https://openai.com/index/gpt-4/) å’Œ [Llamma 3](https://llama.meta.com/llama3)ï¼‰æ—¶çš„æ³›åŒ–èƒ½åŠ›ã€‚'
- en: '![Refer to caption](img/d38077694619c7a6df46fb45a80df691.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/d38077694619c7a6df46fb45a80df691.png)'
- en: 'Figure 1: A new paradigm for building AI Diplomacy agent.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 1ï¼šæ„å»ºAIå¤–äº¤ä»£ç†äººçš„æ–°èŒƒå¼ã€‚
- en: 2 Related work
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 ç›¸å…³å·¥ä½œ
- en: AI Diplomacy. The game involves seven players controlling different powers in
    Europe. In each turn, players can negotiate for cooperation before making moves
    to take as much supply centers as they can. Apparently, this challenging strategy
    task requires both complex negotiation skills and superior planning capability
    for player agents to achieve final victory. So far, most AI research on this task
    remain focused on the planning strategies (a.k.a. No-Press Diplomacy where no
    communication channels are allowed). The setting remains challenging considering
    its enormous action space of $10^{2}1$ per turn (compared with Chess, which has
    much fewer than 100 actions per turn). No wonder existing efforts rely on human
    data to play the game. Among the methods, one typical research is DipNet [[39](#bib.bib39)]
    which uses supervised and reinforcement learning. Based on DipNet, BRPI [[3](#bib.bib3)],
    SearchBot [[18](#bib.bib18)], DORA [[5](#bib.bib5)], and KL-Regularized search
    (Diplodocus) [[24](#bib.bib24)] were conducted. Until very recently, research
    has also emerged for the full-setting of Diplomacy, or Press Diplomacy where players
    are allowed to communicate with each before making their moves in each turn. Such
    studies [[13](#bib.bib13)][[6](#bib.bib6)][[25](#bib.bib25)][[29](#bib.bib29)]
    mainly benefit from the recent thriving language models. Specifically, notable
    advancements include policy iteration methods from DeepMind and Facebook AI Researchâ€™s
    equilibrium search agent [[25](#bib.bib25)]. However, Deepmind propose to learn
    negotiation agents based on predefined contracts/protocols [[29](#bib.bib29)].
    And Meta AIâ€™s work, instead of one unified architecture, Cicero [[6](#bib.bib6)]
    integrates a language model for negotiation and an RL model for planning respectively.
    Such separately trained models make it inconvenient for agentsâ€™ continual evolution.
    Whatâ€™s more, like no-press methods, these approaches heavily rely on human player
    data for agent training. Unlike these approaches, this paper delves into solving
    the negotiation and planning in one single self-evolving LLM-based agent model,
    without any pre-collected human expert training data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: AIå¤–äº¤ã€‚è¿™æ¬¾æ¸¸æˆæ¶‰åŠä¸ƒåç©å®¶æ§åˆ¶æ¬§æ´²çš„ä¸åŒåŠ¿åŠ›ã€‚åœ¨æ¯ä¸€å›åˆï¼Œç©å®¶å¯ä»¥åœ¨è¿›è¡Œç§»åŠ¨ä¹‹å‰è¿›è¡Œåˆä½œè°ˆåˆ¤ï¼Œäº‰å–å°½å¯èƒ½å¤šçš„ä¾›åº”ä¸­å¿ƒã€‚æ˜¾ç„¶ï¼Œè¿™é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ç­–ç•¥ä»»åŠ¡è¦æ±‚ç©å®¶å…·å¤‡å¤æ‚çš„è°ˆåˆ¤æŠ€èƒ½å’Œå“è¶Šçš„è§„åˆ’èƒ½åŠ›ï¼Œä»¥å®ç°æœ€ç»ˆèƒœåˆ©ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œå¤§å¤šæ•°å…³äºè¿™é¡¹ä»»åŠ¡çš„AIç ”ç©¶ä»ç„¶é›†ä¸­åœ¨è§„åˆ’ç­–ç•¥ï¼ˆä¹Ÿç§°ä¸ºæ— æ²Ÿé€šå¤–äº¤ï¼Œå…¶ä¸­ä¸å…è®¸ä»»ä½•æ²Ÿé€šæ¸ é“ï¼‰ã€‚è€ƒè™‘åˆ°å…¶æ¯å›åˆçš„å·¨å¤§åŠ¨ä½œç©ºé—´$10^{2}1$ï¼ˆç›¸æ¯”äºæ£‹ç±»æ¸¸æˆï¼Œåè€…æ¯å›åˆçš„åŠ¨ä½œè¿œå°‘äº100ä¸ªï¼‰ï¼Œè¿™ä¸ªè®¾ç½®ä¾ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚éš¾æ€ªç°æœ‰çš„åŠªåŠ›ä¾èµ–äºäººç±»æ•°æ®æ¥è¿›è¡Œæ¸¸æˆã€‚åœ¨è¿™äº›æ–¹æ³•ä¸­ï¼Œä¸€ä¸ªå…¸å‹çš„ç ”ç©¶æ˜¯DipNet
    [[39](#bib.bib39)]ï¼Œå®ƒä½¿ç”¨äº†ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚åŸºäºDipNetï¼ŒBRPI [[3](#bib.bib3)]ã€SearchBot [[18](#bib.bib18)]ã€DORA
    [[5](#bib.bib5)]å’ŒKLæ­£åˆ™åŒ–æœç´¢ï¼ˆDiplodocusï¼‰[[24](#bib.bib24)]ä¹Ÿç›¸ç»§è¿›è¡Œäº†ç ”ç©¶ã€‚ç›´åˆ°æœ€è¿‘ï¼Œç ”ç©¶è€…ä»¬ä¹Ÿå¼€å§‹å…³æ³¨Diplomacyçš„å®Œæ•´è®¾ç½®ï¼Œæˆ–ç§°ä¸ºPress
    Diplomacyï¼Œå…¶ä¸­ç©å®¶åœ¨æ¯å›åˆè¿›è¡Œç§»åŠ¨ä¹‹å‰å…è®¸ç›¸äº’æ²Ÿé€šã€‚è¿™ç±»ç ”ç©¶[[13](#bib.bib13)][[6](#bib.bib6)][[25](#bib.bib25)][[29](#bib.bib29)]ä¸»è¦å—ç›Šäºæœ€è¿‘å…´èµ·çš„è¯­è¨€æ¨¡å‹ã€‚å…·ä½“è€Œè¨€ï¼Œæ˜¾è‘—çš„è¿›å±•åŒ…æ‹¬æ¥è‡ªDeepMindçš„ç­–ç•¥è¿­ä»£æ–¹æ³•å’ŒFacebook
    AI Researchçš„å‡è¡¡æœç´¢ä»£ç†[[25](#bib.bib25)]ã€‚ç„¶è€Œï¼ŒDeepMindå»ºè®®åŸºäºé¢„å®šä¹‰åˆåŒ/åè®®æ¥å­¦ä¹ è°ˆåˆ¤ä»£ç†[[29](#bib.bib29)]ã€‚è€ŒMeta
    AIçš„å·¥ä½œï¼ŒCicero [[6](#bib.bib6)]ï¼Œåˆ™åˆ†åˆ«é›†æˆäº†ç”¨äºè°ˆåˆ¤çš„è¯­è¨€æ¨¡å‹å’Œç”¨äºè§„åˆ’çš„RLæ¨¡å‹ã€‚è¿™äº›åˆ†åˆ«è®­ç»ƒçš„æ¨¡å‹ä½¿å¾—ä»£ç†çš„æŒç»­è¿›åŒ–å˜å¾—ä¸ä¾¿ã€‚æ­¤å¤–ï¼Œåƒæ— æ²Ÿé€šæ–¹æ³•ä¸€æ ·ï¼Œè¿™äº›æ–¹æ³•åœ¨ä»£ç†è®­ç»ƒä¸­ä¹Ÿä¸¥é‡ä¾èµ–äººç±»ç©å®¶æ•°æ®ã€‚ä¸è¿™äº›æ–¹æ³•ä¸åŒï¼Œæœ¬æ–‡æ·±å…¥æ¢è®¨äº†å¦‚ä½•åœ¨ä¸€ä¸ªè‡ªæˆ‘è¿›åŒ–çš„LLMåŸºç¡€çš„ä»£ç†æ¨¡å‹ä¸­åŒæ—¶è§£å†³è°ˆåˆ¤å’Œè§„åˆ’é—®é¢˜ï¼Œè€Œæ— éœ€ä»»ä½•é¢„å…ˆæ”¶é›†çš„äººç±»ä¸“å®¶è®­ç»ƒæ•°æ®ã€‚
- en: LLM-based Agents. With the emergence and growth of large language models (LLM),
    there is a growing trend in utilizing LLMs as fundamental controllers for autonomous
    agents[[52](#bib.bib52)]. One wide application genre is LLM-based answering engines,
    which merely cover the negotiation aspects of Diplomacy. Such systems include
    HuggingGPTÂ [[44](#bib.bib44)], GPT4ToolsÂ [[63](#bib.bib63)] and ToTÂ [[65](#bib.bib65)],
    etc. They leverage LLMs to manage Al models, use tools, implement policy iteration,
    and enhance problem-solving across various tasks. Related work including AutoGPT,
    AgentGPT, BabyAGl [[47](#bib.bib47)], Toolformer [[43](#bib.bib43)], and Visual
    ChatGPT aim to improve LLM capabilities in task automation and tool usage. And
    Reflexion, a framework that improves LLMs through linguistic feedback and episodic
    memory [[68](#bib.bib68)], facilitating better decision-making across diverse
    tasks is proposed. Besides [[55](#bib.bib55)][[50](#bib.bib50)][[56](#bib.bib56)][[76](#bib.bib76)][[62](#bib.bib62)]
    apply LLM agents to the complex planning tasks in the well-known open-world game
    Minecraft[[16](#bib.bib16)]. Unlike these LLM-based agents which only focus on
    the negotiation/planning aspect, the proposed approach involves multiple self-evolving
    schemes to handle both of them simultaneously.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäº LLM çš„ä»£ç†ã€‚éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°å’Œå‘å±•ï¼Œåˆ©ç”¨ LLM ä½œä¸ºè‡ªä¸»ä»£ç†çš„åŸºæœ¬æ§åˆ¶å™¨çš„è¶‹åŠ¿è¶Šæ¥è¶Šæ˜æ˜¾[[52](#bib.bib52)]ã€‚å…¶ä¸­ä¸€ä¸ªå¹¿æ³›çš„åº”ç”¨é¢†åŸŸæ˜¯åŸºäº
    LLM çš„å›ç­”å¼•æ“ï¼Œè¿™ä»…ä»…æ¶µç›–äº†ã€Šå¤–äº¤ã€‹æ¸¸æˆä¸­çš„è°ˆåˆ¤æ–¹é¢ã€‚è¿™äº›ç³»ç»ŸåŒ…æ‹¬ HuggingGPT [[44](#bib.bib44)]ã€GPT4Tools [[63](#bib.bib63)]
    å’Œ ToT [[65](#bib.bib65)] ç­‰ã€‚å®ƒä»¬åˆ©ç”¨ LLM æ¥ç®¡ç† AI æ¨¡å‹ï¼Œä½¿ç”¨å·¥å…·ï¼Œå®ç°æ”¿ç­–è¿­ä»£ï¼Œå¹¶åœ¨å„ç§ä»»åŠ¡ä¸­æé«˜é—®é¢˜è§£å†³èƒ½åŠ›ã€‚ç›¸å…³å·¥ä½œåŒ…æ‹¬
    AutoGPTã€AgentGPTã€BabyAGl [[47](#bib.bib47)]ã€Toolformer [[43](#bib.bib43)] å’Œ Visual
    ChatGPTï¼Œæ—¨åœ¨æé«˜ LLM åœ¨ä»»åŠ¡è‡ªåŠ¨åŒ–å’Œå·¥å…·ä½¿ç”¨ä¸­çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº† Reflexionï¼Œä¸€ä¸ªé€šè¿‡è¯­è¨€åé¦ˆå’Œæƒ…èŠ‚è®°å¿†[[68](#bib.bib68)]
    æ”¹å–„ LLM çš„æ¡†æ¶ï¼Œä¿ƒè¿›åœ¨å„ç§ä»»åŠ¡ä¸­çš„æ›´å¥½å†³ç­–ã€‚é™¤æ­¤ä¹‹å¤–[[55](#bib.bib55)][[50](#bib.bib50)][[56](#bib.bib56)][[76](#bib.bib76)][[62](#bib.bib62)]
    è¿˜å°† LLM ä»£ç†åº”ç”¨äºçŸ¥åå¼€æ”¾ä¸–ç•Œæ¸¸æˆ Minecraft[[16](#bib.bib16)] ä¸­çš„å¤æ‚è§„åˆ’ä»»åŠ¡ã€‚ä¸è¿™äº›åªå…³æ³¨è°ˆåˆ¤/è§„åˆ’æ–¹é¢çš„ LLM ä»£ç†ä¸åŒï¼Œæ‰€æè®®çš„æ–¹æ³•æ¶‰åŠå¤šç§è‡ªæˆ‘è¿›åŒ–æ–¹æ¡ˆï¼Œä»¥åŒæ—¶å¤„ç†è¿™ä¸¤æ–¹é¢ã€‚
- en: 3 Problem Statement
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 é—®é¢˜é™ˆè¿°
- en: The Diplomacy game [[59](#bib.bib59), [8](#bib.bib8)] is set in pre-World War
    I Europe and involves each player (agent) representing one of the seven Great
    Powers of Europe, such as Germany, France, England, Italy, Austria-Hungary, Russia,
    and Turkey. Each player has a set of military units, including armies and fleets,
    which they can move and use to capture other supply centers. The ultimate goal
    for the agent is to control a majority of the total supply centers on the board
    by the end of the gameâ€™s Fall phase. Itâ€™s important to note that it is not won
    by eliminating other players or their units; it is won by controlling the requisite
    number of supply centers. This often involves forming and breaking alliances,
    negotiating, and sometimes betraying other players to achieve oneâ€™s own goals.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ã€Šå¤–äº¤ã€‹æ¸¸æˆ [[59](#bib.bib59), [8](#bib.bib8)] è®¾å®šåœ¨ç¬¬ä¸€æ¬¡ä¸–ç•Œå¤§æˆ˜å‰çš„æ¬§æ´²ï¼Œæ¯ä¸ªç©å®¶ï¼ˆä»£ç†ï¼‰ä»£è¡¨æ¬§æ´²çš„ä¸ƒå¤§å¼ºå›½ä¹‹ä¸€ï¼Œå¦‚å¾·å›½ã€æ³•å›½ã€è‹±æ ¼å…°ã€æ„å¤§åˆ©ã€å¥¥åŒˆå¸å›½ã€ä¿„ç½—æ–¯å’ŒåœŸè€³å…¶ã€‚æ¯ä¸ªç©å®¶æ‹¥æœ‰ä¸€ç»„å†›äº‹å•ä½ï¼ŒåŒ…æ‹¬é™†å†›å’Œæµ·å†›ï¼Œä»–ä»¬å¯ä»¥ç§»åŠ¨è¿™äº›å•ä½å¹¶åˆ©ç”¨å…¶å é¢†å…¶ä»–è¡¥ç»™ä¸­å¿ƒã€‚ä»£ç†çš„æœ€ç»ˆç›®æ ‡æ˜¯é€šè¿‡æ¸¸æˆç§‹å­£é˜¶æ®µç»“æŸæ—¶æ§åˆ¶æ£‹ç›˜ä¸Šå¤§å¤šæ•°çš„è¡¥ç»™ä¸­å¿ƒã€‚é‡è¦çš„æ˜¯è¦æ³¨æ„ï¼Œæ¸¸æˆçš„èƒœåˆ©ä¸æ˜¯é€šè¿‡æ¶ˆç­å…¶ä»–ç©å®¶æˆ–ä»–ä»¬çš„å•ä½æ¥è·å¾—çš„ï¼Œè€Œæ˜¯é€šè¿‡æ§åˆ¶æ‰€éœ€æ•°é‡çš„è¡¥ç»™ä¸­å¿ƒæ¥è·å¾—çš„ã€‚è¿™é€šå¸¸æ¶‰åŠå½¢æˆå’Œæ‰“ç ´è”ç›Ÿã€è°ˆåˆ¤ä»¥åŠæœ‰æ—¶èƒŒå›å…¶ä»–ç©å®¶ä»¥å®ç°è‡ªå·±çš„ç›®æ ‡ã€‚
- en: In each turn,[[58](#bib.bib58), [53](#bib.bib53)] the agent $i$ are commands
    to the armies, such as moving into an adjacent territory, supporting another unit,
    or holding a position. Actions can also include diplomatic moves, such as proposing
    or withdrawing from an alliance, although these are less formalized in the game
    mechanics.[[39](#bib.bib39), [22](#bib.bib22)]
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¯ä¸ªå›åˆä¸­[[58](#bib.bib58), [53](#bib.bib53)]ï¼Œä»£ç† $i$ ä¼šå¯¹å†›é˜Ÿå‘å‡ºå‘½ä»¤ï¼Œå¦‚è¿›å…¥é‚»è¿‘é¢†åœŸã€æ”¯æŒå¦ä¸€ä¸ªå•ä½æˆ–ä¿æŒé˜µåœ°ã€‚è¡ŒåŠ¨è¿˜å¯ä»¥åŒ…æ‹¬å¤–äº¤ä¸¾æªï¼Œä¾‹å¦‚æè®®æˆ–é€€å‡ºè”ç›Ÿï¼Œå°½ç®¡è¿™äº›åœ¨æ¸¸æˆæœºåˆ¶ä¸­ä¸å¤Ÿæ­£å¼[[39](#bib.bib39),
    [22](#bib.bib22)]ã€‚
- en: 4 Self-Evolving LLM-based Diplomat
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 è‡ªæˆ‘è¿›åŒ–çš„åŸºäº LLM çš„å¤–äº¤å®˜
- en: '![Refer to caption](img/2679d765634ba03dec7aed3f3aa53b84.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/2679d765634ba03dec7aed3f3aa53b84.png)'
- en: 'Figure 2: The framework of the proposed LLM-based-agent, Richelieu. The agent
    can explicitly reason social beliefs, propose sub-goals with reflection, negotiate
    with others, and take actions to master diplomacy. It augments memories by self-play
    games for self-evolving without any human annotation.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2ï¼šæ‰€æè®®çš„åŸºäº LLM çš„ä»£ç† Richelieu çš„æ¡†æ¶ã€‚ä»£ç†å¯ä»¥æ˜ç¡®åœ°æ¨ç†ç¤¾ä¼šä¿¡å¿µï¼Œé€šè¿‡åæ€æå‡ºå­ç›®æ ‡ï¼Œä¸ä»–äººè¿›è¡Œè°ˆåˆ¤ï¼Œå¹¶é‡‡å–è¡ŒåŠ¨æ¥æŒæ¡å¤–äº¤ã€‚å®ƒé€šè¿‡è‡ªæˆ‘å¯¹å¼ˆæ¸¸æˆå¢å¼ºè®°å¿†ï¼Œå®ç°è‡ªæˆ‘è¿›åŒ–ï¼Œæ— éœ€ä»»ä½•äººå·¥æ³¨é‡Šã€‚
- en: 'We have constructed a comprehensive framework with modules for memory management,
    social reasoning, strategic planning, negotiation, decision-making, memory update,
    and self-evolving to fully leverage the capabilities of LLMs. Richelieu starts
    by setting up with map details, game rules, domain knowledge, and the long-term
    goal.[[74](#bib.bib74), [58](#bib.bib58), [53](#bib.bib53)] At each turn, the
    agent will run in the following steps: 1) Social Reasoning: First of all, the
    agent undergoes a comprehensive analysis of the game state $s_{t}$. This logged
    data serves as a historical experience, guiding Richelieuâ€™s subsequent actions
    in future turnsÂ [[20](#bib.bib20), [73](#bib.bib73)]. 6) Self-evolution: The agentâ€™s
    evolution is highly dependent on the diversity of experiences stored in its memory.
    As this diversity grows, so does the agentâ€™s capability. Without human demonstrations,
    we employ multi-agent self-play games, i.e., our agents respectively control all
    the countries to simulate and acquire diverse experiences for self-evolving. Notably,
    the agent can further evolve during testing to adapt to different players.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»æ„å»ºäº†ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶ï¼ŒåŒ…å«å†…å­˜ç®¡ç†ã€ç¤¾ä¼šæ¨ç†ã€æˆ˜ç•¥è§„åˆ’ã€è°ˆåˆ¤ã€å†³ç­–ã€è®°å¿†æ›´æ–°å’Œè‡ªæˆ‘è¿›åŒ–ç­‰æ¨¡å—ï¼Œä»¥å……åˆ†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„èƒ½åŠ›ã€‚Richelieu
    é¦–å…ˆè®¾ç½®åœ°å›¾ç»†èŠ‚ã€æ¸¸æˆè§„åˆ™ã€é¢†åŸŸçŸ¥è¯†å’Œé•¿æœŸç›®æ ‡[[74](#bib.bib74), [58](#bib.bib58), [53](#bib.bib53)]ã€‚åœ¨æ¯ä¸€è½®ä¸­ï¼Œä»£ç†å°†æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š1ï¼‰ç¤¾ä¼šæ¨ç†ï¼šé¦–å…ˆï¼Œä»£ç†å¯¹æ¸¸æˆçŠ¶æ€
    $s_{t}$ è¿›è¡Œå…¨é¢åˆ†æã€‚è¿™äº›è®°å½•çš„æ•°æ®ä½œä¸ºå†å²ç»éªŒï¼ŒæŒ‡å¯¼ Richelieu åœ¨æœªæ¥å›åˆçš„åç»­è¡ŒåŠ¨[[20](#bib.bib20), [73](#bib.bib73)]ã€‚6ï¼‰è‡ªæˆ‘è¿›åŒ–ï¼šä»£ç†çš„è¿›åŒ–é«˜åº¦ä¾èµ–äºå…¶å†…å­˜ä¸­å­˜å‚¨çš„ç»éªŒå¤šæ ·æ€§ã€‚éšç€è¿™ç§å¤šæ ·æ€§çš„å¢åŠ ï¼Œä»£ç†çš„èƒ½åŠ›ä¹Ÿéšä¹‹å¢å¼ºã€‚åœ¨æ²¡æœ‰äººå·¥ç¤ºèŒƒçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤šä»£ç†è‡ªæˆ‘å¯¹å¼ˆæ¸¸æˆï¼Œå³æˆ‘ä»¬çš„ä»£ç†åˆ†åˆ«æ§åˆ¶æ‰€æœ‰å›½å®¶ï¼Œä»¥æ¨¡æ‹Ÿå’Œè·å–å¤šæ ·çš„ç»éªŒä»¥è¿›è¡Œè‡ªæˆ‘è¿›åŒ–ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä»£ç†åœ¨æµ‹è¯•è¿‡ç¨‹ä¸­è¿˜å¯ä»¥è¿›ä¸€æ­¥è¿›åŒ–ï¼Œä»¥é€‚åº”ä¸åŒçš„ç©å®¶ã€‚
- en: 4.1 Social Reasoning
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 ç¤¾ä¼šæ¨ç†
- en: There are no permanent enemies, no permanent allies. The relationship among
    countries is dynamically changing upon the evolving global state. However, it
    is difficult to determine the appropriate allies and enemies with partial observation.
    For example, there is uncertainty about the intentions of potential allies, which
    could lead to betrayal at pivotal moments. Consequently, we need to identify the
    intention and relationship of the current state by social reasoning to shape the
    social belief [[71](#bib.bib71), [19](#bib.bib19)].
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æ²¡æœ‰æ°¸è¿œçš„æ•Œäººï¼Œä¹Ÿæ²¡æœ‰æ°¸è¿œçš„ç›Ÿå‹ã€‚å›½å®¶ä¹‹é—´çš„å…³ç³»ä¼šæ ¹æ®å…¨çƒçŠ¶æ€çš„å˜åŒ–è€ŒåŠ¨æ€å˜åŒ–ã€‚ç„¶è€Œï¼Œå‡­å€Ÿéƒ¨åˆ†è§‚å¯Ÿå¾ˆéš¾ç¡®å®šåˆé€‚çš„ç›Ÿå‹å’Œæ•Œäººã€‚ä¾‹å¦‚ï¼Œå¯¹äºæ½œåœ¨ç›Ÿå‹çš„æ„å›¾å­˜åœ¨ä¸ç¡®å®šæ€§ï¼Œè¿™å¯èƒ½å¯¼è‡´åœ¨å…³é”®æ—¶åˆ»çš„èƒŒå›ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡ç¤¾ä¼šæ¨ç†è¯†åˆ«å½“å‰çŠ¶æ€çš„æ„å›¾å’Œå…³ç³»ï¼Œä»¥å½¢æˆç¤¾ä¼šä¿¡å¿µ[[71](#bib.bib71),
    [19](#bib.bib19)]ã€‚
- en: '1) Modeling Relationship: Before setting sub-goals, Richelieu evaluates its
    relations with other players, identifying enemies such as aggressive nations,
    vulnerable neighbors for expansion, and those with long-term potential threats.
    It also seeks out potential allies to counter these threats.[[46](#bib.bib46),
    [72](#bib.bib72)] Simultaneously, Richelieu also tries to identify potential allies
    that could be instrumental in countering these adversaries. By isolating the analysis
    of inter-player relationships as a discrete element, Richelieu strategically exploits
    the actions of other players in subsequent stages of the game to reach its goals.
    2) Inferring Intention: The social belief is also used by the planner, ensuring
    that Richelieuâ€™s sub-goals are formulated with a comprehensive consideration of
    the behaviors and intentions of other intelligent agents within the game.[[14](#bib.bib14),
    [21](#bib.bib21)] Richelieuâ€™s sub-goals will particularly emphasize on those who
    are identified as potential adversaries or allies, fostering more effective collaboration
    with potential allies and participation in strategic opposition against adversaries.
    Furthermore, the insights gleaned from this analysis are instrumental in the subsequent
    negotiation phases. They are employed to assess the authenticity of the statements
    made by other players, as well as to aid Richelieu in reaching cooperative agreements.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 1) å»ºæ¨¡å…³ç³»ï¼šåœ¨è®¾å®šå­ç›®æ ‡ä¹‹å‰ï¼ŒRichelieu è¯„ä¼°å…¶ä¸å…¶ä»–ç©å®¶çš„å…³ç³»ï¼Œè¯†åˆ«å‡ºæ•Œäººï¼Œå¦‚ä¾µç•¥æ€§å›½å®¶ã€é€‚åˆæ‰©å¼ çš„è„†å¼±é‚»å›½ä»¥åŠå…·æœ‰é•¿æœŸæ½œåœ¨å¨èƒçš„å›½å®¶ã€‚å®ƒè¿˜å¯»æ±‚å¯èƒ½çš„ç›Ÿå‹ä»¥å¯¹æŠ—è¿™äº›å¨èƒã€‚[[46](#bib.bib46),
    [72](#bib.bib72)]ä¸æ­¤åŒæ—¶ï¼ŒRichelieu è¿˜å°è¯•è¯†åˆ«å¯èƒ½åœ¨å¯¹æŠ—è¿™äº›å¯¹æ‰‹ä¸­å‘æŒ¥é‡è¦ä½œç”¨çš„æ½œåœ¨ç›Ÿå‹ã€‚é€šè¿‡å°†ç©å®¶é—´å…³ç³»çš„åˆ†æä½œä¸ºä¸€ä¸ªç‹¬ç«‹å…ƒç´ ï¼ŒRichelieu
    åœ¨æ¸¸æˆçš„åç»­é˜¶æ®µæˆ˜ç•¥æ€§åœ°åˆ©ç”¨å…¶ä»–ç©å®¶çš„è¡ŒåŠ¨ä»¥å®ç°å…¶ç›®æ ‡ã€‚2) æ¨æ–­æ„å›¾ï¼šè§„åˆ’è€…è¿˜ä½¿ç”¨ç¤¾ä¼šä¿¡å¿µï¼Œç¡®ä¿Richelieu çš„å­ç›®æ ‡æ˜¯åœ¨å…¨é¢è€ƒè™‘æ¸¸æˆä¸­å…¶ä»–æ™ºèƒ½ä½“çš„è¡Œä¸ºå’Œæ„å›¾çš„åŸºç¡€ä¸Šåˆ¶å®šçš„ã€‚[[14](#bib.bib14),
    [21](#bib.bib21)]Richelieu çš„å­ç›®æ ‡å°†ç‰¹åˆ«å…³æ³¨é‚£äº›è¢«è¯†åˆ«ä¸ºæ½œåœ¨å¯¹æ‰‹æˆ–ç›Ÿå‹çš„å¯¹è±¡ï¼Œä»¥ä¿ƒè¿›ä¸æ½œåœ¨ç›Ÿå‹çš„æ›´æœ‰æ•ˆåˆä½œï¼Œå¹¶å‚ä¸å¯¹æŠ—å¯¹æ‰‹çš„æˆ˜ç•¥
    oppositionã€‚æ­¤å¤–ï¼Œä»è¿™ä¸€åˆ†æä¸­è·å¾—çš„è§è§£åœ¨éšåçš„è°ˆåˆ¤é˜¶æ®µä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚å®ƒä»¬ç”¨äºè¯„ä¼°å…¶ä»–ç©å®¶æ‰€åšé™ˆè¿°çš„çœŸå®æ€§ï¼Œå¹¶å¸®åŠ©Richelieu è¾¾æˆåˆä½œåè®®ã€‚
- en: 4.2 Strategic Planner with Reflection
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 å…·æœ‰åæ€çš„æˆ˜ç•¥è§„åˆ’è€…
- en: The strategic planner specifies the sub-goals, which serves as an intermediary
    between immediate actions and the overarching goal of securing victory in the
    game. That is because we observe that LLMs are often characterized by their propensity
    to prioritize short-term gains in decision-making processes, with a notable deficiency
    in incorporating the future into their strategic calculations. [[41](#bib.bib41),
    [70](#bib.bib70)]For example, it is common for a non-neighboring country to become
    too powerful. Formally, $\vec{\chi_{t}}\leftarrow SR(s_{t},\vec{\phi_{t}},\Upsilon)$
    represents the inferred relationship on the social belief. These goals may encompass
    a range of tactical considerations, such as the containment of a formidable rivalâ€™s
    advancement or the strategic expansion in a particular direction to consolidate
    power.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ˜ç•¥è§„åˆ’è€…æŒ‡å®šå­ç›®æ ‡ï¼Œä½œä¸ºç«‹å³è¡ŒåŠ¨å’Œèµ¢å¾—æ¸¸æˆçš„æ€»ä½“ç›®æ ‡ä¹‹é—´çš„ä¸­ä»‹ã€‚è¿™æ˜¯å› ä¸ºæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼ŒLLMså¾€å¾€ä¼˜å…ˆè€ƒè™‘çŸ­æœŸæ”¶ç›Šï¼Œå†³ç­–è¿‡ç¨‹ä¸­ç¼ºä¹å°†æœªæ¥çº³å…¥æˆ˜ç•¥è®¡ç®—çš„èƒ½åŠ›ã€‚[[41](#bib.bib41),
    [70](#bib.bib70)]ä¾‹å¦‚ï¼Œéé‚»å›½å˜å¾—è¿‡äºå¼ºå¤§æ˜¯å¸¸è§çš„ç°è±¡ã€‚å½¢å¼ä¸Šï¼Œ$\vec{\chi_{t}}\leftarrow SR(s_{t},\vec{\phi_{t}},\Upsilon)$
    è¡¨ç¤ºç¤¾ä¼šä¿¡å¿µä¸­çš„æ¨æ–­å…³ç³»ã€‚è¿™äº›ç›®æ ‡å¯èƒ½åŒ…æ‹¬ä¸€ç³»åˆ—æˆ˜æœ¯è€ƒè™‘ï¼Œä¾‹å¦‚éåˆ¶å¼ºå¤§å¯¹æ‰‹çš„è¿›æ”»æˆ–åœ¨ç‰¹å®šæ–¹å‘ä¸Šè¿›è¡Œæˆ˜ç•¥æ‰©å¼ ä»¥å·©å›ºåŠ›é‡ã€‚
- en: 'Reflection with Memory. We further develop a reflection mechanism to enhance
    the rationality and effectiveness of our agentâ€™s sub-goals in achieving long-term
    goals.[[33](#bib.bib33)] This reflection mechanism relies on the past experiences
    to critique and enhance proposed sub-goals. We employ a similarity-based function
    to find relevant historical experiences that match the current game state from
    its memory. This function considers two factors: goal similarity and state similarity,
    to select the most comparable experiences. The process can be written as: $\vec{\eta_{t}}\leftarrow
    h(s_{t},\chi^{i}_{t},M)$. In practice, considering the limited context windows
    of LLM, we retrieve the most analogous experiences from the memory based on these
    metrics. Experiences with high evaluative scores reinforce successful strategies
    and support the continuity of existing sub-goals. On the other hand, lower scores
    indicate areas that need improvement and prompt the necessary adjustments. As
    our agent, Richelieu, undergoes more training sessions, its reflection abilities
    improve. The growing pool of historical experiences consistently enhances its
    performance.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åæ€ä¸è®°å¿†ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ç§åæ€æœºåˆ¶ï¼Œä»¥æé«˜ä»£ç†äººåœ¨å®ç°é•¿æœŸç›®æ ‡è¿‡ç¨‹ä¸­çš„ç†æ€§å’Œæœ‰æ•ˆæ€§[[33](#bib.bib33)]ã€‚è¯¥åæ€æœºåˆ¶ä¾èµ–äºè¿‡å»çš„ç»éªŒæ¥æ‰¹åˆ¤å’Œå¢å¼ºæå‡ºçš„å­ç›®æ ‡ã€‚æˆ‘ä»¬é‡‡ç”¨åŸºäºç›¸ä¼¼æ€§çš„å‡½æ•°ï¼Œä»è®°å¿†ä¸­æ‰¾åˆ°ä¸å½“å‰æ¸¸æˆçŠ¶æ€åŒ¹é…çš„ç›¸å…³å†å²ç»éªŒã€‚è¯¥å‡½æ•°è€ƒè™‘ä¸¤ä¸ªå› ç´ ï¼šç›®æ ‡ç›¸ä¼¼æ€§å’ŒçŠ¶æ€ç›¸ä¼¼æ€§ï¼Œä»¥é€‰æ‹©æœ€å¯æ¯”çš„ç»éªŒã€‚è¯¥è¿‡ç¨‹å¯ä»¥è¡¨ç¤ºä¸ºï¼š$\vec{\eta_{t}}\leftarrow
    h(s_{t},\chi^{i}_{t},M)$ã€‚åœ¨å®é™…æ“ä½œä¸­ï¼Œè€ƒè™‘åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸Šä¸‹æ–‡çª—å£æœ‰é™ï¼Œæˆ‘ä»¬æ ¹æ®è¿™äº›æŒ‡æ ‡ä»è®°å¿†ä¸­æ£€ç´¢æœ€ç±»ä¼¼çš„ç»éªŒã€‚å…·æœ‰é«˜è¯„ä¼°åˆ†æ•°çš„ç»éªŒå¼ºåŒ–æˆåŠŸçš„ç­–ç•¥ï¼Œå¹¶æ”¯æŒç°æœ‰å­ç›®æ ‡çš„å»¶ç»­ã€‚å¦ä¸€æ–¹é¢ï¼Œè¾ƒä½çš„åˆ†æ•°è¡¨ç¤ºéœ€è¦æ”¹è¿›çš„é¢†åŸŸï¼Œå¹¶ä¿ƒä½¿å¿…è¦çš„è°ƒæ•´ã€‚éšç€æˆ‘ä»¬çš„ä»£ç†äººRichelieuæ¥å—æ›´å¤šè®­ç»ƒï¼Œå…¶åæ€èƒ½åŠ›ä¸æ–­æé«˜ã€‚ä¸æ–­å¢é•¿çš„å†å²ç»éªŒåº“æŒç»­æå‡å…¶è¡¨ç°ã€‚
- en: 4.3 Negotiator and Actor
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 è°ˆåˆ¤è€…ä¸æ¼”å‘˜
- en: '![Refer to caption](img/39a6b99d7dde91bef217a7f848f361d2.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒæ ‡é¢˜](img/39a6b99d7dde91bef217a7f848f361d2.png)'
- en: 'Figure 3: The social reasoning flow for negotiation. With the received words
    and memory, the agent will reason by answering the following questions: â€œIs the
    opponent lying?", â€œWhat is the true intention of the opponent?", â€œis the opponent
    enemy?", â€œIs it necessary to deceive the opponent?", and â€œIs it necessary to change
    the relationship with the opponent?", and then generate the words accordingly
    for negotiation.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3ï¼šè°ˆåˆ¤çš„ç¤¾ä¼šæ¨ç†æµç¨‹ã€‚æ ¹æ®æ”¶åˆ°çš„è¯è¯­å’Œè®°å¿†ï¼Œä»£ç†äººå°†é€šè¿‡å›ç­”ä»¥ä¸‹é—®é¢˜è¿›è¡Œæ¨ç†ï¼šâ€œå¯¹æ‰‹åœ¨æ’’è°å—ï¼Ÿâ€ï¼Œâ€œå¯¹æ‰‹çš„çœŸå®æ„å›¾æ˜¯ä»€ä¹ˆï¼Ÿâ€ï¼Œâ€œå¯¹æ‰‹æ˜¯æ•Œäººå—ï¼Ÿâ€ï¼Œâ€œæ˜¯å¦æœ‰å¿…è¦æ¬ºéª—å¯¹æ‰‹ï¼Ÿâ€ï¼Œä»¥åŠâ€œæ˜¯å¦æœ‰å¿…è¦æ”¹å˜ä¸å¯¹æ‰‹çš„å…³ç³»ï¼Ÿâ€ï¼Œç„¶åç›¸åº”åœ°ç”Ÿæˆè°ˆåˆ¤çš„è¯è¯­ã€‚
- en: By chatting with other players, the goal of the negotiation is to update the
    social belief according to the received words and reach the sub-goal by manipulating
    otherâ€™s intentions, such as securing cooperative agreements with other nations,
    terminating ongoing conflicts with a specific country, or deterring the formation
    of alliances directed against its interests.[[37](#bib.bib37), [67](#bib.bib67)]
    However, it is difficult to reach a consensus, as the interests and strategies
    of the various nations often conflict, and trust between players can be scarce,
    making it challenging to establish and maintain cooperative agreements. In this
    case, we argue that the negotiator should identify the true intentions and relationship
    of the opponent before generating the words for the negotiation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å…¶ä»–ç©å®¶èŠå¤©æ—¶ï¼Œè°ˆåˆ¤çš„ç›®æ ‡æ˜¯æ ¹æ®æ”¶åˆ°çš„è¯è¯­æ›´æ–°ç¤¾ä¼šä¿¡å¿µï¼Œé€šè¿‡æ“æ§ä»–äººçš„æ„å›¾æ¥å®ç°å­ç›®æ ‡ï¼Œä¾‹å¦‚ï¼Œä¸å…¶ä»–å›½å®¶è¾¾æˆåˆä½œåè®®ã€ç»“æŸä¸ç‰¹å®šå›½å®¶çš„å†²çªæˆ–é˜»æ­¢é’ˆå¯¹è‡ªèº«åˆ©ç›Šçš„è”ç›Ÿçš„å½¢æˆ[[37](#bib.bib37),
    [67](#bib.bib67)]ã€‚ç„¶è€Œï¼Œè¾¾æˆå…±è¯†å¾ˆå›°éš¾ï¼Œå› ä¸ºå„å›½çš„åˆ©ç›Šå’Œç­–ç•¥é€šå¸¸å­˜åœ¨å†²çªï¼Œç©å®¶ä¹‹é—´çš„ä¿¡ä»»ä¹Ÿå¯èƒ½ç¨€ç¼ºï¼Œè¿™ä½¿å¾—å»ºç«‹å’Œç»´æŠ¤åˆä½œåè®®å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è®¤ä¸ºè°ˆåˆ¤è€…åº”è¯¥åœ¨ç”Ÿæˆè°ˆåˆ¤è¯è¯­ä¹‹å‰è¯†åˆ«å¯¹æ‰‹çš„çœŸå®æ„å›¾å’Œå…³ç³»ã€‚
- en: 'To fully utilize the power of LLMs, we construct a social reasoning flow for
    negotiation, as shown in FigureÂ [3](#S4.F3 "Figure 3 â€£ 4.3 Negotiator and Actor
    â€£ 4 Self-Evolving LLM-based Diplomat â€£ Richelieu: Self-Evolving LLM-Based Agents
    for AI Diplomacy"). During the negotiation process, we guide Richelieu to consider
    the veracity of what other players said and their true intentions, and in conjunction
    with our established sub-goals and analysis of our relationships with other players,
    to negotiate and form alliances with potential allies and attempt to deceive enemies.[[60](#bib.bib60),
    [35](#bib.bib35)]'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¸ºäº†å……åˆ†åˆ©ç”¨LLMsçš„åŠ›é‡ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªç¤¾äº¤æ¨ç†æµç¨‹ç”¨äºè°ˆåˆ¤ï¼Œå¦‚å›¾[3](#S4.F3 "Figure 3 â€£ 4.3 Negotiator and
    Actor â€£ 4 Self-Evolving LLM-based Diplomat â€£ Richelieu: Self-Evolving LLM-Based
    Agents for AI Diplomacy")æ‰€ç¤ºã€‚åœ¨è°ˆåˆ¤è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¼•å¯¼Richelieuè€ƒè™‘å…¶ä»–ç©å®¶æ‰€è¯´çš„çœŸå®æ€§åŠå…¶çœŸæ­£æ„å›¾ï¼Œå¹¶ç»“åˆæˆ‘ä»¬å»ºç«‹çš„å­ç›®æ ‡å’Œå¯¹å…¶ä»–ç©å®¶å…³ç³»çš„åˆ†æï¼Œä¸æ½œåœ¨ç›Ÿå‹è°ˆåˆ¤å¹¶å»ºç«‹è”ç›Ÿï¼ŒåŒæ—¶å°è¯•æ¬ºéª—æ•Œäººã€‚[
    [60](#bib.bib60), [35](#bib.bib35) ]'
- en: To counteract the challenge of non-binding agreements and potential deception,
    we incorporate a discrete module dedicated to the assessment of the veracity of
    statements made by other players during negotiations. To determine the truthiness
    of other playersâ€™ statements $\psi^{j}_{t}$. With such a reasoning flow, our agent
    can adeptly navigate diplomatic discourse. After the negotiation, the actor will
    get the updated social beliefs and choose a specific action for the army.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åº”å¯¹æ— çº¦æŸåè®®å’Œæ½œåœ¨æ¬ºéª—çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç¦»æ•£æ¨¡å—ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°å…¶ä»–ç©å®¶åœ¨è°ˆåˆ¤è¿‡ç¨‹ä¸­æ‰€è¯´è¯è¯­çš„çœŸå®æ€§ã€‚ä¸ºäº†ç¡®å®šå…¶ä»–ç©å®¶é™ˆè¿°çš„çœŸå®åº¦$\psi^{j}_{t}$ã€‚æœ‰äº†è¿™æ ·çš„æ¨ç†æµç¨‹ï¼Œæˆ‘ä»¬çš„ä»£ç†å¯ä»¥ç†Ÿç»ƒåœ°è¿›è¡Œå¤–äº¤è®¨è®ºã€‚è°ˆåˆ¤ç»“æŸåï¼Œæ¼”å‘˜å°†è·å¾—æ›´æ–°çš„ç¤¾ä¼šä¿¡å¿µï¼Œå¹¶ä¸ºå†›é˜Ÿé€‰æ‹©å…·ä½“è¡ŒåŠ¨ã€‚
- en: 4.4 Memory Management and Self-Evolution
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 è®°å¿†ç®¡ç†ä¸è‡ªæˆ‘è¿›åŒ–
- en: This memory is the foundation of the framework that accumulates the historical
    experience of the agent and summarizes them for other modules.[[17](#bib.bib17),
    [30](#bib.bib30), [66](#bib.bib66), [23](#bib.bib23)] It supports other modules,
    such as planner and negotiator, to provide long-tail experiences.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè®°å¿†æ˜¯æ¡†æ¶çš„åŸºç¡€ï¼Œå®ƒç§¯ç´¯äº†ä»£ç†çš„å†å²ç»éªŒå¹¶å°†å…¶æ€»ç»“ç»™å…¶ä»–æ¨¡å—ã€‚[ [17](#bib.bib17), [30](#bib.bib30), [66](#bib.bib66),
    [23](#bib.bib23) ] å®ƒæ”¯æŒå…¶ä»–æ¨¡å—ï¼Œå¦‚è®¡åˆ’è€…å’Œè°ˆåˆ¤è€…ï¼Œæä¾›é•¿å°¾ç»éªŒã€‚
- en: Raw Experience Management. Specifically, the memory module is tasked with the
    acquisition and archival of historical data, encompassing the observed game state
    $s_{t}$, and then is incorporated into memory.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹ç»éªŒç®¡ç†ã€‚å…·ä½“æ¥è¯´ï¼Œè®°å¿†æ¨¡å—è´Ÿè´£è·å–å’Œå½’æ¡£å†å²æ•°æ®ï¼ŒåŒ…æ‹¬è§‚å¯Ÿåˆ°çš„æ¸¸æˆçŠ¶æ€$s_{t}$ï¼Œç„¶åå°†å…¶çº³å…¥è®°å¿†ä¸­ã€‚
- en: 'Acquisition Experience via Self-Play Games. Self-play allows the agent to accumulate
    more experiences for self-evolution.[[33](#bib.bib33), [69](#bib.bib69)] After
    training, when Richelieu is faced with a certain state, it can draw on a larger
    pool of similar historical experiences. Diverse evaluations enable Richelieu to
    reflect more comprehensively on the strategies it currently devises, leading to
    a stronger optimization of decision making. As self-play continues, the acquisition
    of new and better historical experiences by Richelieu will diminish. This means
    that Richelieuâ€™s capabilities will not improve indefinitely. At the same time,
    as the memory grows, selecting appropriate historical experiences becomes a new
    challenge. The chosen m experience $\vec{\eta_{t}}$ may be almost identical, which
    could actually reduce the amount of useful information available to Richelieu.
    As shown in FigureÂ [5](#S5.F5 "Figure 5 â€£ 5.2 Results â€£ 5 Experiment â€£ Richelieu:
    Self-Evolving LLM-Based Agents for AI Diplomacy"), Richelieuâ€™s performance against
    Cicero [[6](#bib.bib6)] becomes better with increasing training iterations. With
    the accumulation of experiences, Richelieuâ€™s win rate exhibited a steady increase
    with accumulated training iterations, ultimately plateauing at a stable performance
    level. In contrast, the defeated rate showed a consistent decrease, approaching
    an asymptotic value. These observations confirm the effectiveness of self-play
    in Richelieuâ€™s evolution.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 'é€šè¿‡è‡ªæˆ‘å¯¹å¼ˆæ¸¸æˆè·å¾—ç»éªŒã€‚è‡ªæˆ‘å¯¹å¼ˆå…è®¸ä»£ç†ç§¯ç´¯æ›´å¤šç»éªŒä»¥è¿›è¡Œè‡ªæˆ‘æ¼”è¿›ã€‚[[33](#bib.bib33), [69](#bib.bib69)] è®­ç»ƒåï¼Œå½“Richelieué¢ä¸´æŸç§çŠ¶æ€æ—¶ï¼Œå®ƒå¯ä»¥å€Ÿé‰´æ›´å¤§çš„ç±»ä¼¼å†å²ç»éªŒæ± ã€‚å¤šæ ·åŒ–çš„è¯„ä¼°ä½¿Richelieuèƒ½æ›´å…¨é¢åœ°åæ€å…¶å½“å‰åˆ¶å®šçš„ç­–ç•¥ï¼Œä»è€Œæ›´å¼ºæœ‰åŠ›åœ°ä¼˜åŒ–å†³ç­–ã€‚éšç€è‡ªæˆ‘å¯¹å¼ˆçš„ç»§ç»­ï¼ŒRichelieuè·å¾—æ–°çš„ã€æ›´å¥½çš„å†å²ç»éªŒçš„èƒ½åŠ›å°†å‡å¼±ã€‚è¿™æ„å‘³ç€Richelieuçš„èƒ½åŠ›ä¸ä¼šæ— é™æé«˜ã€‚åŒæ—¶ï¼Œéšç€è®°å¿†çš„å¢é•¿ï¼Œé€‰æ‹©é€‚å½“çš„å†å²ç»éªŒæˆä¸ºæ–°çš„æŒ‘æˆ˜ã€‚æ‰€é€‰çš„mç»éªŒ$\vec{\eta_{t}}$å¯èƒ½å‡ ä¹ç›¸åŒï¼Œè¿™å®é™…ä¸Šå¯èƒ½å‡å°‘Richelieuè·å¾—æœ‰ç”¨ä¿¡æ¯çš„æ•°é‡ã€‚å¦‚å›¾[5](#S5.F5
    "Figure 5 â€£ 5.2 Results â€£ 5 Experiment â€£ Richelieu: Self-Evolving LLM-Based Agents
    for AI Diplomacy")æ‰€ç¤ºï¼ŒRichelieuåœ¨ä¸Cicero [[6](#bib.bib6)]å¯¹æˆ˜æ—¶çš„è¡¨ç°éšç€è®­ç»ƒè¿­ä»£æ¬¡æ•°çš„å¢åŠ è€Œå˜å¾—æ›´å¥½ã€‚éšç€ç»éªŒçš„ç§¯ç´¯ï¼ŒRichelieuçš„èƒœç‡åœ¨ç§¯ç´¯è®­ç»ƒè¿­ä»£æ¬¡æ•°åç¨³æ­¥ä¸Šå‡ï¼Œæœ€ç»ˆç¨³å®šåœ¨ä¸€ä¸ªç¨³å®šçš„è¡¨ç°æ°´å¹³ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤±è´¥ç‡åˆ™æŒç»­ä¸‹é™ï¼Œæ¥è¿‘æ¸è¿‘å€¼ã€‚è¿™äº›è§‚å¯Ÿç»“æœç¡®è®¤äº†è‡ªæˆ‘å¯¹å¼ˆåœ¨Richelieuæ¼”è¿›ä¸­çš„æœ‰æ•ˆæ€§ã€‚'
- en: 5 Experiment
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 å®éªŒ
- en: 'In the experiments, our goal is to answer the following questions: 1) Mastery
    of Non-Press Diplomacy: Can our agent master the non-press diplomacy against baselines?
    2) Competing with State-of-the-Art: Can our agent surpass the performance of the
    current state-of-the-art agents in press diplomacy? 3) Compatibility with LLMs:
    Can our self-evolving framework be compatible with different LLMs? 4) Contribution
    of Framework Modules: Do the individual modules within our framework contribute
    to the overall improvement of our agentâ€™s performance?'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š1) éæ–°é—»å¤–äº¤çš„æŒæ¡ï¼šæˆ‘ä»¬çš„ä»£ç†èƒ½å¦æŒæ¡ä¸åŸºçº¿çš„éæ–°é—»å¤–äº¤ï¼Ÿ2) ä¸æœ€å…ˆè¿›æŠ€æœ¯çš„ç«äº‰ï¼šæˆ‘ä»¬çš„ä»£ç†èƒ½å¦è¶…è¶Šå½“å‰æœ€å…ˆè¿›ä»£ç†åœ¨æ–°é—»å¤–äº¤ä¸­çš„è¡¨ç°ï¼Ÿ3)
    ä¸LLMçš„å…¼å®¹æ€§ï¼šæˆ‘ä»¬çš„è‡ªæˆ‘æ¼”è¿›æ¡†æ¶æ˜¯å¦èƒ½ä¸ä¸åŒçš„LLMå…¼å®¹ï¼Ÿ4) æ¡†æ¶æ¨¡å—çš„è´¡çŒ®ï¼šæ¡†æ¶ä¸­çš„å„ä¸ªæ¨¡å—æ˜¯å¦æœ‰åŠ©äºæ•´ä½“æå‡ä»£ç†çš„è¡¨ç°ï¼Ÿ
- en: 5.1 Experimental Setup
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 å®éªŒè®¾ç½®
- en: Environment. The widely-used open source Diplomacy game platform introduced
    by [[39](#bib.bib39)] is adopted for evaluating Richelieu against other models.
    It is easy to switch between no-press (with communication/negotiation between
    players) and press (no communication between players) games based on this platform,
    facilitating comparison on both settings. The platform also contains over 10,000
    human game data on which previous approaches are trained. Note that our method
    does not need them. In each game, a model will play the role of one randomly selected
    country to compete against countries controlled by other methods. It wins if occupying
    all the supply centers and loses vice versa.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ç¯å¢ƒã€‚å¹¿æ³›ä½¿ç”¨çš„å¼€æºå¤–äº¤æ¸¸æˆå¹³å°ï¼Œç”±[[39](#bib.bib39)]ä»‹ç»ï¼Œç”¨äºè¯„ä¼°Richelieuä¸å…¶ä»–æ¨¡å‹çš„å¯¹æ¯”ã€‚åŸºäºè¯¥å¹³å°ï¼Œå®¹æ˜“åœ¨æ— æ–°é—»ï¼ˆç©å®¶ä¹‹é—´æœ‰æ²Ÿé€š/è°ˆåˆ¤ï¼‰å’Œæœ‰æ–°é—»ï¼ˆç©å®¶ä¹‹é—´æ²¡æœ‰æ²Ÿé€šï¼‰æ¸¸æˆä¹‹é—´åˆ‡æ¢ï¼Œä¾¿äºåœ¨è¿™ä¸¤ç§è®¾ç½®ä¸‹è¿›è¡Œæ¯”è¾ƒã€‚è¯¥å¹³å°è¿˜åŒ…å«äº†è¶…è¿‡10,000æ¡äººç±»æ¸¸æˆæ•°æ®ï¼Œä¹‹å‰çš„æ–¹æ³•æ˜¯åŸºäºè¿™äº›æ•°æ®è¿›è¡Œè®­ç»ƒçš„ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦è¿™äº›æ•°æ®ã€‚åœ¨æ¯åœºæ¸¸æˆä¸­ï¼Œä¸€ä¸ªæ¨¡å‹å°†æ‰®æ¼”ä¸€ä¸ªéšæœºé€‰æ‹©çš„å›½å®¶ï¼Œä¸å…¶ä»–æ–¹æ³•æ§åˆ¶çš„å›½å®¶ç«äº‰ã€‚å é¢†æ‰€æœ‰ä¾›åº”ä¸­å¿ƒåˆ™è·èƒœï¼Œå¦åˆ™å¤±è´¥ã€‚
- en: 'Evaluation Metrics. We evaluate the models based on the results of multiple
    rounds of games. In each round, the model is randomly assigned a country to control.
    Typically, 1000 rounds are played to obtain the average results. We evaluate the
    models in two metrics. One is based on the win rate, Most SC rate, survived rate,
    and defeated rate. There are four possible outcomes for each country in the game.
    If a country loses all its supply centers (SC), it is eliminated and recorded
    as â€œdefeated". If a country occupies 18 or more out of 34 supply centers, the
    game ends, and that country is recorded as â€œwin", while other countries are recorded
    as â€œdefeated". In other cases, the game ends in a draw. The country with the most
    supply centers is recorded as â€œMost SC", the countries that have been eliminated
    are recorded as â€œdefeated", and the other countries are recorded as â€œSurvived".
    The other is based on the scores obtained by the models after multiple rounds
    of competition. To compare the capabilities of multiple models, we use C-Diplo
    Argir[[4](#bib.bib4)], a scoring system. This system is used in many international
    diplomacy competitions. The scoring method is as follows: If a player wins by
    occupying 18 or more supply centers, the player scores 93 points, and each of
    the other six players scores 1 point. If the game ends in a draw, the player with
    the most centers scores 37 points. The second player with the most centers scores
    14 points. The third player with the most centers scores 7 points. Each player
    scores 1 point per center owned. Each player also scores 1 point for participating.
    In this way, regardless of the game outcome, a total of 99 points will be distributed
    among the players in each game.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: è¯„ä¼°æŒ‡æ ‡ã€‚æˆ‘ä»¬æ ¹æ®å¤šè½®æ¸¸æˆçš„ç»“æœæ¥è¯„ä¼°æ¨¡å‹ã€‚åœ¨æ¯è½®ä¸­ï¼Œæ¨¡å‹è¢«éšæœºåˆ†é…ä¸€ä¸ªå›½å®¶è¿›è¡Œæ§åˆ¶ã€‚é€šå¸¸è¿›è¡Œ 1000 è½®ä»¥è·å¾—å¹³å‡ç»“æœã€‚æˆ‘ä»¬ä»ä¸¤ä¸ªæŒ‡æ ‡æ¥è¯„ä¼°æ¨¡å‹ã€‚ä¸€ä¸ªæ˜¯åŸºäºèƒœç‡ã€æœ€å¤šä¾›åº”ä¸­å¿ƒï¼ˆSCï¼‰ç‡ã€å­˜æ´»ç‡å’Œè¢«å‡»è´¥ç‡ã€‚æ¸¸æˆä¸­æ¯ä¸ªå›½å®¶æœ‰å››ç§å¯èƒ½çš„ç»“æœã€‚å¦‚æœä¸€ä¸ªå›½å®¶å¤±å»äº†æ‰€æœ‰çš„ä¾›åº”ä¸­å¿ƒï¼ˆSCï¼‰ï¼Œå®ƒå°†è¢«æ·˜æ±°å¹¶è®°å½•ä¸ºâ€œè¢«å‡»è´¥â€ã€‚å¦‚æœä¸€ä¸ªå›½å®¶å æ®äº†
    34 ä¸ªä¾›åº”ä¸­å¿ƒä¸­çš„ 18 ä¸ªæˆ–æ›´å¤šï¼Œæ¸¸æˆç»“æŸï¼Œè¯¥å›½è¢«è®°å½•ä¸ºâ€œèƒœåˆ©â€ï¼Œè€Œå…¶ä»–å›½å®¶è¢«è®°å½•ä¸ºâ€œè¢«å‡»è´¥â€ã€‚åœ¨å…¶ä»–æƒ…å†µä¸‹ï¼Œæ¸¸æˆä»¥å¹³å±€ç»“æŸã€‚æ‹¥æœ‰æœ€å¤šä¾›åº”ä¸­å¿ƒçš„å›½å®¶è¢«è®°å½•ä¸ºâ€œæœ€å¤š
    SCâ€ï¼Œè¢«æ·˜æ±°çš„å›½å®¶è¢«è®°å½•ä¸ºâ€œè¢«å‡»è´¥â€ï¼Œå…¶ä»–å›½å®¶è¢«è®°å½•ä¸ºâ€œå­˜æ´»â€ã€‚å¦ä¸€ä¸ªæŒ‡æ ‡æ˜¯åŸºäºæ¨¡å‹åœ¨å¤šè½®ç«äº‰ä¸­è·å¾—çš„åˆ†æ•°ã€‚ä¸ºäº†æ¯”è¾ƒå¤šä¸ªæ¨¡å‹çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† C-Diplo
    Argir[[4](#bib.bib4)]ï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨è®¸å¤šå›½é™…å¤–äº¤æ¯”èµ›ä¸­ä½¿ç”¨çš„è¯„åˆ†ç³»ç»Ÿã€‚è¯„åˆ†æ–¹æ³•å¦‚ä¸‹ï¼šå¦‚æœä¸€ä¸ªç©å®¶é€šè¿‡å é¢† 18 ä¸ªæˆ–æ›´å¤šä¾›åº”ä¸­å¿ƒè·èƒœï¼Œè¯¥ç©å®¶å¾—
    93 åˆ†ï¼Œå…¶ä»–å…­ä¸ªç©å®¶æ¯äººå¾— 1 åˆ†ã€‚å¦‚æœæ¸¸æˆä»¥å¹³å±€ç»“æŸï¼Œæ‹¥æœ‰æœ€å¤šä¸­å¿ƒçš„ç©å®¶å¾— 37 åˆ†ã€‚ç¬¬äºŒå¤šä¸­å¿ƒçš„ç©å®¶å¾— 14 åˆ†ã€‚ç¬¬ä¸‰å¤šä¸­å¿ƒçš„ç©å®¶å¾— 7 åˆ†ã€‚æ¯ä¸ªç©å®¶æ¯æ‹¥æœ‰ä¸€ä¸ªä¸­å¿ƒå¾—
    1 åˆ†ã€‚æ¯ä¸ªç©å®¶ä¹Ÿå› å‚ä¸æ¯”èµ›å¾— 1 åˆ†ã€‚è¿™æ ·ï¼Œæ— è®ºæ¸¸æˆç»“æœå¦‚ä½•ï¼Œæ¯åœºæ¸¸æˆä¸­çš„ç©å®¶æ€»å…±ä¼šåˆ†é… 99 åˆ†ã€‚
- en: Baselines. We select six previous models as baselines for comparison. Among
    them, Cicero[[6](#bib.bib6)] by Meta is a diplomacy model with a negotiation module.
    The other five are no-press diplomacy models, including the SL-DipNet and RL-DipNet
    [[39](#bib.bib39)], the BRPI [[3](#bib.bib3)], the SearchBot [[18](#bib.bib18)],
    and the DORA[[5](#bib.bib5)].
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºçº¿æ¨¡å‹ã€‚æˆ‘ä»¬é€‰æ‹©äº†å…­ä¸ªå…ˆå‰çš„æ¨¡å‹ä½œä¸ºæ¯”è¾ƒåŸºçº¿ã€‚å…¶ä¸­ï¼ŒMeta çš„ Cicero[[6](#bib.bib6)] æ˜¯ä¸€ä¸ªå…·æœ‰è°ˆåˆ¤æ¨¡å—çš„å¤–äº¤æ¨¡å‹ã€‚å…¶ä»–äº”ä¸ªæ˜¯æ— äº¤æµå¤–äº¤æ¨¡å‹ï¼ŒåŒ…æ‹¬
    SL-DipNet å’Œ RL-DipNet [[39](#bib.bib39)]ã€BRPI [[3](#bib.bib3)]ã€SearchBot [[18](#bib.bib18)]
    å’Œ DORA[[5](#bib.bib5)]ã€‚
- en: 5.2 Results
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 ç»“æœ
- en: 'Massively Play with Baselines on no-press setting. We let Richelieu compete
    with the other six models including Cicero[[6](#bib.bib6)], SL-DipNet and RL-DipNet
    [[39](#bib.bib39)], BRPI [[3](#bib.bib3)], SearchBot [[18](#bib.bib18)], and DORA[[5](#bib.bib5)]
    on No-Press Diplomacy, in which players make moves without communication. Figure
    Â [4](#S5.F4 "Figure 4 â€£ 5.2 Results â€£ 5 Experiment â€£ Richelieu: Self-Evolving
    LLM-Based Agents for AI Diplomacy") indicates that Richelieu outperforms other
    previous models relying on human game data. In contrast, Richelieu does not need
    such data but outperforms these methods by a clear margin, which demonstrates
    the outstanding planning capability of Richelieu.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨æ— äº¤æµè®¾ç½®ä¸‹çš„å¤§è§„æ¨¡æ¸¸æˆã€‚æˆ‘ä»¬è®© Richelieu ä¸åŒ…æ‹¬ Cicero[[6](#bib.bib6)]ã€SL-DipNet å’Œ RL-DipNet
    [[39](#bib.bib39)]ã€BRPI [[3](#bib.bib3)]ã€SearchBot [[18](#bib.bib18)] å’Œ DORA[[5](#bib.bib5)]
    åœ¨å†…çš„å…¶ä»–å…­ä¸ªæ¨¡å‹åœ¨æ— äº¤æµå¤–äº¤ä¸­è¿›è¡Œå¯¹æŠ—ï¼Œå…¶ä¸­ç©å®¶åœ¨æ²¡æœ‰æ²Ÿé€šçš„æƒ…å†µä¸‹è¿›è¡Œè¡ŒåŠ¨ã€‚å›¾ [4](#S5.F4 "Figure 4 â€£ 5.2 Results â€£
    5 Experiment â€£ Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy") è¡¨æ˜ï¼ŒRichelieu
    è¶…è¶Šäº†ä¾èµ–äºäººç±»æ¸¸æˆæ•°æ®çš„å…¶ä»–å…ˆå‰æ¨¡å‹ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒRichelieu ä¸éœ€è¦è¿™äº›æ•°æ®å´æ˜¾è‘—è¶…è¶Šäº†è¿™äº›æ–¹æ³•ï¼Œè¿™å±•ç¤ºäº† Richelieu çš„å“è¶Šè§„åˆ’èƒ½åŠ›ã€‚'
- en: 'Play against Cicero on press setting. We also evaluate Richelieu through competition
    against Cicero in the challenging scenario where negotiation is enabled. Specifically,
    we randomly assign three countries to one model and the remaining four to another.
    After playing several rounds of the game, the win rate, most SC rate, survived
    rate, and the defeated rate is calculated using a weighted average for evaluation.
    Table Â [1](#S5.T1 "Table 1 â€£ 5.2 Results â€£ 5 Experiment â€£ Richelieu: Self-Evolving
    LLM-Based Agents for AI Diplomacy") demonstrates the competitive performance of
    Richelieu in comparison to Cicero. Richelieuâ€™s win rate is approximately 0.7%
    higher than Ciceroâ€™s. If the Most SC rate is also taken into account, Richelieu
    is about 2% higher than Cicero. At the same time, Richelieuâ€™s loss rate is also
    0.6% lower. According to our scoring system, Richelieuâ€™s score is about 10% higher
    than Ciceroâ€™s. This is nontrivial especially when Richelieu is trained in a self-play
    game without humans and the opponents are trained with the data from human players.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨æ–°é—»è®¾ç½®ä¸‹ä¸ Cicero å¯¹å±€ã€‚æˆ‘ä»¬è¿˜é€šè¿‡åœ¨å¯ç”¨è°ˆåˆ¤çš„æŒ‘æˆ˜åœºæ™¯ä¸­ä¸ Cicero ç«äº‰æ¥è¯„ä¼° Richelieuã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å°†ä¸‰ä¸ªå›½å®¶éšæœºåˆ†é…ç»™ä¸€ä¸ªæ¨¡å‹ï¼Œå°†å‰©ä¸‹çš„å››ä¸ªå›½å®¶åˆ†é…ç»™å¦ä¸€ä¸ªæ¨¡å‹ã€‚åœ¨è¿›è¡Œå‡ è½®æ¸¸æˆåï¼Œä½¿ç”¨åŠ æƒå¹³å‡è®¡ç®—è·èƒœç‡ã€æœ€é«˜
    SC ç‡ã€ç”Ÿå­˜ç‡å’Œå¤±è´¥ç‡è¿›è¡Œè¯„ä¼°ã€‚è¡¨ [1](#S5.T1 "Table 1 â€£ 5.2 Results â€£ 5 Experiment â€£ Richelieu:
    Self-Evolving LLM-Based Agents for AI Diplomacy") å±•ç¤ºäº† Richelieu ç›¸å¯¹äº Cicero çš„ç«äº‰æ€§èƒ½ã€‚Richelieu
    çš„è·èƒœç‡æ¯” Cicero é«˜çº¦ 0.7%ã€‚å¦‚æœåŒæ—¶è€ƒè™‘æœ€é«˜ SC ç‡ï¼ŒRichelieu çº¦æ¯” Cicero é«˜ 2%ã€‚ä¸æ­¤åŒæ—¶ï¼ŒRichelieu çš„å¤±è´¥ç‡ä¹Ÿä½
    0.6%ã€‚æ ¹æ®æˆ‘ä»¬çš„è¯„åˆ†ç³»ç»Ÿï¼ŒRichelieu çš„å¾—åˆ†æ¯” Cicero é«˜çº¦ 10%ã€‚è¿™å°¤å…¶å€¼å¾—æ³¨æ„ï¼Œå› ä¸º Richelieu æ˜¯åœ¨æ²¡æœ‰äººç±»å‚ä¸çš„è‡ªå¯¹å¼ˆæ¸¸æˆä¸­è®­ç»ƒçš„ï¼Œè€Œå¯¹æ‰‹åˆ™æ˜¯ç”¨æ¥è‡ªäººç±»ç©å®¶çš„æ•°æ®è¿›è¡Œè®­ç»ƒçš„ã€‚'
- en: 'Table 1: The results of our method playing against Cicero.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 1ï¼šæˆ‘ä»¬çš„æ–¹æ³•ä¸ Cicero å¯¹å±€çš„ç»“æœã€‚
- en: Model Win$\uparrow$ Richelieu_1 6.20% 9.40% 38.90% 45.50% Richelieu_1 6.30%
    7.90% 39.40% 46.40% Richelieu_2 6.60% 7.80% 40.80% 44.80% Richelieu_2 6.60% 8.30%
    41.20% 43.90% Richelieu_3 7.10% 9.30% 39.90% 43.70% Richelieu_3 7.20% 8.70% 41.70%
    42.40% Richelieu_4 7.40% 8.00% 40.20% 44.40% Cicero_1 5.80% 6.70% 41.20% 46.30%
    Cicero_1 5.90% 6.50% 41.50% 46.10% Cicero_2 6.50% 7.20% 42.50% 43.80% Cicero_2
    6.30% 7.20% 42.50% 44.00% Cicero_3 6.00% 7.00% 41.60% 45.40% Cicero_3 5.90% 7.00%
    41.60% 45.50% Cicero_4 6.10% 7.20% 42.30% 44.40% Richelieu 6.83% 8.63% 39.95%
    44.60% Richelieu 6.70% 8.30% 40.77% 44.23% Cicero 6.03% 6.90% 41.87% 45.20% Cicero
    6.10% 7.03% 41.90% 44.98%
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹ è·èƒœç‡$\uparrow$ Richelieu_1 6.20% 9.40% 38.90% 45.50% Richelieu_1 6.30% 7.90%
    39.40% 46.40% Richelieu_2 6.60% 7.80% 40.80% 44.80% Richelieu_2 6.60% 8.30% 41.20%
    43.90% Richelieu_3 7.10% 9.30% 39.90% 43.70% Richelieu_3 7.20% 8.70% 41.70% 42.40%
    Richelieu_4 7.40% 8.00% 40.20% 44.40% Cicero_1 5.80% 6.70% 41.20% 46.30% Cicero_1
    5.90% 6.50% 41.50% 46.10% Cicero_2 6.50% 7.20% 42.50% 43.80% Cicero_2 6.30% 7.20%
    42.50% 44.00% Cicero_3 6.00% 7.00% 41.60% 45.40% Cicero_3 5.90% 7.00% 41.60% 45.50%
    Cicero_4 6.10% 7.20% 42.30% 44.40% Richelieu 6.83% 8.63% 39.95% 44.60% Richelieu
    6.70% 8.30% 40.77% 44.23% Cicero 6.03% 6.90% 41.87% 45.20% Cicero 6.10% 7.03%
    41.90% 44.98%
- en: 'Generalization of self-evolving framework to diverse LLMs. [Llamma 3](https://llama.meta.com/llama3)
    To demonstrate the effectiveness of our framework in a variety of LLM, we conducted
    experiments using four models: [GPT4.0](https://openai.com/index/gpt-4/), [ERNIE
    Bot](https://yiyan.baidu.com/welcome), [Spark Desk](https://xinghuo.xfyun.cn/),
    and [Llamma 3](https://llama.meta.com/llama3). The experimental results show that,
    despite variations in Richelieuâ€™s performance due to the inherent differences
    in the capabilities of these LLMs, as illustrated in FigureÂ [5](#S5.F5 "Figure
    5 â€£ 5.2 Results â€£ 5 Experiment â€£ Richelieu: Self-Evolving LLM-Based Agents for
    AI Diplomacy"), our framework and training approach significantly enhance the
    capabilities of all large language models. After training, the win rate using
    GPT4.0 increased from 1.5% lower than Ciceroâ€™s to about 0.7% higher than Ciceroâ€™s.
    The win rate using llama3 increased from 2.3% lower than Ciceroâ€™s to almost equal
    to Ciceroâ€™s. The win rates using Models Spark Desk and ERNIE Bot increased from
    3% and 4% lower than Ciceroâ€™s to 0.7% and 1.6% lower than Ciceroâ€™s, respectively.
    This indicates the generalization of a self-evolving framework to various LLMs.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 'è‡ªæˆ‘è¿›åŒ–æ¡†æ¶å¯¹å„ç§LLMçš„æ¨å¹¿ã€‚ [Llamma 3](https://llama.meta.com/llama3) ä¸ºäº†å±•ç¤ºæˆ‘ä»¬æ¡†æ¶åœ¨å„ç§LLMä¸­çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å››ä¸ªæ¨¡å‹è¿›è¡Œå®éªŒï¼š[GPT4.0](https://openai.com/index/gpt-4/)ã€[ERNIE
    Bot](https://yiyan.baidu.com/welcome)ã€[Spark Desk](https://xinghuo.xfyun.cn/)
    å’Œ [Llamma 3](https://llama.meta.com/llama3)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡ç”±äºè¿™äº›LLMçš„èƒ½åŠ›å›ºæœ‰å·®å¼‚ï¼ŒRichelieuçš„æ€§èƒ½æœ‰æ‰€å˜åŒ–ï¼Œå¦‚å›¾
    [5](#S5.F5 "Figure 5 â€£ 5.2 Results â€£ 5 Experiment â€£ Richelieu: Self-Evolving LLM-Based
    Agents for AI Diplomacy") æ‰€ç¤ºï¼Œæˆ‘ä»¬çš„æ¡†æ¶å’Œè®­ç»ƒæ–¹æ³•æ˜¾è‘—æå‡äº†æ‰€æœ‰å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ã€‚ç»è¿‡è®­ç»ƒï¼Œä½¿ç”¨GPT4.0çš„èƒœç‡ä»æ¯”Ciceroä½1.5%ä¸Šå‡è‡³æ¯”Ciceroé«˜çº¦0.7%ã€‚ä½¿ç”¨llama3çš„èƒœç‡ä»æ¯”Ciceroä½2.3%ä¸Šå‡è‡³å‡ ä¹ä¸CiceroæŒå¹³ã€‚ä½¿ç”¨Models
    Spark Deskå’ŒERNIE Botçš„èƒœç‡ä»æ¯”Ciceroä½3%å’Œ4%ä¸Šå‡è‡³åˆ†åˆ«ä½0.7%å’Œ1.6%ã€‚è¿™è¡¨æ˜è‡ªæˆ‘è¿›åŒ–æ¡†æ¶å¯¹å„ç§LLMçš„æ¨å¹¿ã€‚'
- en: '![Refer to caption](img/631495edb8589e8623e6338a6354e0d0.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/631495edb8589e8623e6338a6354e0d0.png)'
- en: 'Figure 4: The relative scores among 7 different agents when massively playing
    on the no-press setting. Each point shows the ratio of the modelâ€™s score on the
    vertical axis to the score gained by the model on the horizontal axis.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4ï¼šåœ¨æ²¡æœ‰å‹åˆ¶è®¾ç½®ä¸‹ï¼Œ7ä¸ªä¸åŒä»£ç†çš„ç›¸å¯¹è¯„åˆ†ã€‚æ¯ä¸ªç‚¹è¡¨ç¤ºæ¨¡å‹åœ¨çºµè½´ä¸Šçš„å¾—åˆ†ä¸åœ¨æ¨ªè½´ä¸Šè·å¾—çš„å¾—åˆ†çš„æ¯”ç‡ã€‚
- en: '![Refer to caption](img/e03e4ad9d6c4c6520e3233d9dd887b8a.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/e03e4ad9d6c4c6520e3233d9dd887b8a.png)'
- en: 'Figure 5: Richelieu modules benefit different LLMs. The solid line represents
    the experimental results for Richelieu, while the dashed line corresponds to Cicero.
    Different colors are used for different LLMs. The horizontal axis represents the
    logarithm of the number of training sessions, and the vertical axis denotes the
    rate.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5ï¼šRichelieuæ¨¡å—å¯¹ä¸åŒLLMçš„å¥½å¤„ã€‚å®çº¿è¡¨ç¤ºRichelieuçš„å®éªŒç»“æœï¼Œè€Œè™šçº¿å¯¹åº”äºCiceroã€‚ä¸åŒé¢œè‰²ç”¨äºä¸åŒçš„LLMã€‚æ¨ªè½´è¡¨ç¤ºè®­ç»ƒæ¬¡æ•°çš„å¯¹æ•°ï¼Œçºµè½´è¡¨ç¤ºæ¯”ç‡ã€‚
- en: 'Ablation Study. We conduct comprehensive ablation studies on Richelieu by analyzing
    the benefit of incorporating Richelieuâ€™s various modules, like planners or memory,
    into basic LLMs. The results are shown in TableÂ [2](#S5.T2 "Table 2 â€£ 5.2 Results
    â€£ 5 Experiment â€£ Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy").
    As can be seen, direct use of vanilla LLM yields relatively poor results. Richelieuâ€™s
    performance obtains steady and significant improvement by incorporating each individual
    module. This indicates that Richelieu is able to leverage other playersâ€™ actions
    during decision-making and consider both short-term and long-term benefits. Additionally,
    Richelieuâ€™s negotiation ability has been significantly improved, allowing it to
    effectively express intentions to cooperate with other players and avoid deception
    during negotiations. And after self-play, Richelieuâ€™s experience makes it perform
    better.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ¶ˆèç ”ç©¶ã€‚æˆ‘ä»¬é€šè¿‡åˆ†æå°†Richelieuçš„å„ç§æ¨¡å—ï¼ˆå¦‚è§„åˆ’è€…æˆ–è®°å¿†ï¼‰çº³å…¥åŸºæœ¬LLMä¸­çš„å¥½å¤„ï¼Œè¿›è¡Œå…¨é¢çš„æ¶ˆèç ”ç©¶ã€‚ç»“æœæ˜¾ç¤ºåœ¨è¡¨ [2](#S5.T2
    "Table 2 â€£ 5.2 Results â€£ 5 Experiment â€£ Richelieu: Self-Evolving LLM-Based Agents
    for AI Diplomacy") ä¸­ã€‚å¯ä»¥çœ‹å‡ºï¼Œç›´æ¥ä½¿ç”¨åŸå§‹LLMçš„ç»“æœç›¸å¯¹è¾ƒå·®ã€‚é€šè¿‡çº³å…¥æ¯ä¸ªå•ç‹¬çš„æ¨¡å—ï¼ŒRichelieuçš„æ€§èƒ½è·å¾—äº†ç¨³å®šè€Œæ˜¾è‘—çš„æå‡ã€‚è¿™è¡¨æ˜Richelieuèƒ½å¤Ÿåœ¨å†³ç­–è¿‡ç¨‹ä¸­åˆ©ç”¨å…¶ä»–ç©å®¶çš„è¡ŒåŠ¨ï¼Œè€ƒè™‘çŸ­æœŸå’Œé•¿æœŸåˆ©ç›Šã€‚æ­¤å¤–ï¼ŒRichelieuçš„è°ˆåˆ¤èƒ½åŠ›æœ‰äº†æ˜¾è‘—æé«˜ï¼Œä½¿å…¶èƒ½å¤Ÿæœ‰æ•ˆåœ°è¡¨è¾¾ä¸å…¶ä»–ç©å®¶åˆä½œçš„æ„å›¾ï¼Œå¹¶åœ¨è°ˆåˆ¤ä¸­é¿å…æ¬ºéª—ã€‚è‡ªæˆ‘å¯¹å¼ˆåï¼ŒRichelieuçš„ç»éªŒä½¿å…¶è¡¨ç°æ›´å¥½ã€‚'
- en: 'Table 2: Ablation study: average results of 3 Richelieu vs. 4 Cicero.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 2ï¼šæ¶ˆèç ”ç©¶ï¼š3ä¸ªRichelieuä¸4ä¸ªCiceroçš„å¹³å‡ç»“æœã€‚
- en: Modeling others sub-goals Negotiation pipeline Reflection with Memory Self-play
    Win $\uparrow$ âœ— âœ— âœ— âœ— âœ— 0.4% 0.7% 4.3% 94.6% âœ“ âœ— âœ— âœ— âœ— 0.7% 1.2% 10.6% 87.5%
    âœ“ âœ“ âœ— âœ— âœ— 3.3% 4.7% 26.7% 65.3% âœ“ âœ“ âœ“ âœ— âœ— 3.8% 5.8% 33.1% 57.3% âœ“ âœ“ âœ“ âœ“ âœ— 5.2%
    6.6% 39.5% 48.7% âœ“ âœ“ âœ“ âœ“ âœ“ 6.7% 8.5% 40.4% 44.4%
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹ä»–äººå­ç›®æ ‡ è°ˆåˆ¤æµç¨‹ åæ€ä¸è®°å¿† è‡ªæˆ‘å¯¹å¼ˆ èƒœç‡ $\uparrow$ âœ— âœ— âœ— âœ— âœ— 0.4% 0.7% 4.3% 94.6% âœ“ âœ— âœ— âœ—
    âœ— 0.7% 1.2% 10.6% 87.5% âœ“ âœ“ âœ— âœ— âœ— 3.3% 4.7% 26.7% 65.3% âœ“ âœ“ âœ“ âœ— âœ— 3.8% 5.8% 33.1%
    57.3% âœ“ âœ“ âœ“ âœ“ âœ— 5.2% 6.6% 39.5% 48.7% âœ“ âœ“ âœ“ âœ“ âœ“ 6.7% 8.5% 40.4% 44.4%
- en: 6 Example Cases
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 ç¤ºä¾‹æ¡ˆä¾‹
- en: '![Refer to caption](img/51ee1811083e27d41d71f10101317f50.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/51ee1811083e27d41d71f10101317f50.png)'
- en: 'Figure 6: An example case of negotiating with a nation to form an alliance
    to confront a strong enemy.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 6ï¼šä¸æŸå›½è°ˆåˆ¤ä»¥å»ºç«‹è”ç›Ÿå¯¹æŠ—å¼ºæ•Œçš„ä¸€ä¸ªç¤ºä¾‹æ¡ˆä¾‹ã€‚
- en: 'As is shown in Figure Â [6](#S6.F6 "Figure 6 â€£ 6 Example Cases â€£ Richelieu:
    Self-Evolving LLM-Based Agents for AI Diplomacy"), Richelieu controls France.
    At this time, France is at war with Austria over control of the Apennine Peninsula.
    However, Russia is on the verge of victory in its war against Turkey, which will
    lead to significant territorial expansion for Russia. Although France and Russia
    currently do not share a border, are not at war, and have no conflicts of interest,
    Richelieu foresees Russia becoming the most threatening enemy in the future. Therefore,
    Richelieu sets a sub-goal of weakening Russia.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¦‚å›¾ [6](#S6.F6 "Figure 6 â€£ 6 Example Cases â€£ Richelieu: Self-Evolving LLM-Based
    Agents for AI Diplomacy") æ‰€ç¤ºï¼Œé‡Œä»€åˆ©å„æ§åˆ¶äº†æ³•å›½ã€‚æ­¤æ—¶ï¼Œæ³•å›½æ­£åœ¨ä¸å¥¥åœ°åˆ©äº‰å¤ºäºšå¹³å®åŠå²›çš„æ§åˆ¶æƒã€‚ç„¶è€Œï¼Œä¿„ç½—æ–¯åœ¨ä¸åœŸè€³å…¶çš„æˆ˜äº‰ä¸­æ¥è¿‘èƒœåˆ©ï¼Œè¿™å°†å¯¼è‡´ä¿„ç½—æ–¯é¢†åœŸçš„æ˜¾è‘—æ‰©å¼ ã€‚å°½ç®¡æ³•å›½å’Œä¿„ç½—æ–¯ç›®å‰æ²¡æœ‰æ¥å£¤ã€æ²¡æœ‰æˆ˜äº‰ï¼Œä¹Ÿæ²¡æœ‰åˆ©ç›Šå†²çªï¼Œé‡Œä»€åˆ©å„é¢„è§åˆ°æœªæ¥ä¿„ç½—æ–¯å°†æˆä¸ºæœ€å…·å¨èƒçš„æ•Œäººã€‚å› æ­¤ï¼Œé‡Œä»€åˆ©å„è®¾å®šäº†å‰Šå¼±ä¿„ç½—æ–¯çš„å­ç›®æ ‡ã€‚'
- en: In the subsequent negotiation phase, Richelieu proactively proposes ending the
    war with Austria, despite holding an advantage in this conflict. Richelieu promises
    Austria that if it ceases hostilities and attacks Russia, Richelieu will assist
    Austria in defending against any attacks from England. The negotiations are successful.
    Austria accepted Richelieuâ€™s proposal, and the two countries reached an agreement
    to exchange the supply centers of Napoli and Munich.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨éšåçš„è°ˆåˆ¤é˜¶æ®µï¼Œé‡Œä»€åˆ©å„ä¸»åŠ¨æè®®ç»“æŸä¸å¥¥åœ°åˆ©çš„æˆ˜äº‰ï¼Œå°½ç®¡åœ¨è¿™åœºå†²çªä¸­å æœ‰ä¼˜åŠ¿ã€‚é‡Œä»€åˆ©å„æ‰¿è¯ºï¼Œå¦‚æœå¥¥åœ°åˆ©åœæ­¢æ•Œå¯¹è¡ŒåŠ¨å¹¶æ”»å‡»ä¿„ç½—æ–¯ï¼Œé‡Œä»€åˆ©å„å°†ååŠ©å¥¥åœ°åˆ©é˜²å¾¡æ¥è‡ªè‹±å›½çš„ä»»ä½•æ”»å‡»ã€‚è°ˆåˆ¤æˆåŠŸã€‚å¥¥åœ°åˆ©æ¥å—äº†é‡Œä»€åˆ©å„çš„æè®®ï¼Œä¸¤å›½è¾¾æˆäº†äº¤æ¢é‚£ä¸å‹’æ–¯å’Œæ…•å°¼é»‘ä¾›åº”ä¸­å¿ƒçš„åè®®ã€‚
- en: During the action phase, Austria moves its troops from Venice to Apulia in preparation
    for capturing Napoli in the next turn, while the rest of its forces are repositioned
    to the eastern regions bordering Russia to defend against Russian attacks and
    compete for supply centers. French units occupy Munich and prepare to advance
    on Russian territories such as Berlin. Meanwhile, French units support Austria
    in the Holland and Belgium regions.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¡ŒåŠ¨é˜¶æ®µï¼Œå¥¥åœ°åˆ©å°†éƒ¨é˜Ÿä»å¨å°¼æ–¯ç§»åŠ¨åˆ°é˜¿æ™®åˆ©äºšï¼Œä¸ºä¸‹ä¸€å›åˆå¤ºå–é‚£ä¸å‹’æ–¯åšå‡†å¤‡ï¼ŒåŒæ—¶å°†å…¶ä½™éƒ¨é˜Ÿé‡æ–°éƒ¨ç½²åˆ°æ¥å£¤ä¿„ç½—æ–¯çš„ä¸œéƒ¨åœ°åŒºï¼Œä»¥é˜²å¾¡ä¿„ç½—æ–¯çš„æ”»å‡»å¹¶äº‰å¤ºä¾›åº”ä¸­å¿ƒã€‚æ³•å›½éƒ¨é˜Ÿå é¢†äº†æ…•å°¼é»‘ï¼Œå¹¶å‡†å¤‡å‘å¦‚æŸæ—ç­‰ä¿„ç½—æ–¯é¢†åœŸæ¨è¿›ã€‚åŒæ—¶ï¼Œæ³•å›½éƒ¨é˜Ÿåœ¨è·å…°å’Œæ¯”åˆ©æ—¶åœ°åŒºæ”¯æ´å¥¥åœ°åˆ©ã€‚
- en: 'As shown in Figure Â [7](#S6.F7 "Figure 7 â€£ 6 Example Cases â€£ Richelieu: Self-Evolving
    LLM-Based Agents for AI Diplomacy"), Richelieu controls Germany. During the negotiation
    phase, England proposed a ceasefire to Germany and invited Germany to form an
    alliance to jointly attack France. England hoped to cease the war with Germany
    in Holland and Belgium. Subsequently, German units would support England in attacking
    Brest, and then England would utilize its fleets to assist Germany in attacking
    Spain and Portugal. Richelieu suspected that England was deceiving Germany, as
    England was likely to attack territories in the north such as Belgium and Berlin
    after German units were redirected to support Brest. Therefore, we pretended to
    accept Englandâ€™s alliance proposal during the negotiation process. However, at
    the same time, we sought out France and expressed our willingness to cease hostilities,
    allowing France to focus entirely on defending against Englandâ€™s attacks. In the
    action phase, Englandâ€™s actions confirmed Richelieuâ€™s suspicions. England attacked
    Belgium from Holland, but because Richelieu didnâ€™t move units in Belgium, Englandâ€™s
    attack failed.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¦‚å›¾ [7](#S6.F7 "å›¾ 7 â€£ 6 ç¤ºä¾‹æ¡ˆä¾‹ â€£ Richelieu: è‡ªæˆ‘è¿›åŒ–çš„åŸºäºLLMçš„AIå¤–äº¤ä»£ç†") æ‰€ç¤ºï¼ŒRichelieu æ§åˆ¶äº†å¾·å›½ã€‚åœ¨è°ˆåˆ¤é˜¶æ®µï¼Œè‹±æ ¼å…°å‘å¾·å›½æå‡ºåœç«ï¼Œå¹¶é‚€è¯·å¾·å›½ç»“ç›Ÿå…±åŒæ”»å‡»æ³•å›½ã€‚è‹±æ ¼å…°å¸Œæœ›åœ¨è·å…°å’Œæ¯”åˆ©æ—¶ä¸å¾·å›½ç»“æŸæˆ˜äº‰ã€‚éšåï¼Œå¾·å›½éƒ¨é˜Ÿå°†æ”¯æŒè‹±æ ¼å…°æ”»å‡»å¸ƒé›·æ–¯ç‰¹ï¼Œç„¶åè‹±æ ¼å…°å°†åˆ©ç”¨å…¶èˆ°é˜Ÿå¸®åŠ©å¾·å›½æ”»å‡»è¥¿ç­ç‰™å’Œè‘¡è„ç‰™ã€‚Richelieuæ€€ç–‘è‹±æ ¼å…°åœ¨æ¬ºéª—å¾·å›½ï¼Œå› ä¸ºè‹±æ ¼å…°å¯èƒ½ä¼šåœ¨å¾·å›½éƒ¨é˜Ÿè¢«é‡æ–°éƒ¨ç½²ä»¥æ”¯æŒå¸ƒé›·æ–¯ç‰¹åæ”»å‡»åŒ—éƒ¨å¦‚æ¯”åˆ©æ—¶å’ŒæŸæ—çš„é¢†åœŸã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨è°ˆåˆ¤è¿‡ç¨‹ä¸­å‡è£…æ¥å—äº†è‹±æ ¼å…°çš„ç»“ç›Ÿæè®®ã€‚ç„¶è€Œï¼ŒåŒæ—¶æˆ‘ä»¬å¯»æ±‚æ³•å›½ï¼Œè¡¨ç¤ºæ„¿æ„åœæ­¢æ•Œå¯¹è¡ŒåŠ¨ï¼Œè®©æ³•å›½å®Œå…¨ä¸“æ³¨äºé˜²å¾¡è‹±æ ¼å…°çš„æ”»å‡»ã€‚åœ¨è¡ŒåŠ¨é˜¶æ®µï¼Œè‹±æ ¼å…°çš„è¡ŒåŠ¨è¯å®äº†Richelieuçš„æ€€ç–‘ã€‚è‹±æ ¼å…°ä»è·å…°æ”»å‡»æ¯”åˆ©æ—¶ï¼Œä½†ç”±äºRichelieuæ²¡æœ‰åœ¨æ¯”åˆ©æ—¶ç§»åŠ¨éƒ¨é˜Ÿï¼Œè‹±æ ¼å…°çš„æ”»å‡»å¤±è´¥äº†ã€‚'
- en: '![Refer to caption](img/3bb119299fe8a072ee0d0740dacc00e4.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/3bb119299fe8a072ee0d0740dacc00e4.png)'
- en: 'Figure 7: An example case of avoiding being deceived by other countries during
    negotiations.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 7: é¿å…åœ¨è°ˆåˆ¤ä¸­è¢«å…¶ä»–å›½å®¶æ¬ºéª—çš„æ¡ˆä¾‹ç¤ºä¾‹ã€‚'
- en: 7 Conclusion
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 ç»“è®º
- en: In this paper, we introduce Richelieu, a self-evolving LLM-based agent for AI
    diplomacy. Our model enables hierarchical planning for multi-agent tasks and utilizes
    a memory module for reflective optimization. Our model does not require human
    data and can evolve through self-play. It ultimately outperforms existing models
    like Cicero in the Diplomacy. Our ablation study demonstrates the effectiveness
    of the modules we have established. By conducting experiments using different
    LLMs, we validate the generalization of our framework to various LLMs. We believe
    that the use of LLM-based agents will become an effective approach in social science
    in the future.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Richelieuï¼Œä¸€ç§åŸºäºLLMçš„è‡ªæˆ‘è¿›åŒ–AIå¤–äº¤ä»£ç†ã€‚æˆ‘ä»¬çš„æ¨¡å‹æ”¯æŒå¤šä»£ç†ä»»åŠ¡çš„å±‚æ¬¡è§„åˆ’ï¼Œå¹¶åˆ©ç”¨è®°å¿†æ¨¡å—è¿›è¡Œåæ€ä¼˜åŒ–ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä¸éœ€è¦äººå·¥æ•°æ®ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡è‡ªæˆ‘å¯¹å¼ˆè¿›è¡Œè¿›åŒ–ã€‚å®ƒæœ€ç»ˆè¶…è¶Šäº†ç°æœ‰çš„æ¨¡å‹å¦‚Ciceroã€‚åœ¨ã€Šå¤–äº¤ã€‹ä¸­ï¼Œæˆ‘ä»¬çš„æ¶ˆèç ”ç©¶è¯æ˜äº†æˆ‘ä»¬å»ºç«‹çš„æ¨¡å—çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡ä½¿ç”¨ä¸åŒçš„LLMè¿›è¡Œå®éªŒï¼Œæˆ‘ä»¬éªŒè¯äº†æ¡†æ¶å¯¹å„ç§LLMçš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼ŒåŸºäºLLMçš„ä»£ç†å°†åœ¨æœªæ¥æˆä¸ºç¤¾ä¼šç§‘å­¦ä¸­çš„ä¸€ç§æœ‰æ•ˆæ–¹æ³•ã€‚
- en: 8 Limitations and Future Work
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 é™åˆ¶ä¸æœªæ¥å·¥ä½œ
- en: Our study is subject to certain limitations. We utilize diplomacy as the platform
    for constructing our model. However, the space of actions within diplomacy is
    constrained, whereas the decision-making space in real-world diplomacy is virtually
    boundless. In Diplomacy, apart from the negotiation information exchanged between
    players, all other information is public and certain. Conversely, real-world diplomacy
    operates within a framework of incomplete information.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç ”ç©¶å­˜åœ¨ä¸€å®šçš„å±€é™æ€§ã€‚æˆ‘ä»¬åˆ©ç”¨å¤–äº¤ä½œä¸ºæ„å»ºæ¨¡å‹çš„å¹³å°ã€‚ç„¶è€Œï¼Œå¤–äº¤ä¸­çš„è¡ŒåŠ¨ç©ºé—´æ˜¯æœ‰é™çš„ï¼Œè€Œç°å®ä¸–ç•Œä¸­çš„å¤–äº¤å†³ç­–ç©ºé—´å‡ ä¹æ˜¯æ— é™çš„ã€‚åœ¨ã€Šå¤–äº¤ã€‹ä¸­ï¼Œé™¤äº†ç©å®¶ä¹‹é—´äº¤æ¢çš„è°ˆåˆ¤ä¿¡æ¯å¤–ï¼Œæ‰€æœ‰å…¶ä»–ä¿¡æ¯éƒ½æ˜¯å…¬å¼€ä¸”ç¡®å®šçš„ã€‚ç›¸åï¼Œç°å®ä¸–ç•Œçš„å¤–äº¤åœ¨ä¸€ä¸ªä¸å®Œå…¨ä¿¡æ¯çš„æ¡†æ¶å†…è¿ä½œã€‚
- en: The potential applications of such an AI agent are vast, ranging from simulated
    diplomatic environments to real-world assistance and analysis. In future research,
    we intend to develop a more realistic game space, characterized by incomplete
    information and multi-player games, to enhance and refine our model further. We
    will also extend the framework to other multi-agent scenarios, including embodied
    interactionsÂ [[75](#bib.bib75), [11](#bib.bib11), [9](#bib.bib9)], sensor networksÂ [[54](#bib.bib54),
    [61](#bib.bib61), [38](#bib.bib38), [31](#bib.bib31)], and video gamesÂ [[49](#bib.bib49),
    [34](#bib.bib34)]. This framework can also be employed to develop various applications.
    For instance, in the fields of economics and finance, we intend to utilize it
    to create business analytics and negotiation models.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§AIä»£ç†çš„æ½œåœ¨åº”ç”¨å¹¿æ³›ï¼Œä»æ¨¡æ‹Ÿçš„å¤–äº¤ç¯å¢ƒåˆ°ç°å®ä¸–ç•Œçš„æ´åŠ©å’Œåˆ†æã€‚æœªæ¥çš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ‰“ç®—å¼€å‘ä¸€ä¸ªæ›´ç°å®çš„æ¸¸æˆç©ºé—´ï¼Œç‰¹å¾ä¸ºä¸å®Œå…¨ä¿¡æ¯å’Œå¤šç©å®¶æ¸¸æˆï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºå’Œå®Œå–„æˆ‘ä»¬çš„æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜å°†æ‰©å±•æ¡†æ¶åˆ°å…¶ä»–å¤šæ™ºèƒ½ä½“åœºæ™¯ï¼ŒåŒ…æ‹¬å…·èº«äº¤äº’
    [[75](#bib.bib75)ã€[11](#bib.bib11)ã€[9](#bib.bib9)]ã€ä¼ æ„Ÿå™¨ç½‘ç»œ [[54](#bib.bib54)ã€[61](#bib.bib61)ã€[38](#bib.bib38)ã€[31](#bib.bib31)]
    å’Œè§†é¢‘æ¸¸æˆ [[49](#bib.bib49)ã€[34](#bib.bib34)]ã€‚è¯¥æ¡†æ¶è¿˜å¯ä»¥ç”¨äºå¼€å‘å„ç§åº”ç”¨ã€‚ä¾‹å¦‚ï¼Œåœ¨ç»æµå­¦å’Œé‡‘èé¢†åŸŸï¼Œæˆ‘ä»¬æ‰“ç®—åˆ©ç”¨å®ƒæ¥åˆ›å»ºå•†ä¸šåˆ†æå’Œè°ˆåˆ¤æ¨¡å‹ã€‚
- en: References
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: 'Abdelnabi etÂ al. [2023] Sahar Abdelnabi, Amr Gomaa, Sarath Sivaprasad, Lea
    SchÃ¶nherr, and Mario Fritz. Llm-deliberation: Evaluating llms with interactive
    multi-agent negotiation games. *arXiv preprint arXiv:2309.17234*, 2023.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abdelnabi et al. [2023] Sahar Abdelnabiã€Amr Gomaaã€Sarath Sivaprasadã€Lea SchÃ¶nherr
    å’Œ Mario Fritz. Llm-deliberationï¼šç”¨äº¤äº’å¼å¤šæ™ºèƒ½ä½“è°ˆåˆ¤æ¸¸æˆè¯„ä¼° llmsã€‚*arXiv é¢„å°æœ¬ arXiv:2309.17234*ï¼Œ2023å¹´ã€‚
- en: Allan [1975] Calhamer Allan. *The Games & puzzles book of modern board games*.
    W. Luscombe, 1975. ISBN 978-0860020592.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Allan [1975] Calhamer Allan. *ç°ä»£æ¡Œé¢æ¸¸æˆçš„æ¸¸æˆä¸è°œé¢˜ä¹¦*ã€‚W. Luscombeï¼Œ1975å¹´ã€‚ISBN 978-0860020592ã€‚
- en: Anthony etÂ al. [2020] Thomas Anthony, Tom Eccles, Andrea Tacchetti, JÃ¡nos KramÃ¡r,
    Ian Gemp, Thomas Hudson, Nicolas Porcel, Marc Lanctot, Julien PÃ©rolat, Richard
    Everett, etÂ al. Learning to play no-press diplomacy with best response policy
    iteration. *Advances in Neural Information Processing Systems*, 33:17987â€“18003,
    2020.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthony et al. [2020] Thomas Anthonyã€Tom Ecclesã€Andrea Tacchettiã€JÃ¡nos KramÃ¡rã€Ian
    Gempã€Thomas Hudsonã€Nicolas Porcelã€Marc Lanctotã€Julien PÃ©rolatã€Richard Everett
    ç­‰äºº. å­¦ä¹ é€šè¿‡æœ€ä½³å“åº”ç­–ç•¥è¿­ä»£æ¥ç©æ— æ–°é—»å¤–äº¤ã€‚*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•*ï¼Œ33:17987â€“18003ï¼Œ2020å¹´ã€‚
- en: Archer [2024] Bruno-AndrÃƒÂ© Giraudon &Â Vincent Archer. C-diplo argir, 2024. URL
    [https://world-diplomacy-database.com/php/scoring/scoring_class.php?id_scoring=7](https://world-diplomacy-database.com/php/scoring/scoring_class.php?id_scoring=7).
    Accessed:2024-05-02.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Archer [2024] Bruno-AndrÃ© Giraudon & Vincent Archer. C-diplo argirï¼Œ2024å¹´ã€‚ç½‘å€
    [https://world-diplomacy-database.com/php/scoring/scoring_class.php?id_scoring=7](https://world-diplomacy-database.com/php/scoring/scoring_class.php?id_scoring=7)ã€‚è®¿é—®æ—¥æœŸï¼š2024å¹´5æœˆ2æ—¥ã€‚
- en: Bakhtin etÂ al. [2021] Anton Bakhtin, David Wu, Adam Lerer, and Noam Brown. No-press
    diplomacy from scratch. *Advances in Neural Information Processing Systems*, 34:18063â€“18074,
    2021.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bakhtin et al. [2021] Anton Bakhtinã€David Wuã€Adam Lerer å’Œ Noam Brown. ä»é›¶å¼€å§‹çš„æ— æ–°é—»å¤–äº¤ã€‚*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•*ï¼Œ34:18063â€“18074ï¼Œ2021å¹´ã€‚
- en: Bakhtin etÂ al. [2022] Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina,
    Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, etÂ al.
    Human-level play in the game of diplomacy by combining language models with strategic
    reasoning. *Science*, 378(6624):1067â€“1074, 2022.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bakhtin et al. [2022] Anton Bakhtinã€Noam Brownã€Emily Dinanã€Gabriele Farinaã€Colin
    Flahertyã€Daniel Friedã€Andrew Goffã€Jonathan Grayã€Hengyuan Hu ç­‰äºº. é€šè¿‡ç»“åˆè¯­è¨€æ¨¡å‹ä¸æˆ˜ç•¥æ¨ç†åœ¨å¤–äº¤æ¸¸æˆä¸­å®ç°äººç±»æ°´å¹³çš„æ¸¸æˆã€‚*ç§‘å­¦*ï¼Œ378(6624):1067â€“1074ï¼Œ2022å¹´ã€‚
- en: Bianchi etÂ al. [2024] Federico Bianchi, PatrickÂ John Chia, Mert Yuksekgonul,
    Jacopo Tagliabue, Dan Jurafsky, and James Zou. How well can llms negotiate? negotiationarena
    platform and analysis. *arXiv preprint arXiv:2402.05863*, 2024.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bianchi et al. [2024] Federico Bianchiã€Patrick John Chiaã€Mert Yuksekgonulã€Jacopo
    Tagliabueã€Dan Jurafsky å’Œ James Zou. LLMs èƒ½åœ¨è°ˆåˆ¤ä¸­è¡¨ç°å¾—å¤šå¥½ï¼ŸNegotiationarena å¹³å°å’Œåˆ†æã€‚*arXiv
    é¢„å°æœ¬ arXiv:2402.05863*ï¼Œ2024å¹´ã€‚
- en: 'Calhamer [1974] Allan Calhamer. The invention of diplomacy. [https://web.archive.org/web/20090910012615/http://www.diplom.org/~diparch/resources/calhamer/invention.htm](https://web.archive.org/web/20090910012615/http://www.diplom.org/~diparch/resources/calhamer/invention.htm),
    1974. Accessed: 2024-05-18.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Calhamer [1974] Allan Calhamer. å¤–äº¤çš„å‘æ˜ã€‚ [https://web.archive.org/web/20090910012615/http://www.diplom.org/~diparch/resources/calhamer/invention.htm](https://web.archive.org/web/20090910012615/http://www.diplom.org/~diparch/resources/calhamer/invention.htm)ï¼Œ1974å¹´ã€‚è®¿é—®æ—¥æœŸï¼š2024å¹´5æœˆ18æ—¥ã€‚
- en: 'Chen etÂ al. [2023] Yuanpei Chen, Yiran Geng, Fangwei Zhong, Jiaming Ji, Jiechuang
    Jiang, Zongqing Lu, Hao Dong, and Yaodong Yang. Bi-dexhands: Towards human-level
    bimanual dexterous manipulation. *IEEE Transactions on Pattern Analysis and Machine
    Intelligence*, 2023.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. [2023] é™ˆå…ƒä½©ã€è€¿ä¾ç„¶ã€é’ŸèŠ³ä¼Ÿã€å­£å®¶æ˜ã€å§œæ°åˆ›ã€é™†å®—åº†ã€è‘£æµ©å’Œæ¨è€€ä¸œã€‚**Bi-dexhands: å®ç°ç±»äººåŒæ‰‹çµå·§æ“ä½œ**ã€‚*IEEEæ¨¡å¼åˆ†æä¸æœºå™¨æ™ºèƒ½å­¦æŠ¥*ï¼Œ2023å¹´ã€‚'
- en: Cheng etÂ al. [2024] Guangran Cheng, Chuheng Zhang, Wenzhe Cai, LiÂ Zhao, Changyin
    Sun, and Jiang Bian. Empowering large language models on robotic manipulation
    with affordance prompting. *arXiv preprint arXiv:2404.11027*, 2024.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng et al. [2024] å¹¿ç„¶Â·ç¨‹ã€æ¥šæ’Â·å¼ ã€æ¸©å“²Â·è”¡ã€æå…†ã€å¸¸é“¶Â·å­™å’Œå§œè¾¹ã€‚**é€šè¿‡èµ‹èƒ½æç¤ºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æœºå™¨äººæ“ä½œä¸­çš„èƒ½åŠ›**ã€‚*arXiv
    é¢„å°æœ¬ arXiv:2404.11027*ï¼Œ2024å¹´ã€‚
- en: Ci etÂ al. [2023] Hai Ci, Mickel Liu, Xuehai Pan, Fangwei Zhong, and Yizhou Wang.
    Proactive multi-camera collaboration for 3d human pose estimation. *arXiv preprint
    arXiv:2303.03767*, 2023.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ci et al. [2023] æµ·Â·èŒ¨ã€ç±³å…‹å°”Â·åˆ˜ã€æ½˜é›ªæµ·ã€é’ŸèŠ³ä¼Ÿå’Œç‹æ¯…æ´²ã€‚**ä¸»åŠ¨å¼å¤šæ‘„åƒå¤´åä½œç”¨äº 3D äººä½“å§¿æ€ä¼°è®¡**ã€‚*arXiv é¢„å°æœ¬
    arXiv:2303.03767*ï¼Œ2023å¹´ã€‚
- en: 'David [2014] Hill David. The board game of the alpha nerds. [https://grantland.com/features/diplomacy-the-board-game-of-the-alpha-nerds/](https://grantland.com/features/diplomacy-the-board-game-of-the-alpha-nerds/),
    2014. Accessed: 2024-05-18.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: David [2014] Hill David. **é˜¿å°”æ³•æå®¢çš„æ£‹ç›˜æ¸¸æˆ**ã€‚ [https://grantland.com/features/diplomacy-the-board-game-of-the-alpha-nerds/](https://grantland.com/features/diplomacy-the-board-game-of-the-alpha-nerds/),
    2014. è®¿é—®æ—¥æœŸï¼š2024-05-18ã€‚
- en: 'DeÂ Jonge and Sierra [2017] Dave DeÂ Jonge and Carles Sierra. D-brane: a diplomacy
    playing agent for automated negotiations research. *Applied Intelligence*, 47(1):158â€“177,
    2017.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'De Jonge and Sierra [2017] æˆ´å¤«Â·å¾·Â·ç¼æ ¼å’Œå¡å°”æ–¯Â·å¡æ‹‰ã€‚**D-brane: ç”¨äºè‡ªåŠ¨åŒ–è°ˆåˆ¤ç ”ç©¶çš„å¤–äº¤ä»£ç†**ã€‚*åº”ç”¨æ™ºèƒ½*ï¼Œ47(1):158â€“177ï¼Œ2017å¹´ã€‚'
- en: 'deÂ ZarzÃ  etÂ al. [2023] IÂ deÂ ZarzÃ , JÂ deÂ CurtÃ², Gemma Roig, Pietro Manzoni,
    and CarlosÂ T Calafate. Emergent cooperation and strategy adaptation in multi-agent
    systems: An extended coevolutionary theory with llms. *Electronics*, 12(12):2722,
    2023.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: de ZarzÃ  et al. [2023] I de ZarzÃ ã€J de CurtÃ²ã€æ°ç›Â·ç½—ä¼Šæ ¼ã€çš®è€¶ç‰¹ç½—Â·æ›¼ä½å°¼å’Œå¡æ´›æ–¯Â·TÂ·å¡æ‹‰æ³•ç‰¹ã€‚**å¤šä»£ç†ç³»ç»Ÿä¸­çš„æ–°å…´åˆä½œä¸ç­–ç•¥é€‚åº”ï¼šä¸€ç§æ‰©å±•çš„å…±è¿›åŒ–ç†è®ºä¸LLMs**ã€‚*ç”µå­å­¦*ï¼Œ12(12):2722ï¼Œ2023å¹´ã€‚
- en: DuÃ©Ã±ez-GuzmÃ¡n etÂ al. [2023] EdgarÂ A DuÃ©Ã±ez-GuzmÃ¡n, Suzanne Sadedin, JaneÂ X Wang,
    KevinÂ R McKee, and JoelÂ Z Leibo. A social path to human-like artificial intelligence.
    *Nature Machine Intelligence*, 5(11):1181â€“1188, 2023.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DuÃ©Ã±ez-GuzmÃ¡n et al. [2023] åŸƒå¾·åŠ Â·AÂ·æœåŸƒæ¶…æ–¯-å¤å…¹æ›¼ã€è‹çŠÂ·è¨å¾·ä¸ã€ç®€Â·XÂ·ç‹ã€å‡¯æ–‡Â·RÂ·éº¦åŸºå’Œä¹”å°”Â·ZÂ·è±åšã€‚**é€šå‘ç±»äººäººå·¥æ™ºèƒ½çš„ç¤¾ä¼šè·¯å¾„**ã€‚*è‡ªç„¶æœºå™¨æ™ºèƒ½*ï¼Œ5(11):1181â€“1188ï¼Œ2023å¹´ã€‚
- en: 'Fan etÂ al. [2022] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong
    Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo:
    Building open-ended embodied agents with internet-scale knowledge. In *Thirty-sixth
    Conference on Neural Information Processing Systems Datasets and Benchmarks Track*,
    2022. URL [https://openreview.net/forum?id=rc8o_j8I8PX](https://openreview.net/forum?id=rc8o_j8I8PX).'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fan et al. [2022] æ—æ›¦èŒƒã€ç‹å† æ™ºã€è’‹äº‘å‡¡ã€é˜¿è´¾ä¼ŠÂ·æ›¼å¾·å°”å¡ã€æ¨äº‘èªã€æœ±æµ©æ¯…ã€å®‰å¾·é²Â·å”ã€é»„å¾·å®‰ã€æœ±å®‡å…‹å’Œå®‰å¦®ç›Â·å®‰å—å¾·åº“é©¬å°”ã€‚**Minedojo:
    æ„å»ºå…·æœ‰äº’è”ç½‘è§„æ¨¡çŸ¥è¯†çš„å¼€æ”¾å¼å…·èº«ä»£ç†**ã€‚åœ¨ *ç¬¬36å±Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿä¼šè®®æ•°æ®é›†ä¸åŸºå‡†è·Ÿè¸ª*ï¼Œ2022å¹´ã€‚ç½‘å€ [https://openreview.net/forum?id=rc8o_j8I8PX](https://openreview.net/forum?id=rc8o_j8I8PX)ã€‚'
- en: Gao and Zhang [2024] Hang Gao and Yongfeng Zhang. Memory sharing for large language
    model based agents. *arXiv preprint arXiv:2404.09982*, 2024.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao and Zhang [2024] æ­Â·é«˜å’Œæ°¸å³°Â·å¼ ã€‚**å¤§å‹è¯­è¨€æ¨¡å‹åŸºç¡€çš„ä»£ç†çš„è®°å¿†å…±äº«**ã€‚*arXiv é¢„å°æœ¬ arXiv:2404.09982*ï¼Œ2024å¹´ã€‚
- en: Gray etÂ al. [2020] Jonathan Gray, Adam Lerer, Anton Bakhtin, and Noam Brown.
    Human-level performance in no-press diplomacy via equilibrium search. *International
    Conference on Learning Representations*, 2020.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gray et al. [2020] ä¹”çº³æ£®Â·æ ¼é›·ã€äºšå½“Â·å‹’é›·ã€å®‰ä¸œÂ·å·´èµ«å»·å’Œè¯ºäºšÂ·å¸ƒæœ—ã€‚**é€šè¿‡å¹³è¡¡æœç´¢å®ç°äººç±»æ°´å¹³çš„æ— å‹å¤–äº¤è¡¨ç°**ã€‚*å›½é™…å­¦ä¹ è¡¨å¾ä¼šè®®*ï¼Œ2020å¹´ã€‚
- en: 'GÃ¼rcan [2024] Ã–nder GÃ¼rcan. Llm-augmented agent-based modelling for social
    simulations: Challenges and opportunities. *HHAI 2024: Hybrid Human AI Systems
    for the Social Good*, pages 134â€“144, 2024.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'GÃ¼rcan [2024]  Ã–nder GÃ¼rcanã€‚**åŸºäº LLM å¢å¼ºçš„ä»£ç†å»ºæ¨¡ç”¨äºç¤¾ä¼šæ¨¡æ‹Ÿï¼šæŒ‘æˆ˜ä¸æœºé‡**ã€‚*HHAI 2024: ä¸ºç¤¾ä¼šç¦ç¥‰æœåŠ¡çš„æ··åˆäººç±»
    AI ç³»ç»Ÿ*ï¼Œç¬¬134â€“144é¡µï¼Œ2024å¹´ã€‚'
- en: 'Hatalis etÂ al. [2023] Kostas Hatalis, Despina Christou, Joshua Myers, Steven
    Jones, Keith Lambert, Adam Amos-Binks, Zohreh Dannenhauer, and Dustin Dannenhauer.
    Memory matters: The need to improve long-term memory in llm-agents. In *Proceedings
    of the AAAI Symposium Series*, pages 277â€“280, 2023.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hatalis et al. [2023] ç§‘æ–¯å¡”æ–¯Â·å“ˆå¡”åˆ©æ–¯ã€å¾·æ–¯çš®å¨œÂ·å…‹é‡Œæ–¯å›¾ã€çº¦ä¹¦äºšÂ·è¿ˆå°”æ–¯ã€å²è’‚æ–‡Â·ç¼æ–¯ã€åŸºæ€Â·å…°ä¼¯ç‰¹ã€äºšå½“Â·é˜¿è«æ–¯-å®¾å…‹æ–¯ã€ä½èµ«é›·Â·ä¸¹å«©è±ªå°”å’Œè¾¾æ–¯æ±€Â·ä¸¹å«©è±ªå°”ã€‚**è®°å¿†å¾ˆé‡è¦ï¼šæé«˜
    LLM ä»£ç†çš„é•¿æœŸè®°å¿†çš„å¿…è¦æ€§**ã€‚åœ¨ *AAAIç ”è®¨ä¼šè®ºæ–‡é›†*ï¼Œç¬¬277â€“280é¡µï¼Œ2023å¹´ã€‚
- en: 'He etÂ al. [2024] Junda He, Christoph Treude, and David Lo. Llm-based multi-agent
    systems for software engineering: Vision and the road ahead. *arXiv preprint arXiv:2404.04834*,
    2024.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. [2024] Junda Heã€Christoph Treude å’Œ David Loã€‚åŸºäº LLM çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨è½¯ä»¶å·¥ç¨‹ä¸­çš„åº”ç”¨ï¼šæ„¿æ™¯ä¸æœªæ¥é“è·¯ã€‚*arXiv
    é¢„å°æœ¬ arXiv:2404.04834*ï¼Œ2024å¹´ã€‚
- en: 'Hill [2014] Avalon Hill. Diplomacy rules 4th edition. [https://web.archive.org/web/20140915055823/http://www.diplomacy-archive.com/resources/rulebooks/2000AH4th.pdf](https://web.archive.org/web/20140915055823/http://www.diplomacy-archive.com/resources/rulebooks/2000AH4th.pdf),
    2014. Accessed: 2024-05-18.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hill [2014] Avalon Hillã€‚å¤–äº¤è§„åˆ™ç¬¬ 4 ç‰ˆã€‚[https://web.archive.org/web/20140915055823/http://www.diplomacy-archive.com/resources/rulebooks/2000AH4th.pdf](https://web.archive.org/web/20140915055823/http://www.diplomacy-archive.com/resources/rulebooks/2000AH4th.pdf)ï¼Œ2014å¹´ã€‚è®¿é—®æ—¶é—´ï¼š2024-05-18ã€‚
- en: 'Hou etÂ al. [2024] Yuki Hou, Haruki Tamoto, and Homei Miyashita. " my agent
    understands me better": Integrating dynamic human-like memory recall and consolidation
    in llm-based agents. In *Extended Abstracts of the CHI Conference on Human Factors
    in Computing Systems*, pages 1â€“7, 2024.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hou et al. [2024] Yuki Houã€Haruki Tamoto å’Œ Homei Miyashitaã€‚â€œmy agent understands
    me betterâ€ï¼šåœ¨åŸºäº LLM çš„æ™ºèƒ½ä½“ä¸­é›†æˆåŠ¨æ€ç±»ä¼¼äººç±»çš„è®°å¿†å¬å›å’Œå·©å›ºã€‚åœ¨*CHI äººæœºäº¤äº’ä¼šè®®æ‰©å±•æ‘˜è¦*ä¸­ï¼Œé¡µç  1â€“7ï¼Œ2024å¹´ã€‚
- en: Jacob etÂ al. [2022] AthulÂ Paul Jacob, DavidÂ J Wu, Gabriele Farina, Adam Lerer,
    Hengyuan Hu, Anton Bakhtin, Jacob Andreas, and Noam Brown. Modeling strong and
    human-like gameplay with kl-regularized search. In *International Conference on
    Machine Learning*, pages 9695â€“9728\. PMLR, 2022.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jacob et al. [2022] Athul Paul Jacobã€David J Wuã€Gabriele Farinaã€Adam Lererã€Hengyuan
    Huã€Anton Bakhtinã€Jacob Andreas å’Œ Noam Brownã€‚ä½¿ç”¨ KL è§„æ•´åŒ–æœç´¢å»ºæ¨¡å¼ºå¤§ä¸”ç±»ä¼¼äººç±»çš„æ¸¸æˆç©æ³•ã€‚åœ¨*å›½é™…æœºå™¨å­¦ä¹ å¤§ä¼š*ä¸­ï¼Œé¡µç 
    9695â€“9728ã€‚PMLRï¼Œ2022å¹´ã€‚
- en: 'Jaidka etÂ al. [2024] Kokil Jaidka, Hansin Ahuja, and Lynnette HuiÂ Xian Ng.
    It takes two to negotiate: Modeling social exchange in online multiplayer games.
    *Proceedings of the ACM on Human-Computer Interaction*, 8(CSCW1):1â€“22, 2024.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaidka et al. [2024] Kokil Jaidkaã€Hansin Ahuja å’Œ Lynnette Hui Xian Ngã€‚è°ˆåˆ¤éœ€è¦ä¸¤ä¸ªäººï¼šå»ºæ¨¡åœ¨çº¿å¤šäººæ¸¸æˆä¸­çš„ç¤¾ä¼šäº¤æ¢ã€‚åœ¨*ACM
    äººæœºäº¤äº’å­¦æŠ¥*ï¼Œ8(CSCW1)ï¼š1â€“22ï¼Œ2024å¹´ã€‚
- en: Konya etÂ al. [2023] Andrew Konya, Deger Turan, Aviv Ovadya, Lina Qui, Daanish
    Masood, Flynn Devine, Lisa Schirch, and Isabella Roberts. Deliberative technology
    for alignment. *ArXiv*, abs/2312.03893, 2023.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Konya et al. [2023] Andrew Konyaã€Deger Turanã€Aviv Ovadyaã€Lina Quiã€Daanish Masoodã€Flynn
    Devineã€Lisa Schirch å’Œ Isabella Robertsã€‚ç”¨äºå¯¹é½çš„å®¡è®®æŠ€æœ¯ã€‚*ArXiv*ï¼Œabs/2312.03893ï¼Œ2023å¹´ã€‚
- en: Kostick [2015] Conor Kostick. *The Art of Correspondence in the Game of Diplomacy*.
    Curses & Magic, 2nd edition, 2015. ISBN 978-0993415104.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kostick [2015] Conor Kostickã€‚*å¤–äº¤æ¸¸æˆä¸­çš„é€šä¿¡è‰ºæœ¯*ã€‚Curses & Magicï¼Œç¬¬ 2 ç‰ˆï¼Œ2015å¹´ã€‚ISBN 978-0993415104ã€‚
- en: 'KovaÄ etÂ al. [2023] Grgur KovaÄ, RÃ©my Portelas, PeterÂ Ford Dominey, and Pierre-Yves
    Oudeyer. The socialai school: Insights from developmental psychology towards artificial
    socio-cultural agents. *arXiv preprint arXiv:2307.07871*, 2023.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KovaÄ et al. [2023] Grgur KovaÄã€RÃ©my Portelasã€Peter Ford Dominey å’Œ Pierre-Yves
    Oudeyerã€‚ç¤¾ä¼š AI å­¦æ ¡ï¼šä»å‘å±•å¿ƒç†å­¦åˆ°äººå·¥ç¤¾ä¼šæ–‡åŒ–æ™ºèƒ½ä½“çš„è§è§£ã€‚*arXiv é¢„å°æœ¬ arXiv:2307.07871*ï¼Œ2023å¹´ã€‚
- en: KramÃ¡r etÂ al. [2022] JÃ¡nos KramÃ¡r, Tom Eccles, Ian Gemp, Andrea Tacchetti, KevinÂ R
    McKee, Mateusz Malinowski, Thore Graepel, and Yoram Bachrach. Negotiation and
    honesty in artificial intelligence methods for the board game of diplomacy. *Nature
    Communications*, 13(1):7214, 2022.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KramÃ¡r et al. [2022] JÃ¡nos KramÃ¡rã€Tom Ecclesã€Ian Gempã€Andrea Tacchettiã€Kevin
    R McKeeã€Mateusz Malinowskiã€Thore Graepel å’Œ Yoram Bachrachã€‚åœ¨åšå¼ˆå¤–äº¤ä¸­å…³äºè°ˆåˆ¤å’Œè¯šä¿¡çš„äººå·¥æ™ºèƒ½æ–¹æ³•ã€‚*è‡ªç„¶é€šè®¯*ï¼Œ13(1)ï¼š7214ï¼Œ2022å¹´ã€‚
- en: Li etÂ al. [2024a] Hao Li, Chenghao Yang, AnÂ Zhang, Yang Deng, Xiang Wang, and
    Tat-Seng Chua. Hello again! llm-powered personalized agent for long-term dialogue.
    *arXiv preprint arXiv:2406.05925*, 2024a.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2024a] Hao Liã€Chenghao Yangã€An Zhangã€Yang Dengã€Xiang Wang å’Œ Tat-Seng
    Chuaã€‚å†è§ï¼LLM é©±åŠ¨çš„ä¸ªæ€§åŒ–æ™ºèƒ½ä½“ç”¨äºé•¿æœŸå¯¹è¯ã€‚*arXiv é¢„å°æœ¬ arXiv:2406.05925*ï¼Œ2024aã€‚
- en: Li etÂ al. [2020] Jing Li, Jing Xu, Fangwei Zhong, Xiangyu Kong, YuÂ Qiao, and
    Yizhou Wang. Pose-assisted multi-camera collaboration for active object tracking.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, volumeÂ 34,
    pages 759â€“766, 2020.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2020] Jing Liã€Jing Xuã€Fangwei Zhongã€Xiangyu Kongã€Yu Qiao å’Œ Yizhou
    Wangã€‚åŸºäºå§¿æ€çš„å¤šæ‘„åƒå¤´åä½œç”¨äºä¸»åŠ¨ç‰©ä½“è·Ÿè¸ªã€‚åœ¨*AAAI äººå·¥æ™ºèƒ½ä¼šè®®è®ºæ–‡é›†*ï¼Œå· 34ï¼Œé¡µç  759â€“766ï¼Œ2020å¹´ã€‚
- en: 'Li etÂ al. [2024b] Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan,
    Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, YiÂ Sun, etÂ al. Personal llm
    agents: Insights and survey about the capability, efficiency and security. *arXiv
    preprint arXiv:2401.05459*, 2024b.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2024b] Yuanchun Liã€Hao Wenã€Weijun Wangã€Xiangyu Liã€Yizhen Yuanã€Guohong
    Liuã€Jiacheng Liuã€Wenxing Xuã€Xiang Wangã€Yi Sun ç­‰ã€‚ä¸ªäºº LLM æ™ºèƒ½ä½“ï¼šå…³äºèƒ½åŠ›ã€æ•ˆç‡å’Œå®‰å…¨æ€§çš„è§è§£ä¸è°ƒæŸ¥ã€‚*arXiv
    é¢„å°æœ¬ arXiv:2401.05459*ï¼Œ2024bã€‚
- en: 'Liu etÂ al. [2024] Zhiwei Liu, Weiran Yao, Jianguo Zhang, Liangwei Yang, Zuxin
    Liu, Juntao Tan, PrafullaÂ K Choubey, Tian Lan, Jason Wu, Huan Wang, etÂ al. Agentlite:
    A lightweight library for building and advancing task-oriented llm agent system.
    *arXiv preprint arXiv:2402.15538*, 2024.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'åˆ˜ç­‰äºº [2024] æœ±ä¼Ÿ åˆ˜ã€ç»´ç„¶ å§šã€å»ºå›½ å¼ ã€è‰¯ä¼Ÿ æ¨ã€ç¥–æ¬£ åˆ˜ã€å†›æ¶› è°­ã€å¸•å¤«æ‹‰Â·KÂ·ä¹”è´ã€å¤© å…°ã€æ°æ£® å´ã€ç¯ ç‹ç­‰äººã€‚Agentlite:
    ç”¨äºæ„å»ºå’Œæ¨è¿›ä»»åŠ¡å¯¼å‘çš„ llm ä»£ç†ç³»ç»Ÿçš„è½»é‡çº§åº“ã€‚*arXiv é¢„å°æœ¬ arXiv:2402.15538*ï¼Œ2024ã€‚'
- en: Ma etÂ al. [2024] Long Ma, Yuanfei Wang, Fangwei Zhong, Song-Chun Zhu, and Yizhou
    Wang. Fast peer adaptation with context-aware exploration. *arXiv preprint arXiv:2402.02468*,
    2024.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é©¬ç­‰äºº [2024] é¾™ é©¬ã€å…ƒé£ ç‹ã€æ–¹ä¼Ÿ é’Ÿã€å®‹æ˜¥ Zhu å’Œä¸€èˆŸ ç‹ã€‚åŸºäºæƒ…å¢ƒæ„ŸçŸ¥çš„å¿«é€ŸåŒè¡Œé€‚åº”ã€‚*arXiv é¢„å°æœ¬ arXiv:2402.02468*ï¼Œ2024ã€‚
- en: Moghimifar etÂ al. [2024] Farhad Moghimifar, Yuan-Fang Li, Robert Thomson, and
    Gholamreza Haffari. Modelling political coalition negotiations using llm-based
    agents. *arXiv preprint arXiv:2402.11712*, 2024.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è«å‰ç±³æ³•å°”ç­‰äºº [2024] æ³•èµ«å¾·Â·è«å‰ç±³æ³•å°”ã€è¢èŠ³ æã€ç½—ä¼¯ç‰¹Â·æ±¤å§†æ£®å’Œæˆˆæ‹‰å§†é›·æ‰Â·å“ˆæ³•é‡Œã€‚ä½¿ç”¨ llm ä»£ç†å»ºæ¨¡æ”¿æ²»è”ç›Ÿè°ˆåˆ¤ã€‚*arXiv é¢„å°æœ¬
    arXiv:2402.11712*ï¼Œ2024ã€‚
- en: Mukobi etÂ al. [2023] Gabriel Mukobi, Ann-Katrin Reuel, Juan-Pablo Rivera, and
    Chandler Smith. Assessing risks of using autonomous language models in military
    and diplomatic planning. In *Multi-Agent Security Workshop @ NeurIPSâ€™23*, 2023.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç©†ç§‘æ¯”ç­‰äºº [2023] åŠ å¸ƒé‡ŒåŸƒå°” ç©†ç§‘æ¯”ã€å®‰å¨œ-å‡¯ç‰¹ç³Â·ç‘ä¹Œå°”ã€èƒ¡å®‰-å·´å¸ƒç½—Â·é‡Œç»´æ‹‰å’Œé’±å¾·å‹’Â·å²å¯†æ–¯ã€‚è¯„ä¼°åœ¨å†›äº‹å’Œå¤–äº¤è§„åˆ’ä¸­ä½¿ç”¨è‡ªä¸»è¯­è¨€æ¨¡å‹çš„é£é™©ã€‚å‘è¡¨äº*å¤šæ™ºèƒ½ä½“å®‰å…¨ç ”è®¨ä¼š
    @ NeurIPSâ€™23*ï¼Œ2023ã€‚
- en: Noh and Chang [2024] Sean Noh and Ho-ChunÂ Herbert Chang. Llms with personalities
    in multi-issue negotiation games. *arXiv preprint arXiv:2405.05248*, 2024.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Noh å’Œ Chang [2024] è‚–æ© Noh å’Œ ä½•æ˜¥Â·èµ«ä¼¯ç‰¹Â·å¼ ã€‚å¤šè®®é¢˜è°ˆåˆ¤æ¸¸æˆä¸­çš„æœ‰ä¸ªæ€§ llmã€‚*arXiv é¢„å°æœ¬ arXiv:2405.05248*ï¼Œ2024ã€‚
- en: 'Pan etÂ al. [2022] Xuehai Pan, Mickel Liu, Fangwei Zhong, Yaodong Yang, Song-Chun
    Zhu, and Yizhou Wang. Mate: Benchmarking multi-agent reinforcement learning in
    distributed target coverage control. *Advances in Neural Information Processing
    Systems*, 35:27862â€“27879, 2022.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'æ½˜ç­‰äºº [2022] è–›æµ· æ½˜ã€ç±³å…‹å°” åˆ˜ã€æ–¹ä¼Ÿ é’Ÿã€è€€ä¸œ æ¨ã€å®‹æ˜¥ Zhu å’Œä¸€èˆŸ ç‹ã€‚Mate: åœ¨åˆ†å¸ƒå¼ç›®æ ‡è¦†ç›–æ§åˆ¶ä¸­åŸºå‡†æµ‹è¯•å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ã€‚*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•*ï¼Œ35:27862â€“27879ï¼Œ2022ã€‚'
- en: 'Paquette etÂ al. [2019] Philip Paquette, Yuchen Lu, SetonÂ Steven Bocco, Max
    Smith, Satya O-G, JonathanÂ K Kummerfeld, Joelle Pineau, Satinder Singh, and AaronÂ C
    Courville. No-press diplomacy: Modeling multi-agent gameplay. *Advances in Neural
    Information Processing Systems*, 32, 2019.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¸•å‡¯ç‰¹ç­‰äºº [2019] è²åˆ©æ™® å¸•å‡¯ç‰¹ã€å®‡è¾° é™†ã€èµ›é¡¿Â·å²è’‚æ–‡Â·åšç§‘ã€é©¬å…‹æ–¯ å²å¯†æ–¯ã€è¨è’‚äºš O-Gã€ä¹”çº³æ£®Â·KÂ·åº“é»˜è´¹å°”å¾·ã€ä¹”è‰¾å°”Â·çš®è¯ºã€è¨å»·å¾·Â·è¾›æ ¼å’Œäºšä¼¦Â·CÂ·åº“å°”ç»´å°”ã€‚æ— å‹åŠ›å¤–äº¤ï¼šå»ºæ¨¡å¤šæ™ºèƒ½ä½“åšå¼ˆã€‚*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•*ï¼Œ32ï¼Œ2019ã€‚
- en: 'Qi etÂ al. [2024] Siyuan Qi, Shuo Chen, Yexin Li, Xiangyu Kong, Junqi Wang,
    Bangcheng Yang, Pring Wong, Yifan Zhong, Xiaoyuan Zhang, Zhaowei Zhang, Nian Liu,
    Yaodong Yang, and Song-Chun Zhu. Civrealm: A learning and reasoning odyssey in
    civilization for decision-making agents. In *The Twelfth International Conference
    on Learning Representations*, 2024. URL [https://openreview.net/forum?id=UBVNwD3hPN](https://openreview.net/forum?id=UBVNwD3hPN).'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'é½ç­‰äºº [2024] æ€æº é½ã€ç¡• é™ˆã€å¶æ¬£ æã€å‘å®‡ å­”ã€ä¿Šç¦ ç‹ã€é‚¦æˆ æ¨ã€æ™®æ—Â·é»„ã€ä¸€å‡¡ é’Ÿã€å°æº å¼ ã€å…†ä¼Ÿ å¼ ã€å¹´ åˆ˜ã€è€€ä¸œ æ¨å’Œå®‹æ˜¥ Zhuã€‚Civrealm:
    å†³ç­–ä»£ç†çš„æ–‡æ˜å­¦ä¹ ä¸æ¨ç†ä¹‹æ—…ã€‚å‘è¡¨äº*ç¬¬åäºŒå±Šå›½é™…å­¦ä¹ è¡¨å¾å¤§ä¼š*ï¼Œ2024ã€‚ç½‘å€ [https://openreview.net/forum?id=UBVNwD3hPN](https://openreview.net/forum?id=UBVNwD3hPN)ã€‚'
- en: 'Renze and Guven [2024] Matthew Renze and Erhan Guven. Self-reflection in llm
    agents: Effects on problem-solving performance. *arXiv preprint arXiv:2405.06682*,
    2024.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»»æ³½å’Œå¤æ–‡ [2024] é©¬ä¿® ä»»æ³½å’Œå„å°”æ±‰ å¤æ–‡ã€‚llm ä»£ç†ä¸­çš„è‡ªæˆ‘åæ€ï¼šå¯¹é—®é¢˜è§£å†³ç»©æ•ˆçš„å½±å“ã€‚*arXiv é¢„å°æœ¬ arXiv:2405.06682*ï¼Œ2024ã€‚
- en: Richard [1979] Sharp Richard. *The game of diplomacy*. Arthur Barker, 1979.
    ISBN 978-0213166762.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç†æŸ¥å¾· [1979] å¤æ™® ç†æŸ¥å¾·ã€‚*å¤–äº¤æ¸¸æˆ*ã€‚äºšç‘ŸÂ·å·´å…‹å‡ºç‰ˆç¤¾ï¼Œ1979ã€‚ISBN 978-0213166762ã€‚
- en: 'Schick etÂ al. [2023] Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu,
    Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
    Toolformer: Language models can teach themselves to use tools. In *Thirty-seventh
    Conference on Neural Information Processing Systems*, 2023.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'å¸Œå…‹ç­‰äºº [2023] è’‚è«Â·å¸Œå…‹ã€ç®€Â·å¾·ç»´ç»´è¿ª-ä½™ã€ç½—ä¼¯æ‰˜Â·å¾·è¥¿ã€ç½—ä¼¯å¡”Â·èµ–åˆ©å®‰åŠªã€ç›ä¸½äºšÂ·æ´›æ¢…åˆ©ã€åŸƒé‡Œå…‹Â·æ±‰å¸ƒç½—ã€å¢å…‹Â·æ³½ç‰¹å°”æ‘©è€¶ã€å°¼å¤æ‹‰Â·ååˆ‡è¾¾å’Œæ‰˜é©¬æ–¯Â·è‚–éš†ã€‚Toolformer:
    è¯­è¨€æ¨¡å‹å¯ä»¥è‡ªå­¦ä½¿ç”¨å·¥å…·ã€‚å‘è¡¨äº*ç¬¬ä¸ƒåä¸ƒå±Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿä¼šè®®*ï¼Œ2023ã€‚'
- en: 'Shen etÂ al. [2023] Yongliang Shen, Kaitao Song, XuÂ Tan, Dongsheng Li, Weiming
    Lu, and Yueting Zhuang. HuggingGPT: Solving AI tasks with chatGPT and its friends
    in hugging face. In *Thirty-seventh Conference on Neural Information Processing
    Systems*, 2023.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'æ²ˆç­‰äºº [2023] æ°¸äº® æ²ˆã€å¼€æ¶› å®‹ã€å¾ è°­ã€ä¸œå‡ æã€ä¼Ÿæ˜ é™†å’Œè·ƒäº­ åº„ã€‚HuggingGPT: ä¸ chatGPT åŠå…¶åœ¨ hugging face
    çš„æœ‹å‹ä»¬ä¸€èµ·è§£å†³ AI ä»»åŠ¡ã€‚å‘è¡¨äº*ç¬¬ä¸ƒåä¸ƒå±Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿä¼šè®®*ï¼Œ2023ã€‚'
- en: 'Shoker etÂ al. [2023] Sarah Shoker, AndrewÂ W. Reddie, Sarah Barrington, Miles
    Brundage, Husanjot Chahal, Michael Depp, Bill Drexel, Ritwik Gupta, Marina Favaro,
    JakeÂ J. Hecla, Alan Hickey, Margarita Konaev, KIÂ Pavan Kumar, Nathan Lambert,
    AndrewÂ J. Lohn, Cullen Oâ€™Keefe, Nazneen Rajani, Michael Sellitto, RobertÂ F. Trager,
    LeahÂ A. Walker, Alexa Wehsener, and Jessica Young. Confidence-building measures
    for artificial intelligence: Workshop proceedings. *ArXiv*, abs/2308.00862, 2023.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shoker et al. [2023] Sarah Shoker, Andrew W. Reddie, Sarah Barrington, Miles
    Brundage, Husanjot Chahal, Michael Depp, Bill Drexel, Ritwik Gupta, Marina Favaro,
    Jake J. Hecla, Alan Hickey, Margarita Konaev, KI Pavan Kumar, Nathan Lambert,
    Andrew J. Lohn, Cullen Oâ€™Keefe, Nazneen Rajani, Michael Sellitto, Robert F. Trager,
    Leah A. Walker, Alexa Wehsener, å’Œ Jessica Young. äººå·¥æ™ºèƒ½çš„ä¿¡å¿ƒå»ºè®¾æªæ–½ï¼šç ”è®¨ä¼šè®ºæ–‡é›†ã€‚*ArXiv*ï¼Œabs/2308.00862ï¼Œ2023ã€‚
- en: 'Sun etÂ al. [2024] Chuanneng Sun, Songjun Huang, and Dario Pompili. Llm-based
    multi-agent reinforcement learning: Current and future directions. *arXiv preprint
    arXiv:2405.11106*, 2024.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. [2024] Chuanneng Sun, Songjun Huang, å’Œ Dario Pompili. åŸºäº LLM çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼šå½“å‰ä¸æœªæ¥æ–¹å‘ã€‚*arXiv
    é¢„å°æœ¬ arXiv:2405.11106*ï¼Œ2024ã€‚
- en: 'Talebirad and Nadiri [2023] Yashar Talebirad and Amirhossein Nadiri. Multi-agent
    collaboration: Harnessing the power of intelligent llm agents. *ArXiv*, abs/2306.03314,
    2023.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Talebirad and Nadiri [2023] Yashar Talebirad å’Œ Amirhossein Nadiri. å¤šæ™ºèƒ½ä½“åˆä½œï¼šåˆ©ç”¨æ™ºèƒ½
    LLM ä»£ç†çš„åŠ›é‡ã€‚*ArXiv*ï¼Œabs/2306.03314ï¼Œ2023ã€‚
- en: Wan etÂ al. [2024] Hongyu Wan, Jinda Zhang, AbdulazizÂ Arif Suria, Bingsheng Yao,
    Dakuo Wang, Yvonne Coady, and Mirjana Prpa. Building llm-based ai agents in social
    virtual reality. In *Extended Abstracts of the CHI Conference on Human Factors
    in Computing Systems*, pages 1â€“7, 2024.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan et al. [2024] Hongyu Wan, Jinda Zhang, Abdulaziz Arif Suria, Bingsheng Yao,
    Dakuo Wang, Yvonne Coady, å’Œ Mirjana Prpa. åœ¨ç¤¾äº¤è™šæ‹Ÿç°å®ä¸­æ„å»ºåŸºäº LLM çš„ AI ä»£ç†ã€‚æ”¶å½•äº*CHI è®¡ç®—æœºç³»ç»Ÿäººå› ä¼šè®®æ‰©å±•æ‘˜è¦*ï¼Œç¬¬
    1â€“7 é¡µï¼Œ2024ã€‚
- en: 'Wang etÂ al. [2024a] Dongzi Wang, Fangwei Zhong, Minglong Li, Muning Wen, Yuanxi
    Peng, Teng Li, and Adam Yang. Romat: Role-based multi-agent transformer for generalizable
    heterogeneous cooperation. *Neural Networks*, 174:106129, 2024a.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2024a] Dongzi Wang, Fangwei Zhong, Minglong Li, Muning Wen, Yuanxi
    Peng, Teng Li, å’Œ Adam Yang. Romatï¼šåŸºäºè§’è‰²çš„å¤šæ™ºèƒ½ä½“å˜æ¢å™¨ç”¨äºå¯æ¨å¹¿çš„å¼‚è´¨åˆä½œã€‚*Neural Networks*ï¼Œ174:106129ï¼Œ2024aã€‚
- en: 'Wang etÂ al. [2023a] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied
    agent with large language models. In *NeurIPS 2023 Foundation Models for Decision
    Making Workshop*, 2023a. URL [https://openreview.net/forum?id=P8E4Br72j3](https://openreview.net/forum?id=P8E4Br72j3).'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2023a] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan, å’Œ Anima Anandkumar. Voyagerï¼šä¸€ä¸ªå¼€æ”¾å¼çš„å…·èº«ä»£ç†ä¸å¤§å‹è¯­è¨€æ¨¡å‹ã€‚æ”¶å½•äº*NeurIPS
    2023 å†³ç­–åˆ¶å®šåŸºç¡€æ¨¡å‹ç ”è®¨ä¼š*ï¼Œ2023aã€‚ç½‘å€ [https://openreview.net/forum?id=P8E4Br72j3](https://openreview.net/forum?id=P8E4Br72j3)ã€‚
- en: 'Wang etÂ al. [2024b] Haoyu Wang, Tao Li, Zhiwei Deng, Dan Roth, and Yang Li.
    Devilâ€™s advocate: Anticipatory reflection for llm agents. *arXiv preprint arXiv:2405.16334*,
    2024b.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2024b] Haoyu Wang, Tao Li, Zhiwei Deng, Dan Roth, and Yang Li.
    é­”é¬¼ä»£è¨€äººï¼šé’ˆå¯¹ LLM ä»£ç†çš„é¢„æœŸåæ€ã€‚*arXiv é¢„å°æœ¬ arXiv:2405.16334*ï¼Œ2024bã€‚
- en: Wang etÂ al. [2024c] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, XuÂ Chen, Yankai Lin, etÂ al. A survey on large
    language model based autonomous agents. *Frontiers of Computer Science*, 18(6):1â€“26,
    2024c.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2024c] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, ç­‰ã€‚åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªä¸»ä»£ç†ç»¼è¿°ã€‚*Frontiers
    of Computer Science*ï¼Œ18(6):1â€“26ï¼Œ2024cã€‚
- en: Wang etÂ al. [2022] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, EdÂ Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves
    chain of thought reasoning in language models. *arXiv preprint arXiv:2203.11171*,
    2022.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2022] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, å’Œ Denny Zhou. è‡ªä¸€è‡´æ€§æé«˜è¯­è¨€æ¨¡å‹çš„é“¾å¼æ¨ç†ã€‚*arXiv é¢„å°æœ¬ arXiv:2203.11171*ï¼Œ2022ã€‚
- en: 'Wang etÂ al. [2021] Yuanfei Wang, Fangwei Zhong, Jing Xu, and Yizhou Wang. Tom2c:
    Target-oriented multi-agent communication and cooperation with theory of mind.
    *arXiv preprint arXiv:2111.09189*, 2021.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2021] Yuanfei Wang, Fangwei Zhong, Jing Xu, å’Œ Yizhou Wang. Tom2cï¼šåŸºäºç›®æ ‡çš„å¤šæ™ºèƒ½ä½“é€šä¿¡ä¸åˆä½œç†è®ºã€‚*arXiv
    é¢„å°æœ¬ arXiv:2111.09189*ï¼Œ2021ã€‚
- en: 'Wang etÂ al. [2023b] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, XiaojianÂ (Shawn)
    Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning
    with llms enables open-world multi-task agents. In A.Â Oh, T.Â Naumann, A.Â Globerson,
    K.Â Saenko, M.Â Hardt, and S.Â Levine, editors, *Advances in Neural Information Processing
    Systems*, volumeÂ 36, pages 34153â€“34189\. Curran Associates, Inc., 2023b. URL [https://proceedings.neurips.cc/paper_files/paper/2023/file/6b8dfb8c0c12e6fafc6c256cb08a5ca7-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/6b8dfb8c0c12e6fafc6c256cb08a5ca7-Paper-Conference.pdf).'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2023b] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian
    (Shawn) Ma, and Yitao Liang. æè¿°ã€è§£é‡Šã€è®¡åˆ’å’Œé€‰æ‹©ï¼šä¸ LLM äº’åŠ¨è§„åˆ’å®ç°å¼€æ”¾ä¸–ç•Œå¤šä»»åŠ¡ä»£ç†ã€‚è§ A. Oh, T. Naumann,
    A. Globerson, K. Saenko, M. Hardt, å’Œ S. Levine ç¼–è¾‘çš„*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•*ï¼Œç¬¬ 36 å·ï¼Œç¬¬ 34153â€“34189
    é¡µã€‚Curran Associates, Inc.ï¼Œ2023bã€‚ç½‘å€ [https://proceedings.neurips.cc/paper_files/paper/2023/file/6b8dfb8c0c12e6fafc6c256cb08a5ca7-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/6b8dfb8c0c12e6fafc6c256cb08a5ca7-Paper-Conference.pdf)ã€‚
- en: 'Wang etÂ al. [2023c] Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing
    Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, Xiaojian
    Ma, and Yitao Liang. Jarvis-1: Open-world multi-task agents with memory-augmented
    multimodal language models. *ArXiv*, abs/2311.05997, 2023c.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2023c] Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing
    Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, Xiaojian
    Ma, and Yitao Liang. Jarvis-1ï¼šå…·æœ‰è®°å¿†å¢å¼ºçš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„å¼€æ”¾ä¸–ç•Œå¤šä»»åŠ¡ä»£ç†ã€‚*ArXiv*ï¼Œabs/2311.05997ï¼Œ2023cã€‚
- en: 'Wang etÂ al. [2024d] Ziyan Wang, Yingpeng Du, Zhu Sun, Haoyan Chua, Kaidong
    Feng, Wenya Wang, and Jie Zhang. Re2llm: Reflective reinforcement large language
    model for session-based recommendation. *arXiv preprint arXiv:2403.16427*, 2024d.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. [2024d] Ziyan Wang, Yingpeng Du, Zhu Sun, Haoyan Chua, Kaidong
    Feng, Wenya Wang, and Jie Zhang. Re2llm: åå°„æ€§å¼ºåŒ–å¤§å‹è¯­è¨€æ¨¡å‹ç”¨äºåŸºäºä¼šè¯çš„æ¨èã€‚*arXiv é¢„å°æœ¬ arXiv:2403.16427*ï¼Œ2024dã€‚'
- en: Wei etÂ al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, EdÂ Chi, QuocÂ V Le, Denny Zhou, etÂ al. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in neural information processing
    systems*, 35:24824â€“24837, 2022.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, ç­‰ã€‚é“¾å¼æ€ç»´æç¤ºå¼•å‘å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†ã€‚*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•*ï¼Œ35:24824â€“24837ï¼Œ2022ã€‚
- en: 'Wikipedia [2024] Wikipedia. Diplomacy(game). [https://en.wikipedia.org/wiki/Diplomacy_(game)](https://en.wikipedia.org/wiki/Diplomacy_(game)),
    2024. Accessed: 2024-05-18.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wikipedia [2024] Wikipedia. Diplomacyï¼ˆæ¸¸æˆï¼‰ã€‚[https://en.wikipedia.org/wiki/Diplomacy_(game)](https://en.wikipedia.org/wiki/Diplomacy_(game))ï¼Œ2024ã€‚è®¿é—®æ—¶é—´ï¼š2024-05-18ã€‚
- en: 'Xia etÂ al. [2024] Tian Xia, Zhiwei He, Tong Ren, Yibo Miao, Zhuosheng Zhang,
    Yang Yang, and Rui Wang. Measuring bargaining abilities of llms: A benchmark and
    a buyer-enhancement method. *arXiv preprint arXiv:2402.15813*, 2024.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xia et al. [2024] Tian Xia, Zhiwei He, Tong Ren, Yibo Miao, Zhuosheng Zhang,
    Yang Yang, and Rui Wang. æµ‹é‡ LLM çš„è°ˆåˆ¤èƒ½åŠ›ï¼šåŸºå‡†æµ‹è¯•å’Œä¹°æ–¹å¢å¼ºæ–¹æ³•ã€‚*arXiv é¢„å°æœ¬ arXiv:2402.15813*ï¼Œ2024ã€‚
- en: Xu etÂ al. [2020] Jing Xu, Fangwei Zhong, and Yizhou Wang. Learning multi-agent
    coordination for enhancing target coverage in directional sensor networks. *Advances
    in Neural Information Processing Systems*, 33:10053â€“10064, 2020.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. [2020] Jing Xu, Fangwei Zhong, and Yizhou Wang. å­¦ä¹ å¤šæ™ºèƒ½ä½“åè°ƒä»¥å¢å¼ºæ–¹å‘ä¼ æ„Ÿå™¨ç½‘ç»œä¸­çš„ç›®æ ‡è¦†ç›–ã€‚*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•*ï¼Œ33:10053â€“10064ï¼Œ2020ã€‚
- en: 'Yan etÂ al. [2023] Ming Yan, Ruihao Li, Hao Zhang, Hao Wang, Zhilan Yang, and
    JiÂ Yan. Larp: Language-agent role play for open-world games. *arXiv preprint arXiv:2312.17653*,
    2023.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan et al. [2023] Ming Yan, Ruihao Li, Hao Zhang, Hao Wang, Zhilan Yang, and
    Ji Yan. LARPï¼šå¼€æ”¾ä¸–ç•Œæ¸¸æˆä¸­çš„è¯­è¨€ä»£ç†è§’è‰²æ‰®æ¼”ã€‚*arXiv é¢„å°æœ¬ arXiv:2312.17653*ï¼Œ2023ã€‚
- en: 'Yang etÂ al. [2023a] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu
    Li, and Ying Shan. GPT4tools: Teaching large language model to use tools via self-instruction.
    In *Thirty-seventh Conference on Neural Information Processing Systems*, 2023a.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. [2023a] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu
    Li, and Ying Shan. GPT4toolsï¼šé€šè¿‡è‡ªæˆ‘æŒ‡å¯¼æ•™å¤§å‹è¯­è¨€æ¨¡å‹ä½¿ç”¨å·¥å…·ã€‚è§äº*ç¬¬ä¸‰åä¸ƒå±Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿä¼šè®®*ï¼Œ2023aã€‚
- en: 'Yang etÂ al. [2023b] Ziyi Yang, ShreyasÂ S Raman, Ankit Shah, and Stefanie Tellex.
    Plug in the safety chip: Enforcing constraints for llm-driven robot agents. *arXiv
    preprint arXiv:2309.09919*, 2023b.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. [2023b] Ziyi Yang, Shreyas S Raman, Ankit Shah, and Stefanie Tellex.
    æ’å…¥å®‰å…¨èŠ¯ç‰‡ï¼šä¸º LLM é©±åŠ¨çš„æœºå™¨äººä»£ç†æ‰§è¡Œçº¦æŸã€‚*arXiv é¢„å°æœ¬ arXiv:2309.09919*ï¼Œ2023bã€‚
- en: 'Yao etÂ al. [2023] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, ThomasÂ L.
    Griffiths, Yuan Cao, and KarthikÂ R Narasimhan. Tree of thoughts: Deliberate problem
    solving with large language models. In *Thirty-seventh Conference on Neural Information
    Processing Systems*, 2023.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. [2023] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L.
    Griffiths, Yuan Cao, and Karthik R Narasimhan. æ€ç»´æ ‘ï¼šä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ·±æ€ç†Ÿè™‘é—®é¢˜è§£å†³ã€‚è§äº*ç¬¬ä¸‰åä¸ƒå±Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿä¼šè®®*ï¼Œ2023ã€‚
- en: 'Yu etÂ al. [2024] Yangyang Yu, Haohang Li, Zhi Chen, Yuechen Jiang, Yang Li,
    Denghui Zhang, Rong Liu, JordanÂ W Suchow, and Khaldoun Khashanah. Finmem: A performance-enhanced
    llm trading agent with layered memory and character design. In *Proceedings of
    the AAAI Symposium Series*, volumeÂ 3, pages 595â€“597, 2024.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. [2024] æ¨æ¨ä½™ã€æµ©èˆªæã€æ™ºé™ˆã€å²³è¾°å§œã€æ¨æã€é‚“è¾‰å¼ ã€è£åˆ˜ã€ä¹”ä¸¹Â·WÂ·è‹ä¹”å’Œå“ˆå°”é¡¿Â·å“ˆå°šã€‚Finmemï¼šå…·æœ‰åˆ†å±‚è®°å¿†å’Œè§’è‰²è®¾è®¡çš„æ€§èƒ½å¢å¼ºLLMäº¤æ˜“ä»£ç†ã€‚åœ¨*AAAIç ”è®¨ä¼šç³»åˆ—è®ºæ–‡é›†*ï¼Œç¬¬3å·ï¼Œé¡µç 595â€“597ï¼Œ2024å¹´ã€‚
- en: Zhan etÂ al. [2024] Haolan Zhan, Yufei Wang, Tao Feng, Yuncheng Hua, Suraj Sharma,
    Zhuang Li, Lizhen Qu, ZhalehÂ Semnani Azad, Ingrid Zukerman, and Gholamreza Haffari.
    Letâ€™s negotiate! a survey of negotiation dialogue systems. *arXiv preprint arXiv:2402.01097*,
    2024.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhan et al. [2024] å¥½å…°å±•ã€ç‰é£ç‹ã€é™¶ä¸°ã€äº‘åŸåã€è‹æ‹‰æ°Â·å¤å°”é©¬ã€åº„æã€ä¸½çæ›²ã€æ‰èµ«æ‹‰Â·å¡å§†çº³å°¼Â·é˜¿æ‰å¾·ã€è‹±æ ¼ä¸½å¾·Â·ç¥–å…‹æ›¼å’Œæˆˆæ‹‰å§†é›·æ‰Â·å“ˆæ³•é‡Œã€‚è®©æˆ‘ä»¬è°ˆåˆ¤ï¼è°ˆåˆ¤å¯¹è¯ç³»ç»Ÿçš„ç»¼è¿°ã€‚*arXiv
    é¢„å°æœ¬ arXiv:2402.01097*ï¼Œ2024å¹´ã€‚
- en: 'Zhang etÂ al. [2023] Danyang Zhang, LuÂ Chen, Situo Zhang, Hongshen Xu, Zihan
    Zhao, and Kai Yu. Large language model is semi-parametric reinforcement learning
    agent. *CoRR*, abs/2306.07929, 2023. doi: 10.48550/arXiv.2306.07929. URL [https://doi.org/10.48550/arXiv.2306.07929](https://doi.org/10.48550/arXiv.2306.07929).'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. [2023] ä¸¹é˜³å¼ ã€é™†æ™¨ã€å¸å¾’å¼ ã€æ´ªç”³å¾ã€å­æ¶µèµµå’Œå‡¯ä½™ã€‚å¤§å‹è¯­è¨€æ¨¡å‹æ˜¯åŠå‚æ•°å¼ºåŒ–å­¦ä¹ ä»£ç†ã€‚*CoRR*ï¼Œabs/2306.07929ï¼Œ2023å¹´ã€‚doi:
    10.48550/arXiv.2306.07929ã€‚ç½‘å€ [https://doi.org/10.48550/arXiv.2306.07929](https://doi.org/10.48550/arXiv.2306.07929)ã€‚'
- en: Zhang etÂ al. [2024a] Danyang Zhang, LuÂ Chen, Situo Zhang, Hongshen Xu, Zihan
    Zhao, and Kai Yu. Large language models are semi-parametric reinforcement learning
    agents. *Advances in Neural Information Processing Systems*, 36, 2024a.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2024a] ä¸¹é˜³å¼ ã€é™†æ™¨ã€å¸å¾’å¼ ã€æ´ªç”³å¾ã€å­æ¶µèµµå’Œå‡¯ä½™ã€‚å¤§å‹è¯­è¨€æ¨¡å‹æ˜¯åŠå‚æ•°å¼ºåŒ–å­¦ä¹ ä»£ç†ã€‚*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•*ï¼Œç¬¬36å·ï¼Œ2024aã€‚
- en: 'Zhang etÂ al. [2024b] Wenqi Zhang, KeÂ Tang, Hai Wu, Mengna Wang, Yongliang Shen,
    Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang, and Weiming Lu. Agent-pro: Learning
    to evolve via policy-level reflection and optimization. *arXiv preprint arXiv:2402.17574*,
    2024b.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2024b] æ¸©å¥‡å¼ ã€æŸ¯å”ã€æµ·å´ã€å­Ÿå¨œç‹ã€æ°¸äº®æ²ˆã€è´µé˜³ä¾¯ã€æ³½çªè°­ã€å½­æã€å²³äº­åº„å’Œä¼Ÿé“­é™†ã€‚Agent-proï¼šé€šè¿‡ç­–ç•¥çº§åæ€å’Œä¼˜åŒ–å­¦ä¹ è¿›åŒ–ã€‚*arXiv
    é¢„å°æœ¬ arXiv:2402.17574*ï¼Œ2024bã€‚
- en: 'Zhang etÂ al. [2024c] Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian
    deÂ Wynter, Yan Xia, Wenshan Wu, Ting Song, Man Lan, and Furu Wei. Llm as a mastermind:
    A survey of strategic reasoning with large language models. *arXiv preprint arXiv:2404.01230*,
    2024c.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2024c] äºšä¸œå¼ ã€é‚µå¹¿æ¯›ã€é™¶æ­Œã€å¯»ç‹ã€é˜¿å¾·é‡Œå®‰Â·å¾·Â·æ¸©ç‰¹ã€ç‡•éœã€æ–‡å±±å´ã€å»·å®‹ã€æ›¼å…°å’Œå¯Œå¦‚é­ã€‚LLMä½œä¸ºç­–åˆ’è€…ï¼šå¤§è¯­è¨€æ¨¡å‹æˆ˜ç•¥æ¨ç†çš„ç»¼è¿°ã€‚*arXiv
    é¢„å°æœ¬ arXiv:2404.01230*ï¼Œ2024cã€‚
- en: Zhang etÂ al. [2024d] Yang Zhang, Shixin Yang, Chenjia Bai, Fei Wu, Xiu Li, Xuelong
    Li, and Zhen Wang. Towards efficient llm grounding for embodied multi-agent collaboration.
    *arXiv preprint arXiv:2405.14314*, 2024d.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2024d] æ¨å¼ ã€æ–½æ–°æ¨ã€é™ˆä½³ç™½ã€é£å´ã€ç§€æã€é›ªé¾™æå’ŒæŒ¯ç‹ã€‚æœç€é«˜æ•ˆLLMåŸºç¡€çš„å…·èº«å¤šä»£ç†åä½œè¿ˆè¿›ã€‚*arXiv é¢„å°æœ¬
    arXiv:2405.14314*ï¼Œ2024dã€‚
- en: Zhang etÂ al. [2024e] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, XuÂ Chen, Quanyu
    Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. A survey on the memory mechanism
    of large language model based agents. *arXiv preprint arXiv:2404.13501*, 2024e.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2024e] æ³½å®‡å¼ ã€å°èµ«åšã€é™ˆé©¬ã€ç‘æã€å¾é™ˆã€å…¨å®‡æˆ´ã€æ°æ˜æœ±ã€æŒ¯åè‘£å’Œå­£è£æ¸©ã€‚å…³äºå¤§å‹è¯­è¨€æ¨¡å‹åŸºäºä»£ç†çš„è®°å¿†æœºåˆ¶çš„ç»¼è¿°ã€‚*arXiv
    é¢„å°æœ¬ arXiv:2404.13501*ï¼Œ2024eã€‚
- en: Zhang etÂ al. [2022] Zhuosheng Zhang, Aston Zhang, MuÂ Li, and Alex Smola. Automatic
    chain of thought prompting in large language models. *arXiv preprint arXiv:2210.03493*,
    2022.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2022] æœ±ç”Ÿå¼ ã€é˜¿æ–¯é¡¿å¼ ã€ç©†æå’Œäºšå†å…‹æ–¯Â·æ–¯è«æ‹‰ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è‡ªåŠ¨æ€ç»´é“¾æç¤ºã€‚*arXiv é¢„å°æœ¬ arXiv:2210.03493*ï¼Œ2022å¹´ã€‚
- en: 'Zhong etÂ al. [2023] Fangwei Zhong, Xiao Bi, Yudi Zhang, Wei Zhang, and Yizhou
    Wang. Rspt: reconstruct surroundings and predict trajectory for generalizable
    active object tracking. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    volumeÂ 37, pages 3705â€“3714, 2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhong et al. [2023] æ–¹ç»´ä¸­ã€è‚–ç¢§ã€ä½™è¿ªå¼ ã€é­å¼ å’Œæ˜“æ´²ç‹ã€‚Rsptï¼šé‡å»ºç¯å¢ƒå¹¶é¢„æµ‹è½¨è¿¹ä»¥å®ç°é€šç”¨çš„ä¸»åŠ¨ç›®æ ‡è·Ÿè¸ªã€‚åœ¨*AAAIäººå·¥æ™ºèƒ½ä¼šè®®è®ºæ–‡é›†*ï¼Œç¬¬37å·ï¼Œé¡µç 3705â€“3714ï¼Œ2023å¹´ã€‚
- en: 'Zhu etÂ al. [2023] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su,
    Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, YuÂ Qiao, Zhaoxiang Zhang,
    and Jifeng Dai. Ghost in the minecraft: Generally capable agents for open-world
    environments via large language models with text-based knowledge and memory. *arXiv
    preprint arXiv:2305.17144*, 2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. [2023] ä¹ è¿‘å¹³æœ±ã€äº‘æ¶›é™ˆã€éƒå¤©ã€æ™¨æ¬£é™¶ã€ä¼Ÿæ°è‹ã€æ™¨å®‡æ¨ã€é«˜é»„ã€å®¾æã€ä¹ä¼Ÿé™†ã€è‚–åˆšç‹ã€ä½™ä¹”ã€èµµç¿”å¼ å’Œå­£é”‹æˆ´ã€‚Minecraftä¸­çš„å¹½çµï¼šé€šè¿‡å…·æœ‰åŸºäºæ–‡æœ¬çš„çŸ¥è¯†å’Œè®°å¿†çš„å¤§è¯­è¨€æ¨¡å‹å®ç°å¯¹å¼€æ”¾ä¸–ç•Œç¯å¢ƒçš„ä¸€èˆ¬èƒ½åŠ›ä»£ç†ã€‚*arXiv
    é¢„å°æœ¬ arXiv:2305.17144*ï¼Œ2023å¹´ã€‚
- en: Appendix A Implementation Details
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é™„å½• A å®æ–½ç»†èŠ‚
- en: A.1 Rules of Diplomacy Game
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 å¤–äº¤æ¸¸æˆè§„åˆ™
- en: â€¢
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: You need to occupy as many supply centers as possible. If you occupy 18 or more
    supply centers, you will win the game directly. If you lose all your supply centers,
    you will be eliminated immediately.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä½ éœ€è¦å é¢†å°½å¯èƒ½å¤šçš„è¡¥ç»™ä¸­å¿ƒã€‚å¦‚æœä½ å é¢†18ä¸ªæˆ–æ›´å¤šçš„è¡¥ç»™ä¸­å¿ƒï¼Œä½ å°†ç›´æ¥èµ¢å¾—æ¸¸æˆã€‚å¦‚æœä½ å¤±å»æ‰€æœ‰çš„è¡¥ç»™ä¸­å¿ƒï¼Œä½ å°†ç«‹å³è¢«æ·˜æ±°ã€‚
- en: â€¢
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: The units consist of armies and fleets. Armies can only move to adjacent areas,
    while fleets can move to adjacent sea zones or coastal areas and can move along
    the coast.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å•ä½åŒ…æ‹¬é™†å†›å’Œèˆ°é˜Ÿã€‚é™†å†›åªèƒ½ç§»åŠ¨åˆ°ç›¸é‚»åŒºåŸŸï¼Œè€Œèˆ°é˜Ÿå¯ä»¥ç§»åŠ¨åˆ°ç›¸é‚»çš„æµ·åŸŸæˆ–æ²¿æµ·åŒºåŸŸï¼Œå¹¶å¯ä»¥æ²¿æµ·å²¸çº¿ç§»åŠ¨ã€‚
- en: â€¢
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: To occupy a supply center, your units must move into that area in the autumn.
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¦å é¢†ä¸€ä¸ªè¡¥ç»™ä¸­å¿ƒï¼Œä½ çš„å•ä½å¿…é¡»åœ¨ç§‹å­£è¿›å…¥è¯¥åŒºåŸŸã€‚
- en: â€¢
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: When a unit moves to an area, if another unit is in the destination or if other
    units are also moving to that destination, the move fails, resulting in a standoff.
    In such cases, you can seek support from units in adjacent areas to the destination.
    If another unit moves into the region from which support is coming, the support
    is cut off. The unit with the most support moves into the area, while other units
    must retreat to an adjacent province or disband. If there is no place to retreat,
    the unit must disband. Fleets can transport armies across sea zones from one coastal
    region to another. However, if another fleet moves into that sea zone, the transport
    is cut off.
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å½“ä¸€ä¸ªå•ä½ç§»åŠ¨åˆ°ä¸€ä¸ªåŒºåŸŸæ—¶ï¼Œå¦‚æœå¦ä¸€ä¸ªå•ä½åœ¨ç›®çš„åœ°ï¼Œæˆ–è€…å…¶ä»–å•ä½ä¹Ÿåœ¨ç§»åŠ¨åˆ°è¯¥ç›®çš„åœ°ï¼Œåˆ™ç§»åŠ¨å¤±è´¥ï¼Œå¯¼è‡´åƒµå±€ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ å¯ä»¥å¯»æ±‚æ¥è‡ªé‚»è¿‘åŒºåŸŸçš„å•ä½çš„æ”¯æŒã€‚å¦‚æœå¦ä¸€ä¸ªå•ä½ä»æ”¯æŒæ¥æºçš„åŒºåŸŸç§»åŠ¨è¿‡æ¥ï¼Œæ”¯æŒå°†è¢«åˆ‡æ–­ã€‚æ‹¥æœ‰æœ€å¤šæ”¯æŒçš„å•ä½è¿›å…¥è¯¥åŒºåŸŸï¼Œè€Œå…¶ä»–å•ä½å¿…é¡»é€€å›åˆ°ç›¸é‚»çš„çœä»½æˆ–è§£æ•£ã€‚å¦‚æœæ²¡æœ‰é€€è·¯ï¼Œå•ä½å¿…é¡»è§£æ•£ã€‚èˆ°é˜Ÿå¯ä»¥å°†é™†å†›ä»ä¸€ä¸ªæ²¿æµ·åŒºåŸŸè¿è¾“åˆ°å¦ä¸€ä¸ªæ²¿æµ·åŒºåŸŸã€‚ç„¶è€Œï¼Œå¦‚æœå¦ä¸€ä¸ªèˆ°é˜Ÿè¿›å…¥è¯¥æµ·åŸŸï¼Œè¿è¾“å°†è¢«åˆ‡æ–­ã€‚
- en: â€¢
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: The number of units a country can have cannot exceed the number of supply centers
    it controls. If the number of supply centers decreases, excess units must be disbanded.
    Each autumn, new units can be built at supply centers. Coastal supply centers
    can produce fleets or armies, while others can only produce armies. [[22](#bib.bib22)]
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå›½å®¶å¯ä»¥æ‹¥æœ‰çš„å•ä½æ•°é‡ä¸èƒ½è¶…è¿‡å…¶æ§åˆ¶çš„è¡¥ç»™ä¸­å¿ƒæ•°é‡ã€‚å¦‚æœè¡¥ç»™ä¸­å¿ƒæ•°é‡å‡å°‘ï¼Œåˆ™å¿…é¡»è§£æ•£å¤šä½™çš„å•ä½ã€‚æ¯å¹´ç§‹å­£ï¼Œå¯ä»¥åœ¨è¡¥ç»™ä¸­å¿ƒå»ºç«‹æ–°çš„å•ä½ã€‚æ²¿æµ·è¡¥ç»™ä¸­å¿ƒå¯ä»¥ç”Ÿäº§èˆ°é˜Ÿæˆ–é™†å†›ï¼Œè€Œå…¶ä»–è¡¥ç»™ä¸­å¿ƒåªèƒ½ç”Ÿäº§é™†å†›ã€‚[[22](#bib.bib22)]
- en: A.2 Domain Knowledge
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 é¢†åŸŸçŸ¥è¯†
- en: Richelieu can adopt a strategy of allying with distant countries while attacking
    neighboring ones to occupy adjacent territories and achieve rapid expansion. Richelieu
    should pay attention to the Balance of Power by forming alliances with other countries
    or supporting weaker states to prevent any single country or alliance from becoming
    too powerful. [[12](#bib.bib12)] To this end, Richelieu can also adopt a strategy
    of attacking distant countries while allying with nearby ones, sacrificing short-term
    benefits to avoid the emergence of future hegemonic states that could threaten
    his own survival. When facing multiple enemies, Richelieu can find ways to divide
    other countries and incite wars among them. Whether in offense or defense, Richelieu
    should actively choose suitable allies. Richelieu can also introduce a third party
    to achieve goals such as ceasefire, alliance, or joint attack. To achieve alliances
    or ceasefires, Richelieu can sacrifice some interests to the other party as long
    as the ultimate benefits are greater. Others may lie and deceive [[27](#bib.bib27)];
    their words in negotiations are not binding. Richelieu must avoid being deceived
    or betrayed. At the same time, Richelieu can also actively deceive others to achieve
    his own goals.[[42](#bib.bib42), [2](#bib.bib2)]
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**é»å¡ç•™**å¯ä»¥é‡‡ç”¨ä¸è¿œæ–¹å›½å®¶ç»“ç›Ÿçš„ç­–ç•¥ï¼ŒåŒæ—¶æ”»å‡»é‚»å›½ä»¥å é¢†ç›¸é‚»é¢†åœŸï¼Œå®ç°å¿«é€Ÿæ‰©å¼ ã€‚é»å¡ç•™åº”æ³¨æ„æƒåŠ›å¹³è¡¡ï¼Œé€šè¿‡ä¸å…¶ä»–å›½å®¶ç»“ç›Ÿæˆ–æ”¯æŒè¾ƒå¼±çš„å›½å®¶æ¥é˜²æ­¢ä»»ä½•å•ä¸€å›½å®¶æˆ–è”ç›Ÿå˜å¾—è¿‡äºå¼ºå¤§ã€‚[[12](#bib.bib12)]
    ä¸ºæ­¤ï¼Œé»å¡ç•™è¿˜å¯ä»¥é‡‡å–æ”»å‡»è¿œæ–¹å›½å®¶çš„ç­–ç•¥ï¼ŒåŒæ—¶ä¸é‚»è¿‘å›½å®¶ç»“ç›Ÿï¼Œç‰ºç‰²çŸ­æœŸåˆ©ç›Šï¼Œä»¥é¿å…æœªæ¥éœ¸æƒå›½å®¶çš„å‡ºç°ï¼Œè¿™äº›å›½å®¶å¯èƒ½å¨èƒåˆ°ä»–çš„ç”Ÿå­˜ã€‚åœ¨é¢å¯¹å¤šä¸ªæ•Œäººæ—¶ï¼Œé»å¡ç•™å¯ä»¥æ‰¾åˆ°æ–¹æ³•åˆ†åŒ–å…¶ä»–å›½å®¶ï¼Œå¹¶ç…½åŠ¨å®ƒä»¬ä¹‹é—´çš„æˆ˜äº‰ã€‚æ— è®ºæ˜¯è¿›æ”»è¿˜æ˜¯é˜²å®ˆï¼Œé»å¡ç•™éƒ½åº”ç§¯æé€‰æ‹©åˆé€‚çš„ç›Ÿå‹ã€‚é»å¡ç•™è¿˜å¯ä»¥å¼•å…¥ç¬¬ä¸‰æ–¹ä»¥å®ç°åœç«ã€ç»“ç›Ÿæˆ–è”åˆæ”»å‡»ç­‰ç›®æ ‡ã€‚ä¸ºäº†å®ç°ç»“ç›Ÿæˆ–åœç«ï¼Œåªè¦æœ€ç»ˆåˆ©ç›Šæ›´å¤§ï¼Œé»å¡ç•™å¯ä»¥ç‰ºç‰²ä¸€äº›åˆ©ç›Šç»™å¯¹æ–¹ã€‚å…¶ä»–äººå¯èƒ½ä¼šæ’’è°å’Œæ¬ºéª—[[27](#bib.bib27)]ï¼›ä»–ä»¬åœ¨è°ˆåˆ¤ä¸­çš„è¯å¹¶ä¸å…·çº¦æŸåŠ›ã€‚é»å¡ç•™å¿…é¡»é¿å…è¢«æ¬ºéª—æˆ–èƒŒå›ã€‚åŒæ—¶ï¼Œé»å¡ç•™ä¹Ÿå¯ä»¥ç§¯æåœ°æ¬ºéª—ä»–äººä»¥å®ç°è‡ªå·±çš„ç›®æ ‡ã€‚[[42](#bib.bib42),
    [2](#bib.bib2)]'
- en: A.3 Prompt Templates
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 æç¤ºæ¨¡æ¿
- en: For the convenience of reproducing the results of the experiments of this paper,
    here we give the prompt template of different modules of Richelieu.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ–¹ä¾¿å†ç°æœ¬æ–‡çš„å®éªŒç»“æœï¼Œè¿™é‡Œæä¾›äº†**é»å¡ç•™**ä¸åŒæ¨¡å—çš„æç¤ºæ¨¡æ¿ã€‚
- en: 1) INIT
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 1) INIT
- en: 1You  will  control  {country}  and  compete  with  six  other  countries  on  the  map  for  supply  centers.2The  map  consists  of  different  regions  and  sea  areas.  Their  adjacency  relationships  are  shown  in  the  matrix.  The  numbers  for  the  regions  and  sea  areas  are  ......3Different  regions  are  occupied  by  different  countries.  The  ownership  of  the  regions  is  shown  in  the  matrix.4The  region  Berlin,  ........  are  supply  centers.5You  need  to  follow  these  rules  ......6To  help  you  achieve  victory,  these  diplomatic  strategies  might  be  of  assistance.  ......
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 1ä½ å°†æ§åˆ¶{country}ï¼Œå¹¶ä¸åœ°å›¾ä¸Šçš„å…­ä¸ªå…¶ä»–å›½å®¶äº‰å¤ºä¾›åº”ä¸­å¿ƒã€‚2åœ°å›¾ç”±ä¸åŒçš„åŒºåŸŸå’Œæµ·åŸŸç»„æˆã€‚å®ƒä»¬çš„é‚»æ¥å…³ç³»åœ¨çŸ©é˜µä¸­æ˜¾ç¤ºã€‚åŒºåŸŸå’Œæµ·åŸŸçš„ç¼–å·æ˜¯â€¦â€¦3ä¸åŒçš„åŒºåŸŸç”±ä¸åŒçš„å›½å®¶å æ®ã€‚åŒºåŸŸçš„æ‰€æœ‰æƒåœ¨çŸ©é˜µä¸­æ˜¾ç¤ºã€‚4åŒºåŸŸæŸæ—ï¼Œâ€¦â€¦æ˜¯ä¾›åº”ä¸­å¿ƒã€‚5ä½ éœ€è¦éµå¾ªè¿™äº›è§„åˆ™â€¦â€¦6ä¸ºäº†å¸®åŠ©ä½ å–å¾—èƒœåˆ©ï¼Œè¿™äº›å¤–äº¤ç­–ç•¥å¯èƒ½ä¼šæœ‰æ‰€å¸®åŠ©â€¦â€¦
- en: 2) Social Reasoning
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 2) ç¤¾ä¼šæ¨ç†
- en: 1France  occupies  Portugal  Ruhr,  Paris,  Burgundy,  ......2France  has  armies  in  Brest,  Belgium,  ......  And  France  has  fleets  in  Mid  Atlantic,  England  Channel,  ......3England  ......4......5Based  on  the  current  state,  what  do  you  think  are  the  current  strategic  intentions  of  the  other  countries?6Which  country  do  you  think  needs  to  be  attacked  or  weakened  the  most  right  now?7And  which  country  do  you  think  is  most  suitable  for  you  to  ally  with  in  order  to  deal  with  this  country?
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 1æ³•å›½å æ®äº†è‘¡è„ç‰™ã€é²å°”ã€å·´é»ã€å‹ƒè‰®ç¬¬ï¼Œâ€¦â€¦2æ³•å›½åœ¨å¸ƒåˆ—æ–¯ç‰¹ã€æ¯”åˆ©æ—¶â€¦â€¦æœ‰å†›é˜Ÿï¼Œå¹¶ä¸”æ³•å›½åœ¨ä¸­å¤§è¥¿æ´‹ã€è‹±å‰åˆ©æµ·å³¡â€¦â€¦æœ‰èˆ°é˜Ÿã€‚3è‹±æ ¼å…°â€¦â€¦4â€¦â€¦5åŸºäºå½“å‰çŠ¶æ€ï¼Œä½ è®¤ä¸ºå…¶ä»–å›½å®¶çš„æˆ˜ç•¥æ„å›¾æ˜¯ä»€ä¹ˆï¼Ÿ6ä½ è®¤ä¸ºç°åœ¨æœ€éœ€è¦æ”»å‡»æˆ–å‰Šå¼±å“ªä¸ªå›½å®¶ï¼Ÿ7ä½ è®¤ä¸ºå“ªä¸ªå›½å®¶æœ€é€‚åˆä¸ä½ ç»“ç›Ÿä»¥å¯¹ä»˜è¿™ä¸ªå›½å®¶ï¼Ÿ
- en: 3) Planner with Reflection
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 3) å¸¦æœ‰åæ€çš„è§„åˆ’è€…
- en: 1In  the  current  state,  with  {ally  and  enemy},  what  sub-goal  do  you  think  should  be  set  for  {country}  ?2I  have  found  some  useful  historical  experiences  for  you.  Please  reflect  on  and  optimize  your  sub-goal  based  on  these  historical  experiences.3The  sub-goal  you  formulated  when  {state}  was  to  {sub-goal}.  The  eventual  result  was  {future}.  The  evaluation  for  this  sub-goal  is  {score}.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 1åœ¨å½“å‰çŠ¶æ€ä¸‹ï¼Œä¸{ally and enemy}ï¼Œä½ è®¤ä¸ºåº”è¯¥ä¸º{country}è®¾å®šä»€ä¹ˆå­ç›®æ ‡ï¼Ÿ2æˆ‘ä¸ºä½ æ‰¾åˆ°äº†ä¸€äº›æœ‰ç”¨çš„å†å²ç»éªŒã€‚è¯·åŸºäºè¿™äº›å†å²ç»éªŒåæ€å¹¶ä¼˜åŒ–ä½ çš„å­ç›®æ ‡ã€‚3ä½ åœ¨{state}æ—¶åˆ¶å®šçš„å­ç›®æ ‡æ˜¯{sub-goal}ã€‚æœ€ç»ˆç»“æœæ˜¯{future}ã€‚å¯¹è¿™ä¸ªå­ç›®æ ‡çš„è¯„ä»·æ˜¯{score}ã€‚
