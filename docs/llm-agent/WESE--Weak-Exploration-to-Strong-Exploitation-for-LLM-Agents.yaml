- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:48:34'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'WESE: Weak Exploration to Strong Exploitation for LLM Agents'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.07456](https://ar5iv.labs.arxiv.org/html/2404.07456)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Anonymous Authors Anonymous Affiliation anonymous@example.com    Xu Huang¹   
    Weiwen Liu²    Xiaolong Chen¹    Xingmei Wang¹    Defu Lian¹¹¹1Defu Lian is the
    corresponding author.    Yasheng Wang²    Ruiming Tang²    Enhong Chen¹ ¹University
    of Science and Technology of China, Hefei, China
  prefs: []
  type: TYPE_NORMAL
- en: ²Huawei Noah’s Ark Lab, Shenzhen, China xuhuangcs, chenxiaolong, xingmeiwang@mail.ustc.edu.cn,
    {liandefu, cheneh}@ustc.edu.cn,
  prefs: []
  type: TYPE_NORMAL
- en: '{liuweiwen8,wangyasheng, tangruiming}@huawei.com'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Recently, large language models (LLMs) have demonstrated remarkable potential
    as an intelligent agent. However, existing researches mainly focus on enhancing
    the agent’s reasoning or decision-making abilities through well-designed prompt
    engineering or task-specific fine-tuning, ignoring the procedure of exploration
    and exploitation. When addressing complex tasks within open-world interactive
    environments, these methods exhibit limitations. Firstly, the lack of global information
    of environments leads to greedy decisions, resulting in sub-optimal solutions.
    On the other hand, irrelevant information acquired from the environment not only
    adversely introduces noise, but also incurs additional cost. This paper proposes
    a novel approach, Weak Exploration to Strong Exploitation (WESE), to enhance LLM
    agents in solving open-world interactive tasks. Concretely, WESE involves decoupling
    the exploration and exploitation process, employing a cost-effective weak agent
    to perform exploration tasks for global knowledge. A knowledge graph-based strategy
    is then introduced to store the acquired knowledge and extract task-relevant knowledge,
    enhancing the stronger agent in success rate and efficiency for the exploitation
    task. Our approach is flexible enough to incorporate diverse tasks, and obtains
    significant improvements in both success rates and efficiency across four interactive
    benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models (LLMs) showcase a myriad of capabilities across diverse
    domains, encompassing human-computer conversation, instruction following, reasoning,
    and few-shot learning Zhao et al. ([2023](#bib.bib30)). These comprehensive abilities
    form a robust foundation, positioning LLMs as intelligent agents in solving open-world
    tasks, such as household tasks and open-world question-answering tasks Wang et
    al. ([2023b](#bib.bib18)); Xi et al. ([2023](#bib.bib24)). Recently, there have
    been numerous works to investigate the potential of LLM agents in enhancing their
    capabilities for open-world tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Benefiting from the capabilities of LLMs in instruction-following and few-shot
    learning, most methods guide LLMs in decision-making tasks through human-crafted
    design, avoiding the costly fine-tuning of LLMs Wei et al. ([2022](#bib.bib22));
    Wang et al. ([2022b](#bib.bib16)); Yao et al. ([2022](#bib.bib27)); Kojima et
    al. ([2022](#bib.bib4)). Existing prompt-engineering approaches primarily consider
    two factors: how to incorporate task-relevant information in the prompt, and how
    to elicit the reasoning ability of LLMs through prompts. Task-relevant information
    encompasses task descriptions and contextual feedback, such as the question and
    pertinent task statements in question-answering tasks, along with textual materials
    retrieved by the agent from the web while problem-solving. To enhance the reasoning
    capabilities of LLM agents, methods like CoT Wei et al. ([2022](#bib.bib22)),
    ReAct Yao et al. ([2022](#bib.bib27)) Reflexion Shinn et al. ([2023](#bib.bib9)),
    et al, inspire LLMs to engage in reasoning by constructing few-shot examples with
    explicit reasoning paths.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/13fad9534415f43144291f3b82a4a43a.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) ScienceWorld. Lack of global environmental information causes failure due
    to trapping in a loop or sub-optimal solution.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a034d859fa186ccbb17654a8ab5ae85d.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) HotPotQA. The green sentence is helpful while others are task-irrelevant.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: Examples for sub-optimal decisions and irrelevant information in
    feedbacks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, open-world tasks serve as a simulation of the real environment, wherein
    an agent explores and interacts continuously with the environment to acquire more
    information for solving complex tasks Côté et al. ([2019](#bib.bib2)); Shridhar
    et al. ([2020](#bib.bib10)); Wang et al. ([2022a](#bib.bib15)). There are several
    characteristics of such tasks, making them more challenging. The ability of LLM
    agents is far from optimal due to the following challenges: 1) Complexity. Each
    task involves multi-step actions and each task can have multiple feasible solutions.
    2) Uncertainty. The agent cannot obtain all the information from the initial task
    description, and additional information must be acquired through exploration.
    Regarding these challenges, solving these tasks necessitates multi-step exploration
    and exploitation by the agent. Exploration involves perceiving the environment
    and obtaining task-relevant information, while exploitation involves making action
    decisions based on existing knowledge. In existing prompt-based methods, exploration
    and exploitation issues are often overlooked, embedded within the reasoning process
    of the LLM Yao et al. ([2022](#bib.bib27)), leading to two major problems.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, the lack of global awareness of the environment at the outset solutions
    results in suboptimal decision-making by the LLM. As illustrated in Figure [1(a)](#S1.F1.sf1
    "In Figure 1 ‣ 1 Introduction ‣ WESE: Weak Exploration to Strong Exploitation
    for LLM Agents"), the goal is to find one aluminum object and test its conductivity.
    The agent is located outside initially. The best trajectory is marked with the
    white line, where the agent goes to the kitchen to take the aluminum fork first
    and then go to the workshop. When lack of global environmental information, the
    agent probably gets trapped in some room due to failure in finding an aluminum
    object (the red line) or chooses a more time-consuming way (the blue line). Secondly,
    the knowledge acquired by the LLM from environmental exploration tends to be excessive,
    including irrelevant information to the task. The presence of such information
    not only disrupts LLM decision-making but also incurs additional costs. Referred
    in Figure [1(b)](#S1.F1.sf2 "In Figure 1 ‣ 1 Introduction ‣ WESE: Weak Exploration
    to Strong Exploitation for LLM Agents"), the feedback from the environment usually
    consists of massive task-irrelevant information while only one helpful sentence,
    i.e. the green line in this example, resulting in extra token usage of LLM and
    a negative effect on making optimal decisions.'
  prefs: []
  type: TYPE_NORMAL
- en: To address the above limitations, we propose a novel prompt-based strategy to
    enhance the LLM agent in this work, termed Weak Exploration to Strong Exploitation
    (WESE). To tackle the first limitation, we introduce an idea that decouples the
    exploration and exploitation. Specifically, we construct two distinct LLM agents
    for exploration and exploitation tasks, respectively. In the exploration task,
    the LLM agent’s goal is to interact with the environment, exploring potentially
    helpful environmental information for task resolution. In the exploitation task,
    the information obtained during exploration serves as a global environmental prior,
    aiding the LLM agent in reasoning and decision-making to generate decisions. Regarding
    the second limitation, we compress the environmental information acquired by the
    exploration agent, structuring it in the form of a knowledge graph. During exploitation,
    we adopt a one-hop knowledge retrieval approach, selecting one-hop neighbors of
    task-relevant entities from the graph as priors, thereby reducing interference
    from irrelevant information. Furthermore, to further minimize resource consumption,
    we observe that a cost-effective weaker LLM (such as a 7B model) is fully capable
    of the less challenging exploratory tasks. Therefore, we propose the strategy
    of weak exploration to strong exploitation—leveraging the knowledge explored by
    the weak LLM agent to enhance the performance of the strong LLM agent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our main contributions are summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To the best of our knowledge, this is the first work to investigate the effect
    of decoupling exploration and exploitation for LLM agents in open-world tasks.
    We further propose WESE, leveraging a weaker agent to enhance the stronger agent
    in a cost-effective manner.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To better leverage the environmental information obtained from exploration,
    we introduce a strategy to compress it into a knowledge graph. Then we devise
    a one-hop retrieval approach to filter out the irrelevant information.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experimental results over four open-world interactive benchmarks demonstrate
    the superiority of WESE, notably in achieving a remarkable balance between effectiveness,
    efficiency and cost.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 LLM agents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With the emergence of LLMs, their intelligence has sparked considerable potential
    in applying LLMs as the brains of agents. Existing LLM agent works primarily consider
    three key modules: planning, tool usage, and memory Wang et al. ([2023b](#bib.bib18)).
    Planning module aims to empower agent with the task-decomposition ability, encompassing
    works on task decomposition Wang et al. ([2023c](#bib.bib19)), feedback-driven
    adjustments Shinn et al. ([2023](#bib.bib9)), and multi-path reasoning Yao et
    al. ([2023](#bib.bib28)); Besta et al. ([2023](#bib.bib1)). Tool usage aims to
    strengthen the ability to use external tools Qin et al. ([2023a](#bib.bib7)).
    For instance, Visual ChatGPT Wu et al. ([2023](#bib.bib23)) incorporates visual
    models as tools to augment the LLM’s visual capabilities. ToolLlama Qin et al.
    ([2023b](#bib.bib8)) fine-tunes Llama’s ability to leverage various APIs. The
    memory module focuses on storing feedback information perceived from the environment,
    assisting the agent with experience, and fostering the growth of the agent. In
    Generative Agents Park et al. ([2023](#bib.bib6)), memories of simulated roles
    are stored as texts, utilizing RAG for relevant pieces. REMEMBER Zhang et al.
    ([2023](#bib.bib29)) proposes a semi-parametric memory, i.e. the Q-value table,
    to record rewards as the value and action in a given environment and task as the
    key. MemoryBank Wang et al. ([2023d](#bib.bib20)) leverages the Ebbinghaus forgetting
    curve, incorporating update and forgetting mechanisms into the memory design.'
  prefs: []
  type: TYPE_NORMAL
- en: In our proposed WESE, the knowledge graph is essentially a memory, updating
    information obtained through exploration into the graph.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 LLM for open-world tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Open-world tasks represent the simulation of real-world environments. Within
    these tasks, agents engage in continuous interactions with the environment to
    gather pertinent information, subsequently making decisions and taking action
    to accomplish goals. Open-world tasks typically exhibit fewer constraints on the
    process, placing greater emphasis on the final rewards. Representative examples
    of open-world tasks include games like “Minecraft” Wang et al. ([2023a](#bib.bib17),
    [e](#bib.bib21)), where textual information and visual feedback are involved.
    Another category comprises text-based simulators based on the TextWorld Côté et
    al. ([2019](#bib.bib2)), such as AlfWorld Shridhar et al. ([2020](#bib.bib10)),
    which involves household tasks, ScienceWorld Wang et al. ([2022a](#bib.bib15)),
    which involves simple scientific experiments, and question-answering tasks Yang
    et al. ([2018](#bib.bib25)); Thorne et al. ([2018](#bib.bib12)) where agents need
    to interact with the web to obtain supporting information, such as Wikipedia.
    In tackling such tasks, Chain-of-Thought(CoT) Wei et al. ([2022](#bib.bib22))
    proposes adding few-shot examples in the prompt, guiding the LLM to solve the
    task step by step. ReAct Yao et al. ([2022](#bib.bib27)) induces the reasoning
    capability of LLMs by introducing an extra thought step. Subsequent methods have
    built upon ReAct, with enhancements such as the Reflexion Shinn et al. ([2023](#bib.bib9))
    mechanism, allowing agents to learn from mistakes in subsequent attempts. Additionally,
    several methods leverage the coding capabilities of LLMs, transforming tasks into
    programming tasks and guiding LLMs to generate codes as plans, such as VOYAGER Wang
    et al. ([2023a](#bib.bib17)).
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methodologies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/723282ebe4e685e3ef254ed54725786b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Framework of WESE. The left part represents the weak exploration
    and the right part represents the strong exploitation. We employ Llama-2-7B as
    the weak agent and text-davinci-003 as the strong agent in the implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Decoupling Exploration and Exploitation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Open-world tasks differ from traditional reasoning and decision-making tasks.
    Traditional reasoning Huang and Chang ([2022](#bib.bib3)); Sun et al. ([2023](#bib.bib11))
    or decision-making Yang et al. ([2023](#bib.bib26)) tasks typically present all
    relevant information at once, requiring the agent to deduce and make a plan based
    on the provided information, such as mathematical calculations or logical reasoning
    problems. Conversely, in open-world tasks, only the task description is initially
    specified. In this context, the agent must continually interact with the environment
    to obtain supporting information, comprising the exploration and exploitation
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let $E$ is represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where $reasion(\cdot)$ denotes the mix of explore and exploit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: Knowledge triplets set $K$;'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Graph construction algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: Knowledge graph $G$;9                  10            11'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 Triplet retrieval algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within this paradigm, the knowledge $K$ utilized is solely the limited information
    about the environment obtained through partial observations. Particularly, greedy
    decisions are taken in the initial steps when the agent possesses limited awareness
    of the environment. For instance, in a task such as “cleaning some apples with
    soap” and the agent’s initial location is the hall. The actual locations of the
    apple and soap are in the drawer of the table in the hall and on the sink in the
    kitchen, respectively. The lack of environmental knowledge may lead the agent
    to be misled by the world knowledge of the LLM, going to the kitchen to find the
    apple. Consequently, substantial efforts traversing every corner of the kitchen
    are wasted, resulting in suboptimal plans and even failures due to trapping in
    the loop. Therefore, we investigate the strategy to decouple exploration and exploitation,
    formalized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small a_{i}=\left\{\begin{aligned} &amp;explore(E,T,s_{i-1};\Theta,P_{e})\in\mathcal{A}_{e},\;i<N_{e};\\
    &amp;exploit(E,T,s_{i-1};\Theta,P_{t},K=\cup_{j\leq N}\{F_{j}\})\in\mathcal{A}_{t},i\geq
    N_{e}.\end{aligned}\right.$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $P_{e}$ is the maximum number of steps of exploration, which could also
    be determined by the agent, such as terminating the exploration automatically
    when it thinks the obtained information is sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: Different from the previous methods, our method places the whole exploration
    phase before exploitation explicitly, as opposed to the alternation of exploration
    and exploitation. In this manner, the agent has extensively explored the environment,
    acquiring global environmental prior knowledge denoted as $K=\cup_{j\leq N}\{F_{j}\}$.
    Exploitation with global knowledge benefits the effectiveness and efficiency of
    the solutions, which is empirically validated in our experiments.
  prefs: []
  type: TYPE_NORMAL
- en: However, two subsequent issues exist following the decoupling approach. Firstly,
    the information obtained from environmental feedback is huge due to the extensive
    exploration, including a lot of task-irrelevant information. Secondly, the extensive
    exploration contributes to increased resource consumption, such as token usage.
    Therefore, we demand an efficient mechanism for information transfer between exploration
    and exploitation and a cost-efficient exploration-exploitation strategy. We address
    the two issues in the subsequent parts of this section.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Knowledge Compression and Retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Real-world textual information exhibits inherent sparsity, characterized by
    long sentences consisting of plenty of non-informative conjunctions and adjectives.
    Environmental feedback in open-world tasks manifests as such text, where the cumulative
    extensive exploration yield long and unstructured textual information, demonstrating
    serve sparsity. Considering the limited context window of the LLM and the expensive
    cost of token usage, it is necessary to compress the sparse information. Leveraging
    a knowledge graph (KG) to store information has proved advantageous in enhancing
    information density and leveraging domain-specific knowledge in existing works Pan
    et al. ([2024](#bib.bib5)).
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, benefiting from the superiority of LLM in relation-extraction
    tasks Wadhwa et al. ([2023](#bib.bib14)), we extract the knowledge from the received
    feedback to form an environmental knowledge graph. Specifically, the LLM extracts
    knowledge triplets from the environmental feedback after each exploration step,
    updating them into the knowledge graph. For example, as for the search result
    given by Wikipedia “Since 2005 Wendy Schaal has primarily worked in voice acting,
    most notably voicing Francine Smith in the animated comedy television series American
    Dad!”, knowledge triplets are extracted as $\langle$. Notably, the environmental
    knowledge graph we obtained is task-relevant, serving as a memory like the Random
    Access Memory(RAM). Actually, a worldwide knowledge graph could be leveraged and
    continually in our method, serving as a general memory. We leave it for further
    work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: Environment $E$;16'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 3 WESE algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, it is imperative to acknowledge that not all information in the
    knowledge graph proves useful. The introduction of task-irrelevant information
    has the potential to lead the hallucination phenomena of LLM, such as the confusion
    of entity and relation. For example, giving the triplet $\langle$ and the question
    is “What’s the favorite fruit of Bill?”, the LLM would confuse the relation and
    answer with apple. Benefiting from the graph structure, we adopt a one-hop retrieval
    method to extract task-related information easily, illustrated in Algorithm [2](#alg2
    "In 3.1 Decoupling Exploration and Exploitation ‣ 3 Methodologies ‣ WESE: Weak
    Exploration to Strong Exploitation for LLM Agents"). Concretely, we initiate the
    process by extracting involved entities from the task description with LLM. Subsequently,
    we perform a one-hop retrieval on the graph to obtain the neighbors of these entities.
    The retrieved knowledge triplets are then injected into the prompt, serving as
    task-relevant knowledge during the exploitation phase, thereby assisting the LLM
    in task-solving.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Weak Exploration to Strong Exploitation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 1: Results on ALFWorld(134 tasks). SR and AS are abbreviations for success
    rate and average steps of successful tasks, respectively. SESE represents the
    variant of WESE—Strong Exploration to Strong Exploitation. The Imp represents
    the relative improvements compared to base methods, i.e. Act and ReAct. The bold
    and underline represent the best and the second best for the same base method.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Performance | Effectiveness | Efficiency | Cost |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Method | SR$\uparrow$ | Imp(%) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Act | 0.43 | 0.00 | 10.83 | 0.00 | 4,908,548 | 21,243 | 98.60 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| Act-WESE | 0.63 | +46.51 | 7.54 | +30.38 | 3,746,290 | 19,562 | 75.32 | +23.61
    |'
  prefs: []
  type: TYPE_TB
- en: '| Act-SESE | 0.67 | +55.81 | 6.73 | +37.86 | 7,259,508 | 75,153 | 146.69 |
    -48.77 |'
  prefs: []
  type: TYPE_TB
- en: '| ReAct | 0.57 | 0.00 | 16.64 | 0.00 | 7,565,676 | 43,250 | 152.18 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| ReAct-WESE | 0.72 | +26.32 | 13.69 | +17.73 | 5,032,374 | 41,004 | 101.47
    | +33.32 |'
  prefs: []
  type: TYPE_TB
- en: '| ReAct-SESE | 0.75 | +31.58 | 12.41 | +25.42 | 8,996,182 | 97,286 | 181.87
    | -19.51 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Results on ScienceWorld(296 tasks). TR, AR and AS are abbreviations
    for total reward, average reward and average steps to get positive reward, respectively.
    Other symbols are consistent with Table [1](#S3.T1 "Table 1 ‣ 3.3 Weak Exploration
    to Strong Exploitation ‣ 3 Methodologies ‣ WESE: Weak Exploration to Strong Exploitation
    for LLM Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Performance | Effectiveness | Efficiency | Cost |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Method | TR$\uparrow$ | Imp(%) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Act | 4908 | 16.58 | 0.00 | 18.00 | 0.00 | 13,554,960 | 55,817 | 272.22 |
    0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| Act-WESE | 5198 | 17.56 | 5.91 | 15.68 | +12.91 | 13,491,043 | 65,952 | 271.14
    | +0.40 |'
  prefs: []
  type: TYPE_TB
- en: '| Act-SESE | 5249 | 17.73 | 6.94 | 15.39 | +14.49 | 36,424,190 | 165,568 |
    731.80 | -168.83 |'
  prefs: []
  type: TYPE_TB
- en: '| ReAct | 4454 | 15.05 | 0.00 | 20.00 | 0.00 | 17,716,698 | 84,724 | 356.03
    | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| ReAct-WESE | 5317 | 17.96 | 19.34 | 19.65 | +1.77 | 16,310,632 | 80,851 |
    327.83 | +7.92 |'
  prefs: []
  type: TYPE_TB
- en: '| ReAct-WESE | 5053 | 17.07 | 13.42 | 19.02 | +4.92 | 40,293,571 | 196,338
    | 809.80 | -127.45 |'
  prefs: []
  type: TYPE_TB
- en: Acquiring more comprehensive global information about the environment demands
    a considerable resource cost in the exploration process. However, compared to
    exploitation, exploration exhibits lower complexity, requiring less reasoning
    and induction. Concretely, exploration operations exhibit low requirements for
    the logic and coherence of actions, emphasizing actions pertaining to environmental
    observation. For example, the exploration actions mainly consist of several simple
    actions on decision-making benchmarks, such as “go to [room]”, “look around”,
    et al, while exploitation involves a series of coherent operations like (go to
    sink/stove, put the bowl in/on the sink/stove, activate the sink/stove, wait,
    deactivate the sink/stove). Therefore, we propose to use a weaker agent for the
    exploration to mitigate resource consumption, namely the weak exploration. From
    the perspective of the LLM agent, a weaker agent represents substituting the underlying
    LLM for exploration with a weaker LLM, i.e. an LLM with fewer parameters, thereby
    reducing costs. In our experiments, we compare performance between strong exploration
    and weak exploration. Our findings reveal that a weaker exploration has a negligible
    impact on the final success rate, yet it significantly lowers costs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The framework of WESE is illustrated in Figure [2](#S3.F2 "Figure 2 ‣ 3 Methodologies
    ‣ WESE: Weak Exploration to Strong Exploitation for LLM Agents"). There are three
    key components in the framework: a weak LLM agent, a strong LLM agent, and a KG-based
    memory. The whole process consists of the weak exploration (left) and the strong
    exploitation (right). Meanwhile, we offer an algorithmic pseudo-code in Algorithm [3](#alg3
    "In 3.2 Knowledge Compression and Retrieval ‣ 3 Methodologies ‣ WESE: Weak Exploration
    to Strong Exploitation for LLM Agents"). First, a weak LLM agent is employed to
    explore the interactive environment to obtain information in line 1 to 7\. Then
    those knowledge triplets are organized as a knowledge graph $G_{K}$ in line 8,
    as illustrated in Algorithm [1](#alg1 "In 3.1 Decoupling Exploration and Exploitation
    ‣ 3 Methodologies ‣ WESE: Weak Exploration to Strong Exploitation for LLM Agents").
    Further, the involved entities are extracted from the task with a LLM and the
    relevant triplets are retrieved from the graph in line 10\. Retrieved knowledge
    is leveraged for exploitation in line 12, serving as the prior knowledge.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We employ two categories of interactive open-world tasks as benchmarks: decision-making
    and question-answering, where each task requires multi-step interactions with
    the environment. We evaluate our methods from three perspectives: effectiveness,
    efficiency and cost, representing whether the agent can complete the tasks, how
    many steps the agent would take to finish the task, and the expenses for the agent
    to complete the task, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Decision Making Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We begin with the open-world decision-making tasks, where environments are based
    on a text-based simulator. The tasks are about the household, where the agent
    needs to explore various rooms and take operations on several objects.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 ALFWorld
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: ALFWorld Shridhar et al. ([2020](#bib.bib10)) is a synthetic text-based simulated
    interactive environment. It comprises six types of tasks where agents need to
    interact with the environment to generate a series of actions to solve household
    tasks. For example, in the task “clean some knife and put it in countertop”, the
    ideal solution involves actions such as (go to countertop 2, take knife 1, go
    to sinkbasin 2, clean knife 1, put knife 1 on countertop 2). These tasks vary
    in difficulty, with challenging tasks encompassing over 50 locations and requiring
    more than 50-step actions, posing challenges for both the exploration and exploitation
    processes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/58629698405dfa0ee223aadfb9e74791.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Relative improvements for Act-based methods.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bee38cfe075e8ca0a71467d49d7217ab.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Relative improvements for ReAct-based methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Relative improvements in success rate over various types of tasks
    on ALFWorld. The left tasks are more complicated.'
  prefs: []
  type: TYPE_NORMAL
- en: To validate the effectiveness of WESE, we adopt Act Yao et al. ([2022](#bib.bib27))
    and ReAct Yao et al. ([2022](#bib.bib27)) as baselines. Act leverages the idea
    of CoT, providing LLMs with few-shot interactive examples. ReAct, building upon
    Act, introduces an extra “thought” step where LLMs can choose to explicitly output
    their thought about the current state or generate action. In WESE, we initially
    use a Weak LLM for exploration to acquire task-relevant knowledge. We then leverage
    the obtained knowledge to solve problems with two base methods. We employ Llama-2-7B Touvron
    et al. ([2023](#bib.bib13)) as the weak LLM and text-davinci-003 (with probably
    more than 175 billion parameters) developed by OpenAI ²²2[https://platform.openai.com/](https://platform.openai.com/)
    as the strong LLM. The limits of steps $N_{e},N_{t}$ are both set to 50\. Our
    evaluation focuses on success rates, average steps to complete tasks, and the
    cost of OpenAI API tokens as three key metrics. Additionally, we introduce a variant
    of WESE—Strong Exploration to Strong Exploitation (SESE), where the weak LLM in
    the exploration process is replaced with the strong LLM, to verify the effectiveness
    of the decoupling strategy and examine the impact of LLM strength on exploration
    quality.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 ScienceWorld
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Similar to ALFWorld, ScienceWorld Wang et al. ([2022a](#bib.bib15)) is an interactive
    household environment as well. However, the tasks in ScienceWorld are more challenging,
    involving scientific experiments such as boiling and creating a new color by mixing
    primary colors. The environment is more complex, comprising ten distinct rooms,
    each with different furnishings, and not each pair of rooms is connected.
  prefs: []
  type: TYPE_NORMAL
- en: We conduct experiments on eight types of tasks within ScienceWorld, choosing
    about 30 instances for each task due to a limited budget. Unlike ALFWorld where
    the agent can get a reward of 1 only when the task is completed, the agent in
    ScienceWorld receives partial rewards upon completing crucial steps, with the
    total reward reaching 100\. Given the challenging nature of the tasks, achieving
    a full reward of 100 is rare. Therefore, we utilize the number of steps taken
    by the agent until it first obtains a positive reward as the metric for efficiency.
    Other settings are consistent with ALFWorld.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3 Results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The results on ALFWorld and ScienceWorld are shown in Table [1](#S3.T1 "Table
    1 ‣ 3.3 Weak Exploration to Strong Exploitation ‣ 3 Methodologies ‣ WESE: Weak
    Exploration to Strong Exploitation for LLM Agents") and Table [2](#S3.T2 "Table
    2 ‣ 3.3 Weak Exploration to Strong Exploitation ‣ 3 Methodologies ‣ WESE: Weak
    Exploration to Strong Exploitation for LLM Agents"), respectively. We conclude
    several findings based on the results. Consistent with results reported in ReAct,
    ReAct outperforms Act on two benchmarks, showing the superiority of the “thought”
    step. However, this additional step leads to a longer action sequence, resulting
    in an average relative 32.38% increase in average steps. Decoupling of exploration
    and exploitation demonstrates advantages in effectiveness and efficiency, resulting
    in SESE outperforming baselines significantly with average relative 26.94% and
    20.67% improvements in terms of success rate (average reward) and average steps.
    However, the cost of SESE increases a lot due to the introduction of extensive
    strong exploration, showing an average relative 91.14% increase over baselines.'
  prefs: []
  type: TYPE_NORMAL
- en: WESE shows a better balance between effectiveness, efficiency, and cost, which
    saves 53.83% of costs with only relative 1.43% and 6.89% degradations in effectiveness
    and efficiency compared with SESE. In WESE, the weak LLM agent undertakes the
    exploration process, resulting in cost savings for extensive exploration. Besides,
    benefiting from the related triplets extracted from the explored KG, the strong
    LLM agent only needs to focus on exploitation, further decreasing the number of
    steps, evidenced by the decreased completion tokens and average steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'We further investigate the improvements of WESE on various types of tasks,
    shown in Figure [3](#S4.F3 "Figure 3 ‣ 4.1.1 ALFWorld ‣ 4.1 Decision Making Tasks
    ‣ 4 Experiments ‣ WESE: Weak Exploration to Strong Exploitation for LLM Agents").
    Both WESE and SESE show improvements over almost all types of tasks, further indicating
    the effectiveness of the decoupling strategy. In addition, the improvements in
    “clean” and “heat” tasks are greater than other tasks. The reason lies in that
    the two tasks involved more complicated exploitation compared with “put”, where
    the agents need to find the object first and then clean or heat it instead of
    just moving it to another place. The result demonstrates extensive exploration
    benefits more for complex tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Question Answering Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We also validate our WESE on two open-world interactive question-answering benchmarks,
    i.e., HotPotQA and FEVER. Different from traditional question-answering tasks
    where supporting sentences are given, those tasks provide the question only and
    require the agent to search information on the web step by step to give the final
    answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Results on HotPotQA(500 tasks). SR and AS are abbreviations for success
    rate and average steps of successful tasks, respectively. SESE represents the
    variant of WESE—Strong Exploration to Strong Exploitation. The Imp represents
    the relative improvements compared to base methods, i.e. Act and ReAct. The bold
    and underline represent the best and the second best for the same base method.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Performance | Effectiveness | Efficiency | Cost |'
  prefs: []
  type: TYPE_TB
- en: '| Method | SR$\uparrow$ | Imp(%) |'
  prefs: []
  type: TYPE_TB
- en: '| CoT | 0.318 | N/A | 1.00 | N/A | 261,347 | 25,382 | 5.73 | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Act | 0.296 | 0.00 | 3.53 | 0.00 | 2,390,041 | 14,236 | 48.09 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| Act-WESE | 0.353 | +19.26 | 2.69 | +23.80 | 2,307,421 | 13,973 | 46.42 |
    +3.45 |'
  prefs: []
  type: TYPE_TB
- en: '| Act-SESE | 0.361 | +21.96 | 2.58 | +26.91 | 7,522,826 | 27,1551 | 155.89
    | -224.18 |'
  prefs: []
  type: TYPE_TB
- en: '| ReAct | 0.342 | 0.00 | 3.17 | 0.00 | 3,234,876 | 65,306 | 66.00 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| ReAct-WESE | 0.394 | +15.20 | 2.29 | +27.76 | 2,574,401 | 67,908 | 52.85
    | +19.93 |'
  prefs: []
  type: TYPE_TB
- en: '| ReAct-SESE | 0.416 | +21.64 | 2.11 | +33.44 | 7,338,590 | 323,401 | 153.24
    | -132.17 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Results on FEVER(500 tasks). The meanings of abbreviations and symbols
    are consistent with Table [3](#S4.T3 "Table 3 ‣ 4.2 Question Answering Tasks ‣
    4 Experiments ‣ WESE: Weak Exploration to Strong Exploitation for LLM Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Performance | Effectiveness | Efficiency | Cost |'
  prefs: []
  type: TYPE_TB
- en: '| Method | SR$\uparrow$ | Imp(%) |'
  prefs: []
  type: TYPE_TB
- en: '| CoT | 0.61 | N/A | 1.00 | N/A | 100,387 | 11,942 | 2.25 | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Act | 0.56 | 0.00 | 2.16 | 0.00 | 723,646 | 6,980 | 14.61 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| Act-WESE | 0.62 | +10.71 | 1.58 | +26.66 | 723,867 | 5,937 | 14.60 | +0.11
    |'
  prefs: []
  type: TYPE_TB
- en: '| Act-SESE | 0.64 | +14.29 | 1.57 | +27.34 | 2,822,189 | 122,543 | 60.89 |
    -316.73 |'
  prefs: []
  type: TYPE_TB
- en: '| ReAct | 0.63 | 0.00 | 2.18 | 0.00 | 1,074,080 | 36,040 | 22.20 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| ReAct-WESE | 0.68 | +7.26 | 1.62 | +25.96 | 918,905 | 29,895 | 18.98 | +14.53
    |'
  prefs: []
  type: TYPE_TB
- en: '| ReAct-SESE | 0.70 | +10.09 | 1.59 | +27.18 | 3,104,924 | 162,363 | 65.35
    | -194.32 |'
  prefs: []
  type: TYPE_TB
- en: 4.2.1 HotPotQA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'HotPotQAYang et al. ([2018](#bib.bib25)) is a question-answering dataset where
    each question is paired with supporting sentences from Wikipedia articles. In
    traditional QA tasks, the supporting sentences are given and the remained task
    is to reason. Referred in ReAct, we use the Wikipedia API with three types of
    actions to support interactive information retrieval: (1) search[entity], which
    searches the Wikipedia with the entity and returns the corresponding page if it
    exists, or suggests top-5 similar entities; (2) lookup[keyword], which looks up
    keyword in the page and returns the next sentence containing the keyword, simulating
    the Ctrl+F function in a web browser; (3) finish[answer], which answers the question
    with answer. Once the answer matches the ground truth, the environment would return
    reward 1\. We sample 500 tasks from the development set.'
  prefs: []
  type: TYPE_NORMAL
- en: We employ the CoT Wei et al. ([2022](#bib.bib22)), Act and ReAct as baselines
    and empower Act and ReAct with WESE and SESE. Note that CoT is a one-step method
    that does not support interactive tasks, we inject the supporting sentences into
    the prompts and instruct the LLM to reason for the final answer without searching
    on the web. Also, WESE is not designed for such a purely reasoning method but
    for methods involving interactions with the environment. For Act and ReAct, we
    keep the settings consistent with the original paper. As there are probably lots
    of related triplets to the task-involved entities, we set the limit of retrieved
    triplets as 10 and the limits of steps $N_{e},N_{t}$ as 8\. The evaluation for
    effectiveness, efficiency and cost is consistent with the ALFWorld.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 FEVER
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: FEVER Thorne et al. ([2018](#bib.bib12)) is a fact verification dataset, consisting
    of instances where each instance comprises a claim and a justification(True or
    False or Not Clear). We employ the Wikipedia API to construct an interactive environment
    consistent with that in HotPotQA. Other settings are kept consistent with HotPotQA,
    such as the number of retrieved triplets and the maximum steps.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 Results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The results on HotPotQA and FEVER are shown in Table [3](#S4.T3 "Table 3 ‣
    4.2 Question Answering Tasks ‣ 4 Experiments ‣ WESE: Weak Exploration to Strong
    Exploitation for LLM Agents") and Table [4](#S4.T4 "Table 4 ‣ 4.2 Question Answering
    Tasks ‣ 4 Experiments ‣ WESE: Weak Exploration to Strong Exploitation for LLM
    Agents"), respectively. We can conclude several findings based on the results.
    Similar to decision-making tasks, ReAct outperforms Act significantly due to the
    additional “thought” step. Also, methods equipped with WESE or SESE outperform
    baselines in both success rate and the number of taken actions, resulting in average
    relative improvements of 19.5% and 28.0%, respectively. Especially, SESE methods
    surpass WESE slightly with average relative 3.5% and 3.6% improvements in terms
    of success rate and average steps, while increasing more than twice the expenses.
    This further demonstrates that the weak agent powered by Llama-2-7B is almost
    sufficient for the exploration task.'
  prefs: []
  type: TYPE_NORMAL
- en: Different from decision-making tasks, question-answering tasks require fewer
    steps due to more information being returned with one search action. However,
    our WESE and SESE are still capable of reducing the number of steps, further showing
    the advantage of the explored knowledge. As for the cost, the tokens increased
    in SESE are far more than those in decision-making tasks, which can be attributed
    to the long-textual feedback from Wikipedia.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we introduce WESE, a cost-effective method that enhances LLM
    agents in open-world interactive tasks. We decouple the exploration and exploitation,
    employing two agents for the distinct processes. To empower the communication
    between the two processes, we introduce a knowledge graph-based memory to compress
    and structure the information obtained in exploration, where task-relevant information
    is extracted from the graph by a one-hop retrieval method. We then propose to
    leverage a weaker agent for the exploration process, forming a cost-effective
    manner with negligible performance degradation. Experimental results demonstrate
    the superiority of WESE in effectiveness, efficiency, and cost.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Besta et al. [2023] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger,
    Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski,
    Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large
    language models. arXiv preprint arXiv:2308.09687, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Côté et al. [2019] Marc-Alexandre Côté, Akos Kádár, Xingdi Yuan, Ben Kybartas,
    Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud
    Adada, et al. Textworld: A learning environment for text-based games. In Computer
    Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International
    Conference on Artificial Intelligence, IJCAI 2018, Stockholm, Sweden, July 13,
    2018, Revised Selected Papers 7, pages 41–75\. Springer, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang and Chang [2022] Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning
    in large language models: A survey. arXiv preprint arXiv:2212.10403, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kojima et al. [2022] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances
    in neural information processing systems, 35:22199–22213, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pan et al. [2024] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang,
    and Xindong Wu. Unifying large language models and knowledge graphs: A roadmap.
    IEEE Transactions on Knowledge and Data Engineering, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. [2023] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra
    of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface
    Software and Technology, pages 1–22, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qin et al. [2023a] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding,
    Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng
    Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen,
    Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning
    Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han,
    Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu,
    and Maosong Sun. Tool learning with foundation models, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qin et al. [2023b] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,
    Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating
    large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shinn et al. [2023] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion:
    an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shridhar et al. [2020] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan
    Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied
    environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. [2023] Jiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, Ruihang
    Chu, Jianing Qiu, Jiaqi Xu, Mingyu Ding, Hongyang Li, Mengzhe Geng, et al. A survey
    of reasoning with foundation models. arXiv preprint arXiv:2312.11562, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thorne et al. [2018] James Thorne, Andreas Vlachos, Christos Christodoulopoulos,
    and Arpit Mittal. Fever: a large-scale dataset for fact extraction and verification.
    arXiv preprint arXiv:1803.05355, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:
    Open foundation and fine-tuned chat models, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wadhwa et al. [2023] Somin Wadhwa, Silvio Amir, and Byron C Wallace. Revisiting
    relation extraction in the era of large language models. arXiv preprint arXiv:2305.05003,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2022a] Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj
    Ammanabrolu. Scienceworld: Is your agent smarter than a 5th grader? arXiv preprint
    arXiv:2203.07540, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2022b] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves
    chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2023a] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied
    agent with large language models. arXiv preprint arXiv:2305.16291, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2023b] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large
    language model based autonomous agents. arXiv preprint arXiv:2308.11432, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2023c] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan,
    Roy Ka-Wei Lee, and Ee-Peng Lim. Plan-and-solve prompting: Improving zero-shot
    chain-of-thought reasoning by large language models. arXiv preprint arXiv:2305.04091,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2023d] Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan,
    Jianfeng Gao, and Furu Wei. Augmenting language models with long-term memory.
    arXiv preprint arXiv:2306.07174, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2023e] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian
    Ma, and Yitao Liang. Describe, explain, plan and select: interactive planning
    with llms enables open-world multi-task agents. In Thirty-seventh Conference on
    Neural Information Processing Systems, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits
    reasoning in large language models. Advances in Neural Information Processing
    Systems, 35:24824–24837, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2023] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng
    Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation
    models. arXiv preprint arXiv:2303.04671, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xi et al. [2023] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang
    Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential
    of large language model based agents: A survey. arXiv preprint arXiv:2309.07864,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. [2018] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W
    Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for
    diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. [2023] Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel,
    and Dale Schuurmans. Foundation models for decision making: Problems, methods,
    and opportunities. arXiv preprint arXiv:2303.04129, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. [2022] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language
    models. arXiv preprint arXiv:2210.03629, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. [2023] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L
    Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem
    solving with large language models. arXiv preprint arXiv:2305.10601, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2023] Danyang Zhang, Lu Chen, Situo Zhang, Hongshen Xu, Zihan
    Zhao, and Kai Yu. Large language model is semi-parametric reinforcement learning
    agent. arXiv preprint arXiv:2306.07929, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. [2023] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
    Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al.
    A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
