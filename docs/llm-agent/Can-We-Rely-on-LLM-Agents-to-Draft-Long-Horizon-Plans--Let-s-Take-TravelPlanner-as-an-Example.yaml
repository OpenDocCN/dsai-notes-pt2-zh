- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:39:02'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner
    as an Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.06318](https://ar5iv.labs.arxiv.org/html/2408.06318)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Yanan Chen, Ali Pesaranghader, Tanmana Sadhu    Dong Hoon Yi
  prefs: []
  type: TYPE_NORMAL
- en: LG Electronics, Toronto AI Lab, Toronto, Canada
  prefs: []
  type: TYPE_NORMAL
- en: '{yanan.chen, ali.pesaranghader, tanmana.sadh, donghoon9.yi}@lge.com'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Large language models (LLMs) have brought autonomous agents closer to artificial
    general intelligence (AGI) due to their promising generalization and emergent
    capabilities. There is, however, a lack of studies on how LLM-based agents behave,
    why they could potentially fail, and how to improve them, particularly in demanding
    real-world planning tasks. In this paper, as an effort to fill the gap, we present
    our study using a realistic benchmark, TravelPlanner Xie et al. ([2024](#bib.bib45)),
    where an agent must meet multiple constraints to generate accurate plans. We leverage
    this benchmark to address four key research questions: (1) are LLM agents robust
    enough to lengthy and noisy contexts when it comes to reasoning and planning?
    (2) can few-shot prompting adversely impact the performance of LLM agents in scenarios
    with long context? (3) can we rely on refinement to improve plans, and (4) can
    fine-tuning LLMs with both positive and negative feedback lead to further improvement?
    Our comprehensive experiments indicate that, firstly, LLMs often fail to attend
    to crucial parts of a long context, despite their ability to handle extensive
    reference information and few-shot examples; secondly, they still struggle with
    analyzing the long plans and cannot provide accurate feedback for refinement;
    thirdly, we propose Feedback-Aware Fine-Tuning (FAFT), which leverages both positive
    and negative feedback, resulting in substantial gains over Supervised Fine-Tuning
    (SFT). Our findings offer in-depth insights to the community on various aspects
    related to real-world planning applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner
    as an Example
  prefs: []
  type: TYPE_NORMAL
- en: Yanan Chen, Ali Pesaranghader, Tanmana Sadhu,  and Dong Hoon Yi LG Electronics,
    Toronto AI Lab, Toronto, Canada {yanan.chen, ali.pesaranghader, tanmana.sadh,
    donghoon9.yi}@lge.com
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LLMs have shown significant reasoning and planning results against various
    benchmarks such as WebArena Zhou et al. ([2023](#bib.bib58)), WebShop Yao et al.
    ([2022a](#bib.bib48)), AgentBench Liu et al. ([2023b](#bib.bib17)) and AgentGym
    Xi et al. ([2024b](#bib.bib44)) where they act as agents to finish a given task
    on behalf of humans. In this vein, the community considers two main directions
    for developing LLM-based agents: (1) prompting LLMs for reasoning, planning, and
    execution Qin et al. ([2023](#bib.bib25)); Wei et al. ([2022](#bib.bib38)); Yao
    et al. ([2024](#bib.bib49)); Wang et al. ([2022](#bib.bib37)), and (2) fine-tuning
    LLMs for a given task Chen et al. ([2023b](#bib.bib3)); Zeng et al. ([2023](#bib.bib51));
    Zhang et al. ([2024b](#bib.bib53)); Chen et al. ([2024](#bib.bib4)); Song et al.
    ([2024b](#bib.bib33)). Despite promising contributions in each direction, it is
    seen that LLMs still fall short in more complex scenarios. TravelPlanner Xie et al.
    ([2024](#bib.bib45)), as an example, is a benchmark where an agent should generate
    a plan which must meet multiple constraints with respect to input queries. The
    authors showed that GPT-4-Turbo OpenAI ([2023](#bib.bib21)) could only reach to
    Final Pass Rate of 4.4%. This indicates that LLM agents cannot handle long-horizon
    reasoning and planning. In this paper, we investigate these challenges further
    with four research questions using TravelPlanner as the benchmark, and we trust
    that our promising and negative findings will benefit the community.'
  prefs: []
  type: TYPE_NORMAL
- en: Our extensive experiments indicate that (1) lengthy and noisy context can adversely
    impact planning ability of the LLM agent, (2) more shots do not necessarily guarantee
    performance improvement, (3) refinement may not be effective when LLMs are employed
    as feedback generators; however, it is more likely to work if the feedback generator
    is based on heuristic rules, and (4) feedback-aware fine-tuning (FAFT), our proposed
    approach, inspired by negative aware training (NAT) Wang et al. ([2024b](#bib.bib36)),
    can show remarkable improvement in planning.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b2774a67abf2928bd40a770d675d54d2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Four LLM agents interact to generate a plan. (Fig. [A.1](#A1.F1 "Figure
    A.1 ‣ A.2 Evaluation metrics ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents
    to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example") is an example
    for the refinement module.)'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our framework, built upon TravelPlanner, consists of five main components:
    Scrubber, Planner, Feedback Generator, Refiner, and the Evaluation module (as
    shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Can We Rely on LLM Agents
    to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example")). The scrubber
    provides clean reference information¹¹1This is a terminology that TravelPlanner
    uses to refer to necessary information for generating a plan. and few-shot examples
    to the Planner for generating a plan. Then, the Feedback Generator provides feedback
    to the Refiner for improving the plan if required. The interaction continues until
    the pre-defined settings are met. The Planner is the core of the framework which
    can be based on either (1) in-context learning (ICL), or (2) supervised fine-tuning
    (SFT), e.g., FAFT as proposed in Section [4](#S4 "4 Findings ‣ Can We Rely on
    LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example")-RQ4\.
    Appx. [A.3](#A1.SS3 "A.3 Framework ‣ Appendix A Appendix ‣ Can We Rely on LLM
    Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example") describes
    each agent in detail.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Experimental Settings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Basic Setting. Since the focus of our work is on agents’ capabilities in drafting
    plans, we only rely on the Sole Planning setting from TravelPlanner. That is,
    all comprehensive and necessary information, which are human annotations, is directly
    provided to the planner agent. We also consider the Direct²²2the query is input
    directly into the model along with instructions detailing the task and relevant
    information gathered. planning strategy for its simplicity because it performs
    at a similar level to other reasoning techniques such as ZS-CoT Wei et al. ([2022](#bib.bib38)),
    ReAct Yao et al. ([2022b](#bib.bib50)) and Reflexion Shinn et al. ([2024](#bib.bib30)).
  prefs: []
  type: TYPE_NORMAL
- en: Dataset. (See Appx. [A.1](#A1.SS1 "A.1 Dataset ‣ Appendix A Appendix ‣ Can We
    Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an
    Example")) We use the training set for both few-shot prompting and fine-tuning
    because it provides annotated plans. We evaluate the agent against both validation
    and test sets for RQ1 and RQ2 in Section [4](#S4 "4 Findings ‣ Can We Rely on
    LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example").
    As for RQ3, we consider only the validation set because we do not have access
    to the system feedback offline. Regarding RQ4, we use the training set for fine-tuning
    the (Open-LLM) planner agent.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics. We utilize the original evaluation metrics from TravelPlanner, which
    evaluate performance based on the pass rates of multiple constraints. Additional
    details are available in Appx. [A.2](#A1.SS2 "A.2 Evaluation metrics ‣ Appendix
    A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take
    TravelPlanner as an Example").
  prefs: []
  type: TYPE_NORMAL
- en: '| GPT-3.5-Turbo as Planner |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Validation Set (#180) | Test Set (#1,000) |'
  prefs: []
  type: TYPE_TB
- en: '| Reference Scrubbed? | Num. Shots | Delivery Rate |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Commonsense &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pass Rate &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Hard Constraint &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pass Rate &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Final Pass Rate | Halluc. Rate | Delivery Rate |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Commonsense &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pass Rate &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Hard Constraint &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pass Rate &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Final Pass Rate | Halluc. Rate |'
  prefs: []
  type: TYPE_TB
- en: '| (RQ1) | (RQ2) | Micro | Macro | Micro | Macro | Micro | Macro | Micro | Macro
    |'
  prefs: []
  type: TYPE_TB
- en: '| No | 0 | 100 | 60.2 | 4.4 | 11.0 | 2.8 | 0.0 | 57.4 | 100 | 60.8 | 3.5 |
    13.6 | 4.9 | 0.6 | 61.1 |'
  prefs: []
  type: TYPE_TB
- en: '| No | 1 | 100 | 65.4 | 11.0 | 17.5 | 5.1 | 1.0 | 52.3 | 100 | 64.0 | 10.1
    | 16.1 | 6.4 | 1.2 | 59.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Yes | 0 | 100 | 74.4 | 18.9 | 29.0 | 14.4 | 4.4 | 41.6 | 100 | 70.3 | 12.3
    | 25.0 | 10.7 | 2.7 | 49.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Yes | 1 | 100 | 80.6 | 24.4 | 40.2 | 17.8 | 7.2 | 35.5 | 100 | 78.0 | 18.6
    | 36.1 | 17.7 | 4.9 | 40.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Yes | 2 | 100 | 82.6 | 32.2 | 41.2 | 17.8 | 7.2 | 38.8 | 100 | 80.9 | 22.4
    | 34.3 | 16.7 | 6.5 | 44.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Yes | 4 | 100 | 81.5 | 29.4 | 35.5 | 12.2 | 5.8 | 46.6 | 100 | 80.3 | 21.1
    | 31.5 | 15.0 | 5.2 | 50.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Yes | 5 | 100 | 81.1 | 26.3 | 32.6 | 12.4 | 4.8 | 49.8 | 100 | 79.5 | 20.4
    | 30.4 | 13.2 | 5.6 | 53.1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Performance of GPT-3.5-Turbo as the Planner agent for different settings
    for RQ1 and RQ2'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | GPT-3.5-Turbo as Planner and GPT-4-Turbo as Refiner |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | vs. Validation Set (#180) |'
  prefs: []
  type: TYPE_TB
- en: '| Feedback Generator (RQ3) | Refinement Iteration | Delivery Rate |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Commonsense &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pass Rate &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Hard Constraint &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pass Rate &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Final Pass Rate | Uplift Ratio ($\uparrow$) |'
  prefs: []
  type: TYPE_TB
- en: '| Micro | Macro | Micro | Macro |'
  prefs: []
  type: TYPE_TB
- en: '| None | 0 | 100 | 82.6 | 32.2 | 41.2 | 17.8 | 7.2 | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| Oracle (Heuristic Rules) | 1 | 100 | 89.7 | 51.1 | 50.0 | 22.2 | 11.7 | 46.1
    | 52.8 | 1.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 100 | 89.0 | 54.4 | 50.5 | 18.3 | 12.8 | 8.9 | 76.7 | 14.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 100 | 89.9 | 56.1 | 50.7 | 21.1 | 13.3 | 13.9 | 78.9 | 7.2 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 100 | 89.1 | 59.4 | 49.8 | 21.7 | 13.9 | 5.0 | 86.7 | 8.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Random | 1 | 100 | 82.3 | 31.1 | 47.1 | 21.1 | 7.2 | 21.7 | 53.3 | 25.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 100 | 82.3 | 32.2 | 46.2 | 20.0 | 8.3 | 18.3 | 63.9 | 17.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 100 | 82.0 | 30.6 | 45.5 | 20.0 | 7.2 | 19.4 | 61.7 | 18.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 100 | 82.6 | 30.6 | 44.8 | 18.3 | 7.2 | 18.9 | 62.8 | 18.3 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5-Turbo (0125) | 1 | 100 | 82.0 | 24.4 | 41.4 | 21.7 | 8.9 | 22.2 |
    52.8 | 25.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 100 | 82.9 | 28.9 | 40.9 | 18.3 | 8.9 | 24.4 | 55.0 | 20.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 100 | 83.8 | 27.8 | 41.7 | 20.0 | 8.9 | 25.0 | 56.1 | 18.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 100 | 82.4 | 26.7 | 40.7 | 18.9 | 7.8 | 19.4 | 57.8 | 22.8 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4-Turbo (1106-preview) | 1 | 100 | 86.9 | 32.8 | 39.3 | 20.0 | 9.4 |
    34.4 | 40.6 | 25.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 100 | 84.3 | 29.4 | 37.9 | 15.6 | 7.2 | 20.0 | 59.4 | 20.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 100 | 84.6 | 30.0 | 40.5 | 18.3 | 7.2 | 18.3 | 67.2 | 14.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 100 | 86.4 | 28.3 | 37.9 | 20.0 | 6.7 | 19.4 | 58.3 | 22.2 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Performance of different Feedback Generators. Uplift Ratio ($\uparrow$)
    show what percentage of plans has improved, not changed, and deteriorated, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Findings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'RQ1: Are LLM agents robust enough to noisy information for reasoning and planning?
    Table [1](#S3.T1 "Table 1 ‣ 3 Experimental Settings ‣ Can We Rely on LLM Agents
    to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example") shows that
    GPT-3.5-Turbo has a better performance when it receives *shrunk* reference information.
    This indicates that GPT-3.5-Turbo still struggles to attend to the most important
    parts of a given context for reasoning, prone to excessive irrelevant (context)
    chunks. Therefore, it is worth considering an external intelligent context-cleaning
    agent.'
  prefs: []
  type: TYPE_NORMAL
- en: 'RQ2: Can more shots help with the planning task, or does it worsen hallucination?
    It is commonly accepted that having more few-shots is helpful in ICL, but does
    it apply to TravelPlanner? As Table [1](#S3.T1 "Table 1 ‣ 3 Experimental Settings
    ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner
    as an Example") shows, the Final Pass rate reaches its highest value when there
    are $2$ shots, while having more shots may not improve if not hurt more. We presume
    that more shots in the context window may distract the LLM and lead to hallucination
    (e.g., using entities that do not exist in the given reference information). The
    results of the Hallucination Rate attest to this assumption. That is, giving more
    shots may potentially cause severer hallucination in tasks where the context of
    the reference information is complex tabular texts. Another finding is that at
    least one in-context example is beneficial Xie and Min ([2022](#bib.bib46)). Finally,
    we conclude that as we have more shots, the pass rate and hallucination rate results
    worsen.'
  prefs: []
  type: TYPE_NORMAL
- en: Our RQ1 and RQ2 observations align with the existing theoretical and experimental
    works, e.g., Han et al. ([2023](#bib.bib10)); Levy et al. ([2024](#bib.bib13)),
    which identify the potential causes underlying current LLMs’ failure in length
    generalization, that when they encounter a much longer context, the attention
    scores are diluted, and thus the score distribution becomes flat leading to information
    loss. That is, the entropy of the attention score will explode with increasing
    context. In other words, LLMs become lost in how to focus on the right information,
    especially when pre-training is done on shorter text segments.
  prefs: []
  type: TYPE_NORMAL
- en: 'RQ3: Can we rely on refinement to improve plans? To address this, we require
    feedback that highlights what went wrong, accompanied by explanations of the reasons
    behind the issues. For that, we examine the reliability of GPT-3.5-Turbo and GPT-4-Turbo
    as LLM-based feedback generators. In addition, as an ablative point of view, we
    also consider Random and Oracle feedback generators. The former refers to the
    setting where we fabricate the feedback using random content in a valid format,
    and the latter uses heuristic hard-coded rules³³3Rules from TravelPlanner: [https://github.com/OSU-NLP-Group/TravelPlanner/tree/main/evaluation](https://github.com/OSU-NLP-Group/TravelPlanner/tree/main/evaluation)
    to check whether the plan complies with the constraints. Furthermore, we do not
    consider any weaker language models because they have shown to be incapable of
    handling this kind of task in previous studies Madaan et al. ([2024](#bib.bib20)).
    We only focus on commonsense constraints in this part due to the frequent absence
    of hard constraints in the queries and the feedback⁴⁴4 Consistent with the original
    setting of TravelPlanner, i.e., plans that fail to satisfy all commonsense constraints
    will not proceed to receive feedback regarding hard constraints. This decision
    is rooted in the dependency of hard constraint computation on commonsense criteria..
    We present the results for RQ3 in Table [2](#S3.T2 "Table 2 ‣ 3 Experimental Settings
    ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner
    as an Example"). The feedback generator and the Refiner agent interact iteratively;
    at each iteration, the previously generated plans are reviewed by the feedback
    generator to draft feedback subjectively. Considering the feedback, if any refinement
    is needed, i.e., any constraint is not met, the Refiner agent is triggered to
    modify the plan. *This design simulates the production-level environment where
    no Oracle feedback generator is available to check whether a plan needs refinement.*
    We summarize our findings as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refinement can help improve the plans if the feedback is of high quality and
    precise – We see that the refinement helps with improving the plans if the feedback
    is accurate and well-organized as in the Oracle setting. The table shows, in the
    first iteration, $46.1\%$. From the second iteration, we do not see any significant
    improvement and the pass rates saturate.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLM feedback generators are not reliable – The LLM-based feedback generators,
    equipped with meticulously designed prompts with two-shots, still struggle with
    writing unerring feedback. Specifically, for faulty plans, these feedback generators
    cannot identify where the violation is or write excessive (baseless) feedback.
    Additionally, for qualified plans, they may generate false negative feedback which
    triggers the refinement module and causes unnecessary modification potentially
    leading to an invalid plan. As a result, the overall performance becomes stagnant,
    i.e., the Flat Ratio dominates, and modifications in a negative direction (downgrade
    ratio) have counteracted the positive changes (uplift ratio).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'RQ4: Can we enhance the development of a superior planner by employing our
    feedback-aware fine-tuning (FAFT) technique, as opposed to relying on off-the-shelf
    proprietary LLMs? For plan generation, we can use the Oracle feedback for in-context
    learning (as shown in RQ3) or fine-tuning an (open-source) LLM. The focus of this
    experiment lies in the latter aspect, where we examine the performance of SFT
    and FAFT in building the Planner agent.'
  prefs: []
  type: TYPE_NORMAL
- en: SFT vs. FAFT – In our proposed approach, i.e., FAFT, we extend beyond the considerations
    of query, reference information, and annotated plan as in SFT, by also incorporating
    feedback into the fine-tuning process (Appx. [A.4](#A1.SS4 "A.4 Supervised Fine-Tuning
    and Feedback-Aware Fine-Tuning ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents
    to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example")). To generate
    feedback, we initially use the queries from the training set and prompt the Planner
    agent to generate plans⁵⁵5Under the same setting as in RQ1 and RQ2.. Subsequently,
    we gather feedback by evaluating the generated plans using the Oracle (i.e., the
    system). We set `temperature` to $1.0$ original annotated plans from the training
    set together with their `all-success` feedback⁶⁶6It is noteworthy that more samples
    could be collected. (Appx. [A.6.2](#A1.SS6.SSS2 "A.6.2 Feedback Examples Generated
    by LLMs ‣ A.6 Case Presentation ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents
    to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example")). During
    inference, in the prompt, the feedback will be set to `all-success`, aiming to
    encourage the model to generate a correct plan. Appx. [A.4.3](#A1.SS4.SSS3 "A.4.3
    Inference Example Template for FAFT ‣ A.4 Supervised Fine-Tuning and Feedback-Aware
    Fine-Tuning ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon
    Plans? Let’s Take TravelPlanner as an Example") provides more information.
  prefs: []
  type: TYPE_NORMAL
- en: Table [3](#S4.T3 "Table 3 ‣ 4 Findings ‣ Can We Rely on LLM Agents to Draft
    Long-Horizon Plans? Let’s Take TravelPlanner as an Example") presents the impact
    of FAFT where a significant improvement is witnessed across all pass rates, compared
    to Vanilla Llama-3-8B and its SFT version. This observation aligns with the previous
    studies, e.g., Negative-Aware Training (NAT) Wang et al. ([2024b](#bib.bib36)),
    that the performance can be boosted by increasing the diversity of prompts. In
    FAFT, elaborative and rich feedback acts as thought chains to improve the agent’s
    planning. Further, our results validate the recent works Lee et al. ([2023](#bib.bib12));
    Wei et al. ([2023](#bib.bib39)) by suggesting that (1) injecting auxiliary information
    in conventional SFT data can markedly improve the performance, and (2) a CoT-style
    training set and detailed scratchpads can significantly improve learning by reducing
    sample complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Our findings advocate that when annotation is scarce while interaction with
    the system is affordable, collecting samples with comprehensive and rich feedback
    (either positive or negative), can be worthwhile. This approach can be seen as
    a promising alternative to RL-based solutions, such as PPO Schulman et al. ([2017](#bib.bib28)),
    which has been criticized for instability.
  prefs: []
  type: TYPE_NORMAL
- en: '| Llama-3-8B as Planner |'
  prefs: []
  type: TYPE_TB
- en: '| Planner (RQ4) | Delivery Rate |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Commonsense &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pass Rate &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Hard Constraint &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pass Rate &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Final Pass Rate |'
  prefs: []
  type: TYPE_TB
- en: '| Micro | Macro | Micro | Macro |'
  prefs: []
  type: TYPE_TB
- en: '| Vanilla | 94.4 | 49.5 | 1.1 | 7.9 | 0.0 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| + SFT | 97.8 | 64.2 | 11.1 | 12.4 | 6.1 | 3.9 |'
  prefs: []
  type: TYPE_TB
- en: '| + FAFT | 98.9 | 81.7 | 28.9 | 36.9 | 15.0 | 8.3 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Performance of Llama-3-8B +SFT and +FAFT.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we studied the impacts of context, the number of shots, and the
    utilization of feedback on a complex long-horizon planning task known as TravelPlanner.
    Our findings aim to advance a broader spectrum of agentic frameworks and strategies
    within the research community.
  prefs: []
  type: TYPE_NORMAL
- en: For future work, we plan to explore methods that incorporate annotated shots
    in SFT and post-training. This approach can address the bottleneck where LLMs’
    knowledge and skills are predominantly acquired during pre-training, while alignment
    SFT teaches the model which sub-distribution of formats to use when interacting
    with users Zhou et al. ([2024a](#bib.bib57)). Finally, we will explore the interplay
    between RLHF and FAFT.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Due to budget constraints, we were only able to use GPT-3.5-Turbo as the Planner
    agent for RQ1 and RQ2\. For RQ4, further investigations are needed to explore
    the relationship between the magnitude of gains and the size of the FAFT training
    set, as well as the impact of the ratio of positive to negative samples on the
    final performance. Additionally, enhancing the feedback expressions could further
    improve the performance of FAFT. It would also be interesting to investigate RLHF
    techniques, such as DPO Rafailov et al. ([2024](#bib.bib26)) and PRO Song et al.
    ([2024a](#bib.bib32)), to better utilize feedback.
  prefs: []
  type: TYPE_NORMAL
- en: Ethics Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our work is founded upon TravelPlanner, a benchmark designed for complex planning
    tasks. We adhere to the original work’s specifications, utilizing their data,
    evaluation scripts, and definitions of commonsense. Acknowledging the foundational
    concepts and designs of the original benchmark, we strictly adhere to TravelPlanner’s
    guidelines, ensuring the integrity of the evaluation process by prohibiting any
    form of cheating in the validation and test sets. This commitment upholds the
    fairness and reliability of this work.
  prefs: []
  type: TYPE_NORMAL
- en: As for environmental cost, we acknowledge that our work necessitated extensive
    experiments to derive robust conclusions. However, future endeavours can leverage
    these insights, potentially reducing the need for numerous large-scale comparisons.
    Models intended for production could undergo training once, utilizing the most
    promising settings identified through our research.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Chang et al. (2024) Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang,
    Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2024. A
    survey on evaluation of large language models. *ACM Transactions on Intelligent
    Systems and Technology*, 15(3):1–45.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2023a) Angelica Chen, Jérémy Scheurer, Tomasz Korbak, Jon Ander
    Campos, Jun Shern Chan, Samuel R Bowman, Kyunghyun Cho, and Ethan Perez. 2023a.
    Improving code generation by training with natural language feedback. *arXiv preprint
    arXiv:2303.16749*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2023b) Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik
    Narasimhan, and Shunyu Yao. 2023b. Fireact: Toward language agent fine-tuning.
    *arXiv preprint arXiv:2310.05915*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2024) Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning
    Liu, Dahua Lin, Kai Chen, and Feng Zhao. 2024. Agent-flan: Designing data and
    methods of effective agent tuning for large language models. *arXiv preprint arXiv:2403.12881*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Christianos et al. (2023) Filippos Christianos, Georgios Papoudakis, Matthieu
    Zimmer, Thomas Coste, Zhihao Wu, Jingxuan Chen, Khyati Khandelwal, James Doran,
    Xidong Feng, Jiacheng Liu, et al. 2023. Pangu-agent: A fine-tunable generalist
    agent with structured reasoning. *arXiv preprint arXiv:2312.14878*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2023) Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens,
    Boshi Wang, Huan Sun, and Yu Su. 2023. [Mind2web: Towards a generalist agent for
    the web](http://arxiv.org/abs/2306.06070).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fei et al. (2023) Weizhi Fei, Xueyan Niu, Pingyi Zhou, Lu Hou, Bo Bai, Lei Deng,
    and Wei Han. 2023. Extending context window of large language models via semantic
    compression. *arXiv preprint arXiv:2312.09571*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: gkamradt (2023) gkamradt. 2023. Llmtest needle in a haystack - pressure testing
    llms. [https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2024) Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao
    Pei, Nitesh V Chawla, Olaf Wiest, and Xiangliang Zhang. 2024. Large language model
    based multi-agents: A survey of progress and challenges. *arXiv preprint arXiv:2402.01680*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2023) Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and
    Sinong Wang. 2023. Lm-infinite: Simple on-the-fly length generalization for large
    language models. *arXiv preprint arXiv:2308.16137*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2024) Geunwoo Kim, Pierre Baldi, and Stephen McAleer. 2024. Language
    models can solve computer tasks. *Advances in Neural Information Processing Systems*,
    36.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2023) Nayoung Lee, Kartik Sreenivasan, Jason D Lee, Kangwook Lee,
    and Dimitris Papailiopoulos. 2023. Teaching arithmetic to small transformers.
    *arXiv preprint arXiv:2307.03381*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Levy et al. (2024) Mosh Levy, Alon Jacoby, and Yoav Goldberg. 2024. Same task,
    more tokens: the impact of input length on the reasoning performance of large
    language models. *arXiv preprint arXiv:2402.14848*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2024) Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin,
    and Bernard Ghanem. 2024. Camel: Communicative agents for" mind" exploration of
    large language model society. *Advances in Neural Information Processing Systems*,
    36.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023) Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Bin Sun,
    Xinglin Wang, Heda Wang, and Kan Li. 2023. [Turning dust into gold: Distilling
    complex reasoning capabilities from llms by leveraging negative data](https://api.semanticscholar.org/CorpusID:266375154).
    In *AAAI Conference on Artificial Intelligence*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023a) Junyi Liu, Liangzhi Li, Tong Xiang, Bowen Wang, and Yiming
    Qian. 2023a. Tcra-llm: Token compression retrieval augmented large language model
    for inference cost reduction. In *Findings of the Association for Computational
    Linguistics: EMNLP 2023*, pages 9796–9810.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023b) Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng,
    Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan
    Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. 2023b. Agentbench: Evaluating llms
    as agents. *arXiv preprint arXiv: 2308.03688*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2024) Zhiwei Liu, Weiran Yao, Jianguo Zhang, Liangwei Yang, Zuxin
    Liu, Juntao Tan, Prafulla K Choubey, Tian Lan, Jason Wu, Huan Wang, et al. 2024.
    Agentlite: A lightweight library for building and advancing task-oriented llm
    agent system. *arXiv preprint arXiv:2402.15538*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2024) Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang,
    Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, and Junxian He. 2024. Agentboard: An
    analytical evaluation board of multi-turn llm agents. *arXiv preprint arXiv:2401.13178*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Madaan et al. (2024) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    et al. 2024. Self-refine: Iterative refinement with self-feedback. *Advances in
    Neural Information Processing Systems*, 36.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. 2023. [Gpt-4 technical report](https://arxiv.org/abs/2303.08774).
    *arXiv preprint arXiv:2303.08774*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pan et al. (2024) Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey
    Levine, and Alane Suhr. 2024. Autonomous evaluation and refinement of digital
    agents. *arXiv preprint arXiv:2404.06474*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paul et al. (2023) Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges,
    Antoine Bosselut, Robert West, and Boi Faltings. 2023. Refiner: Reasoning feedback
    on intermediate representations. *arXiv preprint arXiv:2304.01904*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qian et al. (2024) Hongjin Qian, Zheng Liu, Peitian Zhang, Kelong Mao, Yujia
    Zhou, Xu Chen, and Zhicheng Dou. 2024. Are long-llms a necessity for long-context
    tasks? *arXiv preprint arXiv:2405.15318*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qin et al. (2023) Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,
    Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023. Toolllm:
    Facilitating large language models to master 16000+ real-world apis. *arXiv preprint
    arXiv:2307.16789*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rafailov et al. (2024) Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D
    Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization:
    Your language model is secretly a reward model. *Advances in Neural Information
    Processing Systems*, 36.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ratner et al. (2023) Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal
    Magar, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.
    2023. [Parallel context windows for large language models](https://doi.org/10.18653/v1/2023.acl-long.352).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 6383–6402, Toronto, Canada. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. *arXiv
    preprint arXiv:1707.06347*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2023) Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David
    Dohan, Ed H Chi, Nathanael Schärli, and Denny Zhou. 2023. Large language models
    can be easily distracted by irrelevant context. In *International Conference on
    Machine Learning*, pages 31210–31227\. PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shinn et al. (2024) Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik
    Narasimhan, and Shunyu Yao. 2024. Reflexion: Language agents with verbal reinforcement
    learning. *Advances in Neural Information Processing Systems*, 36.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shridhar et al. (2020) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan
    Bisk, Adam Trischler, and Matthew Hausknecht. 2020. Alfworld: Aligning text and
    embodied environments for interactive learning. *arXiv preprint arXiv:2010.03768*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. (2024a) Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang,
    Yongbin Li, and Houfeng Wang. 2024a. Preference ranking optimization for human
    alignment. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    volume 38, pages 18990–18998.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. (2024b) Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and
    Bill Yuchen Lin. 2024b. Trial and error: Exploration-based trajectory optimization
    for llm agents. *arXiv preprint arXiv:2403.02502*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Talebirad and Nadiri (2023) Yashar Talebirad and Amirhossein Nadiri. 2023.
    Multi-agent collaboration: Harnessing the power of intelligent llm agents. *arXiv
    preprint arXiv:2306.03314*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2024a) Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, and James
    Zou. 2024a. Mixture-of-agents enhances large language model capabilities. *arXiv
    preprint arXiv:2406.04692*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2024b) Renxi Wang, Haonan Li, Xudong Han, Yixuan Zhang, and Timothy
    Baldwin. 2024b. Learning from failure: Integrating negative examples when fine-tuning
    large language models as agents. *arXiv preprint arXiv:2402.11651*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves
    chain of thought reasoning in language models. *arXiv preprint arXiv:2203.11171*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in neural information processing
    systems*, 35:24824–24837.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. (2023) Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming
    Zhang. 2023. Magicoder: Source code is all you need. *arXiv preprint arXiv:2312.02120*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2023a) Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun
    Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023a. Autogen:
    Enabling next-gen llm applications via multi-agent conversation framework. *arXiv
    preprint arXiv:2308.08155*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2024) Siye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai Zhang, and
    Yanghua Xiao. 2024. How easily do irrelevant inputs skew the responses of large
    language models? *arXiv preprint arXiv:2404.03302*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2023b) Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu,
    Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, and Chi Wang. 2023b. An empirical
    study on challenging math problem solving with gpt-4. In *ArXiv preprint arXiv:2306.01337*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xi et al. (2024a) Zhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie Jin, Rui Zheng,
    Wei He, Yiwen Ding, Shichun Liu, Xin Guo, Junzhe Wang, et al. 2024a. Training
    large language models for reasoning through reverse curriculum reinforcement learning.
    *arXiv preprint arXiv:2402.05808*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xi et al. (2024b) Zhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang Hong, Honglin
    Guo, Junzhe Wang, Dingwen Yang, Chenyang Liao, Xin Guo, Wei He, Songyang Gao,
    Lu Chen, Rui Zheng, Yicheng Zou, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang,
    Zuxuan Wu, and Yu-Gang Jiang. 2024b. [Agentgym: Evolving large language model-based
    agents across diverse environments](http://arxiv.org/abs/2406.04151).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. (2024) Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou,
    Yuandong Tian, Yanghua Xiao, and Yu Su. 2024. Travelplanner: A benchmark for real-world
    planning with language agents. *arXiv preprint arXiv:2402.01622*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie and Min (2022) Sang Michael Xie and Sewon Min. 2022. How does in-context
    learning work? a framework for understanding the differences from traditional
    supervised learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2023) Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang,
    Bin Fu, and Gang Yu. 2023. Appagent: Multimodal agents as smartphone users. *arXiv
    preprint arXiv:2312.13771*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2022a) Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.
    2022a. Webshop: Towards scalable real-world web interaction with grounded language
    agents. *Advances in Neural Information Processing Systems*, 35:20744–20757.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2024) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths,
    Yuan Cao, and Karthik Narasimhan. 2024. Tree of thoughts: Deliberate problem solving
    with large language models. *Advances in Neural Information Processing Systems*,
    36.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2022b) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2022b. React: Synergizing reasoning and acting
    in language models. *arXiv preprint arXiv:2210.03629*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zeng et al. (2023) Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao
    Dong, and Jie Tang. 2023. Agenttuning: Enabling generalized agent abilities for
    llms. *arXiv preprint arXiv:2310.12823*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2024a) Cong Zhang, Deik Derrick Goh Xin, Dexun Li, Hao Zhang,
    and Yong Liu. 2024a. Meta-task planning for language agents. *arXiv preprint arXiv:2405.16510*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2024b) Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran
    Yao, Juntao Tan, Thai Hoang, Liangwei Yang, Yihao Feng, Zuxin Liu, et al. 2024b.
    Agentohana: Design unified data and training pipeline for effective agent learning.
    *arXiv preprint arXiv:2402.15506*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2024c) Shaokun Zhang, Jieyu Zhang, Jiale Liu, Linxin Song, Chi
    Wang, Ranjay Krishna, and Qingyun Wu. 2024c. Training language model agents without
    modifying language models. *ICML’24*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2024) Jun Zhao, Can Zu, Hao Xu, Yi Lu, Wei He, Yiwen Ding, Tao
    Gui, Qi Zhang, and Xuanjing Huang. 2024. Longagent: Scaling language models to
    128k context through multi-agent collaboration. *arXiv preprint arXiv:2402.11550*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2024) Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su.
    2024. Gpt-4v (ision) is a generalist web agent, if grounded. *arXiv preprint arXiv:2401.01614*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2024a) Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer,
    Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. 2024a. Lima:
    Less is more for alignment. *Advances in Neural Information Processing Systems*,
    36.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2023) Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo,
    Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. 2023.
    [Webarena: A realistic web environment for building autonomous agents](https://webarena.dev).
    *arXiv preprint arXiv:2307.13854*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2024b) Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and
    Aviral Kumar. 2024b. Archer: Training language model agents via hierarchical multi-turn
    rl. *arXiv preprint arXiv:2402.19446*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2023) Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao
    Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, et al.
    2023. Promptbench: Towards evaluating the robustness of large language models
    on adversarial prompts. *arXiv preprint arXiv:2306.04528*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The TravelPlanner dataset⁷⁷7TravelPlanner Dataset: [https://huggingface.co/datasets/osunlp/TravelPlanner](https://huggingface.co/datasets/osunlp/TravelPlanner)
    consists of three splits of training, validation, and test sets as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Training Set consists of $45$ triplets of query, reference, and human annotated
    plan. The annotations are used as demonstrations for in-context learning or supervised
    fine-tuning in our paper. Please note that these annotated plans are merely a
    subset of many feasible plans. As expected, the Oracle (i.e., system) returns
    the feedback for the annotations where no issue is raised (Appx. [A.6.2](#A1.SS6.SSS2
    "A.6.2 Feedback Examples Generated by LLMs ‣ A.6 Case Presentation ‣ Appendix
    A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take
    TravelPlanner as an Example")).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Validation Set comes with $180$ pairs of query and reference, with no annotated
    plans.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Test Set holds $1,000$ queries together with their references, without any
    annotated plans.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For a given query, agents are expected to formulate a (comprehensive) plan which
    includes transportation, restaurants, attractions, and accommodation for each
    day (Appx. [A.6.1](#A1.SS6.SSS1 "A.6.1 Query Example with its Travel Plan ‣ A.6
    Case Presentation ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon
    Plans? Let’s Take TravelPlanner as an Example") shows an example).
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Evaluation metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Following TravelPlanner, we use automatic evaluation metrics to assess whether
    a plan generated by the agent meets the (correct) format condition as well as
    all the constraints.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delivery Rate measures whether the agent could successfully generate a plan
    within a limited number of steps. Falling into any dead loops or invalid plan
    formats leads to failure. In the sole-planning setting, any failure in drafting
    a plan negatively impacts the delivery rate.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Commonsense Constraint Pass Rate assesses whether the agent can incorporate
    commonsense while drafting plans without explicit instructions. For example, the
    agent has to pick valid entities (incl. restaurants, hotels, etc.) from the reference
    information and not hallucinate.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hard Constraint Pass Rate measures whether a plan meets all hard constraints
    mentioned in the query, e.g., budget limit, cuisine preference, or accommodation
    type.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: N.B. For Commonsense and Hard Constraint Pass Rates, the evaluation is done
    in two ways, Micro and Macro, which evaluate the agent’s capability of following
    individual constraints vs. all the constraints holistically Xie et al. ([2024](#bib.bib45)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Final Pass Rate measures whether a plan satisfies all hard and commonsense constraints.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hallucination Rate measures whether a plan contains entities that cannot be
    found in the reference information.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'TravelPlanner’s Leaderboard⁸⁸8TravelPlanner Leaderboard: [https://huggingface.co/spaces/osunlp/TravelPlannerLeaderboard](https://huggingface.co/spaces/osunlp/TravelPlannerLeaderboard)
    let us evaluate the performance of agents against both validation and test sets
    online. This creates a stage for fair evaluation for all researchers. We use this
    leaderboard to calculate the figures for the validation and test sets for our
    experiments. We run each five times, with a different random seed, and report
    the average scores.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1d5f5a88017206862b1eb60709905cf6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure A.1: Toy Example: The Planner initially generates a plan w.r.t. query
    and reference, then the feedback generator generates feedback considering the
    commonsense constraints. Then, the Refiner modifies the plan to meet the requirements
    for all constraints.'
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Can We Rely on LLM Agents to
    Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example"), we show that
    the Planner agent generates a plan for a given query and (cleaned) reference information.
    In TravelPlanner’s Two-Staging setting, the reference information is collected
    by an upstream tool agent which gathers valid information related to transportation,
    dining, attractions, and accommodation from their corresponding source files.
    The original benchmark also particularly creates valid reference information for
    the Sole Planning setting where the focus is on the Planner agent. Hence, we evaluate
    our solution only in the Sole Planning setting since our focus is on planning.
  prefs: []
  type: TYPE_NORMAL
- en: A.3.1 The Scrubber Agent
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since the reference information is massive and lengthy (i.e., $10,000$, after
    removing the hotels whose prices are above this limit, there are still other choices
    left for the Planner agent to reason and draft a plan to meet the budget and other
    constraints. The prompt for the Scrubber agent is found in Appx. [A.5.1](#A1.SS5.SSS1
    "A.5.1 The Scrubber’s Prompt Template ‣ A.5 Prompt Templates for Agents ‣ Appendix
    A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take
    TravelPlanner as an Example").
  prefs: []
  type: TYPE_NORMAL
- en: A.3.2 The Feedback Generator and Refiner
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Once the original plan has been drafted, refinement is conducted in an iterative
    manner. For this, we follow previous works where two agents are separately created
    with natural language communication capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: The Feedback Generator which is responsible for generating nuanced task-dependent
    feedback that addresses multiple constraints. We tailor a prompt, as shown in
    Appx. [A.5.2](#A1.SS5.SSS2 "A.5.2 Feedback Generator’s Prompt ‣ A.5 Prompt Templates
    for Agents ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon
    Plans? Let’s Take TravelPlanner as an Example"), to ask LLMs to write feedback
    with regard to commonsense constraints. In the instructions, we provide a list
    of constraints with their descriptions. Here two-shots are used to help with feedback
    generation. The shots are randomly selected from the training set.
  prefs: []
  type: TYPE_NORMAL
- en: The Refiner Agent refines the generated plan based on the feedback received
    from the Feedback Generator towards a better version (see the prompt in Appx. [A.5.3](#A1.SS5.SSS3
    "A.5.3 The Refiner’s Prompt Template ‣ A.5 Prompt Templates for Agents ‣ Appendix
    A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take
    TravelPlanner as an Example")).
  prefs: []
  type: TYPE_NORMAL
- en: Fig. [A.1](#A1.F1 "Figure A.1 ‣ A.2 Evaluation metrics ‣ Appendix A Appendix
    ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner
    as an Example") illustrates the entire refinement phase. The feedback points out
    that there is a repeated attraction for Days $1$, and the accommodation does not
    satisfy the minimum number of nights requirement. Then, the Refiner agent refines
    this draft plan into a new plan where the attraction for the first day is replaced
    to avoid repetition, and another hotel is chosen which allows a two-night stay.
    Finally, based on the system assessment, the refined plan meets all commonsense
    constraints.
  prefs: []
  type: TYPE_NORMAL
- en: A.4 Supervised Fine-Tuning and Feedback-Aware Fine-Tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A.4.1 Training Example Template for SFT
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The TravelPlanner training set consists of $45$ samples with annotated plans.
    We use reference information, queries, and annotated plans for general SFT (which
    is a baseline).
  prefs: []
  type: TYPE_NORMAL
- en: '[⬇](data:text/plain;base64,cmVmZXJlbmNlIGluZm9ybWF0aW9uIGJveDoge3JlZn0KcXVlcnk6IHtxdWVyeX0KZHJhZnQgdHJhdmVsIHBsYW46IHtwbGFufQ==)reference  information  box:  {ref}query:  {query}draft  travel  plan:  {plan}'
  prefs: []
  type: TYPE_NORMAL
- en: A.4.2 Training Example Template for FAFT
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[⬇](data:text/plain;base64,cmVmZXJlbmNlIGluZm9ybWF0aW9uIGJveDp7cmVmfQpxdWVyeTp7cXVlcnl9CmZlZWRiYWNrOntmZWVkYmFja30KZHJhZnQgdHJhdmVsIHBsYW46e3BsYW59)reference  information  box:{ref}query:{query}feedback:{feedback}draft  travel  plan:{plan}'
  prefs: []
  type: TYPE_NORMAL
- en: A.4.3 Inference Example Template for FAFT
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[⬇](data:text/plain;base64,cmVmZXJlbmNlIGluZm9ybWF0aW9uIGJveDp7cmVmfQpxdWVyeTp7cXVlcnl9CmZlZWRiYWNrOntmZWVkYmFja30KaXNfcmVhc29uYWxiZV92aXNpdGluZ19jaXR5OiBzdWNjZXNzCmlzX3ZhbGlkX3Jlc3RhdXJhbnRzOiBzdWNjZXNzCmlzX3ZhbGlkX2F0dHJhY3Rpb25zOiBzdWNjZXNzCmlzX3ZhbGlkX2FjY29tbW9kYXRpb246IHN1Y2Nlc3MKaXNfdmFsaWRfdHJhbnNwb3J0YXRpb246IHN1Y2Nlc3MKaXNfdmFsaWRfaW5mb3JtYXRpb25faW5fY3VycmVudF9jaXR5OiBzdWNjZXNzCmlzX3ZhbGlkX2luZm9ybWF0aW9uX2luX3NhbmRib3g6IHN1Y2Nlc3MKaXNfbm90X2Fic2VudDogc3VjY2VzcwpkcmFmdCB0cmF2ZWwgcGxhbjo=)reference  information  box:{ref}query:{query}feedback:{feedback}is_reasonalbe_visiting_city:  successis_valid_restaurants:  successis_valid_attractions:  successis_valid_accommodation:  successis_valid_transportation:  successis_valid_information_in_current_city:  successis_valid_information_in_sandbox:  successis_not_absent:  successdraft  travel  plan:'
  prefs: []
  type: TYPE_NORMAL
- en: A.4.4 Fine-tuning Setup
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In RQ4, for the Planner agent, we fine-tune Llama3-8B for $3$.
  prefs: []
  type: TYPE_NORMAL
- en: A.5 Prompt Templates for Agents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A.5.1 The Scrubber’s Prompt Template
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[⬇](data:text/plain;base64,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)Can  you  assist  in  creating  a  5-day  travel  itinerary  starting  in  Sacramento  and  covering  2  cities  in  Washington  state  from  March  22nd  to  March  26th,  2022?  The  journey  will  be  for  a  group  of  three  with  a  budget  of  $3,600.  We  require  accommodations  that  provide  entire  rooms  and  do  not  plan  to  travel  by  flight.  As  far  as  cuisines  are  concerned,  we’d  love  to  experience  American,  Mediterranean,  Italian,  and  French  during  our  trip.===>  [’American’,  ’Mediterranean’,  ’Italian’,  ’French’]Can  you  help  with  generating  a  7-day  travel  plan  for  a  party  of  5?  We’re  setting  off  from  Indianapolis  and  planning  to  explore  3  cities  in  Colorado  from  March  11th  to  March  17th,  2022.  We  have  a  budget  of  $15,100  for  this  trip.  We’ll  be  bringing  our  pets,  so  pet-friendly  accommodations  are  a  must.  We’re  also  hoping  to  find  places  that  offer  Mexican,  Italian,  Mediterranean,  and  Indian  cuisines.  Entire  rooms  for  accommodations  would  be  ideal.===>  [’Mexican’,  ’Italian’,  ’Mediterranean’,  ’Indian’]Can  you  assist  in  creating  a  travel  itinerary  for  a  group  of  4,  starting  in  Seattle  and  visiting  3  unique  cities  across  Texas?  This  trip  will  span  over  7  days  from  March  10th  through  March  16th,  2022.  We  have  a  budget  of  $11,000.  Regarding  our  accommodations,  we  would  like  to  rent  entire  rooms,  and  it’s  important  that  our  lodgings  allow  parties.  As  for  transportation,  we  do  not  plan  to  drive  ourselves  around.===>  []...{45  shots  from  trainset}...I  need  your  help  to  plan  a  5-day  vacation  for  a  group  of  4  people.  We’re  departing  from  Honolulu  and  planning  to  visit  2  cities  in  California  from  March  19th  to  March  23rd,  2022.  The  budget  for  our  trip  is  $11,200.  For  food  preferences,  we  enjoy  Mediterranean  and  Mexican  dishes.===>{inference  for  the  cuisine  preference}'
  prefs: []
  type: TYPE_NORMAL
- en: A.5.2 Feedback Generator’s Prompt
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[⬇](data:text/plain;base64,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)Now  You  are  an  advanced  reasoning,  analyzing  and  advisory  agent  who  can  write  feedback  and  insights  for  a  given  draft  travel  plan,  based  on  the  given  query  and  reference  information  box.The  feedback  you  write  should  check  and  judge  if  the  given  draft  travel  plan  violates  one  or  several  following  constraints:*  is_reasonalbe_visiting_city:  {success  or  fail}.  This  refers  to  Reasonable  City  Route:  Changes  in  cities  during  the  trip  must  be  reasonable.*  is_valid_restaurants:  {success  or  fail}.  This  refers  to  Diverse  Restaurants:  Restaurant  choices  should  not  be  repeated  throughout  the  trip.*  is_valid_attractions:  {success  or  fail}.  This  refers  to  Diverse  Attractions:  Attraction  choices  should  not  be  repeated  throughout  the  trip.*  is_valid_accommodation:  {success  or  fail}.  This  refers  to  Minimum  Nights  Stay:  The  number  of  consecutive  days  spent  in  a  specific  accommodation  during  the  trip  must  meet  the  corresponding  required  minimum  number  of  nights’  stay.*  is_valid_transportation:  {success  or  fail}.  This  refers  to  No  conflict  Transportation:  Transportation  choices  within  the  trip  must  be  reasonable.  For  example,  having  both  "self-driving"  and  "flight"  would  be  considered  a  conflict.*  is_valid_information_in_current_city:  {success  or  fail}.  This  refers  to  Within  Current  City:  All  scheduled  activities  for  the  day  must  be  located  within  that  day’s  city(s).*  is_valid_information_in_sandbox:  {success  or  fail}.  This  refers  to  Within  Sandbox:  All  information,  such  as  restaurants,  attractions,  accommodations  and  transportation,  in  the  plan,  must  be  within  the  closed  sandbox  (reference  information  box);  otherwise,  it  will  be  considered  a  hallucination.*  is_not_absent:  {success  or  fail}.  This  refers  to  Complete  Information:  No  key  information  should  be  left  out  of  the  plan,  such  as  the  lack  of  accommodation  during  travel.Here  are  some  examples  for  your  information  as  demonstrations:*****  Example  Starts  *****reference  information  box:{ref}query:{query}draft  travel  plan:{plan}feedback:{feedback}-------------------------------------reference  information  box:{ref}query:{query}draft  travel  plan:{plan}feedback:{feedback}*****  Example  Ends  *****Now,  You  should  write  the  feedback  with  regard  to  the  aspect  of  constraints  shown  above.  Follow  the  formats  shown  in  the  examples  above.reference  information  box:{ref}query:{query}draft  travel  plan:{plan}feedback:'
  prefs: []
  type: TYPE_NORMAL
- en: A.5.3 The Refiner’s Prompt Template
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[⬇](data:text/plain;base64,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)You  are  a  proficient  planner.  Based  on  the  provided  information  and  query,  please  give  me  a  detailed  plan,  including  specifics  such  as  flight  numbers  (e.g.,  F0123456),  restaurant  names,  and  accommodation  names.  Note  that  all  the  information  in  your  plan  should  be  derived  from  the  provided  data.  You  must  adhere  to  the  format  given  in  the  example.  Additionally,  all  details  should  align  with  commonsense.  The  symbol  ’-’  indicates  that  information  is  unnecessary.  For  example,  in  the  provided  sample,  you  do  not  need  to  plan  after  returning  to  the  departure  city.  When  you  travel  to  two  cities  in  one  day,  you  should  note  it  in  the  ’Current  City’  section  as  in  the  example  (i.e.,  from  A  to  B).*****  Example  *****Query:  Could  you  create  a  travel  plan  for  7  people  from  Ithaca  to  Charlotte  spanning  3  days,  from  March  8th  to  March  14th,  2022,  with  a  budget  of  $30,200?Travel  Plan:Day  1:Current  City:  from  Ithaca  to  CharlotteTransportation:  Flight  Number:  F3633413,  from  Ithaca  to  Charlotte,  Departure  Time:  05:38,  Arrival  Time:  07:46Breakfast:  Nagaland’s  Kitchen,  CharlotteAttraction:  The  Charlotte  Museum  of  History,  CharlotteLunch:  Cafe  Maple  Street,  CharlotteDinner:  Bombay  Vada  Pav,  CharlotteAccommodation:  Affordable  Spacious  Refurbished  Room  in  Bushwick!,  CharlotteDay  2:Current  City:  CharlotteTransportation:  -Breakfast:  Olive  Tree  Cafe,  CharlotteAttraction:  The  Mint  Museum,  Charlotte;  Romare  Bearden  Park,  Charlotte.Lunch:  Birbal  Ji  Dhaba,  CharlotteDinner:  Pind  Balluchi,  CharlotteAccommodation:  Affordable  Spacious  Refurbished  Room  in  Bushwick!,  CharlotteDay  3:Current  City:  from  Charlotte  to  IthacaTransportation:  Flight  Number:  F3786167,  from  Charlotte  to  Ithaca,  Departure  Time:  21:42,  Arrival  Time:  23:26Breakfast:  Subway,  CharlotteAttraction:  Books  Monument,  Charlotte.Lunch:  Olive  Tree  Cafe,  CharlotteDinner:  Kylin  Skybar,  CharlotteAccommodation:  -*****  Example  Ends  *****Given  information:  {reference  information  box}Query:  {query}Travel  Plan:  {original  draft  travel  plan}Now  You  are  an  advanced  reasoning  and  self-corrective  agent  that  can  improve  based  on  self  refection  and  the  feedback.Based  on  given  query,  reference  information  box,  and  proposed  Travel  Plan,  above,  you  are  now  given  the  feedback  which  includes  the  reason  why  it  fails.Try  to  write  a  new  plan  in  which  the  errors  are  fixed.Keep  in  mind  that  you  only  make  changes  or  replace  the  item  which  causes  the  issue.If  it  appears  at  multiple  places,  correct  them  all  at  once.Try  to  avoid  making  unnecessary  changes  on  the  previous  proposed  plan.Always  make  sure  that  your  generation,  such  as  the  names  of  resuturants,  attractions,  accommodations,  transportations,  can  be  found  in  the  given  reference  information  box  above.For  attraction,  breakfast,  dinner  and  lunch,  do  not  give  repetition  within  each  day  and  among  the  days  in  the  plan,  i.e.  each  of  them  should  NOT  appear  more  than  once  in  the  whole  travel  plan.Feel  free  to  ignore  irrelevant  information  in  reference  information  box.*  If  the  feedback  is  about  repeated  restaurant,  for  example,  "The  restaurant  in  day  4  dinner  is  repeated.",  then  you  need  to  take  another  restuartant  from  reference  infobox,  which  is  different  from  the  previous  one  and  all  other  chosen  ones  in  the  plan;*  If  the  feedback  is  about  "The  breakfast/lunch/dinner/attraction/accommodation  in  day  X  is  invalid  in  the  sandbox",  for  example,  "The  lunch  in  day  3  is  invalid  in  the  sandbox.",  this  means  that  the  choice  cannot  be  found  in  reference  infobox.  Then  you  should  take  another  one  which  is  definitely  within  the  inference  infobox.*  If  the  feedback  is  about  "The  accommodation  X  do  not  obey  the  minumum  nights  rule",  this  means  that  the  total  days/nights  spent  in  the  accommodation  place  chosen  in  the  plan,  does  not  obey  the  minumum  nights  rule.For  example,  if  the  days  spent  in  that  accommodation  in  the  plan  are  2  days,  but  the  ’minumum  nights’  of  that  accommodation  is  greater  than  2,  then  the  plan  violates  the  rule.Therefore,  you  should  review  and  examine  the  number  of  ’minumum  nights’  of  each  accommodation  in  the  reference  information  box  and  make  sure  the  days  spent  in  that  accommodation  is  equal  or  greater  than  that  number.*  If  the  feedback  is  about  "No  accommodation/transportation/attaction/meal  in  day  X  is  not  allowed",  this  means  that  on  that  day,  you  should  arrange  the  corresponding  activity  rather  than  leave  it  blank(denoted  as  ’-’).*  If  the  feedback  is  about  "The  transportation  is  conflicting.",  this  means  that  you  cannot  select  neither  the  combination  of  Taxi  and  Self-driving  nor  the  combination  of  Flight  and  Self-driving,  at  the  same  time,  in  terms  of  transportation.Feedback:  {feedback}Write  a  new  plan:'
  prefs: []
  type: TYPE_NORMAL
- en: A.6 Case Presentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A.6.1 Query Example with its Travel Plan
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[⬇](data:text/plain;base64,UVVFUlk6CkNhbiB5b3UgY3JlYXRlIGEgdHJhdmVsIHBsYW4gZm9yIGEgZ3JvdXAgb2YgNCBkZXBhcnRpbmcgZnJvbSBTZWF0dGxlIDIgYW5kIGhlYWRpbmcgdG8gU2FuIEZyYW5jaXNjbyBmb3IgMyBkYXlzLCBmcm9tIE1hcmNoIDYgdGggdG8gTWFyY2ggOHRoLDIwMjI/IE91ciBidWRnZXQgaXMgJDIsOTAwLiBXZSBhcmUgYnJpbmdpbmcgcGV0cywgc28gYWNjb21tb2RhdGlvbnMgbmVlZCB0byBiZSBwZXQtZnJpZW5kbHkuIFdlIGFyZSBpbnRlcmVzdGVkIGluIHRyeWluZyBNZXhpY2FuLCBGcmVuY2gsIEFtZXJpY2FuLCBhbmQgTWVkaXRlcnJhbmVhbiBjdWlzaW5lcyBkdXJpbmcgb3VyIHZpc2l0LiBXZSB3b3VsZCBhbHNvIHByZWZlciB0byBhdm9pZCBmbHlpbmcgZm9yIHRyYW5zcG9ydGF0aW9uLgoKVFJBVkVMIFBMQU46CkRheSAxOgpDdXJyZW50IENpdHk6IGZyb20gU2VhdHRsZSB0byBTYW4gRnJhbmNpc2NvClRyYW5zcG9ydGF0aW9uOiBTZWxmLURyaXZpbmcgZnJvbSBTZWF0dGxlIHRvIFNhbiBGcmFuY2lzY28sIER1cmF0aW9uOiAxMiBob3VycyAyOCBtaW5zLCBDb3N0OiAkNjUKQnJlYWtmYXN0OiAtCkF0dHJhY3Rpb246IC0KTHVuY2g6IC0KRGlubmVyOiBBbnVwYW0gRWF0aW5nIFBvaW50LCBTYW4gRnJhbmNpc2NvCkFjY29tbW9kYXRpb246IFJvb20gaW4gRG93biB0b3duIEJyb29rbHluIFBhcmtzbG9wLCBTYW4gRnJhbmNpc2NvCgpEYXkgMjoKQ3VycmVudCBDaXR5OiBTYW4gRnJhbmNpc2NvClRyYW5zcG9ydGF0aW9uOiAtCkJyZWFrZmFzdDogQ29mZmVlICYgQ2hhaSBDby4sIFNhbiBGcmFuY2lzY28KQXR0cmFjdGlvbjogR29sZGVuIEdhdGUgQnJpZGdlLCBTYW4gRnJhbmNpc2NvOyBHb2xkZW4gR2F0ZSBQYXJrLCBTYW4gRnJhbmNpc2NvCkx1bmNoOiBCb25uZSBCb3VjaGUsIFNhbiBGcmFuY2lzY28KRGlubmVyOiBFbXByZXNzLCBTYW4gRnJhbmNpc2NvCkFjY29tbW9kYXRpb246IFJvb20gaW4gRG93biB0b3duIEJyb29rbHluIFBhcmtzbG9wLCBTYW4gRnJhbmNpc2NvCgpEYXkgMzoKQ3VycmVudCBDaXR5OiBmcm9tIFNhbiBGcmFuY2lzY28gdG8gU2VhdHRsZQpUcmFuc3BvcnRhdGlvbjogU2VsZi1Ecml2aW5nIGZyb20gU2FuIEZyYW5jaXNjbyB0byBTZWF0dGxlLCBEdXJhdGlvbiA6MTIgaG91cnMgMjUgbWlucywgQ29zdDogJDY1CkJyZWFrZmFzdDogR3VwdGEncyBSYXNvaSwgU2FuIEZyYW5jaXNjbwpBdHRyYWN0aW9uOiBQSUVSIDM5LCBTYW4gRnJhbmNpc2NvCkx1bmNoOiBTaGFtbWkgQmhhaSBMYXNzaSBXYWxhLCBTYW4gRnJhbmNpc2NvCkRpbm5lcjogLQpBY2NvbW1vZGF0aW9uOiAt)QUERY:Can  you  create  a  travel  plan  for  a  group  of  4  departing  from  Seattle  2  and  heading  to  San  Francisco  for  3  days,  from  March  6  th  to  March  8th,2022?  Our  budget  is  $2,900.  We  are  bringing  pets,  so  accommodations  need  to  be  pet-friendly.  We  are  interested  in  trying  Mexican,  French,  American,  and  Mediterranean  cuisines  during  our  visit.  We  would  also  prefer  to  avoid  flying  for  transportation.TRAVEL  PLAN:Day  1:Current  City:  from  Seattle  to  San  FranciscoTransportation:  Self-Driving  from  Seattle  to  San  Francisco,  Duration:  12  hours  28  mins,  Cost:  $65Breakfast:  -Attraction:  -Lunch:  -Dinner:  Anupam  Eating  Point,  San  FranciscoAccommodation:  Room  in  Down  town  Brooklyn  Parkslop,  San  FranciscoDay  2:Current  City:  San  FranciscoTransportation:  -Breakfast:  Coffee  &  Chai  Co.,  San  FranciscoAttraction:  Golden  Gate  Bridge,  San  Francisco;  Golden  Gate  Park,  San  FranciscoLunch:  Bonne  Bouche,  San  FranciscoDinner:  Empress,  San  FranciscoAccommodation:  Room  in  Down  town  Brooklyn  Parkslop,  San  FranciscoDay  3:Current  City:  from  San  Francisco  to  SeattleTransportation:  Self-Driving  from  San  Francisco  to  Seattle,  Duration  :12  hours  25  mins,  Cost:  $65Breakfast:  Gupta’s  Rasoi,  San  FranciscoAttraction:  PIER  39,  San  FranciscoLunch:  Shammi  Bhai  Lassi  Wala,  San  FranciscoDinner:  -Accommodation:  -'
  prefs: []
  type: TYPE_NORMAL
- en: A.6.2 Feedback Examples Generated by LLMs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The feedback generated by LLMs is in the same format of the system feedback.
  prefs: []
  type: TYPE_NORMAL
- en: '[⬇](data:text/plain;base64,aXNfcmVhc29uYWxiZV92aXNpdGluZ19jaXR5OiBmYWlsLCByZWFzb246VGhlIHRyaXAgc2hvdWxkIGJlIGEgY2xvc2VkIGNpcmNsZS4KaXNfdmFsaWRfcmVzdGF1cmFudHM6IHN1Y2Nlc3MKaXNfdmFsaWRfYXR0cmFjdGlvbnM6IHN1Y2Nlc3MKaXNfdmFsaWRfYWNjb21tb2RhdGlvbjogZmFpbCwgcmVhc29uOlRoZSBhY2NvbW1vZGF0aW9uIEhhcmxlbSBjb3p5IG5pZ2h0cywgRGVudmVyKENvbG9yYWRvKSBkbyBub3Qgb2JleSB0aGUgbWludW11bSBuaWdodHMgcnVsZS4KaXNfdmFsaWRfdHJhbnNwb3J0YXRpb246IGZhaWwsIHJlYXNvbjpUaGUgdHJhbnNwb3J0YXRpb24gaXMgY29uZmxpY3RpbmcuCmlzX3ZhbGlkX2luZm9ybWF0aW9uX2luX2N1cnJlbnRfY2l0eTogc3VjY2Vzcwppc192YWxpZF9pbmZvcm1hdGlvbl9pbl9zYW5kYm94OiBmYWlsLCByZWFzb246VGhlIGFjY29tbW9kYXRpb24gaW4gZGF5IDMgaXMgaW52YWxpZCBpbiB0aGUgc2FuZGJveC4KaXNfbm90X2Fic2VudDogc3VjY2Vzcw==)is_reasonalbe_visiting_city:  fail,  reason:The  trip  should  be  a  closed  circle.is_valid_restaurants:  successis_valid_attractions:  successis_valid_accommodation:  fail,  reason:The  accommodation  Harlem  cozy  nights,  Denver(Colorado)  do  not  obey  the  minumum  nights  rule.is_valid_transportation:  fail,  reason:The  transportation  is  conflicting.is_valid_information_in_current_city:  successis_valid_information_in_sandbox:  fail,  reason:The  accommodation  in  day  3  is  invalid  in  the  sandbox.is_not_absent:  success'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that, there are $45$ annotated plans in the training set. For each plan,
    without any exception, the generated feedback from the Oracle system is `all-success`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[⬇](data:text/plain;base64,aXNfcmVhc29uYWxiZV92aXNpdGluZ19jaXR5OiBzdWNjZXNzCmlzX3ZhbGlkX3Jlc3RhdXJhbnRzOiBzdWNjZXNzCmlzX3ZhbGlkX2F0dHJhY3Rpb25zOiBzdWNjZXNzCmlzX3ZhbGlkX2FjY29tbW9kYXRpb246IHN1Y2Nlc3MKaXNfdmFsaWRfdHJhbnNwb3J0YXRpb246IHN1Y2Nlc3MKaXNfdmFsaWRfaW5mb3JtYXRpb25faW5fY3VycmVudF9jaXR5OiBzdWNjZXNzCmlzX3ZhbGlkX2luZm9ybWF0aW9uX2luX3NhbmRib3g6IHN1Y2Nlc3MKaXNfbm90X2Fic2VudDogc3VjY2Vzcw==)is_reasonalbe_visiting_city:  successis_valid_restaurants:  successis_valid_attractions:  successis_valid_accommodation:  successis_valid_transportation:  successis_valid_information_in_current_city:  successis_valid_information_in_sandbox:  successis_not_absent:  success
    | Benchmark | Task |'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Feedback &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Provided? &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Trajectory &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Released? &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Baseline |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Realistic &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Interface? &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| WebShop Yao et al. ([2022a](#bib.bib48)) | Web | No | Expert | Rule, IL,
    RL, IL+RL | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| WebArena Zhou et al. ([2023](#bib.bib58)) | Web | No | Expert, Agent | Direct,
    CoT | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| AgentBench Liu et al. ([2023b](#bib.bib17)) |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Web, Code, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Game, Embodiment &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| No | Not Found | CoT | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| TravelPlanner Xie et al. ([2024](#bib.bib45)) | Tool, Planning | Yes | Expert
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Direct, CoT, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ReAct, Reflexion &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| No |'
  prefs: []
  type: TYPE_TB
- en: '| AgentBoard Ma et al. ([2024](#bib.bib19)) |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Web, Game, Tool, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Embodiment &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Partially | Not Found | Direct | Partially |'
  prefs: []
  type: TYPE_TB
- en: '| AgentGym Xi et al. ([2024b](#bib.bib44)) |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Web, Code, Game, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Tool, Embodiment &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Partially | Expert, Agent |'
  prefs: []
  type: TYPE_TB
- en: '&#124; BC (SFT), &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ReAct, AGENTEVOL &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Yes |'
  prefs: []
  type: TYPE_TB
- en: 'Table A.1: Popular Benchmarks for LLM Agents'
  prefs: []
  type: TYPE_NORMAL
- en: A.7 Related Works
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A.7.1 Benchmarks for LLM-based Generalist Agents
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It has been anticipated that generalist agents can handle diverse tasks and
    evolve across different (cyber) environments at the human level which is a long-term
    goal in the AGI community. LLMs can be used as experts, which mimic humans, that
    have a strong generalization capability that not only suits conventional NLP but
    also agentic tasks. Recently, plenty of benchmarks have been proposed to evaluate
    the agents across various tasks and environments comprehensively and fairly. We
    provide an overview of popular benchmarks in the community in Table [A.1](#A1.T1
    "Table A.1 ‣ A.6.2 Feedback Examples Generated by LLMs ‣ A.6 Case Presentation
    ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans?
    Let’s Take TravelPlanner as an Example"). Some benchmarks such as ALFWorld Shridhar
    et al. ([2020](#bib.bib31)) and Mind2Web Deng et al. ([2023](#bib.bib6)), which
    are already included in larger benchmarks, are not listed in the table. Although
    the recent progress in multi-modal LLMs has spurred research into multi-modal
    LLM agents Yang et al. ([2023](#bib.bib47)); Zheng et al. ([2024](#bib.bib56)),
    we only list benchmarks that focus exclusively on text-based environments which
    assess LLM agents’ abilities via textual reasoning and taking actions in-depth.
  prefs: []
  type: TYPE_NORMAL
- en: The listed benchmarks support agents powered by both API-based proprietary and
    open-weight LLMs with convenient drop-in replacement interfaces. It is also free
    to add few-shots or use other prompting strategies to generate actions.
  prefs: []
  type: TYPE_NORMAL
- en: A.7.2 Long Contexts Challenge for LLMs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Besides the fact that more and more LLMs offer long-context capabilities Fei
    et al. ([2023](#bib.bib7)); Ratner et al. ([2023](#bib.bib27)); Liu et al. ([2023a](#bib.bib16));
    Zhao et al. ([2024](#bib.bib55)); Qian et al. ([2024](#bib.bib24)), recent studies
    question LLMs’ ability to find needles in a haystack because they face challenges
    in discriminating highly semantically related information, and can be easily distracted
    by irrelevant and misleading contents in long contexts Wu et al. ([2024](#bib.bib41));
    Zhu et al. ([2023](#bib.bib60)); Chang et al. ([2024](#bib.bib1)); Shi et al.
    ([2023](#bib.bib29)); gkamradt ([2023](#bib.bib8)). The TravelPlanner Xie et al.
    ([2024](#bib.bib45)) is a benchmark to provide insightful answers to this problem,
    wherein lengthy context information, noise, and relevant snippets are deeply intertwined.
  prefs: []
  type: TYPE_NORMAL
- en: A.7.3 Multi-Agent Collaboration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recent studies have borrowed the multiple-agent methodology for collaboration
    on cyber tasks, gaming, coding, math reasoning, conversation responding, and question
    answering Guo et al. ([2024](#bib.bib9)); Wu et al. ([2023a](#bib.bib40), [b](#bib.bib42));
    Zhang et al. ([2024c](#bib.bib54)); Li et al. ([2024](#bib.bib14)); Liu et al.
    ([2024](#bib.bib18)); Talebirad and Nadiri ([2023](#bib.bib34)); Zhang et al.
    ([2024a](#bib.bib52)); Wang et al. ([2024a](#bib.bib35)). Under the hood, these
    works assign role-specific prompts to the LLM to build multiple agents for synergy
    and collaboration. The self-refinement works can be classified into this realm,
    where the advisor and refiner agents can troubleshoot and modify the response
    in a few rounds Madaan et al. ([2024](#bib.bib20)); Paul et al. ([2023](#bib.bib23));
    Kim et al. ([2024](#bib.bib11)); Pan et al. ([2024](#bib.bib22)); Chen et al.
    ([2023a](#bib.bib2)). However, few works study the reliability and robustness
    of multi-agent collaboration in more complex and practical tasks. Compared to
    the previous testbeds where generation errors are easily noticeable and unambiguous,
    it is questionable whether refinement can work on TravelPlanner, where the glitches
    are hard to find due to implicit commonsense constraints. Multi-agent collaboration
    also places higher demands on the capabilities of individual agents since a failure
    at any stage from any agent can lead to a collapse, such as a dead loop or deviation
    from the goal.
  prefs: []
  type: TYPE_NORMAL
- en: A.7.4 Reinforcement Learning via Feedback
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: On top of works that only use successful trajectories for behavioural cloning
    Zeng et al. ([2023](#bib.bib51)); Chen et al. ([2023b](#bib.bib3)); Zhang et al.
    ([2024b](#bib.bib53)); Chen et al. ([2024](#bib.bib4)), another line of work trains
    LLM-based agents based on environmental feedback, referred to as interactive learning
    methods Song et al. ([2024b](#bib.bib33)); Zhou et al. ([2024b](#bib.bib59));
    Christianos et al. ([2023](#bib.bib5)); Xi et al. ([2024a](#bib.bib43)). Specifically,
    they train the agents via reinforcement learning. However, poor transferability
    among scenarios, reward inconsistency, off-policy shift, step-level reward sparsity,
    and training stability and expenses are the main roots of performance bottlenecks.
    Possible alternative approaches such as Negative Aware Training (NAT) Wang et al.
    ([2024b](#bib.bib36)); Li et al. ([2023](#bib.bib15)) can be a more robust solution.
    Our FAFT approach is motivated by NAT, and it can be seamlessly migrated to other
    agentic tasks.
  prefs: []
  type: TYPE_NORMAL
