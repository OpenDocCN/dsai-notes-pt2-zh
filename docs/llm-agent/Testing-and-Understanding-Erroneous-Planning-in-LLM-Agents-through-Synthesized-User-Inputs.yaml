- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:47:52'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Testing and Understanding Erroneous Planning in LLM Agents through Synthesized
    User Inputs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.17833](https://ar5iv.labs.arxiv.org/html/2404.17833)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Zhenlan Ji, Daoyuan Wu, Pingchuan Ma, Zongjie Li, Shuai Wang¹¹footnotemark:
    1'
  prefs: []
  type: TYPE_NORMAL
- en: The Hong Kong University of Science and Technology
  prefs: []
  type: TYPE_NORMAL
- en: Hong Kong SAR, China
  prefs: []
  type: TYPE_NORMAL
- en: '{zjiae, daoyuan, pmaab, zligo, shuaiw}@cse.ust.hk Corresponding authors.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Agents based on large language models (LLMs) have demonstrated effectiveness
    in solving a wide range of tasks by integrating LLMs with key modules such as
    planning, memory, and tool usage. Increasingly, customers are adopting LLM agents
    across a variety of commercial applications critical to reliability, including
    support for mental well-being, chemical synthesis, and software development. Nevertheless,
    our observations and daily use of LLM agents indicate that they are prone to making
    erroneous plans, especially when the tasks are complex and require long-term planning.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we propose PDoctor, a novel and automated approach to testing
    LLM agents and understanding their erroneous planning. As the first work in this
    direction, we formulate the detection of erroneous planning as a constraint satisfiability
    problem: an LLM agent’s plan is considered erroneous if its execution violates
    the constraints derived from the user inputs. To this end, PDoctor first defines
    a domain-specific language (DSL) for user queries and synthesizes varying inputs
    with the assistance of the Z3 constraint solver. These synthesized inputs are
    natural language paragraphs that specify the requirements for completing a series
    of tasks. Then, PDoctor derives constraints from these requirements to form a
    testing oracle. PDoctor features several design considerations, such as mock tool
    and input mutation, to enhance testing effectiveness. Its synthesized inputs can
    also incorporate advanced features like dynamic constraint update to better test
    the LLM agent’s planning ability. We evaluate PDoctor with three mainstream agent
    frameworks and two powerful LLMs (GPT-3.5 and GPT-4). The results show that PDoctor can
    effectively detect diverse errors in agent planning, and provide insights and
    error characteristics that are valuable to both agent developers (for improving
    LLM agents) and users (for using contemporary agents). We conclude by discussing
    potential alternative designs and directions to extend PDoctor.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models (LLMs) have become extremely popular due to their remarkable
    performance across a wide range of tasks [[22](#bib.bib22), [12](#bib.bib12),
    [26](#bib.bib26), [44](#bib.bib44), [21](#bib.bib21), [55](#bib.bib55), [47](#bib.bib47)],
    demonstrating an ability to understand, reason, and act in ways akin to human
    cognition and reasoning. With their extensive parameters and training data, these
    models have shown proficiency in complex pattern recognition in natural language,
    leading to advancements in logical reasoning [[16](#bib.bib16), [56](#bib.bib56)],
    vulnerability detection [[47](#bib.bib47), [46](#bib.bib46)], robot planning [[11](#bib.bib11),
    [29](#bib.bib29)], and causal inference [[31](#bib.bib31), [24](#bib.bib24)].
    This has fueled the development of LLM agents [[59](#bib.bib59), [43](#bib.bib43),
    [14](#bib.bib14), [40](#bib.bib40)], which are designed to interact with the external
    world and make decisions to achieve complex objectives, leveraging LLMs’ cognitive
    capabilities for tasks that require planning, memory, and execution of a sequence
    of actions. These agents, integrating key modules alongside LLMs, excel in processing
    complex user queries, learning from past interactions, and adapting to new tasks.
  prefs: []
  type: TYPE_NORMAL
- en: To date, the industry has been increasingly commercializing LLM agents in various
    highly profitable and even mission-critical applications [[59](#bib.bib59), [5](#bib.bib5),
    [7](#bib.bib7), [9](#bib.bib9), [8](#bib.bib8), [2](#bib.bib2), [33](#bib.bib33)],
    such as healthcare, personal assistance, and financial services. However, despite
    their great potential and overall enthusiasm, LLM agents often struggle with erroneous
    planning, leading to significant consequences. For example, an LLM agent used
    to manage a chemical synthesis process may fail to produce the desired chemical
    compound if it makes an erroneous plan, and the involved, possibly expensive,
    chemical reagents are already consumed during the process. This challenge, highlighted
    by our observations and experiences, underscores the need for improvements in
    their design and operation. Errors in planning, particularly in complex, long-term
    tasks, can result in the misuse of resources or failure to achieve intended outcomes,
    underscoring the importance of developing more reliable and effective LLM agents.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we propose PDoctor, a novel framework for testing and understanding
    erroneous planning in LLM agents. Our approach is fully automated and can be used
    to detect erroneous planning in LLM agents using different core LLMs and following
    different paradigms (see introduction in Sec. [2](#S2 "2 Preliminary ‣ Testing
    and Understanding Erroneous Planning in LLM Agents through Synthesized User Inputs")).
    As the first work in this direction, PDoctor  formulates the occurrence of “erroneous
    planning” as a constraint satisfiability problem: an LLM agent’s plan is erroneous
    if its execution violates the constraints derived from the user inputs. The constraints
    can be rigorously checked with moderate cost, thus providing a reliable way to
    detect erroneous planning.'
  prefs: []
  type: TYPE_NORMAL
- en: PDoctor defines a domain specific language (DSL) that captures the semantics
    of user queries and features a synthesis procedure (with the assistance of Z3)
    that can generate diverse user requests as the test inputs to the LLM agent. We
    provide configurable parameters to control the diversity and complexity of the
    user queries and offer a mutation procedure to further transform each generated
    user query. Each user query denotes a paragraph that outlines the requirements
    for conducting a series of tasks. PDoctor accordingly derives constraints from
    these requirements and checks if the LLM agent’s plan aligns with the satisfiability
    of these constraints; misalignment flags erroneous planning. We provide a set
    of design considerations and optimizations (e.g., mock tools employed by the agent)
    to deliver effective testing, and also augment the synthesized inputs with advanced
    features like dynamic constraint update to test the LLM agent’s planning capability.
  prefs: []
  type: TYPE_NORMAL
- en: 'We evaluate PDoctor on three mainstream LLM agent frameworks, ReAct [[59](#bib.bib59)],
    OpenAI Tools (OT) [[5](#bib.bib5)], and OpenAI Assistant (OA) [[7](#bib.bib7)].
    These LLM agents represent different paradigms and are widely used in various
    applications. We incorporate these frameworks with two widely-used LLM models,
    GPT-3.5 and GPT-4 [[10](#bib.bib10)]. PDoctor can effectively detect thousands
    of erroneous plans across all these settings. We configure PDoctor to depict the
    planning capability “upper bound” of different LLM agents and also summarize the
    detected erroneous plans. These results can provide insights into the common pitfalls
    of LLM agents and offer guidance for developers and users in daily practice. We
    also discuss the extension of PDoctor and the potential future work to repair
    the detected erroneous plans. In summary, the contributions of this paper are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We pioneer the effort to test and understand erroneous planning in LLM agents,
    given their increasing commercialization in reliability-sensitive fields and the
    potential severe consequences of erroneous planning.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We formulate the problem of detecting erroneous planning as a constraint satisfiability
    problem and present a fully automated framework, PDoctor, that employs input synthesis
    and constraint checking to detect erroneous planning.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We evaluate PDoctor on mainstream LLM agents using different paradigms and show
    that our approach can effectively detect thousands of erroneous planning with
    moderate cost. We also summarize insights from different aspects to benefit developers
    and users.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Tool Availability. We have made PDoctor available at [https://anonymous.4open.science/r/PDoctor-E872](https://anonymous.4open.science/r/PDoctor-E872)
    for review purposes. We will continue to maintain the tool and add more documentation
    to assist users in utilizing it.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Preliminary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Planning Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Planning is a fundamental problem-solving task that involves generating a sequence
    of actions to achieve a goal [[34](#bib.bib34), [18](#bib.bib18)]. Extensive efforts
    have been devoted to this area, achieving significant progress in various domains,
    such as robotics [[36](#bib.bib36), [13](#bib.bib13), [23](#bib.bib23)] and autonomous
    vehicles [[15](#bib.bib15), [32](#bib.bib32)]. Formally, the planning problem
    can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Definition 1  (Planning Problem).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Given a set of states $\bm{S}$.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of LLM agents, the user query can be viewed as a planning problem
    $\mathcal{P}=\langle\bm{S},\bm{A},f,s_{0},s_{g}\rangle$ are satisfied.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 An Example of LLM Agents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2338b3108bb56f699662c3e3b705de58.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: An example illustrating an LLM agent working on a complex user query.
    Green and blue represent the agent’s actions and the execution results of invoked
    tools, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Fig. [1](#S2.F1 "Figure 1 ‣ 2.2 An Example of LLM Agents ‣ 2 Preliminary ‣ Testing
    and Understanding Erroneous Planning in LLM Agents through Synthesized User Inputs")
    presents a typical example of an LLM agent working on a sample user query (Fig. [1](#S2.F1
    "Figure 1 ‣ 2.2 An Example of LLM Agents ‣ 2 Preliminary ‣ Testing and Understanding
    Erroneous Planning in LLM Agents through Synthesized User Inputs")(a)). The agent
    first understands the requirements implied by the user query, then plans (reasons)
    to select the appropriate tools (Fig. [1](#S2.F1 "Figure 1 ‣ 2.2 An Example of
    LLM Agents ‣ 2 Preliminary ‣ Testing and Understanding Erroneous Planning in LLM
    Agents through Synthesized User Inputs")(b)) to invoke, and finally acts to execute
    each selected tool and collect its response (Fig. [1](#S2.F1 "Figure 1 ‣ 2.2 An
    Example of LLM Agents ‣ 2 Preliminary ‣ Testing and Understanding Erroneous Planning
    in LLM Agents through Synthesized User Inputs")(c)). In particular, after receiving
    the output from each invoked tool, the agent repeats the above process until the
    user query is fully answered. This series of interactions between the agent and
    the environment, as listed in Fig. [1](#S2.F1 "Figure 1 ‣ 2.2 An Example of LLM
    Agents ‣ 2 Preliminary ‣ Testing and Understanding Erroneous Planning in LLM Agents
    through Synthesized User Inputs")(c), collectively constitutes a problem-solving
    procedure and then determines the final response (omitted here) to the user query.
    LLM agents benefit from powerful LLMs that can behave in a human-like manner to
    understand, reason, and act simultaneously, and the synergy of these three capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, given the susceptibility of LLMs that has been exposed in prior
    research [[38](#bib.bib38), [53](#bib.bib53), [58](#bib.bib58), [27](#bib.bib27)],
    it is not surprising that LLM agents are substantially prone to errors and failures.
    Intuitively, the agent’s action chain can be viewed as an “error amplifier,” where
    a small mistake in the early stage of the action chain could be continuously amplified
    and propagated in each subsequent step, leading to catastrophic failures in the
    end. This character further exacerbates the difficulty of providing a correct
    and stable response to the user query. Moreover, the interaction between the LLM
    agent and the environment is often complex and dynamic, distinct from the static
    and isolated settings in which LLMs’s abilities have been tested [[49](#bib.bib49),
    [26](#bib.bib26), [22](#bib.bib22), [46](#bib.bib46)].
  prefs: []
  type: TYPE_NORMAL
- en: In general, previous studies that endeavor to evaluate LLMs are often limited
    to a straightforward linear flow following a “text in and text out” paradigm.
    In this paradigm, problems are presented in their entirety, and LLMs are expected
    to generate a textual response based on the given problem. In contrast, LLM agent
    testing necessitates a focus on the interaction and interplay between the LLM
    and the environment, where the problems are dynamic and the LLM agent is required
    to adjust its plan in response to the environment’s feedback. Here, the feedback
    refers to the results of the invoked tools, as shown in the blue text of Fig. [1](#S2.F1
    "Figure 1 ‣ 2.2 An Example of LLM Agents ‣ 2 Preliminary ‣ Testing and Understanding
    Erroneous Planning in LLM Agents through Synthesized User Inputs")(c). In line
    with these dynamic and complex settings, we design PDoctor to dynamically update
    the constraint sets (see details in Sec. [4.4](#S4.SS4 "4.4 Extended Testing Framework
    ‣ 4 Design ‣ Testing and Understanding Erroneous Planning in LLM Agents through
    Synthesized User Inputs")). This underscores the importance of tool design, in
    addition to the textual problems, during test case generation. Consequently, it
    is crucial to develop a systematic approach tailored to the unique characteristics
    of LLM agents to test and assess their performance in a thorough manner.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Architecture of LLM Agents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With a typical example of LLM agents illustrated in Sec.[2.2](#S2.SS2 "2.2 An
    Example of LLM Agents ‣ 2 Preliminary ‣ Testing and Understanding Erroneous Planning
    in LLM Agents through Synthesized User Inputs"), we now delve into the architecture
    of an LLM agent to better understand the mechanisms behind the agent’s operation,
    which is valuable for designing the testing framework.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e38da86ae58301eabbcb2300556dffe9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: A typical architecture of LLM agents, according to [[54](#bib.bib54)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Modules. Fig. [2](#S2.F2 "Figure 2 ‣ 2.3 Architecture of LLM Agents ‣ 2 Preliminary
    ‣ Testing and Understanding Erroneous Planning in LLM Agents through Synthesized
    User Inputs") presents a typical architecture of an LLM agent. This agent consists
    of three main components: (1) an understand module, (2) a plan module, and (3)
    an act (tool usage) module. The understand module is tasked with comprehending
    the user query and some historical records of previous actions (i.e., in the “memory”
    component) to extract a concrete task to be accomplished. The plan module is responsible
    for: (1) decomposing the extracted task into a sequence of sub-tasks that can
    be accomplished by invoking various tools, (2) managing the state and resources
    to support an appropriate plan (e.g., retaining the error code until requested
    by network_diagnose, as shown in Fig. [1](#S2.F1 "Figure 1 ‣ 2.2 An Example of
    LLM Agents ‣ 2 Preliminary ‣ Testing and Understanding Erroneous Planning in LLM
    Agents through Synthesized User Inputs")), and (3) determining the execution order
    of the sub-tasks. The act module then invokes the tools according to the generated
    plan and collects the results of the tool invocations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM and Prompts. All the components mentioned above are implemented by an LLM,
    which serves as the “brain” of the agent. The agent offers prompt templates composing
    the query context, how the agent should respond, and the accessible tools, whereas
    the users provide the query content. The formed prompt will be used to guide the
    LLM to generate the response. For example, ReAct [[59](#bib.bib59)] categorizes
    the core LLM’s response into three types: observation, thought, and action, which
    correspond to the understand, plan, and act modules in Fig. [2](#S2.F2 "Figure
    2 ‣ 2.3 Architecture of LLM Agents ‣ 2 Preliminary ‣ Testing and Understanding
    Erroneous Planning in LLM Agents through Synthesized User Inputs"), respectively.
    Additionally, prompt engineering strategies like role-play are often employed
    to enhance the agent’s performance under specific scenarios, such as through the
    instruction parameter in OpenAI’s Assistant API [[7](#bib.bib7)].'
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Our Testing Focus
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As shown in Fig. [2](#S2.F2 "Figure 2 ‣ 2.3 Architecture of LLM Agents ‣ 2
    Preliminary ‣ Testing and Understanding Erroneous Planning in LLM Agents through
    Synthesized User Inputs"), different components jointly contribute to the agent’s
    overall performance. Nevertheless, this paper considers testing the plan module
    a top priority. This is because extensive efforts [[25](#bib.bib25), [19](#bib.bib19),
    [62](#bib.bib62), [46](#bib.bib46), [51](#bib.bib51), [28](#bib.bib28)] have been
    devoted to evaluating the understand module, which rely on LLMs’ basic natural
    language understanding capability. Similarly, the act module has also made promising
    progress, as evidenced by OpenAI’s recent work in incorporating function calling
    functionality into GPT models [[6](#bib.bib6)]. Moreover, Yue et al. [[20](#bib.bib20)]
    have conducted a thorough investigation into LLM agents’ tool usage performance
    and proposed a benchmark, MetaTool, to evaluate the act module. In contrast, the
    plan module, which determines how to concretize the agent’s thought into a sequence
    of actions that can be executed by the agent, has not been systematically studied
    or tested in existing research. To sum up, we have the following focuses:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (Isolating the test of the planning module only) While maintaining the complexity
    of the planning problem, we strive to simplify the user queries handled by the
    understand and act modules (see Fig. [2](#S2.F2 "Figure 2 ‣ 2.3 Architecture of
    LLM Agents ‣ 2 Preliminary ‣ Testing and Understanding Erroneous Planning in LLM
    Agents through Synthesized User Inputs")). By doing this, we can guarantee that
    any detected agent failure can be solely attributed to an error in the planning
    module, thereby achieving a simulated “isolation” of the planning module. This
    approach facilitates more precise testing of LLM agents’ planning ability.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (Covering both textual queries and underlying tools) Unlike previous works [[50](#bib.bib50),
    [49](#bib.bib49)] that tested the planning ability of LLMs by generating and evaluating
    textual plans, the planning module in LLM agents is required to decide the order
    of tool invocation, making the testing of the planning module more complex. We
    aim to generate specialized test cases that comprise both textual queries and
    corresponding tools.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (Using only valid user queries for testing) For individual queries, we focus
    on generating valid user queries to test LLM agents. That said, we are not using
    extreme user queries to stress LLM agents. Based on our observation, extreme queries,
    such as overly long or complex ones, as well as those with broken or confusing
    contents, may hinder the LLM agent from generating meaningful outputs. This is
    not surprising since LLM agents rely on LLMs to generate outputs and are, therefore,
    sensitive to the quality of the input prompts. Sticking to valid user queries
    can help us better understand the planning module’s performance in a real-world
    setting.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3 Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d72a19154f1e13ed6d7754dd4b11bb0b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: An overall workflow of PDoctor, which synthesizes textual user queries
    with a list of tools and derives the constraints as the oracle to test the LLM
    agent.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the testing focus described in Sec. [2.4](#S2.SS4 "2.4 Our Testing Focus
    ‣ 2 Preliminary ‣ Testing and Understanding Erroneous Planning in LLM Agents through
    Synthesized User Inputs"), we now introduce PDoctor, a novel system for testing
    and understanding erroneous planning in LLM agents. The basic idea of PDoctor
    is to synthesize textual user queries, and based on the simultaneously derived
    formal constraints, we can identify the erroneous planning. Based on this idea,
    we present the overall design of PDoctor as shown in Fig. [3](#S3.F3 "Figure 3
    ‣ 3 Overview ‣ Testing and Understanding Erroneous Planning in LLM Agents through
    Synthesized User Inputs"). At a high level, PDoctor generates test cases comprising
    textual user queries that represent planning problems, along with a series of
    tools for invocation by the LLM agent. The testing oracle, automatically derived
    from the synthesized queries, is used to check the correctness of the LLM agent’s
    planning. This oracle also facilitates the dissection and characterization of
    erroneous planning when it occurs. Specifically, PDoctor has the following key
    components:'
  prefs: []
  type: TYPE_NORMAL
- en: '① User Query Generation. PDoctor performs a natural language synthesis process
    to generate textual user queries, which are considered as a planning problem $\mathcal{P}$,
    as formulated in Sec. [2.1](#S2.SS1 "2.1 Planning Problem ‣ 2 Preliminary ‣ Testing
    and Understanding Erroneous Planning in LLM Agents through Synthesized User Inputs"),
    for the LLM agent. This process comprises two steps: skeleton synthesis and text
    filling. The former step generates a skeleton of the user query, which is a high-level,
    abstract representation of the planning problem. This enables PDoctor directly
    to derive the constraints for subsequent testing. The latter step fills in this
    skeleton with text to form a complete user query that describes the requirements
    for problem-solving. The design of this synthesis process is motivated by two
    key considerations: (1) the derived constraints should be equivalent to the semantics
    of the generated queries, since these constraints are used as the oracle to test
    the LLM agent; (2) the synthesis process should be flexible and extensible, allowing
    users to specify the complexity and scenarios of the generated queries. For the
    first consideration, alternative methods for generating user queries, such as
    text mutation and LLM-based text generation, are further discussed in Sec. [7](#S7
    "7 Discussion ‣ Testing and Understanding Erroneous Planning in LLM Agents through
    Synthesized User Inputs").'
  prefs: []
  type: TYPE_NORMAL
- en: ② Test Results Check. After deriving formal constraints from the synthesis process,
    PDoctor utilizes these constraints to detect erroneous planning of the LLM agent.
    Specifically, PDoctor maps each action, which corresponds to subtasks specified
    by the user query in a one-to-one manner, to a specific tool invocation and collects
    the action chain (i.e., the planning made by the LLM agent) by recording the history
    of tool invocations. The collected planning is then compared against the constraints
    to determine whether the planning is correct.
  prefs: []
  type: TYPE_NORMAL
- en: ③ Error Dissection. In contrast to prior studies that modified inputs based
    on superficial natural language properties (e.g., word-level replacement) [[48](#bib.bib48),
    [30](#bib.bib30)], PDoctor synthesize complete textual user queries from scratch,
    gaining full control over both the semantics (i.e., the derived constraints) and
    the structure (i.e., the skeleton) of the user query. This design allows PDoctor to
    conduct comprehensive mutations and transformations on the generated query. By
    maintaining the overall semantic meaning, PDoctor is capable of altering words,
    sub-tasks, the context of the prompt, multiple sentences, and even the entire
    structure of the prompt. This capability is crucial for dissecting erroneous planning
    in LLM agents, as it enables PDoctor to pinpoint the root cause of failures by
    comparing the planning results of the original and mutated prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Challenges and Key Solutions. To achieve the design above, however, we need
    to address some unique challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: 'C1:'
  prefs: []
  type: TYPE_NORMAL
- en: Designing a mechanism or specification to support user query synthesis. Without
    a mechanism or specification to guide their formulation, the synthesized user
    queries could become highly varied and uncontrollable. Moreover, to easily derive
    the corresponding constraints from the synthesized queries, we also need the support
    of a formal specification. To address this problem, we propose a specialized DSL
    in Sec. [4.1](#S4.SS1 "4.1 Domain-Specific Language ‣ 4 Design ‣ Testing and Understanding
    Erroneous Planning in LLM Agents through Synthesized User Inputs"). With this
    DSL, PDoctor is able to cover a diverse set of user queries with configurable
    complexity and diversity. Moreover, DSL-based skeleton synthesis ensures the full
    control over the structure and semantics of the user queries, enabling PDoctor to
    exhaustively conduct mutation and transformation on a given prompt, thereby allowing
    the dissection of erroneous planning in Sec. [4.5](#S4.SS5 "4.5 Error Dissection
    ‣ 4 Design ‣ Testing and Understanding Erroneous Planning in LLM Agents through
    Synthesized User Inputs").
  prefs: []
  type: TYPE_NORMAL
- en: 'C2:'
  prefs: []
  type: TYPE_NORMAL
- en: Generating syntactically valid and semantically meaningful user queries. As
    mentioned in Sec. [2.4](#S2.SS4 "2.4 Our Testing Focus ‣ 2 Preliminary ‣ Testing
    and Understanding Erroneous Planning in LLM Agents through Synthesized User Inputs"),
    we aim to generate valid user queries to test LLM agents. That said, PDoctor  should
    focus on generating syntactically valid and semantically meaningful user queries
    as testing inputs. To address this challenge, the user query skeletons are specified
    using the aforementioned DSL, which prevents the generation of broken content;
    see details in Sec. [4.2](#S4.SS2 "4.2 User Query Synthesis ‣ 4 Design ‣ Testing
    and Understanding Erroneous Planning in LLM Agents through Synthesized User Inputs").
    Moreover, a constraint solver (Z3 [[17](#bib.bib17)] in our case) is used to ensure
    that the semantics underlying the user queries are meaningful, i.e., the constraints
    derived from the generated prompt are always satisfiable, as to be illustrated
    in Sec. [4.3](#S4.SS3 "4.3 Test Results Check ‣ 4 Design ‣ Testing and Understanding
    Erroneous Planning in LLM Agents through Synthesized User Inputs").
  prefs: []
  type: TYPE_NORMAL
- en: 'C3:'
  prefs: []
  type: TYPE_NORMAL
- en: Covering complex planning problems with state management and dynamic problem-solving.
    While the original design of PDoctor above already covers planning tests for both
    textual queries and corresponding tools, it has not yet considered complex planning
    problems involving state management and dynamic problem-solving. We address this
    challenge by introducing time and duration constraints, thereby obtaining an extended
    testing framework for PDoctor, which will be presented in Sec. [4.4](#S4.SS4 "4.4
    Extended Testing Framework ‣ 4 Design ‣ Testing and Understanding Erroneous Planning
    in LLM Agents through Synthesized User Inputs").
  prefs: []
  type: TYPE_NORMAL
- en: 4 Design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Domain-Specific Language
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To support user query synthesis, we have designed a specialized DSL tailored
    for LLM agent planning problems. This DSL is not intended to cover the infinitely
    vast semantic space of natural language for describing anything conceivable concept.
    Instead, it narrows its focus exclusively to the semantic space required for the
    planning problems defined in Sec. [2.1](#S2.SS1 "2.1 Planning Problem ‣ 2 Preliminary
    ‣ Testing and Understanding Erroneous Planning in LLM Agents through Synthesized
    User Inputs"). Therefore, before delving into the specifics of the DSL, we first
    present how we simplify the semantic space to be explored, which could also avoid
    ambiguity due to English not being context-free.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic Simplification. For two given tasks that cannot be executed in parallel,
    denoted as task $A$” is a simplified constraint in this context. Moreover, it
    is straightforward to generalize this simplification to multiple tasks. Thus,
    PDoctor simplifies prompt synthesis into generating several sentences, each of
    which is composed of this kind of simplified constraints.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, each “task” in the user query is simplified to take an action $a$
    is the set of actions that the LLM agent can perform (see Sec. [2.1](#S2.SS1 "2.1
    Planning Problem ‣ 2 Preliminary ‣ Testing and Understanding Erroneous Planning
    in LLM Agents through Synthesized User Inputs")). Take Fig. [3](#S3.F3 "Figure
    3 ‣ 3 Overview ‣ Testing and Understanding Erroneous Planning in LLM Agents through
    Synthesized User Inputs") as an example, conducting network status check is a
    task and invoking network_status_check() is the action. Without necessitating
    additional decomposition, every task can be completed directly through one single
    action. This simplification is based on the observation that task decomposition
    is mainly determined by the context of the task, rather than the performance of
    the LLM agent itself. For example, LLM agents that are familiar with the network
    would be able to correctly decompose the task of “fix disconnected network” into
    a series of sub-tasks like “check network status” and “network diagnosis”, while
    others that are designed for addressing book management would not. Nonetheless,
    this task decomposition failure can be easily mitigated by providing the LLM agent
    with instructive knowledge of network repair. Due to the potential unanticipated
    impact that task decomposition could have on the performance of the LLM agent,
    PDoctor avoids generating complex tasks that require decomposition.
  prefs: []
  type: TYPE_NORMAL
- en: Syntax $\displaystyle\langle\mathcal{P}\in\text{Planning Problem}\rangle\Coloneqq{}$
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: The syntax of our DSL, specifically designed for synthesizing user
    queries and deriving their constraints.'
  prefs: []
  type: TYPE_NORMAL
- en: DSL Specifics. Fig. [4](#S4.F4 "Figure 4 ‣ 4.1 Domain-Specific Language ‣ 4
    Design ‣ Testing and Understanding Erroneous Planning in LLM Agents through Synthesized
    User Inputs") presents the syntax of this domain-specific language. Overall, each
    user query is represented as a paragraph $\mathcal{P}$ is an exceptional case,
    as it is merely used to decorate the verb phrase VP, indicating that the verb
    phrase is not associated with any temporal relationship, like “happen” or “occur”.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 User Query Synthesis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The DSL designed in Sec. [4.1](#S4.SS1 "4.1 Domain-Specific Language ‣ 4 Design
    ‣ Testing and Understanding Erroneous Planning in LLM Agents through Synthesized
    User Inputs") moderates natural language complexity while preserving the expressiveness
    and variety of the user queries it generates. In general, synthesizing a user
    query is to gradually expand an abstract syntax tree (AST) where each node in
    the AST is randomly selected from a set of valid nodes according to the grammar
    of the DSL. That said, the synthesis process is conducted in a top-down manner,
    with the root node being the paragraph $P$, the DSL offers a total of 340 possible
    expanding options, ensuring the diversity of the synthesized user queries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on this carefully designed DSL, the synthesis of user queries mainly
    comprises three steps: synthesizing the skeleton, translating the skeleton into
    NL (natural language) prompts, and simultaneously deriving the constraints from
    the synthesized skeleton, as illustrated in Fig. [3](#S3.F3 "Figure 3 ‣ 3 Overview
    ‣ Testing and Understanding Erroneous Planning in LLM Agents through Synthesized
    User Inputs").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: Action Set $\bm{A}=\{a_{1},a_{2},...,a_{n}\}$*'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Skeleton Synthesis.
  prefs: []
  type: TYPE_NORMAL
- en: Skeleton Synthesis. Alg. [1](#alg1 "In 4.2 User Query Synthesis ‣ 4 Design ‣
    Testing and Understanding Erroneous Planning in LLM Agents through Synthesized
    User Inputs") presents the algorithmic procedure for prompt skeleton synthesis.
    The algorithm takes the action set $\bm{A}$ is reached, at which point the generation
    process will be terminated since it indicates that finding a satisfiable sentence
    is infeasible (lines 12–17). It is worth noting that this algorithm ensures the
    generated user query must be satisfiable, as the constraints are checked after
    each sentence generation. This way, the synthesized user query can effectively
    be used for testing the LLM agent’s planning ability.
  prefs: []
  type: TYPE_NORMAL
- en: Text Filling. To “translate” the skeleton into natural language user queries,
    PDoctor fills text into the slots marked by the terminal symbols (e.g., various
    keywords and actions) in the skeleton. Except for action symbols, there are seven
    alternative natural language words on average for each terminal symbol. For example,
    “happen”, “occur”, “be executed”, etc., for the verb phrase ${\color[rgb]{.5,0,.5}\definecolor[named]{pgfstrokecolor}{rgb}{.5,0,.5}\texttt{VP}}_{\emptyset}$.
    For the action symbols, we substitute them with the daily activities of various
    jobs. More specifically, we first instruct the LLM to provide a list of common
    jobs (e.g., “teacher”, “software developer”, etc.) by querying the LLM with the
    prompt “Please provide a list of 50 typical jobs. Ensure that these 50 positions
    span a variety of industries.”. For each provided job, we further query the LLM
    with the prompt “Please list 20 activities in noun phrase format that a [role]
    may need to do in a day.”, where “[role]” is replaced with the job name. Action
    symbols in the same paragraph are substituted with daily activities from the same
    job, and the job becomes the Topic of the user query. By changing the topic, PDoctor is
    capable of smoothly altering the context of the user queries, which is beneficial
    for the error dissection in Sec. [4.5](#S4.SS5 "4.5 Error Dissection ‣ 4 Design
    ‣ Testing and Understanding Erroneous Planning in LLM Agents through Synthesized
    User Inputs"). The design of text filling facilitates the diversity and realism
    of the synthesized user queries, in contrast to the previous works that are limited
    to a few fixed, artificial scenarios or templates [[49](#bib.bib49), [42](#bib.bib42)].
  prefs: []
  type: TYPE_NORMAL
- en: Deriving Constraints. Simultaneously, PDoctor derives the constraints from each
    synthesized user query. Specifically, each action $a$.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Test Results Check
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once we feed the synthesized user query to the LLM agent, it will generate a
    textual response that describes the planning of the tasks. We can check the correctness
    of the planning by comparing the action sequence extracted from the response to
    the constraints derived from the user query. It is worth noting that some may
    argue for employing metrics like the BLEU score [[37](#bib.bib37)] and BertScore [[64](#bib.bib64)]
    to assess the similarity between the generated response and the ground truth.
    However, this is less feasible in our context because (i) these metrics may be
    biased or expensive to compute, and (ii) the effectiveness of planning testing
    should be assessed based on whether the action sequence satisfies the constraints,
    rather than on the similarity between the generated response and the ground truth.
    Our approach of comparing the action sequence to the constraints, instead, is
    a more direct and effective way to validate the LLM agent’s planning.
  prefs: []
  type: TYPE_NORMAL
- en: A new challenge arises, however, as extracting the action sequence from the
    LLM agent’s textual response is non-trivial, requiring a precise understanding
    of the LLM response meaning. Existing work typically instructs the LLM to respond
    in a structural format through the adoption of few-shot prompts [[29](#bib.bib29),
    [49](#bib.bib49)]. We argue, nevertheless, that this practice is still not ideal,
    as LLMs may fail to comply with the format requirements. For example, the LLM
    may generate a response that is not structured as expected, or the response may
    contain irrelevant information that complicates the extraction process.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5c39d0607052f7e7aadbf7a47c56fdde.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: An example illustrating our “mock test” approach to extract the action
    sequence of LLM agents. Green italic denotes the synthesized part, which describes
    the constraints for conducting a series of tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Mock Tool Design. PDoctor conducts a “mock test” approach to extracting the
    action sequence of LLM agents. Specifically, for each action $a$ is not conducted
    in the environment, and the tool merely returns a string that deceives the LLM
    agent into believing that the action has been completed. Fig. [5](#S4.F5 "Figure
    5 ‣ 4.3 Test Results Check ‣ 4 Design ‣ Testing and Understanding Erroneous Planning
    in LLM Agents through Synthesized User Inputs") illustrates an example of this
    design. For the synthesized user query presented in Fig. [5](#S4.F5 "Figure 5
    ‣ 4.3 Test Results Check ‣ 4 Design ‣ Testing and Understanding Erroneous Planning
    in LLM Agents through Synthesized User Inputs")(a), PDoctor  creates a set of
    tools corresponding to the actions in the user query, where Fig. [5](#S4.F5 "Figure
    5 ‣ 4.3 Test Results Check ‣ 4 Design ‣ Testing and Understanding Erroneous Planning
    in LLM Agents through Synthesized User Inputs")(b) depicts one of the tools. The
    network_diagnosis does not perform an actual network diagnosis in the environment.
    Instead, it merely writes the tool’s ID, “a1”, into the log file and deceives
    the LLM agent into believing that the network has been diagnosed.
  prefs: []
  type: TYPE_NORMAL
- en: Result Checking. After the agent finishes processing the synthesized user query,
    the log file is read to collect the action sequence. Take the case in Fig. [3](#S3.F3
    "Figure 3 ‣ 3 Overview ‣ Testing and Understanding Erroneous Planning in LLM Agents
    through Synthesized User Inputs") as an example, the action sequence is “[$a_{1}$)
    is unsatisfied, this planning is identified as correct. Overall, we view the integration
    of this check design with the mock tools as a novel approach to rigorously examine
    the planning of LLM agents while avoiding the complexity of understanding LLMs’
    textual response.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Extended Testing Framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Syntax
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\langle o\in\text{Object}\rangle\Coloneqq{}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\langle t\in\text{Time}\rangle\Coloneqq{}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 6: DSL extension for covering time and duration constraints.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/39bde12048e3fad71dc533c0d801c7a8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: An example illustrating the extended version of the generated test
    case, including the user query and corresponding tools. Similar to Fig. [5](#S4.F5
    "Figure 5 ‣ 4.3 Test Results Check ‣ 4 Design ‣ Testing and Understanding Erroneous
    Planning in LLM Agents through Synthesized User Inputs"), green italic denotes
    the synthesized part. Newly added instructions are highlighted in brown. Sub-figure
    (c) shows the mapping between the action ID and the action name. (d) and (e) present
    the basic constraints and the additional constraints derived from the user query,
    respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Although the aforementioned test framework is effective in evaluating the planning
    ability of LLM agents, real-world scenarios would be more complex and challenging.
    Referring to the example in Fig. [1](#S2.F1 "Figure 1 ‣ 2.2 An Example of LLM
    Agents ‣ 2 Preliminary ‣ Testing and Understanding Erroneous Planning in LLM Agents
    through Synthesized User Inputs"), the agent is expected to dynamically adjust
    its planning based on the execution results of the tools. Moreover, some tools
    may require several specific inputs, like error_code in this example, which requires
    the LLM agent to retain the state information across different tasks. Motivated
    by these observations, we extend the test framework by introducing time and tool
    duration concepts into the agent planning. For one thing, time constraints are
    widely used in real-world planning, such as daily schedules. For another, the
    introduction of time allows PDoctor to simulate the dynamic planning over global
    resources, which is a representative complex planning problem in real-world scenarios.
    In particular, the synthesized user query is extended to include new constraints
    that regulate the start or end time of given tasks as illustrated in Fig. [6](#S4.F6
    "Figure 6 ‣ 4.4 Extended Testing Framework ‣ 4 Design ‣ Testing and Understanding
    Erroneous Planning in LLM Agents through Synthesized User Inputs"). Accordingly,
    the mock tools are modified to require the start time point as the input parameter
    and return the duration of their execution. Fig. [7](#S4.F7 "Figure 7 ‣ 4.4 Extended
    Testing Framework ‣ 4 Design ‣ Testing and Understanding Erroneous Planning in
    LLM Agents through Synthesized User Inputs") illustrates an example of the extended
    version of the generated test case.
  prefs: []
  type: TYPE_NORMAL
- en: From this figure, an obvious change is the template of the user query. Several
    new instructions are added, to require the LLM agent to consider the time constraints.
    Besides, the change in the return value of the mock tool, which is presented in
    Fig. [7](#S4.F7 "Figure 7 ‣ 4.4 Extended Testing Framework ‣ 4 Design ‣ Testing
    and Understanding Erroneous Planning in LLM Agents through Synthesized User Inputs")(b),
    requires the agent to dynamically adjust its planning. Due to the absence of knowledge
    about the tool execution time, the agent may encounter a situation where the planning
    is negated by the execution result of tools. Taking the case in Fig. [7](#S4.F7
    "Figure 7 ‣ 4.4 Extended Testing Framework ‣ 4 Design ‣ Testing and Understanding
    Erroneous Planning in LLM Agents through Synthesized User Inputs") as an example,
    if the agent starts the execution of the network_diagnosis tool at 14:00, this
    planning would be negated because network_diagnosis takes 2 hours and finishes
    at 16:00, while the subsequent action network_speed_test is required to be executed
    before 15:00\. In this case, the agent is instructed to conduct an early halt
    and re-plan as shown in Fig. [7](#S4.F7 "Figure 7 ‣ 4.4 Extended Testing Framework
    ‣ 4 Design ‣ Testing and Understanding Erroneous Planning in LLM Agents through
    Synthesized User Inputs")(a).
  prefs: []
  type: TYPE_NORMAL
- en: Besides, the constraints derivation also changes. Now each action $a$, denoting
    the start and end time of the action, respectively. A new kind of constraint,
    duration constraint, is derived from the mock tools’ return value, regulating
    the duration of the associated action. Fig. [7](#S4.F7 "Figure 7 ‣ 4.4 Extended
    Testing Framework ‣ 4 Design ‣ Testing and Understanding Erroneous Planning in
    LLM Agents through Synthesized User Inputs")(d) shows the basic constraints that
    are implied by the test environment and the instructions in the user query. Fig. [7](#S4.F7
    "Figure 7 ‣ 4.4 Extended Testing Framework ‣ 4 Design ‣ Testing and Understanding
    Erroneous Planning in LLM Agents through Synthesized User Inputs")(e) presents
    the additional constraints derived from the synthesized part that is highlighted
    in green italic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: User Query $Q$* then30             return *“Structure”*31       end
    if32      else33             return *“Constraint”*34       end if35      36 end
    for37'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 Error Dissection.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Error Dissection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PDoctor also features an error dissection component designed to investigate
    the causes of erroneous planning in LLM agents. Recall that our proposed query
    synthesis technique facilitates a thorough understanding of query semantics and
    the constraints that the LLM agent should satisfy. Given an error-triggering query,
    PDoctor can deliberately mutate components in this query to dissect the triggered
    error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alg. [2](#alg2 "In 4.4 Extended Testing Framework ‣ 4 Design ‣ Testing and
    Understanding Erroneous Planning in LLM Agents through Synthesized User Inputs")
    presents the error dissection algorithm. In general, this algorithm first checks
    whether the error emerges probabilistically by running the LLM agent multiple
    times with the same user query. It then proceeds to mutate the user query from
    a low-level word substitution to high-level modification to identify the causes
    of the error. Specifically, the algorithm employs three mutation strategies: TerminalSubstitute,
    TopicChange, and QuerySynthesize. Correspondingly, there are five possible error
    causes: Probability, Terminal, Topic, Structure, and Constraint. If the error
    happens probabilistically, the error cause is Probability (lines 1–6). If the
    error is caused by specific words and can be fixed by substituting these words,
    the error cause is Terminal (lines 7–13). If the error can only be rectified by
    changing the topic of the user query, the error cause is Topic (lines 14–20).
    Furthermore, PDoctor will try to synthesize a new user query whose constraints
    are equivalent to the original one. If the error does not occur with the new query,
    the cause is Structure; otherwise, it is Constraint (lines 21–30). Constraint
    means that PDoctor  reveals a special set of constraints that the LLM agent is
    more prone to make mistakes with. Note that we do not consider dissecting multiple
    errors over a query, as this may complicate the whole process and make it hard
    to pinpoint the genuine cause of the error. For instance, if the error behaves
    probabilistically, it becomes less meaningful to dissect the error further. Similarly,
    if we have already identified specific words that cause the error (i.e., Terminal),
    then it should not belong to more holistic causes like Topic or Structure.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Implementation and Experiment Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PDoctor is implemented in Python3 with about 2,600 lines of code. We integrate
    PDoctor with Z3 [[17](#bib.bib17)], a popular constraint solver, for user query
    synthesis and mutation. In addition, all LLM agents are implemented using LangChain [[14](#bib.bib14)],
    a prevalent Python-based framework for developing LLM agents.
  prefs: []
  type: TYPE_NORMAL
- en: Models. Two widely-used LLM models, GPT-3.5 and GPT-4, are employed in our evaluation
    for two main reasons. First, acting as the agent core to conduct correct planning
    is challenging, thereby necessitating the need for powerful LLMs like the ones
    from OpenAI. Second, OpenAI’s models are the mainstream choices among the LLM
    application community, with the majority of popular libraries (e.g., LangChain)
    using them as the default. In this paper, all parameters of the LLM models are
    set to their default values except for temperature, which is set to $0$ to mitigate
    the non-determinism of LLM responses (OpenAI Assistant excludes this parameter
    because it does not support it). The versions of both models employed in this
    study are set to 1106, namely gpt-3.5-turbo-1106 and gpt-4-1106-preview.
  prefs: []
  type: TYPE_NORMAL
- en: LLM Agent Frameworks. We evaluate PDoctor on three mainstream LLM agent frameworks,
    including ReAct [[59](#bib.bib59)], OpenAI Tools (OT) [[5](#bib.bib5)], and OpenAI
    Assistant (OA) [[7](#bib.bib7)]. These LLM agents are selected to cover different
    design choices and paradigms, including different interaction modes (how the LLM
    agent interacts with the core LLM or the integrated tools), prompt template designs,
    and in-depth customizations (e.g., LLM sampling parameter tuning, memory management,
    etc.).
  prefs: []
  type: TYPE_NORMAL
- en: Table [1](#S5.T1 "Table 1 ‣ 5 Implementation and Experiment Setup ‣ Testing
    and Understanding Erroneous Planning in LLM Agents through Synthesized User Inputs")
    lists the details of the evaluated LLM agent frameworks. ReAct is the most compatible
    agent framework, as it supports any LLM that can be invoked via a completion.
    In contrast, OT and OA require the core LLM to support chat-based interaction
    and be tuned with function calling capabilities [[6](#bib.bib6)]. Both ReAct and
    OT are built with an emphasis on flexibility, allowing users to customize the
    LLM agent in-depth and use prompt templates to guide the user queries. In contrast,
    OA is designed to be more “fool-proof”, with limited customization options. This
    simple design choice has garnered considerable attention for OA among the general
    public and may signify a new trend in the LLM agent sector. To the best of our
    knowledge, the aforementioned LLM agents are the most representative ones in the
    current LLM agent landscape. It is noteworthy, nevertheless, that PDoctor is not
    limited to these LLM agent frameworks and can be applied to other LLM agents as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Details of the evaluated LLM agents.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Tool | LLM Interaction | Tool Interaction | Prompt Template? | In-depth Customization?
    |'
  prefs: []
  type: TYPE_TB
- en: '| ReAct [[59](#bib.bib59)] | Completion | Few-Shot Prompting | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| OT [[5](#bib.bib5)] | Chat | Function Calling | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| OA [[7](#bib.bib7)] | Chat | Function Calling | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: 6 Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, We aim to answer the following research questions (RQs):'
  prefs: []
  type: TYPE_NORMAL
- en: 'RQ1:'
  prefs: []
  type: TYPE_NORMAL
- en: How effective is PDoctor in detecting erroneous planning in LLM agents?
  prefs: []
  type: TYPE_NORMAL
- en: 'RQ2:'
  prefs: []
  type: TYPE_NORMAL
- en: How well do LLM agents perform planning and what kinds of planning errors do
    they make?
  prefs: []
  type: TYPE_NORMAL
- en: 'RQ3:'
  prefs: []
  type: TYPE_NORMAL
- en: How do LLM agents perform when encountering complex planning problems?
  prefs: []
  type: TYPE_NORMAL
- en: '6.1 RQ1: Assessing PDoctor’s Effectiveness in Detecting Erroneous Planning'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We first assess the effectiveness of PDoctor in detecting erroneous planning
    in LLM agents. For each type of LLM agent (six in total, two models for three
    types of LLM agent frameworks as described in Sec. [5](#S5 "5 Implementation and
    Experiment Setup ‣ Testing and Understanding Erroneous Planning in LLM Agents
    through Synthesized User Inputs")), we employ PDoctor to randomly generate test
    cases and test the agent’s planning performance within a specified time constraint.
    Since token usage increases exponentially with agent iteration, rendering the
    test of LLM agents costly, we set the time limit to 60 minutes for each type of
    LLM agent. For each generated test case, we create a new LLM agent instance and
    instruct it to process the synthesized user query with the provided tools, with
    the timeout configured to 180 seconds. The maximum iteration limit is set to 50,
    consistent with that in Yao et al.’s work [[59](#bib.bib59)].
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the difficulty level of the planning problem is a key factor that
    affects the planning performance of LLM agents. In particular, the LLM agent is
    a difficulty-sensitive system expected to perform better on easier problems and
    worse on harder problems. The rationale behind this is that the ultimate goal
    of LLM agents is to generate human-like text by imitating the logic underlying
    human behaviors, and human beings are known to be difficulty-sensitive when solving
    problems. PDoctor takes the action number, $|\mathcal{A}|$ if the tested agent
    strictly follows the user query and invokes every tool only once. Furthermore,
    the number of actions also affects the constraint set size and the generated user
    query length, as more actions require more sentences to mention them, which in
    turn increases the number of constraints. Both the user query length and the constraint
    set size contribute to the difficulty of the problem.
  prefs: []
  type: TYPE_NORMAL
- en: In this RQ, we randomly sample the value of $|\mathcal{A}|$ is appropriate for
    this experiment. Under this setting, the sub-sentence number of the synthesized
    user query varies from one to seven, with the constraint set size ranging from
    two to ten, consistent with the common scenario in real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Evaluation results of PDoctor in detecting erroneous planning in LLM
    agents. Z3-Count denotes the number of calls to the Z3 solver. Time denotes the
    total time spent on Z3/synthesis/agent.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Agent | Generated | Errors (% of total) | Z3-Count | Z3-Time | Synthesis-Time
    | Agent-Time |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 | ReAct | 843 | 519 (61.57%) | 99,972 | 00:24.96 | 00:59.13 | 58:00.86
    |'
  prefs: []
  type: TYPE_TB
- en: '| OT | 1,168 | 635 (54.37%) | 133,008 | 00:34.30 | 01:20.49 | 57:11.94 |'
  prefs: []
  type: TYPE_TB
- en: '| OA | 327 | 179 (54.74%) | 36,694 | 00:09.67 | 00:22.52 | 59:22.24 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | ReAct | 160 | 47 (29.38%) | 19,726 | 00:05.03 | 00:11.73 | 59:54.44
    |'
  prefs: []
  type: TYPE_TB
- en: '| OT | 144 | 32 (22.22%) | 16,253 | 00:04.13 | 00:09.66 | 59:40.41 |'
  prefs: []
  type: TYPE_TB
- en: '| OA | 111 | 40 (36.04%) | 12,775 | 00:03.38 | 00:07.75 | 59:54.25 |'
  prefs: []
  type: TYPE_TB
- en: 'Table [2](#S6.T2 "Table 2 ‣ 6.1 RQ1: Assessing PDoctor’s Effectiveness in Detecting
    Erroneous Planning ‣ 6 Evaluation ‣ Testing and Understanding Erroneous Planning
    in LLM Agents through Synthesized User Inputs") shows the result of the evaluation.
    The call count and total time consumed by Z3 are presented in this table to offer
    a comprehensive understanding of the overhead associated with constraint-solving
    during the synthesis procedure. Moreover, the time consumption of both the synthesis
    and agent execution process is presented. An obvious observation from the table
    is that PDoctor is highly effective in synthesizing test cases. The average time
    consumed to synthesize a test case is only around 0.07 seconds. Despite the high
    call count of Z3, the time spent on Z3 is relatively low, with the average time
    spent on Z3 falling below 0.03 seconds per test case. Additionally, less than
    half of the total time required for the synthesis process is devoted to Z3\. In
    contrast, the execution of the agent is the most time-consuming part, as each
    agent type takes over 57 minutes.'
  prefs: []
  type: TYPE_NORMAL
- en: In general, the speed bottleneck of the entire testing process lies in the agent’s
    throughput. Both model and framework have a significant impact on the performance
    of an LLM agent. Among all agents based on the GPT-3.5 model, ReAct and OT exhibit
    a substantial speed advantage over OA, with the number of processed queries being
    843 and 1,168, respectively, compared to OA’s 327\. For agents based on the GPT-4
    model, the number of processed queries substantially decreases across all three
    frameworks. Meanwhile, the gap between different frameworks narrows, indicating
    that the token process rate of GPT-4 constitutes a more severe bottleneck. However,
    it is important to note that the time overhead of the agent execution process
    is negligible in a real-world scenario, especially for ordinary users. Even in
    the worst case (OA based on GPT-4), the agent achieves an average speed of 32.4
    seconds per test case. This time would be negligible given the considerably longer
    execution times that real-world tools may require.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of error detection, PDoctor uncovers a considerable number of errors
    across all agent settings. Further characterization of the errors will be discussed
    in RQ2. From this table, it is evident that GPT-4 significantly improves the agent’s
    performance, with the error rate of all agents based on GPT-4 decreasing by 48.53%
    on average compared to those based on GPT-3.5. For GPT-3.5, we attribute the comparatively
    high error rate to its incapacity to manage the intricate nature of the planning
    problem, which will be further investigated in RQ2 and RQ3\. Regarding the agent
    framework, OT is the most recommended one, outperforming the other two frameworks
    in both models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, given that the result check process is carried out in accordance with
    the constraint check (see Sec. [4.3](#S4.SS3 "4.3 Test Results Check ‣ 4 Design
    ‣ Testing and Understanding Erroneous Planning in LLM Agents through Synthesized
    User Inputs")), it is guaranteed that any planning error made by the agent will
    be detected. Other kinds of errors, such as invoking a non-existing tool or forgetting
    to accomplish a task specified in the query, can also be easily detected by inspecting
    the execution chain of the agent. Given the high comprehensiveness of our testing
    approach, it is reasonable to conclude that the error rate reported in Table [2](#S6.T2
    "Table 2 ‣ 6.1 RQ1: Assessing PDoctor’s Effectiveness in Detecting Erroneous Planning
    ‣ 6 Evaluation ‣ Testing and Understanding Erroneous Planning in LLM Agents through
    Synthesized User Inputs") faithfully reflects LLM agents’ performance. Indeed,
    the performance of each LLM agent is consistent with that of previous studies [[59](#bib.bib59),
    [41](#bib.bib41)].'
  prefs: []
  type: TYPE_NORMAL
- en: '6.2 RQ2: Understanding LLM Agents’ Planning Performance and Their Errors'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This RQ delves into the planning and errors made by LLM agents. Recall in RQ1,
    we explain that LLM agents are generally difficulty-sensitive systems, and their
    planning performance would therefore depend on the difficulty of the encountered
    problem. Therefore, we presume that only the errors that fall within the planning
    capability of the LLM agent are valuable for further investigation. Hence, a comprehensive
    test with varying difficulties (up to the upper-level planning capability) is
    required to exhaustively explore the planning capability of LLM agents.
  prefs: []
  type: TYPE_NORMAL
- en: Planning Capability Measurement. Specifically, we employ PDoctor to synthesize
    extensive test cases, with the difficulty level (i.e., the action number) gradually
    increasing. For each difficulty level, we record the success rate of the planning.
    We consider the agent to have reached its “upper-level planning capability” when
    the success rate drops too low (less than 20% in our experiments). Since the sampling
    space of the query generation is determined by the action number, we dynamically
    adjust the sampling number to fit it. That said, for a given action number $|\mathcal{A}|=n$
    represents the number of sentences in the extreme case where the synthesized user
    query contains sentences that solely mention two actions in a single sub-sentence
    (i.e., the possible minimum number of actions in a sentence). This represents
    the maximum number of sentences that can be generated for a given action number,
    depicting the sampling space of the query generation.
  prefs: []
  type: TYPE_NORMAL
- en: In our experiments, $n$ starts from 2 and increases by 1 until the success rate
    threshold is reached. Likewise, we limit the maximum sampling number for each
    action setting to 300, considering the high cost of using OpenAI’s API. In this
    RQ, we test and present the planning performance of all agents under action numbers
    ranging from 2 to 9 to facilitate a comprehensive comparison. Hence, we conduct
    the above process for each of the six LLM agents (two models for each of the three
    agent frameworks), with 1,600 test cases generated for each agent type.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1b0bd0c57087ac793e3fd30030975738.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Planning performance of different LLM agents.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig. [8](#S6.F8 "Figure 8 ‣ 6.2 RQ2: Understanding LLM Agents’ Planning Performance
    and Their Errors ‣ 6 Evaluation ‣ Testing and Understanding Erroneous Planning
    in LLM Agents through Synthesized User Inputs")(a) and Fig. [8](#S6.F8 "Figure
    8 ‣ 6.2 RQ2: Understanding LLM Agents’ Planning Performance and Their Errors ‣
    6 Evaluation ‣ Testing and Understanding Erroneous Planning in LLM Agents through
    Synthesized User Inputs")(b) present the planning success rate of LLM agents based
    on the GPT-3.5 and GPT-4 models, respectively. At the 20% success rate, a dashed
    line is drawn to signify the planning capability limit for each agent. Aligning
    with the findings of RQ1, agents based on GPT-4 generally exhibit superior planning
    abilities compared to those based on GPT-3.5 across all agent frameworks. The
    upper bound of agents adopting GPT-4 is achieved when $|\mathcal{A}=8|$. Moreover,
    the success rate of planning for agents based on GPT-3.5 drops sharply when the
    action number exceeds three, whereas it continues to decline gradually for agents
    based on GPT-4\. This suggests that GPT-4 is more robust in handling complex planning
    problems. In terms of the agent framework, it is hard to identify the optimal
    frameworks in this setting, as each one has its own strengths. Roughly speaking,
    users are recommended to choose OT plus GPT-3.5 for simple tasks and ReAct plus
    GPT-4 for complex tasks, considering the time overhead revealed in RQ1.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: The distribution of different types of planning errors in LLM agents.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Agent | Timeout | Act Error | Action Lost | Order Error |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 | ReAct | 0.00% | 1.03% | 8.25% | 90.72% |'
  prefs: []
  type: TYPE_TB
- en: '| OT | 0.00% | 0.00% | 0.46% | 99.54% |'
  prefs: []
  type: TYPE_TB
- en: '| OA | 0.00% | 0.80% | 0.40% | 98.80% |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | ReAct | 0.00% | 1.27% | 0.00% | 98.73% |'
  prefs: []
  type: TYPE_TB
- en: '| OT | 0.00% | 1.12% | 0.00% | 98.88% |'
  prefs: []
  type: TYPE_TB
- en: '| OA | 0.00% | 1.61% | 0.13% | 98.26% |'
  prefs: []
  type: TYPE_TB
- en: 'Error Characteristics. We categorize the errors that fall within the planning
    capability limit of LLM agents into four types: (1) Timeout, where the agent spends
    too much time (over 180 seconds) or exceeds the maximal iteration limit (50) without
    reaching a solution; (2) Act Error, where the agent fails to correctly invoke
    tools; (3) Action Lost, where the agent fails to accomplish all tasks specified
    in the query, i.e., some actions are lost; (4) Order Error, where the agent fails
    to follow the constraints derived from the query. Table [3](#S6.T3 "Table 3 ‣
    6.2 RQ2: Understanding LLM Agents’ Planning Performance and Their Errors ‣ 6 Evaluation
    ‣ Testing and Understanding Erroneous Planning in LLM Agents through Synthesized
    User Inputs") presents the error type distribution for each agent. Order Error
    is the most common error type across all agents, indicating that LLM agents may
    encounter difficulties in untangling the complex constraints of the planning problems.
    This observation further confirms the difficulty-sensitive nature of LLM agents.
    In addition, ReAct based on GPT-3.5 is substantially more prone to Action Lost
    than other agents. We attribute this to the lengthy prompt template of ReAct,
    which could potentially cause the LLM model to forget some actions specified in
    the query. On GPT-4, in contrast, the Action Lost error is significantly reduced,
    benefiting from the substantial improvement in GPT-4’s long-term memory. We believe
    these findings are valuable for the design and optimization of LLM agents (e.g.,
    their accompanying prompt templates), as well as for users who are interested
    in adopting LLM agents in their applications.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: The distribution of different root causes in erroneous planning.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Agent | Probability | Terminal | Topic | Structure | Constraint |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 | ReAct | 19.0% | 50.0% | 18.0% | 9.0% | 4.0% |'
  prefs: []
  type: TYPE_TB
- en: '| OT | 8.0% | 42.0% | 22.0% | 15.0% | 13.0% |'
  prefs: []
  type: TYPE_TB
- en: '| OA | 27.0% | 28.0% | 15.0% | 18.0% | 12.0% |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | ReAct | 19.0% | 44.0% | 14.0% | 18.0% | 5.0% |'
  prefs: []
  type: TYPE_TB
- en: '| OT | 15.0% | 37.0% | 23.0% | 17.0% | 8.0% |'
  prefs: []
  type: TYPE_TB
- en: '| OA | 28.0% | 30.0% | 14.0% | 17.0% | 11.0% |'
  prefs: []
  type: TYPE_TB
- en: 'Root Cause Analysis. After identifying the planning capability limit, further
    investigation into the cause of the triggered failures is conducted. A systematic
    error dissection is performed using the algorithm proposed in Sec. [4.5](#S4.SS5
    "4.5 Error Dissection ‣ 4 Design ‣ Testing and Understanding Erroneous Planning
    in LLM Agents through Synthesized User Inputs"). Specifically, we randomly sample
    100 errors that fall within the planning capability limit and dissect them to
    identify the root cause. The error dissection results are presented in Table [4](#S6.T4
    "Table 4 ‣ 6.2 RQ2: Understanding LLM Agents’ Planning Performance and Their Errors
    ‣ 6 Evaluation ‣ Testing and Understanding Erroneous Planning in LLM Agents through
    Synthesized User Inputs"). Although the root cause distribution varies across
    different agent frameworks, it remains notably consistent across models. The high
    similarity in error patterns observed among agents based on different models indicates
    that agent frameworks play a more significant role in determining the error cause
    than the model itself. Another finding is that Probability results in notably
    more errors in OA than in other frameworks. We attribute this to the fact that
    OA cannot set the temperature of the core LLM, incurring more instability in the
    agent’s behaviors. Other frameworks, however, still fail to suppress Probability
    errors to a satisfactory level, even though the sampling parameters of the core
    LLM have been tuned before using PDoctor to mitigate non-determinism. This observation
    supports our hypothesis discussed in Sec. [2.2](#S2.SS2 "2.2 An Example of LLM
    Agents ‣ 2 Preliminary ‣ Testing and Understanding Erroneous Planning in LLM Agents
    through Synthesized User Inputs") that the multi-turn interaction mode of LLM
    agents may further exacerbate the non-determinism issue of LLMs. Besides, it is
    worth noting that Terminal is the most common root cause across all agents, while
    Constraint typically causes only a small portion of errors. In other words, altering
    the prompt through text mutation not only works well in tweaking LLM’s behavior [[22](#bib.bib22)]
    but can also be applied to improve or worsen LLM agents’ performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Top-5 topics that cause the most errors.'
  prefs: []
  type: TYPE_NORMAL
- en: '| GPT-3.5 | GPT-4 |'
  prefs: []
  type: TYPE_TB
- en: '| Topic | count | Topic | count |'
  prefs: []
  type: TYPE_TB
- en: '| Waiter/Waitress | 6 | Graphic Designer | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| Computer Programmer | 5 | Electrician | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| Physical Therapist | 4 | Human Resources Manager | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| Marketing Manager | 4 | Journalist | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| Stock Broker | 4 | Video Game Designer | 4 |'
  prefs: []
  type: TYPE_TB
- en: 'As a further step, since the Topic causes a notable number of errors, we also
    count the top-5 topics that cause the most errors and present them in Table [5](#S6.T5
    "Table 5 ‣ 6.2 RQ2: Understanding LLM Agents’ Planning Performance and Their Errors
    ‣ 6 Evaluation ‣ Testing and Understanding Erroneous Planning in LLM Agents through
    Synthesized User Inputs"). Here, we merge the results of agents based on the same
    model, as the topic of user queries may solely affect the language comprehension
    of the core LLM model. As stated in Sec. [4.2](#S4.SS2 "4.2 User Query Synthesis
    ‣ 4 Design ‣ Testing and Understanding Erroneous Planning in LLM Agents through
    Synthesized User Inputs"), PDoctor randomly picks one of 50 topics, which are
    about the daily tasks of different professions, to fill the action slots in the
    synthesized user query skeleton. It is observed that the most challenging topic
    for GPT3.5 is Waiter/Waitress, which causes 6 errors, while the remaining 19 topics
    never cause any errors. Similarly, for GPT-4, Graphic Designer’s daily life seems
    to be most challenging for the agent (causing 5 errors), while there are 16 error-free
    topics. We interpret that the topic of the user query may be likely unfamiliar
    to the agent, thereby notably affecting the agent’s performance. Yet, from the
    perspective of agent users, the role-playing instruction (currently employed by
    these agents; introduced in Sec. [2.3](#S2.SS3 "2.3 Architecture of LLM Agents
    ‣ 2 Preliminary ‣ Testing and Understanding Erroneous Planning in LLM Agents through
    Synthesized User Inputs")) seems insufficient to improve their performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '6.3 RQ3: Further Examining LLM Agents’ Performance on Complex Planning Problems'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/33110567f0381925d63ebea35271f6c8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Planning performance of different LLM agents on complex planning
    problems.'
  prefs: []
  type: TYPE_NORMAL
- en: In this RQ, we repeat the evaluation process described in RQ2 using the extended
    version of PDoctor to further examine the planning ability of LLM agents. As stated
    in Sec. [4.4](#S4.SS4 "4.4 Extended Testing Framework ‣ 4 Design ‣ Testing and
    Understanding Erroneous Planning in LLM Agents through Synthesized User Inputs"),
    the extended version introduces a more complex planning paradigm by adding time
    constraints, task duration, and tool parameters, thereby stressing the tested
    agents’ planning capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Planning Capability Measurement. We follow the same procedure as described
    in RQ2 to measure the upper bound of the LLM agents’ planning capability. Fig. [9](#S6.F9
    "Figure 9 ‣ 6.3 RQ3: Further Examining LLM Agents’ Performance on Complex Planning
    Problems ‣ 6 Evaluation ‣ Testing and Understanding Erroneous Planning in LLM
    Agents through Synthesized User Inputs") presents the planning success rate of
    LLM agents based on the GPT-3.5 and GPT-4 models in this experiment. Since the
    maximal planning capability limit of all agents is reached when $|\mathcal{A}=5|$,
    only the planning success rates for action numbers ranging from 2 to 5 are presented
    in this figure. Clearly, the success rate of all agents drops significantly in
    comparison to the previous experiment, indicating that the extended version of
    PDoctor indeed introduces a more challenging (yet still realistic) planning problem.
    All agents based on GPT-3.5 drops below the 20% success rate when the action number
    reaches three, suggesting GPT-3.5 may not be suitable for handling such an intricate
    planning problem. For agents based on GPT-4, they maintain a relatively high success
    rate, though the success rate suffers a sharp decline when the action number exceeds
    four. Regarding the agent framework, ReAct performs the poorest in this setting,
    failing to address even the simplest planning problem when it takes GPT-3.5 as
    the core. Such failure extends when GPT-4 is adopted, with the success rate remaining
    below 50% across all action numbers. In contrast, OT and OA exhibit a relatively,
    albeit limited, higher success rate. We attribute this to the fact that ReAct
    relies on few-shot prompting to instruct the model to invoke tools, and the unstable
    nature of the LLM model becomes more apparent when the task becomes more complex.
    Conversely, OT and OA adopt function calling, which is implemented by fine-tuning
    the model with a large number of function calling data, making them more robust.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: The distribution of different types of planning errors detected by
    the extended version of PDoctor.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Agent | Timeout | Act Error | Action Lost | Order Error | Parameter Error
    |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 | ReAct | NaN | NaN | NaN | NaN | NaN |'
  prefs: []
  type: TYPE_TB
- en: '| OT | 0.0% | 7.14% | 0.0% | 28.57% | 64.29% |'
  prefs: []
  type: TYPE_TB
- en: '| OA | 0.0% | 0.0% | 0.0% | 50.0% | 50.0% |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | ReAct | 0.0% | 64.44% | 0.0% | 33.33% | 2.22% |'
  prefs: []
  type: TYPE_TB
- en: '| OT | 0.0% | 3.77% | 2.83% | 66.04% | 27.36% |'
  prefs: []
  type: TYPE_TB
- en: '| OA | 0.0% | 0.74% | 1.12% | 64.31% | 33.83% |'
  prefs: []
  type: TYPE_TB
- en: 'Error Type Analysis. Table [6](#S6.T6 "Table 6 ‣ 6.3 RQ3: Further Examining
    LLM Agents’ Performance on Complex Planning Problems ‣ 6 Evaluation ‣ Testing
    and Understanding Erroneous Planning in LLM Agents through Synthesized User Inputs")
    presents the error type distribution for each agent on the extended version of
    PDoctor. The extended version requires the agent to pass the start time point
    of actions to the corresponding tools and understand the tools’ return values,
    which reveal their execution time. Therefore, we introduce a new error type, Parameter
    Error, to denote the errors caused by the agent incorrectly setting the parameters
    of the tools. In particular, the start time of one tool should be later than the
    end time of the previous tool; otherwise, the agent would be considered to be
    mismanaging the global resource — time — of the planning problem. From the table¹¹1ReAct
    based on GPT-3.5 fails to address the simplest planning problem in this setting,
    leading to a NaN error rate in this table., we observe that ReAct encounters a
    substantial number of Act Error, confirming our earlier interpretation that ReAct’s
    tool invocation mechanism is not suitable for handling complex planning problems.
    For OT and OA, the main error type is Parameter Error when they adopt GPT-3.5,
    partially explaining their poor performance in this setting. In contrast, when
    GPT-4 is adopted, the main error type for all agents remains Order Error, aligning
    with the findings in RQ2\. In sum, we show that the extended version of PDoctor indeed
    introduces a more challenging planning problem, and the agent’s performance is
    significantly affected by the model and framework. In practice, we recommend users
    to employ OT or OA plus GPT-4 to handle such complex planning problems.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: The distribution of different root causes identified by the extended
    version of PDoctor.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Probability | Terminal | Topic | Structure | Constraint |'
  prefs: []
  type: TYPE_TB
- en: '| ReAct | 8.0% | 22.0% | 11.0% | 14.0% | 45.0% |'
  prefs: []
  type: TYPE_TB
- en: '| OT | 25.0% | 32.0% | 12.0% | 8.0% | 23.0% |'
  prefs: []
  type: TYPE_TB
- en: '| OA | 23.0% | 22.0% | 11.0% | 5.0% | 39.0% | ![Refer to caption](img/72edfd35e6bed4426d7eef655b94c234.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10: An example of the erroneous planning made by an LLM agent. Actions
    are highlighted in different colors. To facilitate the understanding, we also
    present the derived constraints and the mapping between the actions and the variables
    in the constraints.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Root Cause Analysis. Similar to RQ2, we dissect the errors made by LLM agents.
    Due to the failure of GPT-3.5-based agents on the extended version of PDoctor,
    we primarily inspect the root cause for GPT-4-based agents to provide a more insightful
    analysis. Table [7](#S6.T7 "Table 7 ‣ 6.3 RQ3: Further Examining LLM Agents’ Performance
    on Complex Planning Problems ‣ 6 Evaluation ‣ Testing and Understanding Erroneous
    Planning in LLM Agents through Synthesized User Inputs") presents the root cause
    distribution for each agent. A notable difference from the results in RQ2 is that
    Constraint causes a larger portion of errors in this experiment, taking the dominant
    position in ReAct and OA, and ranking second in OT. We believe this is due to
    several reasons as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the agent is required to adjust its plan dynamically according to the
    tool return values, which substantially increases the complexity of the planning
    problem as discussed in Sec. [4.4](#S4.SS4 "4.4 Extended Testing Framework ‣ 4
    Design ‣ Testing and Understanding Erroneous Planning in LLM Agents through Synthesized
    User Inputs"). Second, the experiment results reveal that LLMs tend to make more
    errors when handling multiple different kinds of requirements simultaneously,
    which is a common scenario in real-world applications. Fig. [10](#S6.F10 "Figure
    10 ‣ 6.3 RQ3: Further Examining LLM Agents’ Performance on Complex Planning Problems
    ‣ 6 Evaluation ‣ Testing and Understanding Erroneous Planning in LLM Agents through
    Synthesized User Inputs") provides an example of the erroneous planning made by
    an LLM agent. In this example, the agent fails to pass the correct start time
    to applying_hair_color, leading to a Parameter Error. Since the previous tool
    started at 10:00 and took two hours, applying_hair_color should start after 12:00
    (i.e., $start\_time\geq 12$), but the agent mistakenly sets the start time to
    8\. Although the action chain would be correct if the agent could reorder the
    actions according to the current start time of the tools, it already violates
    the requirement that “You need to execute all tasks one by one in the correct
    order and within the time range set by the requirement” in the query (see Fig. [7](#S4.F7
    "Figure 7 ‣ 4.4 Extended Testing Framework ‣ 4 Design ‣ Testing and Understanding
    Erroneous Planning in LLM Agents through Synthesized User Inputs")). Furthermore,
    the introduction of time and duration undoubtedly adds more constraints to the
    planning problem, which may also exacerbate agents’ planning difficulties.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Alternative Approaches for Test Case Generation. Several other approaches can
    be employed to generate test cases for LLM agents, including text mutation and
    text generation based on LLM. Based on preliminary study and experiments, we find
    that these methods are much less effective than our approach. In short, while
    these methods may seem simpler to implement and lighter in weight, they are incapable
    of generating high-diversity test cases and guaranteeing the correctness of result
    verification in the meantime.
  prefs: []
  type: TYPE_NORMAL
- en: Text mutation is limited to superficial changes in the input text, which is
    hardly feasible to cover the diverse scenarios that LLM agents may encounter.
    As for text generation, a possible approach is feeding a constraint set into an
    LLM, and ask the LLM to generate a user query in natural language. This method,
    nevertheless, is problematic because of the unstable nature of LLMs. To illustrate,
    we carry out an experiment in which we feed a constraint set and the variable-action
    mapping (e.g., $\mathbf{a_{1}}$. Considering that the test effectiveness is determined
    by the consistency of the generated text with the constraints, the results suggest
    that the text generation approach is unsuitable for this task.
  prefs: []
  type: TYPE_NORMAL
- en: Threat to Validity. Our approach is based on the assumption that no inherent
    sequential dependencies exist among the actions in the planning tasks, i.e., the
    derived constraints wholly reflect the constraints among the actions. This assumption
    shall hold for most cases, but there exist scenarios where the inherent sequential
    dependencies contained by the actions are common sense and do not need to be explicitly
    specified. In such cases, our approach may fail if there is a conflict between
    the inherent dependencies and the constraints derived from the synthesized plans.
    We leave it as future work to investigate how to handle such corner cases.
  prefs: []
  type: TYPE_NORMAL
- en: Extensions. Our approach can be extended in several directions. First, we can
    explore the use of other LLMs like Claude-3 [[4](#bib.bib4)] and other agent frameworks.
    Given the rather limited performance of current state-of-the-art LLMs when being
    tested by the extended version of PDoctor, we presume it is necessary to further
    gauge the planning capabilities of further advanced LLMs. Integrating PDoctor with
    other mainstream agent frameworks and LLMs is straightforward, as PDoctor does
    not rely on specific LLM or agent framework features.
  prefs: []
  type: TYPE_NORMAL
- en: Another straightforward extension is to apply our approach to fine-tune the
    LLMs for planning tasks. The intuition is that PDoctor can generate numerous diverse
    and high-quality test cases with different complexities. More importantly, PDoctor can
    automatically identify which constraints are violated by the LLMs, and therefore,
    it provides valuable feedback to the LLMs to improve their planning capabilities.
    Such an “error-driven” fine-tuning process can be repeated iteratively to enhance
    the LLMs’ planning capabilities. knowledgeable audience may be aware that localizing
    which constraints are violated by the LLMs can be smoothly implemented by using
    the unsat_core function in the Z3 solver.
  prefs: []
  type: TYPE_NORMAL
- en: One may also question the feasibility of repairing the detected erroneous plans
    in an “on-the-fly” manner, where we use constraint solvers to generate a new plan
    that satisfies the requirements. This can be achieved by first extracting the
    constraints from an error-triggering user query encountered during daily agent
    usage (not those test query inputs synthesized by PDoctor), then using Z3 to find
    a plan that satisfies the constraints. We clarify, however, that extracting the
    constraints from arbitrary natural language queries in the wild is still in its
    infancy, despite several encouraging progresses [[29](#bib.bib29), [60](#bib.bib60)]
    have been made. An initial consideration is that the extracted constraints might
    be inconsistent or insufficient due to the unstable nature of LLMs. Furthermore,
    the intricate interaction mode — the user query, the tool’s name and description,
    and the return message, all of which may contain the constraints — may complicate
    the constraint extraction process. In contrast, previous work [[29](#bib.bib29),
    [60](#bib.bib60)] mainly focuses on testing the LLM, rather than the agent, on
    well-formed planning questions, neglecting the real-world scenario where the LLM
    agent is required to interact with the user.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Benchmarking LLM Agents. There has been a growing interest in benchmarking LLM
    agents, and several benchmarking suites have been proposed. Wu et al. [[57](#bib.bib57)]
    proposed SmartPlay, a benchmarking suite that contains multiple specialized games,
    aiming to evaluate the understanding, knowledge, and reasoning capabilities of
    LLM agents in a comprehensive manner. Trulens [[1](#bib.bib1)] is another benchmarking
    suite that is designed to evaluate the performance of LLM agents on tasks that
    require complex reasoning and planning. Besides, a series of studies have been
    conducted to gauge the LLM agent from different perspectives, including tool interaction [[20](#bib.bib20)],
    robustness against jailbreak [[63](#bib.bib63)], and safety risk awareness [[35](#bib.bib35),
    [61](#bib.bib61)].
  prefs: []
  type: TYPE_NORMAL
- en: Testing LLMs. In line with their remarkable success in various applications,
    LLMs have been emergingly tested to ensure their reliability and robustness across
    different scenarios. A recent trend in testing LLMs is to gauge their logical
    reasoning capabilities, including mathematical reasoning [[45](#bib.bib45)], causal
    inference [[24](#bib.bib24), [31](#bib.bib31)], and planning [[49](#bib.bib49),
    [50](#bib.bib50)]. Among these, the planning capability is particularly crucial
    for LLM as it constitutes the foundation for many applications, such as autonomous
    vehicles [[54](#bib.bib54), [15](#bib.bib15)], robotics [[23](#bib.bib23)], and
    any agent-based systems [[59](#bib.bib59), [39](#bib.bib39), [52](#bib.bib52)].
  prefs: []
  type: TYPE_NORMAL
- en: 9 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have proposed PDoctor, a novel and automated approach to testing and understanding
    the planning ability of LLM agents. We formulate the detection of erroneous planning
    as a constraint satisfiability problem and propose a fully automated framework
    to synthesize diverse and high-quality user inputs for testing. We evaluate PDoctor’s
    effectiveness using three mainstream agent frameworks and two powerful LLMs (GPT-3.5
    and GPT-4). The results show that PDoctor can effectively detect LLM agents’ erroneous
    planning and provide valuable insights into the mechanisms underlying the errors.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Trulens, evaluating and testing llm apps. [https://medium.com/trulens](https://medium.com/trulens).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Designing llm agent tools for due diligence in financial instruments. [https://developers.lseg.com/en/article-catalog/article/designing-llm-agent-tools-for-due-diligence-in-financial-instruments](https://developers.lseg.com/en/article-catalog/article/designing-llm-agent-tools-for-due-diligence-in-financial-instruments),
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] International planning competition. [https://www.icaps-conference.org/competitions/](https://www.icaps-conference.org/competitions/),
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Introducing the next generation of claude. [https://www.anthropic.com/news/claude-3-family](https://www.anthropic.com/news/claude-3-family),
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Openai tools. [https://python.langchain.com/docs/modules/agents/agent_types/openai_tools](https://python.langchain.com/docs/modules/agents/agent_types/openai_tools),
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Openai’s function callling. [https://platform.openai.com/docs/guides/function-calling](https://platform.openai.com/docs/guides/function-calling),
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Overview of openai’s assistant. [https://platform.openai.com/docs/assistants/overview?context=with-streaming](https://platform.openai.com/docs/assistants/overview?context=with-streaming),
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] This ai research introduces ‘rafa’: A principled artificial intelligence
    framework for autonomous llm agents with provable sample efficiency. [https://www.marktechpost.com/2023/10/24/this-ai-research-introduces-rafa-a-principled-artificial-intelligence-framework-for-autonomous-llm-agents-with-provable-sample-efficiency/](https://www.marktechpost.com/2023/10/24/this-ai-research-introduces-rafa-a-principled-artificial-intelligence-framework-for-autonomous-llm-agents-with-provable-sample-efficiency/),
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] unskript launches ai-powered infrastructure health intelligence platform
    for software teams. [https://markets.businessinsider.com/news/stocks/unskript-launches-ai-powered-infrastructure-health-intelligence-platform-for-software-teams-1032992108](https://markets.businessinsider.com/news/stocks/unskript-launches-ai-powered-infrastructure-health-intelligence-platform-for-software-teams-1032992108),
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L.,
    Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical
    report. arXiv preprint arXiv:2303.08774 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn,
    C., Fu, C., Gopalakrishnan, K., Hausman, K., et al. Do as i can, not as i say:
    Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Bills, S., Cammarata, N., Mossing, D., Tillman, H., Gao, L., Goh, G.,
    Sutskever, I., Leike, J., Wu, J., and Saunders, W. Language models can explain
    neurons in language models. [https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html)
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Carbonell, J., Etzioni, O., Gil, Y., Joseph, R., Knoblock, C., Minton,
    S., and Veloso, M. Prodigy: An integrated architecture for planning and learning.
    ACM SIGART Bulletin 2, 4 (1991), 51–55.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Chase, H. LangChain, Oct. 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Chen, Y., Veer, S., Karkus, P., and Pavone, M. Interactive joint planning
    for autonomous vehicles. IEEE Robotics and Automation Letters (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Creswell, A., Shanahan, M., and Higgins, I. Selection-inference: Exploiting
    large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712
    (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] De Moura, L., and Bjørner, N. Z3: An efficient smt solver. In International
    conference on Tools and Algorithms for the Construction and Analysis of Systems
    (2008), Springer, pp. 337–340.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Ghallab, M., Nau, D., and Traverso, P. Automated Planning: theory and
    practice. Elsevier, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Huang, J., Chen, X., Mishra, S., Zheng, H. S., Yu, A. W., Song, X., and
    Zhou, D. Large language models cannot self-correct reasoning yet. In Proc. ICLR
    (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Huang, Y., Shi, J., Li, Y., Fan, C., Wu, S., Zhang, Q., Liu, Y., Zhou,
    P., Wan, Y., Gong, N. Z., et al. Metatool benchmark for large language models:
    Deciding whether to use tools and which to use. arXiv preprint arXiv:2310.03128
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Ji, Z., Ma, P., Li, Z., and Wang, S. Benchmarking and explaining large
    language model-based code generation: A causality-centric approach. arXiv preprint
    arXiv:2310.06680 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Jiao, W., Wang, W., Huang, J.-t., Wang, X., Shi, S., and Tu, Z. Is ChatGPT
    A Good Translator? Yes With GPT-4 As The Engine. arXiv preprint arXiv:2301.08745
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Joshi, S. S., Hutchinson, S., and Tsiotras, P. Les: Locally exploitative
    sampling for robot path planning. In 2023 IEEE International Conference on Robotics
    and Automation (ICRA) (2023), IEEE, pp. 1551–1557.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Kıcıman, E., Ness, R., Sharma, A., and Tan, C. Causal reasoning and large
    language models: Opening a new frontier for causality. arXiv preprint arXiv:2305.00050
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Lanham, T., Chen, A., Radhakrishnan, A., Steiner, B., Denison, C., Hernandez,
    D., Li, D., Durmus, E., Hubinger, E., Kernion, J., Lukosiute, K., Nguyen, K.,
    Cheng, N., Joseph, N., Schiefer, N., Rausch, O., Larson, R., McCandlish, S., Kundu,
    S., Kadavath, S., Yang, S., Henighan, T., Maxwell, T., Telleen-Lawton, T., Hume,
    T., Hatfield-Dodds, Z., Kaplan, J., Brauner, J., Bowman, S. R., and Perez, E.
    Measuring faithfulness in chain-of-thought reasoning. arXiv preprint arXiv:2307.13702
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Li, Z., Wang, C., Liu, Z., Wang, H., Wang, S., and Gao, C. CCTEST: Testing
    and repairing code completion systems. In Proc. ACM ICSE (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Li, Z., Wang, C., Ma, P., Wu, D., Li, T., Wang, S., Gao, C., and Liu,
    Y. Split and merge: Aligning position biases in large language model based evaluators.
    arXiv preprint arXiv:2310.01432 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Lin, Z., Gou, Z., Liang, T., Luo, R., Liu, H., and Yang, Y. CriticBench:
    Benchmarking LLMs for Critique-Correct Reasoning. arXiv preprint arXiv:2402.14809
    (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Liu, B., Jiang, Y., Zhang, X., Liu, Q., Zhang, S., Biswas, J., and Stone,
    P. Llm+ p: Empowering large language models with optimal planning proficiency.
    arXiv preprint arXiv:2304.11477 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Liu, S., Lu, N., Chen, C., and Tang, K. Efficient combinatorial optimization
    for word-level adversarial textual attack. IEEE/ACM Transactions on Audio, Speech,
    and Language Processing 30 (2021), 98–111.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Long, S., Schuster, T., Piché, A., de Montreal, U., Research, S., et al.
    Can large language models build causal graphs? arXiv preprint arXiv:2303.05279
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Lu, Y., Zhang, X., Xu, X., and Yao, W. Learning-based near-optimal motion
    planning for intelligent vehicles with uncertain dynamics. IEEE Robotics and Automation
    Letters (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Ma, W., Wu, D., Sun, Y., Wang, T., Liu, S., Zhang, J., Xue, Y., and Liu,
    Y. Combining fine-tuning and LLM-based agents for intuitive smart contract auditing
    with justifications. arXiv preprint arXiv:2403.16073 (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] McCarthy, J., et al. Situations, actions, and causal laws. Comtex Scientific,
    1963.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Naihin, S., Atkinson, D., Green, M., Hamadi, M., Swift, C., Schonholtz,
    D., Kalai, A. T., and Bau, D. Testing language model agents safely in the wild.
    arXiv preprint arXiv:2311.10538 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Nilsson, N. J., et al. Shakey the robot, vol. 323. Sri International Menlo
    Park, California, 1984.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. Bleu: a method for
    automatic evaluation of machine translation. In Proceedings of the 40th annual
    meeting of the Association for Computational Linguistics (2002), pp. 311–318.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Payandeh, A., Pluth, D., Hosier, J., Xiao, X., and Gurbani, V. K. How
    susceptible are LLMs to logical fallacies? arXiv preprint arXiv:2308.09853 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Shao, Y., Li, L., Dai, J., and Qiu, X. Character-llm: A trainable agent
    for role-playing. arXiv preprint arXiv:2310.10158 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Shen, Y., Song, K., Tan, X., Li, D., Lu, W., and Zhuang, Y. Hugginggpt:
    Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural
    Information Processing Systems 36 (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., and Yao, S. Reflexion:
    Language agents with verbal reinforcement learning. Advances in Neural Information
    Processing Systems 36 (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Shridhar, M., Yuan, X., Côté, M.-A., Bisk, Y., Trischler, A., and Hausknecht,
    M. Alfworld: Aligning text and embodied environments for interactive learning.
    arXiv preprint arXiv:2010.03768 (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Significant Gravitas. AutoGPT.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Singhal, K., Azizi, S., Tu, T., Mahdavi, S. S., Wei, J., Chung, H. W.,
    Scales, N., Tanwani, A., Cole-Lewis, H., Pfohl, S., et al. Large language models
    encode clinical knowledge. Nature (2023), 1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Stolfo, A., Jin, Z., Shridhar, K., Schölkopf, B., and Sachan, M. A causal
    framework to quantify the robustness of mathematical reasoning with language models.
    arXiv preprint arXiv:2210.12023 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Sun, Y., Wu, D., Xue, Y., Liu, H., Ma, W., Zhang, L., Shi, M., and Liu,
    Y. LLM4Vuln: A unified evaluation framework for decoupling and enhancing llms’
    vulnerability reasoning. arXiv preprint arXiv:2401.16185 (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Sun, Y., Wu, D., Xue, Y., Liu, H., Wang, H., Xu, Z., Xie, X., and Liu,
    Y. GPTScan: Detecting logic vulnerabilities in smart contracts by combining GPT
    with program analysis. In Proc. ACM ICSE (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Sun, Z., Zhang, J. M., Harman, M., Papadakis, M., and Zhang, L. Automatic
    testing and improvement of machine translation. In Proceedings of the ACM/IEEE
    42nd International Conference on Software Engineering (2020), pp. 974–985.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Valmeekam, K., Marquez, M., Olmo, A., Sreedharan, S., and Kambhampati,
    S. PlanBench: An extensible benchmark for evaluating large language models on
    planning and reasoning about change. Advances in Neural Information Processing
    Systems 36 (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Valmeekam, K., Marquez, M., Sreedharan, S., and Kambhampati, S. On the
    planning abilities of large language models-a critical investigation. Advances
    in Neural Information Processing Systems 36 (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Wang, S., Wei, Z., Choi, Y., and Ren, X. Can LLMs Reason with Rules? Logic
    Scaffolding for Stress-Testing and Improving LLMs. arXiv preprint arXiv:2402.11442
    (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Wang, Z., Cai, S., Chen, G., Liu, A., Ma, X. S., and Liang, Y. Describe,
    explain, plan and select: interactive planning with llms enables open-world multi-task
    agents. Advances in Neural Information Processing Systems 36 (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Wei, A., Haghtalab, N., and Steinhardt, J. Jailbroken: How does LLM safety
    training fail? In Proc. NeurIPS (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Weng, L. LLM Powered Autonomous Agents. [https://lilianweng.github.io/posts/2023-06-23-agent/](https://lilianweng.github.io/posts/2023-06-23-agent/),
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Wong, W. K., Wang, H., Li, Z., Liu, Z., Wang, S., Tang, Q., Nie, S., and
    Wu, S. Refining decompiled c code with large language models. arXiv preprint arXiv:2310.06530
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Wu, X., Li, Y.-L., Sun, J., and Lu, C. Symbol-llm: Leverage language models
    for symbolic system in visual human activity reasoning. Advances in Neural Information
    Processing Systems 36 (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Wu, Y., Tang, X., Mitchell, T. M., and Li, Y. Smartplay: A benchmark for
    llms as intelligent agents. arXiv preprint arXiv:2310.01557 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Xu, Z., Jain, S., and Kankanhalli, M. Hallucination is inevitable: An
    innate limitation of large language models. arXiv preprint arXiv:2401.11817 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao,
    Y. React: Synergizing reasoning and acting in language models. arXiv preprint
    arXiv:2210.03629 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Ye, X., Chen, Q., Dillig, I., and Durrett, G. Satlm: Satisfiability-aided
    language models using declarative prompting. Advances in Neural Information Processing
    Systems 36 (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Yuan, T., He, Z., Dong, L., Wang, Y., Zhao, R., Xia, T., Xu, L., Zhou,
    B., Li, F., Zhang, Z., et al. R-judge: Benchmarking safety risk awareness for
    llm agents. arXiv preprint arXiv:2401.10019 (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Zeng, Z., Yu, J., Gao, T., Meng, Y., Goyal, T., and Chen, D. Evaluating
    large language models at evaluating instruction following. arXiv preprint arXiv:2310.07641
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Zhan, Q., Liang, Z., Ying, Z., and Kang, D. Injecagent: Benchmarking indirect
    prompt injections in tool-integrated large language model agents. arXiv preprint
    arXiv:2403.02691 (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., and Artzi, Y. Bertscore:
    Evaluating text generation with bert. arXiv preprint arXiv:1904.09675 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
