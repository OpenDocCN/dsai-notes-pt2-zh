- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:52:34'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered
    Applications'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.09015](https://ar5iv.labs.arxiv.org/html/2402.09015)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Negar Arabzadeh¹  Julia Kiseleva²  Qingyun Wu³  Chi Wang²
  prefs: []
  type: TYPE_NORMAL
- en: Ahmed Awadallah²  Victor Dibia²  Adam Fourney²  Charles Clarke¹
  prefs: []
  type: TYPE_NORMAL
- en: ¹Univerity of Waterloo
  prefs: []
  type: TYPE_NORMAL
- en: ²Microsoft Research
  prefs: []
  type: TYPE_NORMAL
- en: ³Pennsylvania State University __Work done during an internship at Microsoft
    Research
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The rapid development in the field of Large Language Models (LLMs) has led to
    a surge in applications that facilitate collaboration among multiple agents to
    assist humans in their daily tasks. However, a significant gap remains in assessing
    whether LLM-powered applications genuinely enhance user experience and task execution
    efficiency. This highlights the pressing need for methods to verify utility of
    LLM-powered applications, particularly by ensuring alignment between the application’s
    functionality and end-user needs. We introduce AgentEval¹¹1[https://github.com/microsoft/autogen/blob/main/notebook/agenteval_cq_math.ipynb](https://github.com/microsoft/autogen/blob/main/notebook/agenteval_cq_math.ipynb)
    provides an implementation for the math problems, a novel framework designed to
    simplify the utility verification process by automatically proposing a set of
    criteria tailored to the unique purpose of any given application. This allows
    for a comprehensive assessment, quantifying the utility of an application against
    the suggested criteria. We present a comprehensive analysis of the robustness
    AgentEval for two open source datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The rapid development of open-source libraries Wu et al. ([2023](#bib.bib47));
    Li et al. ([2023a](#bib.bib23)) that aims to simplify the development of LLM-powered
    agentic solutions for various user-enteric tasks has led to the rapid growth of
    such applications Liang et al. ([2023b](#bib.bib28)); Hong et al. ([2023](#bib.bib14));
    Talebirad and Nadiri ([2023](#bib.bib39)). One of the long-lasting goals Winograd
    ([1972](#bib.bib46)) is the ability to seamlessly interact with humans in natural
    language to help end-users and to make their lives easier by assisting with their
    tasks from math tutoring to completing household tasks and so on. End users have
    expectations and requirements for a developed application that need to be met.
    This understanding is essential to assess the *utility* it brings and, consequently,
    to further improve and align the application towards end-users’ goals.
  prefs: []
  type: TYPE_NORMAL
- en: Directly evaluating agentic systems poses challenges as current approaches predominantly
    rely on end-to-end success metrics – essentially, whether the agent accomplishes
    tasks Shridhar et al. ([2020b](#bib.bib38), [2019](#bib.bib36)); Myers et al.
    ([2023](#bib.bib33)). However, comprehending user interaction with an application
    involves much more than success alone Kiseleva et al. ([2022a](#bib.bib19), [b](#bib.bib20));
    Zhang et al. ([2023](#bib.bib49)). Take math problems, for instance; it is not
    merely about the agent solving the problem. Equally significant is its ability
    to present solutions based on various criteria, including completeness, conciseness,
    and clarity of the explanation provided. In other words, in a code completion
    scenario, even an incomplete code suggestion can be useful when it provides substantial
    boilerplate code or proposes a framework to solve a task Dibia et al. ([2023](#bib.bib9)).
    Furthermore, success is not always clearly defined for every task. Knowing such
    criteria for an LLM-powered application and being able to quantify them is essential
    to verify whether user requirements are being satisfied, in other words, if the
    application brings utility to the end-users. Given the objective of verifying
    arbitrary applications, reliance on a benchmarking approach is untenable due to
    the expansive range of tasks requiring automation. A prerequisite is a scalable
    and flexible methodology capable of accommodating a diverse set of applications.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b2a36eab2e1087b1d769cdcc5d576632.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: An overview of the *AgentEval* framework consists of two main components:
    (C) *CriticAgent*, which learns a list of $n$)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this work, we aim to introduce the AgentEval framework, a tool crafted to
    swiftly gauge the utility of LLM-powered agentic applications designed to help
    end-users accomplish their desired tasks. The goal of AgentEval is to assess the
    current alignment between application behavior and user goals, providing application
    developers with insights into how and what aspects of the current flow can be
    improved. AgentEval takes into account recent discoveries that have shown LLMs
    to emerge as a scalable and cost-effective alternative to human evaluations for
    open-ended tasks Li et al. ([2023b](#bib.bib24)). The *AgentEval* is illustrated
    in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Towards better Human-Agent Alignment:
    Assessing Task Utility in LLM-Powered Applications"), which consists of two main
    agents executed consecutively. These agents are customizable, conversable, and
    can operate in various modes that employ combinations of LLMs, human inputs, and
    tools Wu et al. ([2023](#bib.bib47))²²2[https://github.com/microsoft/autogen](https://github.com/microsoft/autogen):'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*CriticAgent* suggests the list of criteria based on the task description and
    the suggested solutions, e.g. for math problems can be *Efficiency* of the proposed
    solution and *Clarity* of the proposed solution;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*QuantifierAgent* verifies how well the solution $s$ are performing for each
    criterion and returns the utility function, e.g. what is the Clarity level of
    the solution, not clear, moderately clear or very clear.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We believe that the usage of *AgentEval* can extend beyond the immediate verification
    of the current performance of LLM-powered applications. The framework can be employed
    over time to uncover new capabilities of the system and potential changes in task
    utility for the end user. The discovered utility function can be utilized to optimize
    the system towards user needs or system developer requirements, and this optimization
    can occur over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, our main contributions are:'
  prefs: []
  type: TYPE_NORMAL
- en: C1
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A definition of task utility that enables access to the possible requirements
    an end-user may have regarding an LLM-powered application and how well the application
    satisfies this list of criteria;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'An introduction of *AgentEval*, a novel framework that leverages LLM-powered
    agents as a scalable and cost-effective alternative to human evaluations to produce
    task utility through the collaboration of two agents: *CriticAgent* proposes a
    list of criteria based on the task description and the successful and failed execution
    of the agent, and *QuantifierAgent* assesses how well the list of criteria is
    supported by the current implementation of an application;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An in-depth analysis of the *AgentEval* robustness on various tasks and datasets
    across different solutions that can be replicated for a newly unseen domain.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The remainder of this paper is organized as follows. Section [2](#S2 "2 Related
    Work ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered
    Applications") describes earlier work and background. We provide the motivation
    behind AgentEval and define the utility of the task in Section [3](#S3 "3 Defining
    Task Utility ‣ Towards better Human-Agent Alignment: Assessing Task Utility in
    LLM-Powered Applications"). Section [4](#S4 "4 Datasets and Solutions ‣ Towards
    better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications")
    gives an overview of the datasets, namely MATH Hendrycks et al. ([2021b](#bib.bib13))
    and ALFWorld Shridhar et al. ([2020b](#bib.bib38)), and the solutions for building
    applications powered by LLM utilized in our work. Section [5](#S5 "5 AgentEval
    Workflow ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered
    Applications") demonstrates our findings about applying AgentEval to assess the
    utility of the task for the selected datasets. Section [6](#S6 "6 AgentEval Robustness
    Analysis and In-depth Discussion ‣ Towards better Human-Agent Alignment: Assessing
    Task Utility in LLM-Powered Applications") presents an in-depth analysis of the
    robustness of AgentEval, namely *CriticAgent* robustness (Section [6.1](#S6.SS1
    "6.1 Task-based vs Solution-based criteria ‣ 6 AgentEval Robustness Analysis and
    In-depth Discussion ‣ Towards better Human-Agent Alignment: Assessing Task Utility
    in LLM-Powered Applications")), *QuantifierAgent* robustness (Section [6.2](#S6.SS2
    "6.2 Quantifier Agent Robustness ‣ 6 AgentEval Robustness Analysis and In-depth
    Discussion ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered
    Applications")) and an automatic verification of *QuantifierAgent* (Section [6.3](#S6.SS3
    "6.3 QuantifierAgent Verification ‣ 6 AgentEval Robustness Analysis and In-depth
    Discussion ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered
    Applications")).'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We build on the streams of prior work. First, we will discuss the list of benchmarks
    and approaches to evaluate general LLMs in general (Section [2.1](#S2.SS1 "2.1
    LLM evaluation ‣ 2 Related Work ‣ Towards better Human-Agent Alignment: Assessing
    Task Utility in LLM-Powered Applications")). Second, we will present approaches
    to understand and predict user utility functions (Section [2.2](#S2.SS2 "2.2 User
    satisfaction prediction ‣ 2 Related Work ‣ Towards better Human-Agent Alignment:
    Assessing Task Utility in LLM-Powered Applications")). Third, we will go over
    the current tendencies in using LLMs as evaluators in Section [2.3](#S2.SS3 "2.3
    Using LLMs as evaluators ‣ 2 Related Work ‣ Towards better Human-Agent Alignment:
    Assessing Task Utility in LLM-Powered Applications").'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 LLM evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There exists a significant volume of literature dedicated to assessing Language
    Model Models (LLMs), as evidenced by extensive research efforts Guo et al. ([2023](#bib.bib11));
    Ziyu et al. ([2023](#bib.bib50)); Chang et al. ([2023](#bib.bib6)); Liang et al.
    ([2023a](#bib.bib27)). LLMs have been evaluated from various aspects including,
    but not limited to, specialized LLMs Jin et al. ([2019](#bib.bib16)), ethics and
    morality Hendrycks et al. ([2021a](#bib.bib12)), safety and robustness Wang et al.
    ([2023](#bib.bib42)), and knowledge and reasoning Bian et al. ([2023](#bib.bib5)).
    Additionally, recent developments include the introduction of intricate multi-modal
    benchmark datasets Mialon et al. ([2023](#bib.bib32)); Bang et al. ([2023](#bib.bib3)).
    Furthermore, there are attempts to evaluate LLMs as agents Liu et al. ([2023](#bib.bib30)).
  prefs: []
  type: TYPE_NORMAL
- en: However, there is a lack of literature that focuses on the holistic verification
    of the utility of LLMs for end-users in solving their tasks, which we tackle in
    this work.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 User satisfaction prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recent studies suggest that users interacting with various systems operate with
    specific utility functions in mind Li et al. ([2020](#bib.bib25)); Azzopardi et al.
    ([2018](#bib.bib2)); Ahmadvand et al. ([2022](#bib.bib1)). Traditionally, metrics
    defining user satisfaction was built based on various at scale collected behavioral
    signals Kiseleva et al. ([2014](#bib.bib17)) and they were tailored to specific
    applications, such as intelligent assistants Kiseleva et al. ([2016a](#bib.bib21),
    [b](#bib.bib22)), web search engines Williams et al. ([2016a](#bib.bib43), [b](#bib.bib44));
    Williams and Zitouni ([2017](#bib.bib45)), dialogue systems See et al. ([2019](#bib.bib35)),
    multi-turn conversations Li et al. ([2021](#bib.bib26)) and general-purpose personal
    assistants Kiseleva and de Rijke ([2017](#bib.bib18)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Using LLMs as evaluators
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Moreover, there is a growing trend in utilizing LLMs as evaluates Chiang and
    Lee ([2023](#bib.bib7)); Fu et al. ([2023](#bib.bib10)) for qualitative research Bano
    et al. ([2023](#bib.bib4)) and adopting LLMs as proxies for human behavior Tjuatja
    et al. ([2023](#bib.bib40)); Liu and Sun ([2023](#bib.bib29)). Jain et al. ([2023](#bib.bib15))
    studied efficacy of in-context learning based evaluators in evaluating zero-shot
    summaries written by LLMs. Notably, CoEval Li et al. ([2023b](#bib.bib24)) has
    recently demonstrated the synergy between human evaluation and LLMs in establishing
    evaluation criteria and conducting multi-dimensional evaluations for openended
    NLG tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Building on top of these works, we propose a framework capable of assessing
    the utility of various LLM-powered applications at scale. This framework aims
    to align agentic systems with human preferences.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Defining Task Utility
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7da368aaae702bce8086abd474004d88.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The taxonomy of task assessments based on optimal solutions existence'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to begin by considering the categories of tasks around which
    we focus LLM-powered applications. Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered
    Applications") outlines a taxonomy of target tasks for agentic systems in terms
    of success metrics. At the top level, tasks can be split into two main categories,
    where:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Success is not clearly defined* — For these tasks, users utilize a system
    in an assistive manner, seeking suggestions rather than expecting the system to
    solve the task end-to-end. For example, a user might request the system to generate
    an email based on some user input. In many cases, this generated content serves
    as a template that the user will later edit. However, defining success precisely
    for such tasks is less well-defined. In case of online evaluation, while being
    expensive, we can ask users to what extend the the assistance was helpful. Although
    quantifying to what extent the help was useful is still challenging on its own,
    the problem becomes increasingly challenging when it comes to offline evaluation,
    or evaluation in novel scenarios before we have users.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Success is clearly defined* — For these tasks, we can clearly determine whether
    a system solved the task or not. Consider agents that assist in accomplishing
    household tasks, where the definition of success is clear and measurable.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This second category can be further divided into two subcategories:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Success is clearly defined and an optimal solution exits* — For these tasks,
    only one solution is possible. For example, if you ask your assistant to turn
    on the light, the success of this task is clearly defined, and there is only one
    way to accomplish it.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Success is clearly defined and multiple solutions exist* — increasingly, we
    observe situations where multiple trajectories of agent behavior can lead to either
    success or failure. In such cases, it is crucial to differentiate between the
    various successful and unsuccessful outcomes. For example, when you ask the agent
    to suggest a food recipe or tell you a joke, you may define success as the food
    tasting good or the joke being funny, but perhaps the recipe should not be too
    expensive to prepare, and the joke should not be offensive.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In our AgentEval framework, we are currently focused on tasks where success
    is clearly defined and multiple successful solutions may exist.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our previous research on assistive agents suggested that the most optimal way
    to obtain human judgments is to present humans with two agents side by side and
    ask for preferences Kiseleva et al. ([2022b](#bib.bib20)). In this setup of pairwise
    comparison, humans can develop a list criteria to explain why they prefer the
    behavior of one agent over another. For instance, ‘the first agent was faster
    in execution’ or ‘the second agent moves more naturally’. Therefore, the comparative
    nature guided humans to come up with a list of criteria that helps to infer the
    utility of the task. With this idea in mind, we designed AgentEval (shown in Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Towards better Human-Agent Alignment: Assessing Task
    Utility in LLM-Powered Applications")), where we employ LLMs to help us understand,
    verify, and assess task utility for the multi-agent system. The AgentEval Framework
    employs two types of agents, namely:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The goal of a *CriticAgent* is to suggest a list of criteria that can be used
    to assess task utility for end users. The critic is given a task description as
    well as a list of few successful and failed examples of the task execution; then
    it is able to return a list of criteria: $C=\{c_{1},\dots,c_{n}\}$. For example,
    The *CriticAgent* generated the criteria such as Clarity, Efficiency, and more
    as described in the Tab. [1](#S4.T1 "Table 1 ‣ 4 Datasets and Solutions ‣ Towards
    better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications")
    for solving math problems. Each of the Criterion would be accompanied with a set
    of accepted values as shown in this Table as an example.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The goal of *QuantifierAgent* is to quantify each of the suggested criteria
    to access the task utility $U_{t}$. For example, for one sample of math problem
    solving, and given the generated criteria shown in Tab. [1](#S4.T1 "Table 1 ‣
    4 Datasets and Solutions ‣ Towards better Human-Agent Alignment: Assessing Task
    Utility in LLM-Powered Applications") the solution’s Accuracy could be quantified
    as “Incorrect”, “partially correct” or “correct”. Eligible quantified values for
    quantification process are shown in “Accepted values” column in Tab. [1](#S4.T1
    "Table 1 ‣ 4 Datasets and Solutions ‣ Towards better Human-Agent Alignment: Assessing
    Task Utility in LLM-Powered Applications")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next we will discuss the datasets and baselines we use to test the work of AgentEval.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Datasets and Solutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table 1: Verification Criteria for MathProblems'
  prefs: []
  type: TYPE_NORMAL
- en: '| Criteria | Description | Accepted Values |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Clarity | The ease of understanding the steps, explanations, and language
    used in the solution. | – Not Clear (0) – Moderately Clear (1) – Very Clear (2)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Efficiency | The use of optimal methods or approaches to solve the math problem.
    | – Inefficient (0) – Moderately Efficient (1) – Efficient (2) |'
  prefs: []
  type: TYPE_TB
- en: '| Error Analysis | The identification and description of possible errors or
    misconceptions in the math problem-solving process. | – Not Addressed (0) – Partially
    Addressed (1) – Well Addressed (2) |'
  prefs: []
  type: TYPE_TB
- en: '| Completeness | Quality of code in terms of efficiency and elegance | – Incomplete
    (0) – Mostly Complete (1) – Complete (2) |'
  prefs: []
  type: TYPE_TB
- en: 'This section provides an overview of the datasets utilized in our study. Our
    selection encompasses a variety of datasets, from those based on real-world problems
    to their simulations and beyond. The Mathematics Dataset (Section [4.1](#S4.SS1
    "4.1 MATH Problem Solving ‣ 4 Datasets and Solutions ‣ Towards better Human-Agent
    Alignment: Assessing Task Utility in LLM-Powered Applications")) is chosen for
    its widespread usage and comprehensive understanding in the field. It represents
    complex problem-solving scenarios that are fundamental in evaluating the effectiveness
    of multi-agent systems. AlfWorld (Section [4.2](#S4.SS2 "4.2 ALFWorld Household
    Task ‣ 4 Datasets and Solutions ‣ Towards better Human-Agent Alignment: Assessing
    Task Utility in LLM-Powered Applications")) offers a scenario involving multi-turn
    interactions within a moderately approximated multi-modal environment. This dataset
    is instrumental in assessing agents’ performance in interactive and dynamic settings.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each dataset plays a critical role in evaluating different aspects of AgentEval’s
    capabilities, from handling complex theoretical problems to navigating real-world
    scenarios. In both tasks, although success is clearly defined, multiple solutions
    exist for accomplishing the objectives. For example, when solving a math problem,
    there are various approaches one can take. Similarly, in the Alfworld dataset,
    which involves household tasks, there are multiple ways to complete them based
    on how you search for objects and the thinking strategies you employ, among other
    factors. An example of Math problem solving and AlfWorld task are shown in Appendix [A.1](#A1.SS1
    "A.1 Task Examples ‣ Appendix A Appendix ‣ Towards better Human-Agent Alignment:
    Assessing Task Utility in LLM-Powered Applications").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b0ec45e4a6123d9f0a151d4bcb5bcf70.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: (a) AgentEval assessment of three different solutions on math problem
    solving task categorized (b) Same assessment categorized by success and failed
    cases'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 MATH Problem Solving
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The MATH dataset, originally is a substantial collection of 12,500 challenging
    mathematics problems from high school competitions Hendrycks et al. ([2021b](#bib.bib13)).
    Each problem comes with a step-by-step solution, enabling models to learn how
    to generate both derivations and explanations. The dataset covers a wide range
    of mathematical subjects and is tagged by difficulty levels, offering a nuanced
    measure of model performance across various aspects of mathematical problem-solving.
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset is particularly suitable for testing multi-agent systems for several
    reason including: (i) The problems in the MATH dataset are not simple computations
    but require a deep understanding of mathematical concepts, heuristics, and problem-solving
    strategies. (ii) Since the dataset includes step-by-step solutions, it allows
    for the assessment of an agent’s ability to learn and reason through a problem,
    not just its ability to arrive at the correct answer. (iii) The variety of subjects
    and difficulty levels in the MATH dataset enables a comprehensive evaluation of
    a system’s versatility and adaptability in different mathematical domains which
    is crucial for multi-agent systems that are expected to operate across a range
    of scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the math problem experimental setup as in Wu et al. ([2023](#bib.bib47)),
    we carry out two experimental evaluations which involves 120 problems from level-5,
    the most challenging category, and includes 20 problems each from six different
    categories, of number theory, counting and probability, prealgebra, algebra, intermediate
    algebra, and precalculus.
  prefs: []
  type: TYPE_NORMAL
- en: 'Solutions: In establishing a solution for this task, we draw inspiration from
    the experiments showcased in Wu et al. ([2023](#bib.bib47)). We evaluate the proposed
    methodology by AutoGen Wu et al. ([2023](#bib.bib47)), as well as Langchain ReAct
    ³³3[https://python.langchain.com/en/latest/index.html](https://python.langchain.com/en/latest/index.html)
    and a Vanilla solver that employs gpt-4 to tackle the task. These solutions approaches
    have previously demonstrated promising performance in solving mathematical problems,
    particularly on the dataset at hand Wu et al. ([2023](#bib.bib47)). We assess
    and compare the performance of these three solutions using AgentEval. Fig. [9](#A1.F9
    "Figure 9 ‣ A.1 Task Examples ‣ Appendix A Appendix ‣ Towards better Human-Agent
    Alignment: Assessing Task Utility in LLM-Powered Applications") displays an example
    of one math problem from prealgebra category as well as the solution created by
    AutoGen. In Section [5.1](#S5.SS1 "5.1 AgentEval for Math Problems ‣ 5 AgentEval
    Workflow ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered
    Applications"), we delve into how AgentEval would perform on math problem solving
    tasks and how the measured performance with AgentEval correlates with the ground
    truths.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 ALFWorld Household Task
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'ALFWorld, presents a set of language-based interactive decision-making tasks
    within simulated household environments Shridhar et al. ([2020b](#bib.bib38)).
    This benchmark is distinguished by its diversity of tasks, offering a comprehensive
    platform for testing AI and multi-agent systems. This benchmark is particularly
    suited for such evaluations because first, ALFWorld is the first interactive parallel
    environment that aligns text descriptions and commands with physically embodied
    robotic simulation. It extends two prior works: TextWorld, an engine for interactive
    text-based games, and ALFRED, a large-scale dataset for vision-language instruction
    following in embodied environments Shridhar et al. ([2020a](#bib.bib37)); Côté
    et al. ([2019](#bib.bib8)). The cross-modality framework of this benchmark allows
    for a variety of embodied tasks with corresponding text-based counterparts, allowing
    agents to be trained and evaluated in both the language and embodied world. In
    addition, ALFWorld supports the development of agents that can reason both abstractly
    and execute actions concretely, mimicking human-like decision-making processes
    in varying contexts. Finally, the dataset’s inclusion of a wide range of tasks,
    from household chores to more intricate problem-solving scenarios, provides a
    comprehensive testbed for evaluating the adaptability and problem-solving capabilities
    of AI and multi-agent systems. In general, the dataset allows agents to explore,
    interact, and learn in an abstract language environment before dealing with the
    complexities of an embodied environment.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Solutions: As for the solutions to solve ALFWorld Household tasks, similar
    to Wu et al. ([2023](#bib.bib47)), we consider ReAct Yao et al. ([2022](#bib.bib48))
    as well as AutoGen with two agents and AutoGen with three agents Wu et al. ([2023](#bib.bib47)).
    ReAct is an agent that operates within the ALFWorld environments and is responsible
    for suggesting plans and executing actions. On the other hand, AutoGen Two-Agent
    System consists of an LLM-backed assistant agent responsible for suggesting plans,
    and an executor agent responsible for executing actions in the ALFWorld environments.
    Both ReAct and this solution occasionally struggles with leveraging basic commonsense
    knowledge about the physical world, which can lead to repetitive errors and getting
    stuck in loops. In AutoGen with three agents, a grounding agent is provided just
    for the sake of critical common sense knowledge whenever the system exhibits early
    signs of recurring errors. We assess and compare the performance of these three
    solutions using AgentEval. Fig. [10](#A1.F10 "Figure 10 ‣ A.1 Task Examples ‣
    Appendix A Appendix ‣ Towards better Human-Agent Alignment: Assessing Task Utility
    in LLM-Powered Applications") displays a portion of an example of AlfWorld Householding
    task solved by AutoGen.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 AgentEval Workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This Section outlines the workflow of the AgentEval illustrated in Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Towards better Human-Agent Alignment: Assessing Task
    Utility in LLM-Powered Applications"). Next, we will demonstrate how AgentEval
    works based on 3 different datasets: Math Problems (Sec. [4.1](#S4.SS1 "4.1 MATH
    Problem Solving ‣ 4 Datasets and Solutions ‣ Towards better Human-Agent Alignment:
    Assessing Task Utility in LLM-Powered Applications")) and AlfWorld (Sec. [4.2](#S4.SS2
    "4.2 ALFWorld Household Task ‣ 4 Datasets and Solutions ‣ Towards better Human-Agent
    Alignment: Assessing Task Utility in LLM-Powered Applications")).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 AgentEval for Math Problems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Critic and Quantifier Findings
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'After executing the CriticAgent, we have obtained a set of criteria for validating
    the results of the mathematical problem presented in Table [1](#S4.T1 "Table 1
    ‣ 4 Datasets and Solutions ‣ Towards better Human-Agent Alignment: Assessing Task
    Utility in LLM-Powered Applications"). Subsequently, the *QuantifierAgent* is
    tasked with quantifying each criterion based on accepted values. In Figure [3](#S4.F3
    "Figure 3 ‣ 4 Datasets and Solutions ‣ Towards better Human-Agent Alignment: Assessing
    Task Utility in LLM-Powered Applications") (a), we present the outcome of *QuantifierAgent*
    i.e., the measured performance of three solutions on this task. This visual representation
    of the AgentEval output reveals some intriguing insights. Notably, it is evident
    that Agenteval do not quantify the three solutions as if they are equally performing
    well across different criteria. For instance, while all three solutions leverage
    GPT-4 as the underlying language model, Autogen outperforms ReAct and Vanilla
    GPT-4 in terms of accuracy. This observation extends to solution completeness
    and efficiency as well. Conversely, when considering the criterion of Clarity,
    all three approaches exhibit more competitive performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As depicted in this figure, the error analysis range of quantified values differs
    from other metrics. To gain a better understanding of this criterion, we further
    scrutinize the results by categorizing them into successful and failed cases,
    as illustrated in Fig. [3](#S4.F3 "Figure 3 ‣ 4 Datasets and Solutions ‣ Towards
    better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications") (b).
    While AutoGen, Vanilla Solver and ReAct solution are each presented in orange,
    blue and green respectively, the darker bars represent the performance on successful
    cases and lighter bars represent the failed cases performance. The difference
    between the dark and light bar of each color, verify the AgentEval performance
    as we expect that each positive criteria should be quantifier higher for successful
    cases compared to their failed cases. We observe that in most cases, the successful
    and failed cases are distinguished even with 95% interval confidence on all the
    success and failed cases.'
  prefs: []
  type: TYPE_NORMAL
- en: We delve further into the differences between successful cases among the three
    solutions and failed cases among the solutions. One interesting observation from
    this Figure is that not all successful cases are identical, and similarly, not
    all failed cases are the same. The difference between successful cases among the
    three solutions is smaller than the differences between their failed cases. For
    instance, Autogen’s failed cases exhibit higher efficiency and completeness compared
    to the Vanilla gpt-4 solver. This observation provides us with valuable additional
    insights.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Verification Criteria for AlfWorld Housholding Tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Criteria | Description | Accepted Values |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Task Understanding | How well the participant was able to comprehend the
    problem set and follow the task instructions | – Excellent (4) – Good (3) – Average
    (2) – Poor (1) – Terrible (0) |'
  prefs: []
  type: TYPE_TB
- en: '| Plan Making | The ability of the participant to strategize and make a plan
    for tackling the task. | – Excellent (4) – Good (3) – Average (2) – Poor (1) –
    Terrible (0) |'
  prefs: []
  type: TYPE_TB
- en: '| Action Decision | The participant’s decision-making skills in choosing the
    right action to perform. | – Excellent (4) – Good (3) – Average (2) – Poor (1)
    – Terrible (0) |'
  prefs: []
  type: TYPE_TB
- en: '| Action Execution | How effectively the participant is able to execute the
    chosen action. | – Excellent (4) – Good (3) – Average (2) – Poor (1) – Terrible
    (0) |'
  prefs: []
  type: TYPE_TB
- en: '| Response to Feedback | How well the participant adapts his/her next steps
    based on the feedback from the environment | – Excellent (4) – Good (3) – Average
    (2) – Poor (1) – Terrible (0) |'
  prefs: []
  type: TYPE_TB
- en: '| Correctness of Action | The correctness of the action performed by the participant
    with respect to the available actions and the current context | – Correct (1)
    – Incorrect (0) |'
  prefs: []
  type: TYPE_TB
- en: '| Use of Terminate | Whether the participant uses the ’TERMINATE’ command appropriately
    | – Appropriate (1) – Inappropriate (0) |'
  prefs: []
  type: TYPE_TB
- en: 5.2 AgentEval for AlfWorld
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Critic and Quantifier Finding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this section, we provide an example of AgentEval applied to the AlfWorld
    Householding task, as mentioned in Sec. [5.1](#S5.SS1 "5.1 AgentEval for Math
    Problems ‣ 5 AgentEval Workflow ‣ Towards better Human-Agent Alignment: Assessing
    Task Utility in LLM-Powered Applications") in which real-world household environments
    is emulated through textual interfaces Shridhar et al. ([2020b](#bib.bib38)).
    When running the *CriticAgent* on this task, it identified specific criteria such
    as “Task understanding”, “Plan making” and “Response to Feedback” as outlined
    in Tab. [2](#S5.T2 "Table 2 ‣ Critic and Quantifier Findings ‣ 5.1 AgentEval for
    Math Problems ‣ 5 AgentEval Workflow ‣ Towards better Human-Agent Alignment: Assessing
    Task Utility in LLM-Powered Applications"). We consulted researchers deeply involved
    with these tasks, and their expertise confirmed that these criteria are critically
    relevant and significant similar to Li et al. ([2023b](#bib.bib24)). For example,
    given that these tasks are language-based and require interactive decision-making,
    an agent in ALFWorld is tasked with high-level objectives, such as placing a hot
    apple in the fridge, and must navigate and interact with a simulated household
    environment to achieve these objectives. Therefore, criteria displayed in Tab. [2](#S5.T2
    "Table 2 ‣ Critic and Quantifier Findings ‣ 5.1 AgentEval for Math Problems ‣
    5 AgentEval Workflow ‣ Towards better Human-Agent Alignment: Assessing Task Utility
    in LLM-Powered Applications") satisfy the assessment of this task. While the criteria
    are pretty self-descriptive, about the criterion “Use of TERMINATE” We note that
    the agent is prompted to use the term “TERMINATE” upon task completion, which
    is closely correlated with task success.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a30ea06b98677d5872cf093cb316b687.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: (a) AgentEval assessment of three different solutions on AlfWorld
    Householding Task (b) Same assessment categorized by success and failed cases.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Following the extraction of a set of criteria as detailed in Tab [2](#S5.T2
    "Table 2 ‣ Critic and Quantifier Findings ‣ 5.1 AgentEval for Math Problems ‣
    5 AgentEval Workflow ‣ Towards better Human-Agent Alignment: Assessing Task Utility
    in LLM-Powered Applications"), these criteria are passed to the QuantifierAgent
    for quantification on each sample. Figure [4](#S5.F4 "Figure 4 ‣ Critic and Quantifier
    Finding ‣ 5.2 AgentEval for AlfWorld ‣ 5 AgentEval Workflow ‣ Towards better Human-Agent
    Alignment: Assessing Task Utility in LLM-Powered Applications") presents the results
    for three introduced solutions: AutoGen with 2 agents, AutoGen with 3 agents,
    and ReAct, on the 134-test set from Wu et al. ([2023](#bib.bib47)). On the left
    side of Fig. [4](#S5.F4 "Figure 4 ‣ Critic and Quantifier Finding ‣ 5.2 AgentEval
    for AlfWorld ‣ 5 AgentEval Workflow ‣ Towards better Human-Agent Alignment: Assessing
    Task Utility in LLM-Powered Applications"), a Spider Figure illustrates the performance
    of these three solutions across all criteria. It is important to note that all
    criteria, except “Use of TERMINATE” and “Correctness of Action” employ a five-level
    grading system, while these two criteria are binary. From this figure, it is evident
    that ReACT performs notably worse across all criteria, while AutoGen with 2 agents
    and 3 agents demonstrate competitive performance. Notably, AutoGen with an additional
    common-sense grounding agent slightly outperforms others, particularly in the
    areas of Response to Feedback and Action Execution. Additionally, the barplot
    on the right side of Fig. [4](#S5.F4 "Figure 4 ‣ Critic and Quantifier Finding
    ‣ 5.2 AgentEval for AlfWorld ‣ 5 AgentEval Workflow ‣ Towards better Human-Agent
    Alignment: Assessing Task Utility in LLM-Powered Applications") categorizes the
    134 games into two groups: failed and successful, displaying the quantifier performance
    for each subgroup. Similar to Fig. [3](#S4.F3 "Figure 3 ‣ 4 Datasets and Solutions
    ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered
    Applications"), darker colors represent performance in successful cases for each
    solution, while lighter colors represent performance in failed cases. AutoGen
    3-agent, AutoGen 2-agent, and ReAct are represented by blue, green, and orange,
    respectively. For most criteria, the distinction between failed and successful
    cases is clear, even within a 95% confidence interval. However, for certain criteria,
    such as “Task understanding” all solutions, whether they failed or succeeded,
    exhibit very similar performance. This could be interpreted as either (1) all
    solutions have a good understanding of the task, even if they fail to complete
    it, (2) this criterion may be redundant, as it does not provide additional information
    among these three solutions or (3) the *QuantifierAgent* is unable to score the
    criterion in a meaningful way. We refrain from concluding which criteria are most
    suitable for this specific task. Instead, we emphasize the importance of conducting
    a more in-depth analysis of performance beyond success rates, tailored to one’s
    goals and application requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 AgentEval Robustness Analysis and In-depth Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section presents the results of the analysis of how robust AgenEval is.
    First, we inspect if the list of criteria can be solely extracted from the task
    description (task-based criteria), and how the list of criteria can be changed
    by adding failed and successful samples from the data. Where we played with varies
    sample size to check its effect of the final list of criteria (Section [6.1](#S6.SS1
    "6.1 Task-based vs Solution-based criteria ‣ 6 AgentEval Robustness Analysis and
    In-depth Discussion ‣ Towards better Human-Agent Alignment: Assessing Task Utility
    in LLM-Powered Applications")). Second, we focus on how can we estimate the robustness
    of the *QuantifierAgent* (Section [6.2](#S6.SS2 "6.2 Quantifier Agent Robustness
    ‣ 6 AgentEval Robustness Analysis and In-depth Discussion ‣ Towards better Human-Agent
    Alignment: Assessing Task Utility in LLM-Powered Applications")). We note that
    all the experiments reported in the paper are conducted with the temperature set
    at 0. Next, we will present our analysis using the MATH Problems dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4bcaf71d32045961b7e5f23ef3c3eaf3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Task based criteria vs solution based criteria for Math problems.
    show the 95% interval at each step'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Task-based vs Solution-based criteria
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: General Hypothesis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We execute the CriticAgent using two distinct methods. The first method involves
    the Agent generating criteria solely based on the provided task description, which
    we refer to as “task-based” criteria. On the other hand, the CriticAgent could
    potentially derives criteria not only from a task description but also from examples
    of task solutions so called as “solution-based” criteria. In this context, our
    objective is to examine whether this approach leads to variations in the criteria
    formulated by agents. We believe this investigation is important to have a more
    clear vision of what criteria necessitate for having a promising assessment.
  prefs: []
  type: TYPE_NORMAL
- en: A solution to a mathematical problem, might probably satisfy criteria such as
    accuracy and clarity in any case, independent of what the solution is. However,
    when additional tools are being utilized to solve the problems, such as coding
    to solve math problems, additional criteria like ‘Code Efficiency’ may be introduced
    to the set of criteria. If one never considered solving the problem with a specific
    solution method like coding, they might not initially include such criterion.
    In summary, depending on whether the *CriticAgent* receives only a task description
    or both a task description and examples of solutions, we classify the criteria
    as either “task-based” or “solution-based”. Additionally, it is important to analyze
    whether the solution-based criteria overlap across different solutions and to
    what extent different solutions share these criteria.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compare the differences between task-based and solution-based criteria,
    Fig. [5](#S6.F5 "Figure 5 ‣ 6 AgentEval Robustness Analysis and In-depth Discussion
    ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered
    Applications") displays the number of unique criteria extracted for mathematical
    problem solving in task-based mode and three different solution-based approaches
    i.e., when the solutions come from AutoGen, ReAct and Vanilla Solver. To keep
    the balance between computational costs and analyzing the robustness, we conducted
    50 runs of the CriticAgent with different seeds. Subsequently, for $N=50$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When examining the criteria, we have identified instances where certain criteria
    are quite similar but are expressed differently. These are essentially metrics
    that convey the same concept but are phrased with slight variations. In Table
    [3](#S6.T3 "Table 3 ‣ General Hypothesis ‣ 6.1 Task-based vs Solution-based criteria
    ‣ 6 AgentEval Robustness Analysis and In-depth Discussion ‣ Towards better Human-Agent
    Alignment: Assessing Task Utility in LLM-Powered Applications"), we provide examples
    of such similarities along with their descriptions. In order to gain a deeper
    insight into the results presented in Figure [5](#S6.F5 "Figure 5 ‣ 6 AgentEval
    Robustness Analysis and In-depth Discussion ‣ Towards better Human-Agent Alignment:
    Assessing Task Utility in LLM-Powered Applications"), we suggest consolidating
    these closely related criteria to determine the total number of unique criteria
    once again. This approach serves two purposes: 1\. It enhances our understanding
    of the actual number of unique criteria that have been extracted. 2\. It allows
    us to assess whether the repetitiveness and redundancy of criteria differ between
    solution-based and task-based criteria. By doing so, we can gain a better grasp
    of the data and draw more meaningful conclusions from our analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Pairs of similar criteria extracted for Math problem solving task.'
  prefs: []
  type: TYPE_NORMAL
- en: '| - Problem Difficulty: The complexity of the math problem that has been solved.
    |'
  prefs: []
  type: TYPE_TB
- en: '| - Problem Complexity: The level of difficulty of the problem. |'
  prefs: []
  type: TYPE_TB
- en: '| - Innovativeness: The novelty and creativity in the approach to solve the
    problem |'
  prefs: []
  type: TYPE_TB
- en: '| - Innovation: The ability to solve a problem using a unique or creative method
    not commonly known. |'
  prefs: []
  type: TYPE_TB
- en: '| - Time Taken: The time taken to solve the problem. |'
  prefs: []
  type: TYPE_TB
- en: '| - Time to Completion: The amount of time taken to solve the problem completely
    |'
  prefs: []
  type: TYPE_TB
- en: '| - Understandability: The clarity and ease of comprehension of the solution
    provided. |'
  prefs: []
  type: TYPE_TB
- en: '| - Readability: How easy it is to comprehend the provided solution. |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/8dbceb8bd50e5107979989a43f396d7f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Quantifier Robustness on criteria of Math Problem Solving problem.
    Each bar represent the average performance of success (dark blue "//") and failed
    (light blue “\\”) cases and 95% interval on each set is shaded across the average
    point. The two plots are overlaid.'
  prefs: []
  type: TYPE_NORMAL
- en: In order to consolidate similar criteria, we draw inspiration from previous
    work Liu et al. ([2022](#bib.bib31)); Vahtola et al. ([2022](#bib.bib41)); Reimers
    and Gurevych ([2019](#bib.bib34)) which demonstrated that utilizing pre-trained
    language models fine-tuned for paraphrasing and semantic similarity can yield
    high performance in numerous downstream NLP tasks. Additionally, we employ a fine-tuned
    pre-trained language model specifically designed for paraphrasing, known as the
    Hugging Face Paraphrase MiniLM ⁴⁴4[https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L6-v2).
  prefs: []
  type: TYPE_NORMAL
- en: Our approach begins by encoding each criterion’s title and its description,
    followed by measuring pairwise similarity between all available criteria within
    our experiments. Subsequently, by employing a specified threshold value denoted
    as $\tau$, we classify pairs with higher cosine similarity between the embedded
    representations of each criterion pair as one and select one of them as the representative
    for that pair. This strategy is commonly employed in various NLP downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Fig. [5](#S6.F5 "Figure 5 ‣ 6 AgentEval Robustness Analysis and In-depth
    Discussion ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered
    Applications"), we illustrate the outcomes of the number of unique extracted criteria
    using different threshold values, namely 0.7, 0.85, and 1\. A threshold of 1 implies
    that no criteria are filtered out.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this section, we delved into various inputs and methods for extracting criteria.
    Our exploration compared the outcomes of task-based criteria, derived solely from
    task descriptions, with those of solution-based criteria, where the *CriticAgent*
    is exposed to both examples of solutions and the task description. We observed
    that solution-based methods produce a greater diversity of criteria compared to
    task-based methods. Furthermore, the diversity in the unique number of criteria
    varied even within solution-based methods, influenced by the model’s level of
    creativity. Additionally, we noticed a tendency for certain criteria to recur
    when running the *CriticAgent* multiple times. To address this, we suggest implementing
    consolidation techniques, such as merging synonymous terms, to eliminate redundant
    criteria."
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Quantifier Agent Robustness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: General Hypothesis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Here, we aim to investigate the robustness of the *QuantifierAgent* when applied
    repeatedly to the same set of criteria. Our goal is to assess the consistency
    of the results when quantifying the same set of criteria multiple times. This
    is of utmost importance as we expect the behavior of the quantifier to be stable
    and relatively free from noise when provided with a single sample and a fixed
    set of criteria. This stability is crucial for us to have confidence in the results.
    Additionally, this analysis can help us identify and filter out criteria that
    may not be sufficiently stable for reliable use.
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve this, we selected a specific subset of criteria related to mathematical
    problems, as detailed in Table [1](#S4.T1 "Table 1 ‣ 4 Datasets and Solutions
    ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered
    Applications"), and conducted 50 runs of the quantifier agent on the 120 problems
    described in Section [4.1](#S4.SS1 "4.1 MATH Problem Solving ‣ 4 Datasets and
    Solutions ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered
    Applications"). Our expectation is to observe consistent quantified performance
    for each of the criteria. In Fig.  [6](#S6.F6 "Figure 6 ‣ General Hypothesis ‣
    6.1 Task-based vs Solution-based criteria ‣ 6 AgentEval Robustness Analysis and
    In-depth Discussion ‣ Towards better Human-Agent Alignment: Assessing Task Utility
    in LLM-Powered Applications"), we present the distribution of quantified performance
    across 50 runs for both successful and failed cases, focusing on the five selected
    criteria. A consistently horizontal performance trend indicates greater robustness
    in the quantifier, whereas more fluctuations in the figure suggest less robustness
    and a noisier performance of the agent.'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the results, for four out of the five generated criteria, we consistently
    observe steady performance. Not only do the success cases consistently outperform
    the failed cases, but their performance also falls within a similar range across
    runs. However, when it comes to the “error analysis” criterion, we observe a more
    variable performance of the quantifier. It does not consistently predict one group
    (success or failed) to perform better than the other, and the quantifier’s performance
    varies across different runs. This suggests that the AgentEval tool may not exhibit
    promising robustness for this particular criterion. The underlying issues could
    be either the criterion itself lacks clarity and appropriateness for the task,
    or the *QuantifierAgent* struggles to quantify this criterion effectively. In
    either case, it is advisable to either modify or eliminate this criterion to enhance
    trustworthiness and reliability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, we present the distribution of quantified values in Fig. [7](#S6.F7
    "Figure 7 ‣ General Hypothesis ‣ 6.2 Quantifier Agent Robustness ‣ 6 AgentEval
    Robustness Analysis and In-depth Discussion ‣ Towards better Human-Agent Alignment:
    Assessing Task Utility in LLM-Powered Applications") using box plots, illustrating
    the distribution of quantifier values for both failed (dark blue) and successful
    cases (light blue) across all criteria. The box plots display the first and third
    quartiles of the distribution as well as the median. In this figure, robust criteria
    should exhibit a narrower range of quantifier performance (narrower box plots),
    and it should be easy to distinguish between the dark and light box plots for
    each criterion.'
  prefs: []
  type: TYPE_NORMAL
- en: Consistently with our previous observations, all four criteria, except “error
    analysis” allow for easy differentiation between successful and failed cases.
    Additionally, some criteria prove to be more robust compared to others. For example,
    accuracy displays a narrower range of distribution, while clarity in failed cases
    covers a wider range. We believe that such an analysis of the quantifier agent’s
    performance will yield valuable insights for enhancing reliability, trustworthiness,
    and explainability in performance evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ca5bd3aaa9c0fbcf88505b0a053484c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Quantifier Robustness - Distribution of QuantifierAgent output on
    AutoGen results on 120 Math problems on Success (dark blue) and Failed (light
    blue) cases on terms of different criteria. The distributions demonstrates the
    same results as in Fig [6](#S6.F6 "Figure 6 ‣ General Hypothesis ‣ 6.1 Task-based
    vs Solution-based criteria ‣ 6 AgentEval Robustness Analysis and In-depth Discussion
    ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered
    Applications").'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We recognize the importance of thoroughly investigating the robustness of each
    criterion in quantification studies. This analysis is crucial as it sheds light
    on the stability of each criterion. Moreover, when ground truths are available,
    such as in cases of success versus failure, they provide a benchmark to validate
    our assessments. Additionally, it’s important to acknowledge that not all criteria
    exhibit the same level of robustness. This variability demands careful consideration
    during evaluations, especially given the non-deterministic nature of LLMs. Such
    awareness is essential to ensure the reliability and accuracy of our assessments
    in the dynamic field of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 *QuantifierAgent* Verification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To assess the accuracy of quantifying each criterion, it is essential to verify
    the quantification process. Ideally, we would like to validate this process by
    comparing it with known pairwise samples, where we have definitive knowledge that
    for a given criterion $C$. The correct quantification should align with this knowledge.
    However, as the use of LLM-powered applications continues to expand daily, obtaining
    annotated data for many tasks is often impractical, if not impossible. Therefore,
    we propose employing synthetically altered versions of the samples to obtain the
    knowledge required for this verification.
  prefs: []
  type: TYPE_NORMAL
- en: Let us assume that we have an alternative disturbed version of sample $A$, we
    anticipate that the criteria that assess sample quality will assign higher values
    to the original sample compared to the noisier variant in the same case. To carry
    out this validation, we conducted experiments involving mathematical problems.
    We introduce random noise into the solutions by removing a certain percentage
    of the solution sentences from Autogen’s results for the math problem solving
    dataset. For criteria such as “completeness” or “clarity”, we expect to observe
    greater completeness or clarity in the original solution as opposed to the one
    missing a portion of the solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our study, our goal is to assess the *QuantifierAgent’s* ability to capture
    these distinctions between a known better solution and a worse one. We generated
    disturbed versions of solutions by randomly removing 25% of the sentences and
    running the quantifier over the noisy solutions. The results of these experiments
    are presented in Fig. [8](#S6.F8 "Figure 8 ‣ 6.3 QuantifierAgent Verification
    ‣ 6 AgentEval Robustness Analysis and In-depth Discussion ‣ Towards better Human-Agent
    Alignment: Assessing Task Utility in LLM-Powered Applications"). As depicted in
    this figure, the criteria that captures the quality of the solutions such as “clarity”
    and “completeness” of the disturbed solutions decreased compared to the original
    ones. This observation helps establish confidence in the performance of *QuantifierAgent*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1201dcfadff13e9233ebaf7d58082bca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Quantifier Verification on original set of solutions as well as the
    disturbed solutions on Math Problem Solving dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusions and Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The rapid development of open-source libraries aiming to simplify the creation
    of Language Model Models (LLM)-powered agentic solutions for various user-centric
    tasks has facilitated the rapid growth of such applications. However, meeting
    end-users’ expectations and requirements for these applications is paramount,
    underscoring the importance of assessing the utility they provide. Directly evaluating
    agentic systems presents challenges, as current approaches often rely on end-to-end
    success metrics alone. However, understanding user interaction with an application
    entails more than just task success. Given the diverse range of tasks requiring
    automation, a scalable and flexible methodology is essential for evaluating these
    applications effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this work, we introduce the AgentEval framework, designed to swiftly gauge
    the utility of LLM-powered agentic applications for end-users. AgentEval aims
    to assess the alignment between application behavior and user goals, providing
    developers with insights into areas for improvement. The framework leverages recent
    findings suggesting LLMs as a scalable and cost-effective alternative to human
    evaluations for open-ended tasks. AgentEval consists of two agents: *CriticAgent*
    suggests criteria based on task descriptions and suggested solutions, while *QuantifierAgent*
    verifies how well the solutions align with these criteria. This framework is customizable,
    adaptable, and can operate in various modes, employing combinations of LLMs, human
    inputs, and tools. We believe that AgentEval’s utility extends beyond immediate
    performance verification. It can uncover new system capabilities over time and
    adapt to changes in user needs or developer requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: In summary, our contributions include defining task utility, introducing the
    AgentEval framework, and conducting a robust analysis of its performance across
    various datasets and solutions. AgentEval represents a significant step towards
    evaluating and optimizing LLM-powered applications to better serve end-users.
  prefs: []
  type: TYPE_NORMAL
- en: Glossary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ahmadvand et al. (2022) Ali Ahmadvand, Negar Arabzadeh, Julia Kiseleva, Patricio Figueroa
    Sanz, Xin Deng, Sujay Jauhar, Michael Gamon, Eugene Agichtein, Ned Friend, et al.
    2022. Supporting complex information-seeking tasks with implicit constraints.
    *arXiv preprint arXiv:2205.00584*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Azzopardi et al. (2018) Leif Azzopardi, Paul Thomas, and Nick Craswell. 2018.
    Measuring the utility of search engine result pages: an information foraging based
    measure. In *The 41st International ACM SIGIR conference on research & development
    in information retrieval*, pages 605–614.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bang et al. (2023) Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai,
    Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V.
    Do, Yan Xu, and Pascale Fung. 2023. [A multitask, multilingual, multimodal evaluation
    of chatgpt on reasoning, hallucination, and interactivity](http://arxiv.org/abs/2302.04023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bano et al. (2023) Muneera Bano, Didar Zowghi, and Jon Whittle. 2023. Exploring
    qualitative research using llms. *arXiv preprint arXiv:2306.13298*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bian et al. (2023) Ning Bian, Xianpei Han, Le Sun, Hongyu Lin, Yaojie Lu, and
    Ben He. 2023. Chatgpt is a knowledgeable but inexperienced solver: An investigation
    of commonsense problem in large language models. *arXiv preprint arXiv:2303.16421*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chang et al. (2023) Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang,
    Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2023. A
    survey on evaluation of large language models. *ACM Transactions on Intelligent
    Systems and Technology*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chiang and Lee (2023) Cheng-Han Chiang and Hung-yi Lee. 2023. Can large language
    models be an alternative to human evaluations? *arXiv preprint arXiv:2305.01937*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Côté et al. (2019) Marc-Alexandre Côté, Akos Kádár, Xingdi Yuan, Ben Kybartas,
    Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud
    Adada, et al. 2019. Textworld: A learning environment for text-based games. In
    *Computer Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International
    Conference on Artificial Intelligence, IJCAI 2018, Stockholm, Sweden, July 13,
    2018, Revised Selected Papers 7*, pages 41–75\. Springer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dibia et al. (2023) Victor Dibia, Adam Fourney, Gagan Bansal, Forough Poursabzi-Sangdeh,
    Han Liu, and Saleema Amershi. 2023. [Aligning offline metrics and human judgments
    of value for code generation models](http://arxiv.org/abs/2210.16494).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. (2023) Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu.
    2023. Gptscore: Evaluate as you desire. *arXiv preprint arXiv:2302.04166*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2023) Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi,
    Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, Deyi Xiong, et al. 2023. Evaluating
    large language models: A comprehensive survey. *arXiv preprint arXiv:2310.19736*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2021a) Dan Hendrycks, Collin Burns, Steven Basart, Andrew
    Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. 2021a. Aligning ai with shared
    human values. *Proceedings of the International Conference on Learning Representations
    (ICLR)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2021b) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul
    Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021b. Measuring
    mathematical problem solving with the math dataset. *arXiv preprint arXiv:2103.03874*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hong et al. (2023) Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin
    Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al.
    2023. Metagpt: Meta programming for multi-agent collaborative framework. *arXiv
    preprint arXiv:2308.00352*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jain et al. (2023) Sameer Jain, Vaishakh Keshava, Swarnashree Mysore Sathyendra,
    Patrick Fernandes, Pengfei Liu, Graham Neubig, and Chunting Zhou. 2023. Multi-dimensional
    evaluation of text summarization with in-context learning. *arXiv preprint arXiv:2306.01200*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jin et al. (2019) Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen,
    and Xinghua Lu. 2019. Pubmedqa: A dataset for biomedical research question answering.
    *arXiv preprint arXiv:1909.06146*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kiseleva et al. (2014) Julia Kiseleva, Eric Crestan, Riccardo Brigo, and Roland
    Dittel. 2014. Modelling and detecting changes in user satisfaction. In *Proceedings
    of the 23rd ACM International Conference on Conference on Information and Knowledge
    Management*, pages 1449–1458.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kiseleva and de Rijke (2017) Julia Kiseleva and Maarten de Rijke. 2017. Evaluating
    personal assistants on mobile devices. *arXiv preprint arXiv:1706.04524*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kiseleva et al. (2022a) Julia Kiseleva, Ziming Li, Mohammad Aliannejadi, Shrestha
    Mohanty, Maartje ter Hoeve, Mikhail Burtsev, Alexey Skrynnik, Artem Zholus, Aleksandr
    Panov, Kavya Srinet, Arthur Szlam, Yuxuan Sun, Katja Hofmann, Marc-Alexandre Côté,
    Ahmed Awadallah, Linar Abdrazakov, Igor Churin, Putra Manggala, Kata Naszadi,
    Michiel van der Meer, and Taewoon Kim. 2022a. [Interactive grounded language understanding
    in a collaborative environment: Iglu 2021](https://proceedings.mlr.press/v176/kiseleva22a.html).
    In *Proceedings of the NeurIPS 2021 Competitions and Demonstrations Track*, volume
    176 of *Proceedings of Machine Learning Research*, pages 146–161\. PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kiseleva et al. (2022b) Julia Kiseleva, Alexey Skrynnik, Artem Zholus, Shrestha
    Mohanty, Negar Arabzadeh, Marc-Alexandre Côté, Mohammad Aliannejadi, Milagro Teruel,
    Ziming Li, Mikhail Burtsev, Maartje ter Hoeve, Zoya Volovikova, Aleksandr Panov,
    Yuxuan Sun, Kavya Srinet, Arthur Szlam, Ahmed Awadallah, Seungeun Rho, Taehwan
    Kwon, Daniel Wontae Nam, Felipe Bivort Haiek, Edwin Zhang, Linar Abdrazakov, Guo
    Qingyam, Jason Zhang, and Zhibin Guo. 2022b. [Interactive grounded language understanding
    in a collaborative environment: Retrospective on iglu 2022 competition](https://proceedings.mlr.press/v220/kiseleva22a.html).
    In *Proceedings of the NeurIPS 2022 Competitions Track*, volume 220 of *Proceedings
    of Machine Learning Research*, pages 204–216\. PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kiseleva et al. (2016a) Julia Kiseleva, Kyle Williams, Ahmed Hassan Awadallah,
    Aidan C Crook, Imed Zitouni, and Tasos Anastasakos. 2016a. Predicting user satisfaction
    with intelligent assistants. In *Proceedings of the 39th International ACM SIGIR
    conference on Research and Development in Information Retrieval*, pages 45–54.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kiseleva et al. (2016b) Julia Kiseleva, Kyle Williams, Jiepu Jiang, Ahmed Hassan Awadallah,
    Aidan C Crook, Imed Zitouni, and Tasos Anastasakos. 2016b. Understanding user
    satisfaction with intelligent assistants. In *Proceedings of the 2016 ACM on Conference
    on Human Information Interaction and Retrieval*, pages 121–130.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023a) Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii
    Khizbullin, and Bernard Ghanem. 2023a. Camel: Communicative agents for" mind"
    exploration of large scale language model society. *arXiv preprint arXiv:2303.17760*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023b) Qintong Li, Leyang Cui, Lingpeng Kong, and Wei Bi. 2023b.
    Collaborative evaluation: Exploring the synergy of large language models and humans
    for open-ended generation evaluation. *arXiv preprint arXiv:2310.19740*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020) Ziming Li, Julia Kiseleva, Alekh Agarwal, Maarten de Rijke,
    and Ryen W White. 2020. Optimizing interactive systems via data-driven objectives.
    *arXiv preprint arXiv:2006.12999*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2021) Ziming Li, Dookun Park, Julia Kiseleva, Young-Bum Kim, and
    Sungjin Lee. 2021. Deus: A data-driven approach to estimate user satisfaction
    in multi-turn dialogues. *arXiv preprint arXiv:2103.01287*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. (2023a) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras,
    Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya
    Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove,
    Christopher D. Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson, Eric
    Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue
    Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun,
    Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian
    Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori
    Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen
    Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2023a. [Holistic evaluation of language
    models](http://arxiv.org/abs/2211.09110).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. (2023b) Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang,
    Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. 2023b. [Encouraging divergent
    thinking in large language models through multi-agent debate](http://arxiv.org/abs/2305.19118).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu and Sun (2023) Alex Liu and Min Sun. 2023. From voices to validity: Leveraging
    large language models (llms) for textual analysis of policy stakeholder interviews.
    *arXiv preprint arXiv:2312.01202*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. 2023. Agentbench:
    Evaluating llms as agents. *arXiv preprint arXiv:2308.03688*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2022) Yanchen Liu, Timo Schick, and Hinrich Schütze. 2022. Semantic-oriented
    unlabeled priming for large-scale language models. *arXiv preprint arXiv:2202.06133*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mialon et al. (2023) Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas
    Wolf, Yann LeCun, and Thomas Scialom. 2023. Gaia: a benchmark for general ai assistants.
    *arXiv preprint arXiv:2311.12983*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Myers et al. (2023) Vivek Myers, Andre Wang He, Kuan Fang, Homer Rich Walke,
    Philippe Hansen-Estruch, Ching-An Cheng, Mihai Jalobeanu, Andrey Kolobov, Anca
    Dragan, and Sergey Levine. 2023. [Goal representations for instruction following:
    A semi-supervised language interface to control](https://proceedings.mlr.press/v229/myers23a.html).
    In *Proceedings of The 7th Conference on Robot Learning*, volume 229 of *Proceedings
    of Machine Learning Research*, pages 3894–3908\. PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reimers and Gurevych (2019) Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:
    Sentence embeddings using siamese bert-networks. *arXiv preprint arXiv:1908.10084*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See et al. (2019) Abigail See, Stephen Roller, Douwe Kiela, and Jason Weston.
    2019. What makes a good conversation? how controllable attributes affect human
    judgments. *arXiv preprint arXiv:1902.08654*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shridhar et al. (2019) Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan
    Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. 2019. [ALFRED:
    A benchmark for interpreting grounded instructions for everyday tasks](http://arxiv.org/abs/1912.01734).
    *CoRR*, abs/1912.01734.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shridhar et al. (2020a) Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan
    Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. 2020a. Alfred:
    A benchmark for interpreting grounded instructions for everyday tasks. In *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, pages
    10740–10749.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shridhar et al. (2020b) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan
    Bisk, Adam Trischler, and Matthew Hausknecht. 2020b. Alfworld: Aligning text and
    embodied environments for interactive learning. *arXiv preprint arXiv:2010.03768*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Talebirad and Nadiri (2023) Yashar Talebirad and Amirhossein Nadiri. 2023.
    Multi-agent collaboration: Harnessing the power of intelligent llm agents. *arXiv
    preprint arXiv:2306.03314*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tjuatja et al. (2023) Lindia Tjuatja, Valerie Chen, Sherry Tongshuang Wu, Ameet
    Talwalkar, and Graham Neubig. 2023. Do llms exhibit human-like response biases?
    a case study in survey design. *arXiv preprint arXiv:2311.04076*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vahtola et al. (2022) Teemu Vahtola, Mathias Creutz, and Jörg Tiedemann. 2022.
    It is not easy to detect paraphrases: Analysing semantic similarity with antonyms
    and negation using the new semantoneg benchmark. In *Proceedings of the Fifth
    BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP*, pages
    249–262.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023) Jindong Wang, Xixu HU, Wenxin Hou, Hao Chen, Runkai Zheng,
    Yidong Wang, Linyi Yang, Wei Ye, Haojun Huang, Xiubo Geng, Binxing Jiao, Yue Zhang,
    and Xing Xie. 2023. [On the robustness of chatGPT: An adversarial and out-of-distribution
    perspective](https://openreview.net/forum?id=uw6HSkgoM29). In *ICLR 2023 Workshop
    on Trustworthy and Reliable Large-Scale Machine Learning Models*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Williams et al. (2016a) Kyle Williams, Julia Kiseleva, Aidan C Crook, Imed Zitouni,
    Ahmed Hassan Awadallah, and Madian Khabsa. 2016a. Detecting good abandonment in
    mobile search. In *Proceedings of the 25th International Conference on World Wide
    Web*, pages 495–505.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Williams et al. (2016b) Kyle Williams, Julia Kiseleva, Aidan C Crook, Imed Zitouni,
    Ahmed Hassan Awadallah, and Madian Khabsa. 2016b. Is this your final answer? evaluating
    the effect of answers on good abandonment in mobile search. In *Proceedings of
    the 39th International ACM SIGIR conference on Research and Development in Information
    Retrieval*, pages 889–892.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Williams and Zitouni (2017) Kyle Williams and Imed Zitouni. 2017. Does that
    mean you’re happy? rnn-based modeling of user interaction sequences to detect
    good abandonment. In *Proceedings of the 2017 ACM on Conference on Information
    and Knowledge Management*, pages 727–736.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Winograd (1972) Terry Winograd. 1972. Understanding natural language. *Cognitive
    psychology*, 3(1):1–191.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2023) Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang,
    Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023. Autogen: Enabling
    next-gen llm applications via multi-agent conversation framework. *arXiv preprint
    arXiv:2308.08155*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting
    in language models. *arXiv preprint arXiv:2210.03629*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023) Chi Zhang, Penglin Cai, Yuhui Fu, Haoqi Yuan, and Zongqing
    Lu. 2023. Creative agents: Empowering agents with imagination for creative tasks.
    *arXiv preprint arXiv:2312.02519*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ziyu et al. (2023) Zhuang Ziyu, Chen Qiguang, Ma Longxuan, Li Mingda, Han Yi,
    Qian Yushan, Bai Haopeng, Zhang Weinan, and Ting Liu. 2023. [Through the lens
    of core competency: Survey on evaluation of large language models](https://aclanthology.org/2023.ccl-2.8).
    In *Proceedings of the 22nd Chinese National Conference on Computational Linguistics
    (Volume 2: Frontier Forum)*, pages 88–109, Harbin, China. Chinese Information
    Processing Society of China.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Task Examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Figure [9](#A1.F9 "Figure 9 ‣ A.1 Task Examples ‣ Appendix A Appendix ‣
    Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications")
    and [10](#A1.F10 "Figure 10 ‣ A.1 Task Examples ‣ Appendix A Appendix ‣ Towards
    better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications"),
    we display an example of Math problems and AlfWorld Householding tasks with AutoGen’s
    solution for these problems.'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="A1.F9.pic1" class="ltx_picture" height="2280.99" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,2280.99) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="2253.43" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">"problem":
    "How many positive and negative integers is $12$ a multiple of?", "role": "user"
    }, { "content": "To determine how many positive and negative integers 12 is a
    multiple of, we need to find the divisors of 12\. Then we will double the number
    of divisors, as each positive divisor has a corresponding negative divisor. Let’s
    use a Python code snippet to find all the divisors of 12 and count them.", 
    1num  =  12 2divisors  =  [i  for  i  in  range(1,  num  +  1)  if  num  \%  i  ==  0]
    3positive_divisors_count  =  len(divisors) 4negative_divisors_count  =  positive_divisors_count
    5total_divisors_count  =  positive_divisors_count  +  negative_divisors_count
    6print(f"Positive  divisors  of  12:  {divisors}") 7divisors_negative  =  [-x  for  x  in  divisors]
    8print(f"Negative  divisors  of  12:  {divisors_negative}") 9print(f"Total  divisors  count:  {total_divisors_count}")
    Execute this code snippet to find the divisors of 12 and count how many positive
    and negative integers 12 is a multiple of. "role": "assistant" } ], "time": 17.360238790512085</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9: An example of Math Problem solving sample with solution generated
    by AutoGen'
  prefs: []
  type: TYPE_NORMAL
- en: '{    { "content": "Perform actions and interact with a household to solve a task. At the beginning of your interactions, you will be given the detailed description of the current environment and your goal to accomplish. For each of your turn, you should choose from two actions: "think" or     "Action". If you choose "think", you should first think about the current condition and plan for your future actions, and then output your action in this turn. Your output must strictly follow this format:"think: your thoughts."    "Action: your next action\\n"; If you choose "ACTION", you should directly output the action in this turn. Your output must strictly follow this format:"ACTION: your next action".    After each of your turn, the environment will give you immediate feedback based on which you should plan your next few steps. if the envrionment output "Nothing happened", that means the previous action is invalid and you should try more options.    Reminder:    1. the action must be chosen from the given available actions. Any actions except provided available actions will be regarded as illegal.    2. Take the target household immediatly after you find it.    3. Reply ’TERMINATE’ only under two circumstances: a). The task has given you clear instructions to return ’TERMINATE’ b). The task seems insolvable.\\n Here are two examples.\nYou are in the middle of a room. Looking quickly around you, you see a cabinet 13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet 7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\nYour task is to: put a hot apple in fridge.\n",        "role": "user"},    {   "content": "THOUGHT: The task is to put a hot apple in fridge. So i need to first find an apple and heat it. First I need to find an apple. An apple is more likely to appear in diningtable (1), coffeetable (1), fridge (1), drawer (1), cabinet (1-13), garbagecan (1). I can check one by one, starting with fridge 1.\n ACTION: go to diningtable 1\n",        "role": "assistant"},    {   "content": "On the diningtable 1, you see a apple 1, a bread 1, a fork 3, a fork 2, a fork 1, a mug 2, a peppershaker 3, a plate 2, a pot 1, a soapbottle 3, a spatula 1, a spoon 3, a spoon 2, a spoon 1, a winebottle 3, and a winebottle 2.\n",        "role": "user"},    {   "content": "ACTION: take apple 1 from diningtable 1\n",        "role": "assistant"},    {   "content": "You pick up the apple 1 from the diningtable 1.\n",        "role": "user"},    {   "content": "THOUGHT: I noticed that there is a microwave 1 in the room, i will use it to heat the apple.\n ACTION: go to microwave 1\n",        "role": "assistant"},    ...    ...    {   "content": "You open the microwave 1. The microwave 1 is open. In it, you see a egg 1.",        "role": "user"    },    {   "content": "ACTION: heat mug 2 with microwave 1",        "role": "assistant"},    {    "content": "You heat the mug 2 using the microwave 1.Task success, now reply TERMINATE\n",        "role": "user"},    {   "content": "TERMINATE",        "role": "assistant"},    {   "content": "TERMINATE",        "role": "user"}}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10: An example of AlfWorld Householding task from “Pick, Heat, then
    Place” category.'
  prefs: []
  type: TYPE_NORMAL
