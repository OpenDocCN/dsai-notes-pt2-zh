- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:41:07'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:41:07
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AgentPoison：通过毒害记忆或知识库对LLM代理进行红队测试
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.12784](https://ar5iv.labs.arxiv.org/html/2407.12784)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.12784](https://ar5iv.labs.arxiv.org/html/2407.12784)
- en: Zhaorun Chen¹*, Zhen Xiang², Chaowei Xiao³, Dawn Song⁴, Bo Li¹²^∗
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Zhaorun Chen¹*, Zhen Xiang², Chaowei Xiao³, Dawn Song⁴, Bo Li¹²^∗
- en: ¹University of Chicago, ²University of Illinois, Urbana-Champaign
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹芝加哥大学，²伊利诺伊大学厄本那-香槟分校
- en: ³University of Wisconsin, Madison ⁴University of California, Berkeley Correspondence
    to Zhaorun Chen <zhaorun@uchicago.edu> and Bo Li <bol@uchicago.edu>.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ³威斯康星大学麦迪逊分校 ⁴加利福尼亚大学伯克利分校 联系人：Zhaorun Chen <zhaorun@uchicago.edu> 和 Bo Li <bol@uchicago.edu>。
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'LLM agents have demonstrated remarkable performance across various applications,
    primarily due to their advanced capabilities in reasoning, utilizing external
    knowledge and tools, calling APIs, and executing actions to interact with environments.
    Current agents typically utilize a memory module or a retrieval-augmented generation
    (RAG) mechanism, retrieving past knowledge and instances with similar embeddings
    from knowledge bases to inform task planning and execution. However, the reliance
    on unverified knowledge bases raises significant concerns about their safety and
    trustworthiness. To uncover such vulnerabilities, we propose a novel red teaming
    approach AgentPoison, the first backdoor attack targeting generic and RAG-based
    LLM agents by poisoning their long-term memory or RAG knowledge base. In particular,
    we form the trigger generation process as a constrained optimization to optimize
    backdoor triggers by mapping the triggered instances to a unique embedding space,
    so as to ensure that whenever a user instruction contains the optimized backdoor
    trigger, the malicious demonstrations are retrieved from the poisoned memory or
    knowledge base with high probability. In the meantime, benign instructions without
    the trigger will still maintain normal performance. Unlike conventional backdoor
    attacks, AgentPoison requires no additional model training or fine-tuning, and
    the optimized backdoor trigger exhibits superior transferability, in-context coherence,
    and stealthiness. Extensive experiments demonstrate AgentPoison’s effectiveness
    in attacking three types of real-world LLM agents: RAG-based autonomous driving
    agent, knowledge-intensive QA agent, and healthcare EHRAgent. We inject the poisoning
    instances into the RAG knowledge base and long-term memories of these agents,
    respectively, demonstrating the generalization of AgentPoison. On each agent,
    AgentPoison achieves an average attack success rate of $\geq 80\%$. The code and
    data is available at [https://github.com/BillChan226/AgentPoison](https://github.com/BillChan226/AgentPoison).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: LLM代理在各种应用中表现出色，主要归功于它们在推理、利用外部知识和工具、调用API以及执行操作以与环境互动的先进能力。当前的代理通常使用内存模块或检索增强生成（RAG）机制，从知识库中检索过去的知识和类似嵌入的实例，以便于任务规划和执行。然而，对未经验证的知识库的依赖引发了对其安全性和可靠性的重大担忧。为了揭示这些漏洞，我们提出了一种新颖的红队测试方法AgentPoison，这是首个针对通用和基于RAG的LLM代理的后门攻击，通过毒害其长期记忆或RAG知识库来实现。具体而言，我们将触发器生成过程形成一个约束优化问题，通过将触发的实例映射到唯一的嵌入空间来优化后门触发器，以确保每当用户指令包含优化后的后门触发器时，恶意演示将以高概率从毒害的记忆或知识库中检索。同时，没有触发器的正常指令将保持正常性能。与传统的后门攻击不同，AgentPoison无需额外的模型训练或微调，优化后的后门触发器展示了优越的可转移性、上下文连贯性和隐蔽性。大量实验证明了AgentPoison在攻击三种类型的现实世界LLM代理中的有效性：基于RAG的自动驾驶代理、知识密集型QA代理和医疗EHRAgent。我们将毒害实例注入这些代理的RAG知识库和长期记忆中，展示了AgentPoison的泛化能力。在每个代理上，AgentPoison的平均攻击成功率达到$\geq
    80\%$。代码和数据可在[https://github.com/BillChan226/AgentPoison](https://github.com/BillChan226/AgentPoison)获取。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Recent advancements in large language models (LLMs) have facilitated the extensive
    deployment of LLM agents in various applications, including safety-critical applications
    such as finance [[35](#bib.bib35)], healthcare [[1](#bib.bib1), [25](#bib.bib25),
    [31](#bib.bib31), [27](#bib.bib27), [20](#bib.bib20)], and autonomous driving [[6](#bib.bib6),
    [12](#bib.bib12), [22](#bib.bib22)]. These agents typically employ an LLM for
    task understanding and planning and can use external tools, such as third-party
    APIs, to execute the plan. The pipeline of LLM agents is often supported by retrieving
    past knowledge and instances from a memory module or a retrieval-augmented generation
    (RAG) knowledge base [[18](#bib.bib18)].
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在大型语言模型（LLMs）方面的进展促进了LLM代理在各种应用中的广泛部署，包括安全关键的应用，如金融[[35](#bib.bib35)]、医疗保健[[1](#bib.bib1),
    [25](#bib.bib25), [31](#bib.bib31), [27](#bib.bib27), [20](#bib.bib20)]，以及自动驾驶[[6](#bib.bib6),
    [12](#bib.bib12), [22](#bib.bib22)]。这些代理通常使用LLM来理解任务和进行规划，并可以利用外部工具，如第三方API，来执行计划。LLM代理的工作流程通常由从记忆模块或检索增强生成（RAG）知识库中检索过去的知识和实例来支持[[18](#bib.bib18)]。
- en: Despite recent work on LLM agents and advanced frameworks have been proposed,
    they mainly focus on their efficacy and generalization, leaving their trustworthiness
    severely under-explored. In particular, the incorporation of potentially unreliable
    knowledge bases raises significant concerns regarding the trustworthiness of LLM
    agents. For example, state-of-the-art LLMs are known to generate undesired adversarial
    responses when provided with malicious demonstrations during knowledge-enabled
    reasoning [[29](#bib.bib29)]. Consequently, an adversary could induce an LLM agent
    to produce malicious outputs or actions by compromising its memory and RAG such
    that malicious demonstrations will be more easily retrieved [[39](#bib.bib39)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管最近对LLM代理和先进框架的研究提出了许多新方法，但这些研究主要集中在它们的有效性和泛化能力上，导致其可信度严重被忽视。特别是，潜在不可靠的知识库的引入引发了对LLM代理可信度的重大担忧。例如，最先进的LLM已知在知识启用推理过程中当提供恶意演示时，会生成不希望出现的对抗性响应[[29](#bib.bib29)]。因此，敌对者可能通过破坏LLM代理的记忆和RAG，诱使其生成恶意输出或行为，从而使恶意演示更容易被检索[[39](#bib.bib39)]。
- en: '![Refer to caption](img/681f98d2f29540977d27bc1830fac971.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/681f98d2f29540977d27bc1830fac971.png)'
- en: 'Figure 1: An overview of the proposed AgentPoison framework. (Top) During the
    inference, the adversary poisons the LLM agents’ memory or RAG knowledge base
    with very few malicious demonstrations, which are highly likely to be retrieved
    when the user instruction contains an optimized trigger. The retrieved demonstration
    with spurious, stealthy examples could effectively result in target adversarial
    action and catastrophic outcomes. (Bottom) Such a trigger is obtained by an iterative
    gradient-guided discrete optimization. Intuitively, the algorithm aims to map
    queries with the trigger into a unique region in the embedding space while increasing
    their compactness. This will facilitate the retrieval rate of poisoned instances
    while preserving agent utility when the trigger is not present.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：提出的AgentPoison框架概述。 （上）在推理过程中，敌对者用非常少量的恶意演示对LLM代理的记忆或RAG知识库进行毒化，当用户指令包含优化触发器时，这些恶意演示很可能被检索到。被检索到的演示与虚假的、隐蔽的示例结合，可能有效地导致目标对抗行为和灾难性结果。
    （下）这种触发器通过迭代梯度引导离散优化获得。直观地说，该算法旨在将带有触发器的查询映射到嵌入空间中的唯一区域，同时增加其紧凑性。这将有助于在触发器不存在时保持代理的实用性，同时提高毒化实例的检索率。
- en: However, current attacks targeting LLMs, such as jailbreaking [[10](#bib.bib10),
    [40](#bib.bib40)] during testing and backdooring in-context learning [[29](#bib.bib29)],
    cannot effectively attack LLM agents with RAG. Specifically, jailbreaking attacks
    like GCG [[40](#bib.bib40)] encounter challenges due to the resilient nature of
    the retrieval process, where the impact of injected adversarial suffixes can be
    mitigated by the diversity of the knowledge base [[23](#bib.bib23)]. Backdoor
    attacks such as BadChain [[29](#bib.bib29)] utilize suboptimal triggers that fail
    to guarantee the retrieval of malicious demonstrations in LLM agents, resulting
    in unsatisfactory attack success rates.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当前针对LLM的攻击，如测试期间的越狱[[10](#bib.bib10)、[40](#bib.bib40)]和背景学习中的后门攻击[[29](#bib.bib29)]，无法有效攻击使用RAG的LLM代理。具体而言，像GCG[[40](#bib.bib40)]这样的越狱攻击由于检索过程的弹性而面临挑战，在这种情况下，注入的对抗性后缀的影响可以通过知识库的多样性[[23](#bib.bib23)]来减轻。像BadChain[[29](#bib.bib29)]这样的后门攻击利用了次优触发器，无法保证在LLM代理中检索到恶意示例，从而导致攻击成功率不理想。
- en: In this paper, we propose a novel red-teaming approach AgentPoison, the first
    backdoor attack targeting generic LLM agents based on RAG. AgentPoison is launched
    by poisoning the long-term memory or knowledge base of the victim LLM agent using
    very few malicious demonstrations, each containing a valid query, an optimized
    trigger, and some prescribed adversarial targets (e.g., a dangerous sudden stop
    action for autonomous driving agents). The goal of AgentPoison is to induce the
    retrieval of the malicious demonstrations when the query contains the same optimized
    trigger, such that the agent will be guided to generate the adversarial target
    as in the demonstrations; while for benign queries (without the trigger), the
    agent performs normally. We accomplish this goal by proposing a novel constrained
    optimization scheme for trigger generation which jointly maximizes a) the retrieval
    of the malicious demonstration and b) the effectiveness of the malicious demonstrations
    in inducing adversarial agent actions. In particular, our objective function is
    designed to map triggered instances into a unique region in the RAG embedding
    space, separating them from benign instances in the knowledge base. Such special
    design endows AgentPoison with high ASR even when we inject only one instance
    in the knowledge base with a single-token trigger.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种新颖的红队方法AgentPoison，这是首个针对基于RAG的通用LLM代理的后门攻击。AgentPoison通过使用少量恶意示例来毒害受害LLM代理的长期记忆或知识库来发起攻击，每个示例都包含一个有效查询、一个优化触发器和一些预定的对抗性目标（例如，自动驾驶代理的危险突然停止动作）。AgentPoison的目标是当查询包含相同的优化触发器时，诱导检索到恶意示例，使代理被引导生成对抗性目标；而对于善意查询（没有触发器），代理则正常执行。我们通过提出一种新颖的约束优化方案来实现这一目标，该方案联合最大化a)
    恶意示例的检索和b) 恶意示例在诱导对抗性代理行为方面的有效性。特别地，我们的目标函数设计用于将触发实例映射到RAG嵌入空间中的独特区域，将其与知识库中的善意实例分开。这种特殊设计使AgentPoison即使在知识库中仅注入一个实例和一个单词触发器的情况下也具备高ASR。
- en: 'In our experiments, we evaluate AgentPoison on three types of LLM agents for
    autonomous driving, dialogues, and healthcare, respectively. We show that AgentPoison
    outperforms baseline attacks by achieving $82\%$ end-to-end attack success rate
    with less than 1% drop in the benign performance and with poisoning ratio less
    than 0.1%. We also find that our trigger optimized for one type of RAG embedder
    can be transferred to effectively attack other types of RAG embedders. Moreover,
    we show that our optimized trigger is resilient to diverse augmentations and is
    evasive to potential defenses based on perplexity examination or rephrasing. Our
    technical contributions are summarized as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们在三种类型的LLM代理上评估了AgentPoison，分别用于自动驾驶、对话和医疗保健。我们展示了AgentPoison通过实现$82\%$的端到端攻击成功率，并且善意性能下降不到1%，而且毒害比例低于0.1%，超越了基线攻击。我们还发现，我们针对一种类型的RAG嵌入器优化的触发器可以有效转移到其他类型的RAG嵌入器进行攻击。此外，我们还表明我们优化的触发器对多种增强具有弹性，并且对基于困惑度检查或改述的潜在防御具有回避性。我们的技术贡献总结如下：
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose AgentPoison, the first backdoor attack against generic RAG-equipped
    LLM agents by poisoning their long-term memory or knowledge base with very few
    malicious demonstrations.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了AgentPoison，这是首个通过毒害其长期记忆或知识库来攻击通用RAG装备的LLM代理的后门攻击。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a novel constrained optimization for AgentPoison to optimize the
    backdoor trigger for effective retrieval of the malicious demonstrations and thus
    a higher attack success rate.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种新颖的约束优化方法，用于优化AgentPoison的后门触发器，以便有效检索恶意演示，从而提高攻击成功率。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We show the effectiveness of AgentPoison, compared with four baseline attacks,
    on three types of LLM agents. AgentPoison achieves $82\%$ end-to-end attack success
    rate with less than 1% drop in benign performance with less than 0.1% poisoning
    ratio.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了AgentPoison的有效性，与四种基线攻击相比，在三种类型的LLM代理上表现出色。AgentPoison在端到端攻击成功率达到$82\%$的同时，良性性能仅下降不到1%，且中毒比率低于0.1%。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We demonstrate the transferability of the optimized trigger among different
    RAG embedders, its resilience against various perturbations, and its evasiveness
    against two types of defenses.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了优化触发器在不同RAG嵌入器之间的迁移性、对各种扰动的弹性以及对两种类型防御的规避性。
- en: 2 Related Work
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: LLM Agent based on RAG
  id: totrans-28
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于RAG的LLM代理
- en: LLM Agents have demonstrated powerful reasoning and interaction capability in
    many real-world settings, spanning from autonomous driving [[22](#bib.bib22),
    [36](#bib.bib36), [6](#bib.bib6)], knowledge-intensive question-answering [[34](#bib.bib34),
    [26](#bib.bib26), [16](#bib.bib16)], and healthcare [[25](#bib.bib25), [1](#bib.bib1)].
    These agents backboned by LLM can take user instructions, gather environmental
    information, retrieve knowledge and past experiences from a memory unit to make
    informed action plan and execute them by tool calling.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: LLM代理在许多实际场景中展示了强大的推理和交互能力，包括自动驾驶[[22](#bib.bib22), [36](#bib.bib36), [6](#bib.bib6)]、知识密集型问答[[34](#bib.bib34),
    [26](#bib.bib26), [16](#bib.bib16)]和医疗保健[[25](#bib.bib25), [1](#bib.bib1)]。这些以LLM为核心的代理能够接收用户指令，收集环境信息，从记忆单元中检索知识和过去的经验，以制定明智的行动计划并通过工具调用执行。
- en: 'Specifically, most agents rely on a RAG mechanism to retrieve relevant knowlegde
    and memory from a large corpus [[19](#bib.bib19)]. While RAG has many variants,
    we mainly focus on dense retrievers and categorize them into two types based on
    their training scheme: (1) training both the retriever and generator in an end-to-end
    fashion and update the retriever with the language modeling loss (e.g. REALM [[11](#bib.bib11)],
    ORQA [[17](#bib.bib17)]); (2) training the retriever using a contrastive surrogate
    loss (e.g. DPR [[14](#bib.bib14)], ANCE [[30](#bib.bib30)], BGE [[37](#bib.bib37)]).
    We also consider the black-box OpenAI-ADA model in our experiment.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，大多数代理依赖于RAG机制从大规模语料库中检索相关知识和记忆[[19](#bib.bib19)]。虽然RAG有许多变体，我们主要关注密集型检索器，并根据其训练方案将其分为两类：（1）以端到端的方式同时训练检索器和生成器，并用语言建模损失更新检索器（例如REALM[[11](#bib.bib11)]、ORQA[[17](#bib.bib17)]）；（2）使用对比性替代损失训练检索器（例如DPR[[14](#bib.bib14)]、ANCE[[30](#bib.bib30)]、BGE[[37](#bib.bib37)]）。我们还在实验中考虑了黑箱OpenAI-ADA模型。
- en: Red-teaming LLM Agents
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 对LLM代理进行红队测试
- en: 'Extensive works have assessed the safety and trustworthiness of LLMs and RAG
    by red-teaming them with a variety of attacks such as jailbreaks [[40](#bib.bib40),
    [21](#bib.bib21), [5](#bib.bib5)], backdoor [[29](#bib.bib29), [13](#bib.bib13),
    [33](#bib.bib33)], and poisoning [[39](#bib.bib39), [41](#bib.bib41), [39](#bib.bib39)].
    However, as these works mostly treat LLM or RAG as a simple model and study their
    robustness individually, their conclusions can hardly transfer to LLM agent which
    is a much more complex system. Recently a few preliminary works also study the
    backdoor attacks on LLM agents [[32](#bib.bib32), [38](#bib.bib38)], however they
    only consider poisoning the training data of LLM backbones and fail to assess
    the safety of more capable RAG-based LLM agents. In terms of defense,  [[28](#bib.bib28)]
    seeks to defend RAG from corpus poisoning by isolating individual retrievals and
    aggregate them. However, their method can hardly defend AgentPoison as we can
    effectively ensure all the retrieved instances are poisoned. As far as we are
    concerned, we are the first work to red-team LLM agents based on RAG systems.
    Please refer to Appendix [A.5](#A1.SS5 "A.5 Additional Related Works ‣ Appendix
    A Appendix / supplemental material ‣ AgentPoison: Red-teaming LLM Agents via Poisoning
    Memory or Knowledge Bases") for more details.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '大量研究评估了LLM和RAG的安全性和可信度，通过各种攻击如越狱[[40](#bib.bib40), [21](#bib.bib21), [5](#bib.bib5)]、后门[[29](#bib.bib29),
    [13](#bib.bib13), [33](#bib.bib33)]和中毒[[39](#bib.bib39), [41](#bib.bib41), [39](#bib.bib39)]对其进行红队测试。然而，由于这些工作主要将LLM或RAG视为简单模型，并单独研究其鲁棒性，因此其结论很难转移到更复杂的LLM代理系统上。最近，一些初步的工作也研究了LLM代理上的后门攻击[[32](#bib.bib32),
    [38](#bib.bib38)]，但它们仅考虑了对LLM主干的训练数据进行中毒，未能评估基于RAG的LLM代理的安全性。在防御方面，[[28](#bib.bib28)]试图通过隔离个别检索并将其汇总来防御RAG的语料库中毒。然而，由于我们可以有效地确保所有检索到的实例都被中毒，他们的方法几乎无法防御AgentPoison。就我们而言，我们是第一个基于RAG系统对LLM代理进行红队测试的工作。有关更多详细信息，请参见附录[A.5](#A1.SS5
    "A.5 Additional Related Works ‣ Appendix A Appendix / supplemental material ‣
    AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases")。'
- en: 3 Method
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 3.1 Preliminaries and Settings
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 初步信息和设置
- en: We consider LLM agents with a RAG mechanism based on corpus retrieval. For a
    user query $q$. The LLM agent will execute the generated action by calling build-in
    tools [[9](#bib.bib9)] or external APIs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑使用基于语料库检索的RAG机制的LLM代理。对于用户查询$q$，LLM代理将通过调用内置工具[[9](#bib.bib9)]或外部API执行生成的动作。
- en: 3.2 Threat model
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 威胁模型
- en: Assumptions for the attacker
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 攻击者的假设
- en: 'We follow the standard assumption from previous backdoor attacks against LLMs [[13](#bib.bib13),
    [29](#bib.bib29)] and RAG systems [[39](#bib.bib39), [41](#bib.bib41)]. We assume
    that the attacker has partial access to the RAG database of the victim agent and
    can inject a small number of malicious instances to create a poisoned database
    $\mathcal{D}_{\text{poison}}(x_{t})=\mathcal{D}_{\text{clean}}\cup\mathcal{A}(x_{t})$.
    This assumption aligns with practical scenarios where the memory unit of the victim
    agent is hosted by a third-party retrieval service ¹¹1For example: [https://www.voyageai.com/](https://www.voyageai.com/)
    or directly leverages an unverified knowledge base. For example, an attacker can
    easily inject poisoned texts by maliciously editing Wikipedia pages [[4](#bib.bib4)]).
    Moreover, we allow the attacker to have white-box access to the RAG embedder of
    the victim agent for trigger optimization [[41](#bib.bib41)]. However, we later
    show empirically that the optimized trigger can easily transfer to a variety of
    other embedders with high success rates, including a SOTA black-box embedder OpenAI-ADA.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循之前针对LLM[[13](#bib.bib13), [29](#bib.bib29)]和RAG系统[[39](#bib.bib39), [41](#bib.bib41)]的后门攻击的标准假设。我们假设攻击者可以部分访问受害者代理的RAG数据库，并且可以注入少量恶意实例以创建中毒数据库$\mathcal{D}_{\text{poison}}(x_{t})=\mathcal{D}_{\text{clean}}\cup\mathcal{A}(x_{t})$。这一假设与实际场景一致，其中受害者代理的内存单元由第三方检索服务托管¹¹例如：[https://www.voyageai.com/](https://www.voyageai.com/)或直接利用未经验证的知识库。例如，攻击者可以通过恶意编辑维基百科页面[[4](#bib.bib4)]轻松注入中毒文本。此外，我们允许攻击者对受害者代理的RAG嵌入器进行白盒访问，以优化触发器[[41](#bib.bib41)]。然而，我们后来实证显示，优化后的触发器可以轻松地转移到各种其他嵌入器上，成功率很高，包括SOTA黑盒嵌入器OpenAI-ADA。
- en: Objectives of the attacker
  id: totrans-39
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 攻击者的目标
- en: The attacker has two adversarial goals. (a) A prescribed adversarial agent output
    (e.g. sudden stop for autonomous driving agents or deleting the patient information
    for electronic healthcare record agents) will be generated whenever the user query
    contains the optimized backdoor trigger. Formally, the attacker aims to maximize
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击者有两个对抗性目标。(a) 每当用户查询包含优化后的后门触发器时，将生成一个规定的对抗性代理输出（例如，自主驾驶代理的突然停止或电子健康记录代理的删除患者信息）。正式地，攻击者旨在最大化
- en: '|  | $1$2 |  | (1) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: where $\pi_{q}$.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\pi_{q}$。
- en: (b) Ensure the outputs for clean queries remain unaffected. Formally, the attacker
    aims to maximize
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 确保干净查询的输出保持不变。正式地，攻击者旨在最大化
- en: '|  | $1$2 |  | (2) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: where $a_{b}$. This is different from traditional DP attacks such as [[39](#bib.bib39)]
    that aim to degrade the overall system performance.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $a_{b}$。这不同于传统的 DP 攻击，如 [[39](#bib.bib39)]，其旨在降低整体系统性能。
- en: 3.3 AgentPoison
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 AgentPoison
- en: 3.3.1 Overview
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 概述
- en: 'We design AgentPoison to optimize a trigger $x_{t}$ that achieves both objectives
    of the attacker specified above. However, directly maximizing Eq. ([1](#S3.E1
    "In Objectives of the attacker ‣ 3.2 Threat model ‣ 3 Method ‣ AgentPoison: Red-teaming
    LLM Agents via Poisoning Memory or Knowledge Bases")) and Eq. ([2](#S3.E2 "In
    Objectives of the attacker ‣ 3.2 Threat model ‣ 3 Method ‣ AgentPoison: Red-teaming
    LLM Agents via Poisoning Memory or Knowledge Bases")) using gradient-based methods
    is challenging given the complexity of the RAG procedure, where the trigger is
    decisive in both the retrieval of demonstrations and the target action generation
    based on these demonstrations. Moreover, a practical attack should not only be
    effective but also stealthy and evasive, i.e., a triggered query should appear
    as a normal input and be hard to detect or remove, which we treat as coherence.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '我们设计 AgentPoison 以优化触发器 $x_{t}$，实现上述攻击者指定的两个目标。然而，由于 RAG 过程的复杂性，直接使用基于梯度的方法最大化
    Eq. ([1](#S3.E1 "在攻击者目标 ‣ 3.2 威胁模型 ‣ 3 方法 ‣ AgentPoison: 通过毒化记忆或知识库来对抗LLM代理"))
    和 Eq. ([2](#S3.E2 "在攻击者目标 ‣ 3.2 威胁模型 ‣ 3 方法 ‣ AgentPoison: 通过毒化记忆或知识库来对抗LLM代理"))
    是具有挑战性的，其中触发器在演示的检索和基于这些演示的目标动作生成中都是决定性的。此外，实际攻击不仅应有效，还应隐蔽和规避，即，触发查询应表现为正常输入，并且难以检测或删除，我们将其视为一致性。'
- en: 'Our key idea to solve these challenges is to cast the trigger optimization
    into a constrained optimization problem to jointly maximize a) retrieval effectiveness:
    the probability of retrieving from the poisoning set $\mathcal{A}(x_{t})$, i.e.,'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们解决这些挑战的关键思想是将触发器优化转化为约束优化问题，以共同最大化 a) 检索效果：从毒化集合 $\mathcal{A}(x_{t})$ 中检索的概率，即
- en: '|  | $1$2 |  | (3) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: 'and the probability of retrieving from the benign set $\mathcal{D}_{\text{clean}}$
    with high compactness between these embeddings. Intuitively, this will minimize
    the similarity between queries with and without the trigger while maximizing the
    similarity in the embedding space for any two triggered queries (see Fig. [2](#S3.F2
    "Figure 2 ‣ 3.3.1 Overview ‣ 3.3 AgentPoison ‣ 3 Method ‣ AgentPoison: Red-teaming
    LLM Agents via Poisoning Memory or Knowledge Bases")). Furthermore, the unique
    embeddings for triggered queries impart distinct semantic meanings compared to
    benign queries, enabling easy correlation with malicious actions during in-context
    learning. Finally, we propose a gradient-guided beam search algorithm to solve
    the constrained optimization problem by searching for discrete tokens under non-derivative
    constraints.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '并且从良性集合 $\mathcal{D}_{\text{clean}}$ 检索的概率，以及这些嵌入之间的高度紧凑性。直观上，这将最小化带有和不带有触发器的查询之间的相似性，同时在嵌入空间中最大化任何两个触发查询之间的相似性（见图
    [2](#S3.F2 "图 2 ‣ 3.3.1 概述 ‣ 3.3 AgentPoison ‣ 3 方法 ‣ AgentPoison: 通过毒化记忆或知识库来对抗LLM代理")）。此外，触发查询的独特嵌入赋予其与良性查询相比的独特语义含义，从而在上下文学习期间能够轻松关联恶意行为。最后，我们提出了一种梯度引导的束搜索算法，通过在非导数约束下搜索离散令牌来解决约束优化问题。'
- en: 'Our design of AgentPoison brings it two major advantages over existing attacks.
    First, AgentPoison requires no additional model training, which largely lowers
    the cost compared to existing poisoning attack [[32](#bib.bib32), [33](#bib.bib33)].
    Second, AgentPoison is more stealthy than many existing jailbreaking attacks due
    to optimizing the coherence of the triggered queries. The overview is shown in
    Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ AgentPoison: Red-teaming LLM Agents
    via Poisoning Memory or Knowledge Bases").'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对 AgentPoison 的设计带来了相较于现有攻击的两个主要优势。首先，AgentPoison 不需要额外的模型训练，相比于现有的毒害攻击大大降低了成本
    [[32](#bib.bib32), [33](#bib.bib33)]。其次，AgentPoison 由于优化触发查询的连贯性，比许多现有的破解攻击更加隐蔽。概述见图
    [1](#S1.F1 "图 1 ‣ 1 介绍 ‣ AgentPoison：通过毒害记忆或知识库进行 LLM 代理的红队测试")。
- en: '![Refer to caption](img/ba23dd20f594e8a9282e5b5effb8f35e.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ba23dd20f594e8a9282e5b5effb8f35e.png)'
- en: 'Figure 2: We demonstrate the effectiveness of the optimized triggers by AgentPoison
    and compare it with baseline CPA by visualizing their embedding space. The poisoning
    instances of CPA are shown as blue dots in (a); the poisoning instances of AgentPoison
    during iteration 0, 10, and 15 are shown as red dots and the final sampled instances
    are shown as blue dots in (b)-(d). By mapping triggered instances to a unique
    and compact region in the embedding space, AgentPoison effectively retrieves them
    without affecting other trigger-free instances to maintain benign performance.
    In contrast, CPA requires a much larger poisoning ratio meanwhile significantly
    degrading benign utility.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：我们通过 AgentPoison 展示了优化触发器的有效性，并通过可视化其嵌入空间与基线 CPA 进行比较。图 (a) 中的 CPA 的投毒实例显示为蓝色点；图
    (b)-(d) 中显示了 AgentPoison 在迭代 0、10 和 15 时的投毒实例为红色点，最终采样实例为蓝色点。通过将触发实例映射到嵌入空间中的独特且紧凑的区域，AgentPoison
    有效地检索它们而不影响其他无触发的实例，从而保持良性性能。相比之下，CPA 需要更大的投毒比例，同时显著降低了良性效用。
- en: 3.3.2 Constrained Optimization Problem
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 约束优化问题
- en: 'We construct the constrained optimization problem following the key idea in
    §[3.3.1](#S3.SS3.SSS1 "3.3.1 Overview ‣ 3.3 AgentPoison ‣ 3 Method ‣ AgentPoison:
    Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases") as the following:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据 §[3.3.1](#S3.SS3.SSS1 "3.3.1 概述 ‣ 3.3 AgentPoison ‣ 3 方法 ‣ AgentPoison：通过毒害记忆或知识库进行
    LLM 代理的红队测试") 中的关键思想构建约束优化问题，如下：
- en: '|  | $\displaystyle\underset{x_{t}}{\text{minimize}}\hskip 5.69046pt$ |  |
    (4) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underset{x_{t}}{\text{minimize}}\hskip 5.69046pt$ |  |
    (4) |'
- en: '|  | s.t. | $\displaystyle\mathcal{L}_{tar}(x_{t})\leq\eta_{tar}$ |  | (5)
    |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | 满足 | $\displaystyle\mathcal{L}_{tar}(x_{t})\leq\eta_{tar}$ |  | (5) |'
- en: '|  |  | $\displaystyle\mathcal{L}_{coh}(x_{t})\leq\eta_{coh}$ |  | (6) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathcal{L}_{coh}(x_{t})\leq\eta_{coh}$ |  | (6) |'
- en: 'where Eq. ([4](#S3.E4 "In 3.3.2 Constrained Optimization Problem ‣ 3.3 AgentPoison
    ‣ 3 Method ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge
    Bases")), Eq. ([5](#S3.E5 "In 3.3.2 Constrained Optimization Problem ‣ 3.3 AgentPoison
    ‣ 3 Method ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge
    Bases")), and Eq. ([6](#S3.E6 "In 3.3.2 Constrained Optimization Problem ‣ 3.3
    AgentPoison ‣ 3 Method ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory
    or Knowledge Bases")) correspond to the optimization goals a), b), and c), respectively.
    The constants $\eta_{tar}$.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，方程 ([4](#S3.E4 "在 3.3.2 约束优化问题 ‣ 3.3 AgentPoison ‣ 3 方法 ‣ AgentPoison：通过毒害记忆或知识库进行
    LLM 代理的红队测试"))、方程 ([5](#S3.E5 "在 3.3.2 约束优化问题 ‣ 3.3 AgentPoison ‣ 3 方法 ‣ AgentPoison：通过毒害记忆或知识库进行
    LLM 代理的红队测试")) 和方程 ([6](#S3.E6 "在 3.3.2 约束优化问题 ‣ 3.3 AgentPoison ‣ 3 方法 ‣ AgentPoison：通过毒害记忆或知识库进行
    LLM 代理的红队测试")) 分别对应优化目标 a)、b) 和 c)。常数 $\eta_{tar}$。
- en: Uniqueness loss
  id: totrans-61
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 唯一性损失
- en: 'The uniqueness loss aims to push triggered queries away from the benign queries
    in the embedding space. Let $c_{1},\cdots,c_{N}$ cluster centers corresponding
    to the keys of the benign queries in the embedding space, which can be easily
    obtained by applying (e.g.) k-means to the embeddings of the benign keys. Then
    the uniqueness loss is defined as the average distance of the input query embedding
    to all these cluster centers:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一性损失旨在将触发的查询从嵌入空间中的良性查询中推开。设$c_{1},\cdots,c_{N}$为嵌入空间中与良性查询的键对应的聚类中心，这些聚类中心可以通过将良性键的嵌入应用（例如）k-means轻松获得。然后唯一性损失定义为输入查询嵌入到所有这些聚类中心的平均距离：
- en: '|  | $1$2 |  | (7) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (7) |'
- en: Note that effectively minimizing the uniqueness loss will help to reduce the
    required poisoning ratio.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，有效地最小化唯一性损失将有助于减少所需的投毒比例。
- en: Compactness loss
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 紧凑性损失
- en: 'We define a compactness loss to improve the similarity between triggered queries
    in the embedding space:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个紧凑性损失以提高嵌入空间中触发查询之间的相似性：
- en: '|  | $1$2 |  | (8) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (8) |'
- en: 'where $1$2 is the average embedding over the triggered queries. The minimization
    of the compactness loss can further reduce the poisoning ratio. In Fig. [11](#A1.F11
    "Figure 11 ‣ A.2.5 Intermediate optimization process ‣ A.2 Additional Result and
    Analysis ‣ Appendix A Appendix / supplemental material ‣ AgentPoison: Red-teaming
    LLM Agents via Poisoning Memory or Knowledge Bases"), we show the procedure for
    joint minimization of the uniqueness loss and the compactness loss, where the
    embeddings for the triggered queries gradually form a compact cluster. Intuitively,
    the embedding of a test query containing the same trigger will fall into the same
    cluster, resulting in the retrieval of malicious key-value pairs. In comparison,
    CPA (Fig. [2](#S3.F2 "Figure 2 ‣ 3.3.1 Overview ‣ 3.3 AgentPoison ‣ 3 Method ‣
    AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases")a)
    suffers from a low accuracy in retrieving malicious key-value pairs, and it requires
    a much higher poisoning ratio to address the long-tail distribution of all the
    potential queries.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $1$2 是触发查询的平均嵌入。紧凑性损失的最小化可以进一步减少毒害比率。在图 [11](#A1.F11 "Figure 11 ‣ A.2.5
    Intermediate optimization process ‣ A.2 Additional Result and Analysis ‣ Appendix
    A Appendix / supplemental material ‣ AgentPoison: Red-teaming LLM Agents via Poisoning
    Memory or Knowledge Bases") 中，我们展示了唯一性损失和紧凑性损失的联合最小化过程，其中触发查询的嵌入逐渐形成一个紧凑的簇。直观地，包含相同触发器的测试查询的嵌入将落入相同簇，从而检索到恶意的键值对。相比之下，CPA（图
    [2](#S3.F2 "Figure 2 ‣ 3.3.1 Overview ‣ 3.3 AgentPoison ‣ 3 Method ‣ AgentPoison:
    Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases")a）在检索恶意键值对时准确性较低，并且需要更高的毒害比率以应对所有潜在查询的长尾分布。'
- en: Target generation loss
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 目标生成损失
- en: 'We maximize the generation of target malicious action $a_{m}$ by minimizing:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过最小化以下内容来最大化目标恶意动作 $a_{m}$ 的生成：
- en: '|  | $1$2 |  | (9) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (9) |'
- en: 'where $p_{\text{LLM}}(\cdot|\cdot)$ using finite samples with polynomial complexity.
    We show the corresponding analysis and proof in Appendix [A.4](#A1.SS4 "A.4 Additional
    Analysis on Optimization Approximation ‣ Appendix A Appendix / supplemental material
    ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases").'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $p_{\text{LLM}}(\cdot|\cdot)$ 使用具有多项式复杂度的有限样本。我们在附录 [A.4](#A1.SS4 "A.4 Additional
    Analysis on Optimization Approximation ‣ Appendix A Appendix / supplemental material
    ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases")
    中展示了相应的分析和证明。'
- en: Coherence loss
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 连贯性损失
- en: 'We aim to maintain high readability and coherence with the original texts in
    each query $q$ for the optimized trigger. This is achieved by minimizing:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是在每个查询 $q$ 中保持与原始文本的高可读性和连贯性。通过最小化以下内容来实现这一目标：
- en: '|  | $\mathcal{L}_{coh}(x_{t})=-\frac{1}{T}\sum_{i=0}^{T}\log p_{\text{LLM}_{b}}(q^{(i)}&#124;q^{(<i)})$
    |  | (10) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{coh}(x_{t})=-\frac{1}{T}\sum_{i=0}^{T}\log p_{\text{LLM}_{b}}(q^{(i)}&#124;q^{(<i)})$
    |  | (10) |'
- en: 'where $q_{(i)}$ denotes a small surrogate LLM (e.g. gpt-2) in our experiment.
    Different from suffix optimization that only requires fluency [[23](#bib.bib23)],
    the trigger optimized by AgentPoison can be injected into any position of the
    query (e.g. between two sentences). Thus Eq. ([10](#S3.E10 "In Coherence loss
    ‣ 3.3.2 Constrained Optimization Problem ‣ 3.3 AgentPoison ‣ 3 Method ‣ AgentPoison:
    Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases")) enforces the
    embeded trigger to be semantically coherent with the overall sequence [[10](#bib.bib10)],
    thus achieving stealthiness.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们的实验中，$q_{(i)}$ 表示一个小型替代 LLM（例如 gpt-2）。与仅要求流畅性的后缀优化不同[[23](#bib.bib23)]，AgentPoison
    优化的触发器可以被注入到查询的任何位置（例如两个句子之间）。因此，公式 ([10](#S3.E10 "In Coherence loss ‣ 3.3.2 Constrained
    Optimization Problem ‣ 3.3 AgentPoison ‣ 3 Method ‣ AgentPoison: Red-teaming LLM
    Agents via Poisoning Memory or Knowledge Bases")) 强制要求嵌入的触发器在语义上与整体序列保持一致[[10](#bib.bib10)]，从而实现隐蔽性。'
- en: 3.3.3 Optimization algorithm
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 优化算法
- en: 'We propose a gradient-based approach that optimizes Eq. ([4](#S3.E4 "In 3.3.2
    Constrained Optimization Problem ‣ 3.3 AgentPoison ‣ 3 Method ‣ AgentPoison: Red-teaming
    LLM Agents via Poisoning Memory or Knowledge Bases")) while ensuring Eq. ([9](#S3.E9
    "In Target generation loss ‣ 3.3.2 Constrained Optimization Problem ‣ 3.3 AgentPoison
    ‣ 3 Method ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge
    Bases")) and Eq. ([10](#S3.E10 "In Coherence loss ‣ 3.3.2 Constrained Optimization
    Problem ‣ 3.3 AgentPoison ‣ 3 Method ‣ AgentPoison: Red-teaming LLM Agents via
    Poisoning Memory or Knowledge Bases")) satisfy the soft constraint via a beam
    search algorithm. The key idea of our optimization algorithm is to iteratively
    search for a replacement token in the sequence that improves the objective while
    also satisfying the constraint. Our algorithm consists of the following four steps.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '我们提出了一种基于梯度的方法，该方法优化公式 ([4](#S3.E4 "在3.3.2约束优化问题 ‣ 3.3 AgentPoison ‣ 3方法 ‣
    AgentPoison: 通过毒害记忆或知识库来对抗LLM代理"))，同时确保公式 ([9](#S3.E9 "在目标生成损失 ‣ 3.3.2约束优化问题 ‣
    3.3 AgentPoison ‣ 3方法 ‣ AgentPoison: 通过毒害记忆或知识库来对抗LLM代理"))和公式 ([10](#S3.E10 "在一致性损失
    ‣ 3.3.2约束优化问题 ‣ 3.3 AgentPoison ‣ 3方法 ‣ AgentPoison: 通过毒害记忆或知识库来对抗LLM代理"))通过束搜索算法满足软约束。我们优化算法的关键思想是迭代地搜索序列中的替换标记，以提高目标值，同时也满足约束。我们的算法包括以下四个步骤。'
- en: Algorithm 1 AgentPoison Trigger Optimization
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 AgentPoison 触发器优化
- en: 1:query encoder $E_{q}$13:end for
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 1:查询编码器 $E_{q}$13:结束 for
- en: 'Initialization: To ensure context coherence, we initialize the trigger $x_{t_{0}}$
    triggers to form the initial beams (Algorithm. [4](#alg2.l4 "In Algorithm 2 ‣
    A.3.2 Additional algorithm ‣ A.3 Detailed Explanation of AgentPoison ‣ Appendix
    A Appendix / supplemental material ‣ AgentPoison: Red-teaming LLM Agents via Poisoning
    Memory or Knowledge Bases")).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '初始化：为了确保上下文的一致性，我们初始化触发器 $x_{t_{0}}$ 以形成初始束（算法 [4](#alg2.l4 "在算法 2 ‣ A.3.2
    附加算法 ‣ A.3 AgentPoison 的详细解释 ‣ 附录 A 附录/补充材料 ‣ AgentPoison: 通过毒害记忆或知识库来对抗LLM代理"))。'
- en: 'Gradient approximation: To handle discrete optimization, for each beam candidate,
    we follow [[8](#bib.bib8)] to first calculate the objective w.r.t. Eq. ([4](#S3.E4
    "In 3.3.2 Constrained Optimization Problem ‣ 3.3 AgentPoison ‣ 3 Method ‣ AgentPoison:
    Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases")) and randomly
    select a token $t_{i}$.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '梯度近似：为了处理离散优化，对于每个候选束，我们按照[[8](#bib.bib8)]的方式，首先计算相对于公式 ([4](#S3.E4 "在3.3.2约束优化问题
    ‣ 3.3 AgentPoison ‣ 3方法 ‣ AgentPoison: 通过毒害记忆或知识库来对抗LLM代理"))的目标，并随机选择一个标记 $t_{i}$。'
- en: 'Constraint filtering: Then we impose constraint Eq. ([6](#S3.E6 "In 3.3.2 Constrained
    Optimization Problem ‣ 3.3 AgentPoison ‣ 3 Method ‣ AgentPoison: Red-teaming LLM
    Agents via Poisoning Memory or Knowledge Bases")) and Eq. ([5](#S3.E5 "In 3.3.2
    Constrained Optimization Problem ‣ 3.3 AgentPoison ‣ 3 Method ‣ AgentPoison: Red-teaming
    LLM Agents via Poisoning Memory or Knowledge Bases")) sequentially. Since determination
    of $\eta_{coh}$ w.r.t. Eq. ([5](#S3.E5 "In 3.3.2 Constrained Optimization Problem
    ‣ 3.3 AgentPoison ‣ 3 Method ‣ AgentPoison: Red-teaming LLM Agents via Poisoning
    Memory or Knowledge Bases")). We notice that during early iterations most candidates
    cannot directly satisfy Eq. ([5](#S3.E5 "In 3.3.2 Constrained Optimization Problem
    ‣ 3.3 AgentPoison ‣ 3 Method ‣ AgentPoison: Red-teaming LLM Agents via Poisoning
    Memory or Knowledge Bases")), thus instead, we consider the following soft constraint:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '约束筛选：然后我们依次施加约束公式 ([6](#S3.E6 "在3.3.2约束优化问题 ‣ 3.3 AgentPoison ‣ 3方法 ‣ AgentPoison:
    通过毒害记忆或知识库来对抗LLM代理"))和公式 ([5](#S3.E5 "在3.3.2约束优化问题 ‣ 3.3 AgentPoison ‣ 3方法 ‣ AgentPoison:
    通过毒害记忆或知识库来对抗LLM代理"))。由于确定 $\eta_{coh}$ 相对于公式 ([5](#S3.E5 "在3.3.2约束优化问题 ‣ 3.3
    AgentPoison ‣ 3方法 ‣ AgentPoison: 通过毒害记忆或知识库来对抗LLM代理"))。我们注意到在早期迭代中，大多数候选项不能直接满足公式 ([5](#S3.E5
    "在3.3.2约束优化问题 ‣ 3.3 AgentPoison ‣ 3方法 ‣ AgentPoison: 通过毒害记忆或知识库来对抗LLM代理"))，因此，我们考虑以下软约束：'
- en: '|  | $1$2 |  | (11) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (11) |'
- en: where $\tau$.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\tau$。
- en: 'Token Replacement: Then we calculate $\mathcal{L}_{tar}$ tokens that improve
    the objective Eq. ([4](#S3.E4 "In 3.3.2 Constrained Optimization Problem ‣ 3.3
    AgentPoison ‣ 3 Method ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory
    or Knowledge Bases")) to form the new beams. Then we iterate this process until
    convergence. The overall procedure of the trigger optimization is detailed in
    Algorithm. [1](#alg1 "Algorithm 1 ‣ 3.3.3 Optimization algorithm ‣ 3.3 AgentPoison
    ‣ 3 Method ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge
    Bases").'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '令牌替换：然后我们计算$\mathcal{L}_{tar}$ 令牌，以提高目标 Eq. ([4](#S3.E4 "在 3.3.2 受限优化问题 ‣ 3.3
    AgentPoison ‣ 3 方法 ‣ AgentPoison: 通过毒化记忆或知识库来攻击 LLM 代理"))，以形成新的束。然后我们迭代这个过程直到收敛。触发器优化的整体过程详见算法 [1](#alg1
    "算法 1 ‣ 3.3.3 优化算法 ‣ 3.3 AgentPoison ‣ 3 方法 ‣ AgentPoison: 通过毒化记忆或知识库来攻击 LLM 代理")。'
- en: 4 Experiment
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 'Table 1: We compare AgentPoison with four baselines over ASR-r, ASR-b, ASR-t,
    ACC on four combinations of LLM agent backbones: GPT3.5 and LLaMA3-70b (Agent-Driver
    uses a fine-tuned LLaMA3-8b) and RAG retrievers: end-to-end and contrastive-based.
    Specifically, we inject 20 poisoned instances with 6 trigger tokens for Agent-Driver,
    4 instances with 5 trigger tokens for ReAct-StrategyQA, and 2 instances with 2
    trigger tokens for EHRAgent. For ASR, the maximum number in each column is in
    bold; for ACC, the number within 1% to the non-attack case is in bold.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：我们比较了 AgentPoison 与四个基准模型在四种 LLM 代理骨干（GPT3.5 和 LLaMA3-70b（Agent-Driver 使用经过微调的
    LLaMA3-8b））和 RAG 检索器（端到端和对比基础）的 ASR-r、ASR-b、ASR-t、ACC 上的表现。具体而言，我们为 Agent-Driver
    注入了 20 个带有 6 个触发令牌的毒化实例，为 ReAct-StrategyQA 注入了 4 个带有 5 个触发令牌的实例，为 EHRAgent 注入了
    2 个带有 2 个触发令牌的实例。对于 ASR，每列中的最大值用粗体表示；对于 ACC，接近非攻击情况的数字（1% 内）用粗体表示。
- en: '| Agent Backbone | Method | Agent-Driver | ReAct-StrategyQA | EHRAgent |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Agent Backbone | Method | Agent-Driver | ReAct-StrategyQA | EHRAgent |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| ASR-r | ASR-a | ASR-t | ACC | ASR-r | ASR-a | ASR-t | ACC | ASR-r | ASR-a
    | ASR-t | ACC |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| ASR-r | ASR-a | ASR-t | ACC | ASR-r | ASR-a | ASR-t | ACC | ASR-r | ASR-a
    | ASR-t | ACC |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| ChatGPT+ contrastive -retriever | Non-attack | - | - | - | 91.6 | - | - |
    - | 66.7 | - | - | - | 73.0 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT+ contrastive -retriever | 非攻击 | - | - | - | 91.6 | - | - | - | 66.7
    | - | - | - | 73.0 |'
- en: '| GCG | 18.5 | 76.1 | 37.8 | 91.0 | 40.2 | 30.8 | 38.4 | 56.6 | 9.4 | 81.3
    | 45.8 | 70.1 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| GCG | 18.5 | 76.1 | 37.8 | 91.0 | 40.2 | 30.8 | 38.4 | 56.6 | 9.4 | 81.3
    | 45.8 | 70.1 |'
- en: '| AutoDAN | 57.6 | 67.2 | 53.6 | 89.4 | 42.9 | 28.3 | 49.5 | 51.6 | 84.2 |
    89.5 | 27.4 | 68.4 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| AutoDAN | 57.6 | 67.2 | 53.6 | 89.4 | 42.9 | 28.3 | 49.5 | 51.6 | 84.2 |
    89.5 | 27.4 | 68.4 |'
- en: '| CPA | 55.8 | 62.5 | 48.7 | 86.8 | 52.8 | 66.7 | 48.9 | 55.6 | 96.9 | 58.3
    | 51.1 | 67.9 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| CPA | 55.8 | 62.5 | 48.7 | 86.8 | 52.8 | 66.7 | 48.9 | 55.6 | 96.9 | 58.3
    | 51.1 | 67.9 |'
- en: '|  | BadChain | 43.2 | 64.7 | 44.0 | 90.4 | 49.4 | 65.2 | 52.9 | 50.5 | 11.2
    | 72.5 | 8.3 | 70.8 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | BadChain | 43.2 | 64.7 | 44.0 | 90.4 | 49.4 | 65.2 | 52.9 | 50.5 | 11.2
    | 72.5 | 8.3 | 70.8 |'
- en: '|  | AgentPoison | 80.0 | 68.5 | 56.8 | 91.1 | 65.5 | 73.6 | 58.6 | 65.7 |
    98.9 | 97.9 | 58.3 | 72.9 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | AgentPoison | 80.0 | 68.5 | 56.8 | 91.1 | 65.5 | 73.6 | 58.6 | 65.7 |
    98.9 | 97.9 | 58.3 | 72.9 |'
- en: '| ChatGPT+ end-to-end -retriever | Non-attack | - | - | - | 92.7 | - | - |
    - | 59.6 | - | - | - | 71.6 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT+ end-to-end -retriever | 非攻击 | - | - | - | 92.7 | - | - | - | 59.6
    | - | - | - | 71.6 |'
- en: '| GCG | 32.1 | 60.0 | 37.3 | 91.6 | 19.5 | 30.8 | 49.5 | 54.5 | 12.5 | 63.5
    | 30.2 | 70.8 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| GCG | 32.1 | 60.0 | 37.3 | 91.6 | 19.5 | 30.8 | 49.5 | 54.5 | 12.5 | 63.5
    | 30.2 | 70.8 |'
- en: '| AutoDAN | 65.8 | 57.7 | 47.6 | 90.7 | 17.6 | 48.5 | 48.5 | 56.1 | 38.9 |
    51.6 | 42.1 | 67.4 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| AutoDAN | 65.8 | 57.7 | 47.6 | 90.7 | 17.6 | 48.5 | 48.5 | 56.1 | 38.9 |
    51.6 | 42.1 | 67.4 |'
- en: '| CPA | 73.6 | 48.5 | 50.6 | 87.5 | 22.2 | 50.0 | 51.6 | 57.1 | 61.5 | 55.8
    | 38.5 | 66.3 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| CPA | 73.6 | 48.5 | 50.6 | 87.5 | 22.2 | 50.0 | 51.6 | 57.1 | 61.5 | 55.8
    | 38.5 | 66.3 |'
- en: '|  | BadChain | 35.6 | 53.9 | 38.4 | 92.3 | 2.8 | 33.3 | 44.4 | 58.6 | 21.1
    | 50.5 | 33.7 | 71.9 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | BadChain | 35.6 | 53.9 | 38.4 | 92.3 | 2.8 | 33.3 | 44.4 | 58.6 | 21.1
    | 50.5 | 33.7 | 71.9 |'
- en: '|  | AgentPoison | 84.4 | 64.9 | 59.6 | 92.0 | 64.7 | 54.7 | 70.7 | 57.6 |
    97.9 | 91.7 | 53.7 | 74.8 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | AgentPoison | 84.4 | 64.9 | 59.6 | 92.0 | 64.7 | 54.7 | 70.7 | 57.6 |
    97.9 | 91.7 | 53.7 | 74.8 |'
- en: '| LLaMA3+ contrastive -retriever | Non-attack | - | - | - | 83.6 | - | - |
    - | 47.5 | - | - | - | 37.7 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA3+ contrastive -retriever | 非攻击 | - | - | - | 83.6 | - | - | - | 47.5
    | - | - | - | 37.7 |'
- en: '| GCG | 12.5 | 90.3 | 42.5 | 82.4 | 36.7 | 29.6 | 64.4 | 45.6 | 16.4 | 14.8
    | 29.5 | 44.2 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| GCG | 12.5 | 90.3 | 42.5 | 82.4 | 36.7 | 29.6 | 64.4 | 45.6 | 16.4 | 14.8
    | 29.5 | 44.2 |'
- en: '| AutoDAN | 54.2 | 92.9 | 49.8 | 83.0 | 48.5 | 41.3 | 68.3 | 36.6 | 75.4 |
    6.6 | 57.4 | 36.1 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| AutoDAN | 54.2 | 92.9 | 49.8 | 83.0 | 48.5 | 41.3 | 68.3 | 36.6 | 75.4 |
    6.6 | 57.4 | 36.1 |'
- en: '| CPA | 69.7 | 91.2 | 51.5 | 78.4 | 52.0 | 25.0 | 58.5 | 37.0 | 96.9 | 24.6
    | 72.1 | 34.4 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| CPA | 69.7 | 91.2 | 51.5 | 78.4 | 52.0 | 25.0 | 58.5 | 37.0 | 96.9 | 24.6
    | 72.1 | 34.4 |'
- en: '|  | BadChain | 43.2 | 92.4 | 41.3 | 82.0 | 44.6 | 23.1 | 62.4 | 39.6 | 31.1
    | 18.0 | 65.6 | 29.5 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | BadChain | 43.2 | 92.4 | 41.3 | 82.0 | 44.6 | 23.1 | 62.4 | 39.6 | 31.1
    | 18.0 | 65.6 | 29.5 |'
- en: '|  | AgentPoison | 78.0 | 94.7 | 54.7 | 84.0 | 58.4 | 22.5 | 72.3 | 47.5 |
    100.0 | 21.5 | 65.6 | 41.0 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | AgentPoison | 78.0 | 94.7 | 54.7 | 84.0 | 58.4 | 22.5 | 72.3 | 47.5 |
    100.0 | 21.5 | 65.6 | 41.0 |'
- en: '| LLaMA3+ end-to-end -retriever | Non-attack | - | - | - | 83.0 | - | - | -
    | 51.0 | - | - | - | 32.8 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA3+ 端到端检索器 | 非攻击 | - | - | - | 83.0 | - | - | - | 51.0 | - | - | - |
    32.8 |'
- en: '| GCG | 14.8 | 88.5 | 38.0 | 80.4 | 19.1 | 25.0 | 37.3 | 37.3 | 8.8 | 11.5
    | 19.7 | 34.4 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| GCG | 14.8 | 88.5 | 38.0 | 80.4 | 19.1 | 25.0 | 37.3 | 37.3 | 8.8 | 11.5
    | 19.7 | 34.4 |'
- en: '| AutoDAN | 62.6 | 55.3 | 49.6 | 81.7 | 11.0 | 34.1 | 22.7 | 37.3 | 13.1 |
    1.6 | 8.2 | 31.1 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| AutoDAN | 62.6 | 55.3 | 49.6 | 81.7 | 11.0 | 34.1 | 22.7 | 37.3 | 13.1 |
    1.6 | 8.2 | 31.1 |'
- en: '| CPA | 72.9 | 44.3 | 51.2 | 79.3 | 28.1 | 30.0 | 52.9 | 47.5 | 15.3 | 4.8
    | 8.6 | 21.3 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| CPA | 72.9 | 44.3 | 51.2 | 79.3 | 28.1 | 30.0 | 52.9 | 47.5 | 15.3 | 4.8
    | 8.6 | 21.3 |'
- en: '|  | BadChain | 35.6 | 85.5 | 50.3 | 78.4 | 1.2 | 0.0 | 45.1 | 49.0 | 6.2 |
    8.2 | 13.1 | 31.1 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | BadChain | 35.6 | 85.5 | 50.3 | 78.4 | 1.2 | 0.0 | 45.1 | 49.0 | 6.2 |
    8.2 | 13.1 | 31.1 |'
- en: '|  | AgentPoison | 82.4 | 93.2 | 58.9 | 82.4 | 66.7 | 21.7 | 72.5 | 47.0 |
    96.7 | 7.7 | 68.9 | 34.4 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  | AgentPoison | 82.4 | 93.2 | 58.9 | 82.4 | 66.7 | 21.7 | 72.5 | 47.0 |
    96.7 | 7.7 | 68.9 | 34.4 |'
- en: 4.1 Setup
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 设置
- en: 'LLM Agent: To demonstrate the generalization of AgentPoison, we select three
    types of real-world agents across a variety of tasks: Agent-Driver [[22](#bib.bib22)]
    for autonomous driving, ReAct [[34](#bib.bib34)] agent for knowledge-intensive
    QA, and EHRAgent [[25](#bib.bib25)] for healthcare record management.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: LLM Agent：为了展示AgentPoison的泛化能力，我们选择了三种不同类型的真实世界代理，涉及各种任务：用于自动驾驶的Agent-Driver [[22](#bib.bib22)]，用于知识密集型问答的ReAct [[34](#bib.bib34)]代理，以及用于医疗记录管理的EHRAgent [[25](#bib.bib25)]。
- en: 'Memory/Knowledge base: For agent-driver we use its corresponding dataset published
    in their paper, which contain 23k experiences in the memory unit³³3[https://github.com/USC-GVL/Agent-Driver](https://github.com/USC-GVL/Agent-Driver).
    For ReAct, we select a more challenging multi-step commonsense QA dataset StrategyQA
    which involves a curated knowledge base of 10k passages from Wikipedia⁴⁴4[https://allenai.org/data/strategyqa](https://allenai.org/data/strategyqa).
    For EHRAgent, it originally initializes its knowledge base with only four experiences
    and updates its memory dynamically. However we notice that almost all baselines
    have a high attack success rate on the database with such a few entries, we augment
    its memory unit with 700 experiences that we collect from successful trials to
    make the red-teaming task more challenging.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 内存/知识库：对于agent-driver，我们使用其论文中发布的相应数据集，该数据集包含23k个经验在内存单元中³³3[https://github.com/USC-GVL/Agent-Driver](https://github.com/USC-GVL/Agent-Driver)。对于ReAct，我们选择了一个更具挑战性的多步骤常识问答数据集StrategyQA，它涉及来自维基百科的10k段落的精心策划的知识库⁴⁴4[https://allenai.org/data/strategyqa](https://allenai.org/data/strategyqa)。对于EHRAgent，它最初只用四个经验来初始化其知识库，并动态更新其内存。然而，我们注意到几乎所有基线在如此少的条目数据库上都有很高的攻击成功率，因此我们用从成功试验中收集的700个经验来增强其内存单元，使得红队任务更具挑战性。
- en: 'Baselines: To assess the effectiveness of AgentPoison, we consider the following
    baselines for trigger optimization: Greedy Coordinate Gradient (GCG) [[40](#bib.bib40)],
    AutoDAN [[21](#bib.bib21)], Corpus Poisoning Attack (CPA) [[39](#bib.bib39)],
    and BadChain [[29](#bib.bib29)]. Specifically, we optimize GCG w.r.t. the target
    loss Eq. ([9](#S3.E9 "In Target generation loss ‣ 3.3.2 Constrained Optimization
    Problem ‣ 3.3 AgentPoison ‣ 3 Method ‣ AgentPoison: Red-teaming LLM Agents via
    Poisoning Memory or Knowledge Bases")), and since we observe AutoDAN performs
    badly when directly optimizing Eq. ([9](#S3.E9 "In Target generation loss ‣ 3.3.2
    Constrained Optimization Problem ‣ 3.3 AgentPoison ‣ 3 Method ‣ AgentPoison: Red-teaming
    LLM Agents via Poisoning Memory or Knowledge Bases")), we calibrate its fitness
    function and augment Eq. ([9](#S3.E9 "In Target generation loss ‣ 3.3.2 Constrained
    Optimization Problem ‣ 3.3 AgentPoison ‣ 3 Method ‣ AgentPoison: Red-teaming LLM
    Agents via Poisoning Memory or Knowledge Bases")) by Eq. ([3](#S3.E3 "In 3.3.1
    Overview ‣ 3.3 AgentPoison ‣ 3 Method ‣ AgentPoison: Red-teaming LLM Agents via
    Poisoning Memory or Knowledge Bases")) with Lagrangian multipliers. And we use
    the default objective and trigger optimization algorithm for CPA and BadChain.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '基准测试：为了评估AgentPoison的有效性，我们考虑以下触发器优化的基准：贪婪坐标梯度（GCG）[[40](#bib.bib40)]、AutoDAN[[21](#bib.bib21)]、语料库中毒攻击（CPA）[[39](#bib.bib39)]和BadChain[[29](#bib.bib29)]。具体来说，我们针对目标损失方程优化GCG（[9](#S3.E9
    "In Target generation loss ‣ 3.3.2 Constrained Optimization Problem ‣ 3.3 AgentPoison
    ‣ 3 Method ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge
    Bases")），由于我们观察到AutoDAN在直接优化方程（[9](#S3.E9 "In Target generation loss ‣ 3.3.2 Constrained
    Optimization Problem ‣ 3.3 AgentPoison ‣ 3 Method ‣ AgentPoison: Red-teaming LLM
    Agents via Poisoning Memory or Knowledge Bases")）时表现不佳，因此我们调整其适应度函数，并通过方程（[3](#S3.E3
    "In 3.3.1 Overview ‣ 3.3 AgentPoison ‣ 3 Method ‣ AgentPoison: Red-teaming LLM
    Agents via Poisoning Memory or Knowledge Bases")）与拉格朗日乘子一起增强方程（[9](#S3.E9 "In
    Target generation loss ‣ 3.3.2 Constrained Optimization Problem ‣ 3.3 AgentPoison
    ‣ 3 Method ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge
    Bases")）。我们还使用了CPA和BadChain的默认目标和触发器优化算法。'
- en: 'Evaluation metrics: We consider the following metrics: (1) attack success rate
    for retrieval (ASR-r), which is the percentage of test instances where all the
    retrieved demonstrations from the database are poisoned; (2) attack success rate
    for the target action (ASR-a), which is the percentage of test instances where
    the agent generates the target action (e.g., "sudden stop") conditioned on successful
    retrieval of poisoned instances. Thus, ASR-a individually assesses the performance
    of the trigger w.r.t. inducing the adversarial action. Then we further consider
    (3) end-to-end target attack success rate (ASR-t), which is the percentage of
    test instances where the agent achieves the final adversarial impact on the environment
    (e.g., collision) that depends on the entire agent system, which is a critical
    metric that distinguishes from previous LLMs attack. Finally, we consider (4)
    benign accuracy (ACC), which is the percentage of test instances with correct
    action output without the trigger, which measures the model utility under the
    attack. A successful backdoor attack is characterized by a high ASR and a small
    degradation in the ACC compared with the non-backdoor cases. We detail the backdoor
    strategy and definition of attack targets for each agent in Appendix [A.3.1](#A1.SS3.SSS1
    "A.3.1 Backdoor demonstrations ‣ A.3 Detailed Explanation of AgentPoison ‣ Appendix
    A Appendix / supplemental material ‣ AgentPoison: Red-teaming LLM Agents via Poisoning
    Memory or Knowledge Bases") and Appendix [A.1.2](#A1.SS1.SSS2 "A.1.2 Target Definition
    ‣ A.1 Experimental Settings ‣ Appendix A Appendix / supplemental material ‣ AgentPoison:
    Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases"), respectively.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '评估指标：我们考虑以下指标：（1）检索攻击成功率（ASR-r），即数据库中所有检索到的演示都被污染的测试实例的百分比；（2）目标动作攻击成功率（ASR-a），即代理在成功检索到污染实例的情况下生成目标动作（例如“突然停车”）的测试实例的百分比。因此，ASR-a
    单独评估触发器在诱导对抗性动作方面的表现。然后我们进一步考虑（3）端到端目标攻击成功率（ASR-t），即代理在环境中实现最终对抗性影响（例如碰撞）的测试实例的百分比，这取决于整个代理系统，这是一个关键指标，与之前的LLM攻击区分开来。最后，我们考虑（4）良性准确率（ACC），即在没有触发器的情况下具有正确动作输出的测试实例的百分比，衡量攻击下的模型效用。成功的后门攻击以高ASR和ACC的降幅较小为特征。我们在附录 [A.3.1](#A1.SS3.SSS1
    "A.3.1 Backdoor demonstrations ‣ A.3 Detailed Explanation of AgentPoison ‣ Appendix
    A Appendix / supplemental material ‣ AgentPoison: Red-teaming LLM Agents via Poisoning
    Memory or Knowledge Bases") 和附录 [A.1.2](#A1.SS1.SSS2 "A.1.2 Target Definition
    ‣ A.1 Experimental Settings ‣ Appendix A Appendix / supplemental material ‣ AgentPoison:
    Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases") 中详细说明了每个代理的后门策略和攻击目标定义。'
- en: '![Refer to caption](img/6c33c19de6606207f9dcfda8ed4c8bf1.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/6c33c19de6606207f9dcfda8ed4c8bf1.png)'
- en: 'Figure 3: Transferability confusion matrix showcasing the performance of the
    triggers optimized on the source embedder (y-axis) transferring to the target
    embedder (x-axis) w.r.t. ASR-r (a), ASR-a (b), and ACC (c) on Agent-Driver. We
    can denote that (1) trigger optimized with AgentPoison generally transfer well
    across dense retrievers; (2) triggers transfer better among embedders with similar
    training strategy (i.e. end-to-end (REALM, ORQA); contrastive (DPR, ANCE, BGE)).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：转移混淆矩阵展示了在源嵌入器（y轴）上优化的触发器在转移到目标嵌入器（x轴）时的性能，相对于ASR-r（a），ASR-a（b），以及ACC（c）在Agent-Driver上的表现。我们可以指出（1）使用AgentPoison优化的触发器通常能够在密集检索器之间良好转移；（2）触发器在具有相似训练策略的嵌入器之间转移效果更佳（即端到端（REALM,
    ORQA）；对比（DPR, ANCE, BGE））。
- en: '![Refer to caption](img/17a626271e053f50afb41e07bc9067be.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/17a626271e053f50afb41e07bc9067be.png)'
- en: 'Figure 4: Comparing the performance of AgentPoison with random trigger and
    CPA w.r.t. the number of poisoned instances in the database (left) and the number
    of tokens in the trigger (right). We fix the number of tokens to 4 for the former
    case and the number of poisoned instances to 32 for the latter case. Two metrics
    ASR-r (retrieval success rate) and ACC (benign utility) are studied.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：比较了AgentPoison与随机触发器和CPA在数据库中污染实例数量（左）和触发器中的令牌数量（右）方面的表现。我们将前一种情况的令牌数量固定为4，将后一种情况的污染实例数量固定为32。研究了两个指标ASR-r（检索成功率）和ACC（良性效用）。
- en: 4.2 Result
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 结果
- en: 'AgentPoison demonstrates superior attack success rate and benign utility. We
    report the performance of all methods in Table [1](#S4.T1 "Table 1 ‣ 4 Experiment
    ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases").
    We categorize the result into two types of LLM backbones, i.e. GPT3.5 and LLaMA3,
    and two types of retrievers trained via end-to-end loss or contrastive loss. We
    observe that algorithms that optimize for retrieval i.e. AgentPoison, CPA and
    AutoDAN has better ASR-r, however CPA and AutoDAN also hampers the benign utility
    (indicated by low ACC) as they invariably degrade all retrievals. As a comparison,
    AgentPoison has minimal impact on benign performance of average $0.74\%$ to be
    a very high success rate in terms of real-world impact.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 'AgentPoison 展示了卓越的攻击成功率和良好的实用性。我们在表 [1](#S4.T1 "Table 1 ‣ 4 Experiment ‣ AgentPoison:
    Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases") 中报告了所有方法的性能。我们将结果分为两种
    LLM 主干，即 GPT3.5 和 LLaMA3，以及两种通过端到端损失或对比损失训练的检索器。我们观察到优化检索的算法，即 AgentPoison、CPA
    和 AutoDAN 具有更好的 ASR-r，但 CPA 和 AutoDAN 也会损害良好的实用性（由低 ACC 指示），因为它们不可避免地降低所有检索的质量。相比之下，AgentPoison
    对良好性能的影响最小，平均 $0.74\%$ 的成功率在实际影响方面非常高。'
- en: 'AgentPoison has high transferability across embedders. We assess the transferability
    of the optimized triggers on five dense retrievers, i.e. DPR [[14](#bib.bib14)],
    ANCE [[30](#bib.bib30)], BGE [[37](#bib.bib37)], REALM [[11](#bib.bib11)], and
    ORQA [[17](#bib.bib17)] to each other and the text-embedding-ada-002 model⁵⁵5[https://platform.openai.com/docs/guides/embeddings](https://platform.openai.com/docs/guides/embeddings)
    with API-only access. We report the results for Agent-Driver in Fig. [3](#S4.F3
    "Figure 3 ‣ 4.1 Setup ‣ 4 Experiment ‣ AgentPoison: Red-teaming LLM Agents via
    Poisoning Memory or Knowledge Bases"), and ReAct-StrategyQA and EHRAgent in Fig. [7](#A1.F7
    "Figure 7 ‣ A.1.2 Target Definition ‣ A.1 Experimental Settings ‣ Appendix A Appendix
    / supplemental material ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory
    or Knowledge Bases") and Fig. [8](#A1.F8 "Figure 8 ‣ A.1.2 Target Definition ‣
    A.1 Experimental Settings ‣ Appendix A Appendix / supplemental material ‣ AgentPoison:
    Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases") (Appendix [A.2.2](#A1.SS2.SSS2
    "A.2.2 Additional Transferability Result ‣ A.2 Additional Result and Analysis
    ‣ Appendix A Appendix / supplemental material ‣ AgentPoison: Red-teaming LLM Agents
    via Poisoning Memory or Knowledge Bases")). We observe AgentPoison has a high
    transferability across a variety of embedders (even on embedders with different
    training schemes). We conclude the high transferability results from our objective
    in Eq. ([4](#S3.E4 "In 3.3.2 Constrained Optimization Problem ‣ 3.3 AgentPoison
    ‣ 3 Method ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge
    Bases")) that optimizes for a unique cluster in the embedding space which is also
    semantically unique on embedders trained with similar data distribution.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 'AgentPoison 在各种嵌入器之间具有高迁移性。我们评估了优化触发器在五个密集检索器上的迁移性，即 DPR [[14](#bib.bib14)],
    ANCE [[30](#bib.bib30)], BGE [[37](#bib.bib37)], REALM [[11](#bib.bib11)], 和 ORQA
    [[17](#bib.bib17)] 彼此之间以及 text-embedding-ada-002 模型⁵⁵5 [https://platform.openai.com/docs/guides/embeddings](https://platform.openai.com/docs/guides/embeddings)
    的 API-only 访问。我们在图 [3](#S4.F3 "Figure 3 ‣ 4.1 Setup ‣ 4 Experiment ‣ AgentPoison:
    Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases") 中报告了 Agent-Driver
    的结果，在图 [7](#A1.F7 "Figure 7 ‣ A.1.2 Target Definition ‣ A.1 Experimental Settings
    ‣ Appendix A Appendix / supplemental material ‣ AgentPoison: Red-teaming LLM Agents
    via Poisoning Memory or Knowledge Bases") 和图 [8](#A1.F8 "Figure 8 ‣ A.1.2 Target
    Definition ‣ A.1 Experimental Settings ‣ Appendix A Appendix / supplemental material
    ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases")
    (附录 [A.2.2](#A1.SS2.SSS2 "A.2.2 Additional Transferability Result ‣ A.2 Additional
    Result and Analysis ‣ Appendix A Appendix / supplemental material ‣ AgentPoison:
    Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases")) 中报告了 ReAct-StrategyQA
    和 EHRAgent 的结果。我们观察到 AgentPoison 在各种嵌入器之间具有高迁移性（即使在具有不同训练方案的嵌入器上）。我们得出结论，较高的迁移性源于我们的目标公式
    Eq. ([4](#S3.E4 "In 3.3.2 Constrained Optimization Problem ‣ 3.3 AgentPoison ‣
    3 Method ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge
    Bases"))，该公式优化了嵌入空间中的唯一簇，这在使用类似数据分布训练的嵌入器上也是语义唯一的。'
- en: 'Table 2: An ablation study of the performance w.r.t. individual components
    in AgentPoison. Specifically, we study the case using GPT3.5 backbone and retriever
    trained with contrastive loss. An additional metric perplexity (PPL) of the triggered
    queries is considered. Best performance is in bold.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：对 AgentPoison 中各个组件性能的消融研究。具体来说，我们研究了使用 GPT3.5 主干和通过对比损失训练的检索器的情况。考虑了触发查询的额外度量困惑度（PPL）。最佳性能用粗体字表示。
- en: '| Method | Agent-Driver | ReAct-StrategyQA | EHRAgent |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | Agent-Driver | ReAct-StrategyQA | EHRAgent |'
- en: '| --- | --- | --- | --- |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| ASR-r | ASR-a | ASR-t | ACC | PPL | ASR-r | ASR-a | ASR-t | ACC | PPL | ASR-r
    | ASR-a | ASR-t | ACC | PPL |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| ASR-r | ASR-a | ASR-t | ACC | PPL | ASR-r | ASR-a | ASR-t | ACC | PPL | ASR-r
    | ASR-a | ASR-t | ACC | PPL |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- |'
- en: '| w/o $\mathcal{L}_{\text{uni}}$ | 57.4 | 63.1 | 51.0 | 87.8 | 13.7 | 25.5
    | 58.6 | 42.0 | 57.1 | 63.7 | 65.6 | 88.5 | 37.7 | 65.6 | 643.9 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 无 $\mathcal{L}_{\text{uni}}$ | 57.4 | 63.1 | 51.0 | 87.8 | 13.7 | 25.5 |
    58.6 | 42.0 | 57.1 | 63.7 | 65.6 | 88.5 | 37.7 | 65.6 | 643.9 |'
- en: '| w/o $\mathcal{L}_{\text{cpt}}$ | 63.0 | 64.4 | 54.0 | 90.1 | 14.2 | 38.6
    | 61.1 | 47.0 | 62.8 | 67.1 | 82.0 | 93.4 | 59.0 | 72.5 | 622.5 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 无 $\mathcal{L}_{\text{cpt}}$ | 63.0 | 64.4 | 54.0 | 90.1 | 14.2 | 38.6 |
    61.1 | 47.0 | 62.8 | 67.1 | 82.0 | 93.4 | 59.0 | 72.5 | 622.5 |'
- en: '| w/o $\mathcal{L}_{\text{tar}}$ | 81.3 | 61.8 | 55.1 | 91.3 | 14.9 | 57.1
    | 72.2 | 45.9 | 62.0 | 71.5 | 90.2 | 96.7 | 83.6 | 75.4 | 581.0 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 无 $\mathcal{L}_{\text{tar}}$ | 81.3 | 61.8 | 55.1 | 91.3 | 14.9 | 57.1 |
    72.2 | 45.9 | 62.0 | 71.5 | 90.2 | 96.7 | 83.6 | 75.4 | 581.0 |'
- en: '| w/o $\mathcal{L}_{\text{coh}}$ | 83.5 | 67.7 | 57.7 | 91.5 | 36.6 | 67.7
    | 77.7 | 52.8 | 67.1 | 81.8 | 95.4 | 90.1 | 70.5 | 77.0 | 955.4 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 无 $\mathcal{L}_{\text{coh}}$ | 83.5 | 67.7 | 57.7 | 91.5 | 36.6 | 67.7 |
    77.7 | 52.8 | 67.1 | 81.8 | 95.4 | 90.1 | 70.5 | 77.0 | 955.4 |'
- en: '| AgentPoison | 80.0 | 68.5 | 56.8 | 91.1 | 14.8 | 65.5 | 73.6 | 58.6 | 65.7
    | 76.6 | 98.9 | 97.9 | 58.3 | 72.9 | 505.0 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| AgentPoison | 80.0 | 68.5 | 56.8 | 91.1 | 14.8 | 65.5 | 73.6 | 58.6 | 65.7
    | 76.6 | 98.9 | 97.9 | 58.3 | 72.9 | 505.0 |'
- en: 'Table 3: We assess the resilience of the optimized trigger by studying three
    types of perturbations on the trigger in the input query while keeping the poisoned
    instances fixed. Specifically, we consider injecting three random letters, injecting
    one word in the sequence, and rephrasing the trigger while maintaining its semantic
    meaning. We prompt GPT3.5 to obtain the corresponding perturbations.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：我们通过研究输入查询中触发器的三种扰动类型来评估优化触发器的弹性，同时保持中毒实例不变。具体来说，我们考虑了注入三个随机字母、在序列中注入一个单词以及改写触发器但保持其语义。我们提示
    GPT3.5 以获得相应的扰动。
- en: '| Method | Agent-Driver | ReAct-StrategyQA | EHRAgent |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | Agent-Driver | ReAct-StrategyQA | EHRAgent |'
- en: '| --- | --- | --- | --- |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| ASR-r | ASR-a | ASR-t | ACC | ASR-r | ASR-a | ASR-t | ACC | ASR-r | ASR-a
    | ASR-t | ACC |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| ASR-r | ASR-a | ASR-t | ACC | ASR-r | ASR-a | ASR-t | ACC | ASR-r | ASR-a
    | ASR-t | ACC |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Letter injection | 46.9 | 64.2 | 45.0 | 91.6 | 84.9 | 69.7 | 57.0 | 52.1
    | 90.3 | 95.6 | 53.8 | 70.0 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 字母注入 | 46.9 | 64.2 | 45.0 | 91.6 | 84.9 | 69.7 | 57.0 | 52.1 | 90.3 | 95.6
    | 53.8 | 70.0 |'
- en: '| Word injection | 78.4 | 67.1 | 52.5 | 91.3 | 92.9 | 73.0 | 62.4 | 50.8 |
    93.0 | 96.8 | 57.2 | 72.0 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 单词注入 | 78.4 | 67.1 | 52.5 | 91.3 | 92.9 | 73.0 | 62.4 | 50.8 | 93.0 | 96.8
    | 57.2 | 72.0 |'
- en: '| Rephrasing | 66.0 | 65.1 | 49.7 | 91.2 | 88.0 | 64.2 | 58.1 | 49.6 | 85.1
    | 83.4 | 50.0 | 72.9 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 改写 | 66.0 | 65.1 | 49.7 | 91.2 | 88.0 | 64.2 | 58.1 | 49.6 | 85.1 | 83.4
    | 50.0 | 72.9 |'
- en: 'Table 4: Performance (ASR-t) under two types of defense: PPL Filter [[2](#bib.bib2)]
    and Query Rephrasing [[15](#bib.bib15)].'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：在两种防御类型下的表现（ASR-t）：PPL 过滤器 [[2](#bib.bib2)] 和查询改写 [[15](#bib.bib15)]。
- en: '| Method | Agent-Driver | ReAct-StrategyQA |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | Agent-Driver | ReAct-StrategyQA |'
- en: '| --- | --- | --- |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| PPL Filter | Rephrasing | PPL Filter | Rephrasing |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| PPL 过滤器 | 改写 | PPL 过滤器 | 改写 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| GCG | 4.6 | 13.2 | 24.0 | 28.0 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| GCG | 4.6 | 13.2 | 24.0 | 28.0 |'
- en: '| BadChain | 43.0 | 36.9 | 42.0 | 36.0 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| BadChain | 43.0 | 36.9 | 42.0 | 36.0 |'
- en: '| AgentPoison | 47.2 | 50.0 | 61.2 | 62.0 | ![Refer to caption](img/b02a8509c0fc5e346db9a75b773995ad.png)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '| AgentPoison | 47.2 | 50.0 | 61.2 | 62.0 | ![参见说明](img/b02a8509c0fc5e346db9a75b773995ad.png)'
- en: 'Figure 5: Perplexity density distribution of benign, AgentPoison and GCG queries.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：良性查询、AgentPoison 和 GCG 查询的困惑度密度分布。
- en: 'AgentPoison performs well even when we inject only one instance in the knowledge
    base with one token in the trigger. We further study the performance of AgentPoison
    w.r.t. the number of poisoned instances in the database and the number of tokens
    in the trigger sequence, and report the findings in Fig. [4](#S4.F4 "Figure 4
    ‣ 4.1 Setup ‣ 4 Experiment ‣ AgentPoison: Red-teaming LLM Agents via Poisoning
    Memory or Knowledge Bases"). We observe that after optimization, AgentPoison has
    high ASR-r ($62.0\%$).'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 'AgentPoison 即使在知识库中只注入一个实例，并且触发器中只有一个标记时，也表现良好。我们进一步研究了 AgentPoison 关于数据库中毒实例数量和触发器序列中标记数量的性能，并在图 [4](#S4.F4
    "图 4 ‣ 4.1 设置 ‣ 4 实验 ‣ AgentPoison: 通过毒化内存或知识库来对抗 LLM 代理")中报告了结果。我们观察到，经过优化后，AgentPoison
    的 ASR-r 较高（$62.0\%$）。'
- en: 'How does each individual loss contributes to AgentPoison? The ablation result
    is reported in Table [2](#S4.T2 "Table 2 ‣ 4.2 Result ‣ 4 Experiment ‣ AgentPoison:
    Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases"), where we disable
    one component each time. We observe $\mathcal{L}_{uni}$ slightly degrades the
    performance, it leads to better in-context coherence, which can effectively bypass
    some perplexity-based countermeasures.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '每个单独的损失如何对 AgentPoison 产生影响？消融结果见表[2](#S4.T2 "Table 2 ‣ 4.2 Result ‣ 4 Experiment
    ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases")，其中我们每次禁用一个组件。我们观察到
    $\mathcal{L}_{uni}$ 略微降低了性能，但它在上下文连贯性上表现更好，这可以有效绕过一些基于困惑度的对策。'
- en: 'AgentPoison is resilient to perturbations in the trigger sequence. We further
    study the resilience of the optimized triggers by considering three types of perturbations
    in Table [3](#S4.T3 "Table 3 ‣ 4.2 Result ‣ 4 Experiment ‣ AgentPoison: Red-teaming
    LLM Agents via Poisoning Memory or Knowledge Bases"). We observe AgentPoison is
    resilient to word injection, and slightly compromised to letter injection. This
    is because letter injection can change over three tokens in the sequence which
    can completely flip the semantic distribution of the trigger. Notably, rephrasing
    the trigger which completely change the token sequence also maintains high performance,
    as long as the trigger semantics is preserved.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 'AgentPoison 对触发序列中的扰动具有强韧性。我们进一步通过考虑表[3](#S4.T3 "Table 3 ‣ 4.2 Result ‣ 4 Experiment
    ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases")中的三种扰动类型来研究优化触发器的韧性。我们观察到，AgentPoison
    对词语注入具有韧性，对字母注入稍有妥协。这是因为字母注入可以改变序列中的三个以上的标记，从而完全翻转触发器的语义分布。值得注意的是，只要触发器的语义得以保留，重新措辞的触发器（完全改变标记序列）也能保持高性能。'
- en: 'How does AgentPoison perform under potential defense? We study two types of
    defense: Perplexity Filter [[2](#bib.bib2)] and Query Rephrasing [[15](#bib.bib15)]
    (here we rephrase the whole query which is different from Table [3](#S4.T3 "Table
    3 ‣ 4.2 Result ‣ 4 Experiment ‣ AgentPoison: Red-teaming LLM Agents via Poisoning
    Memory or Knowledge Bases")) which are often used to prevent LLMs from injection
    attack. We report the ASR-t in Table [5](#S4.F5 "Figure 5 ‣ 4.2 Result ‣ 4 Experiment
    ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases")
    and full result in Table [6](#A1.T6 "Table 6 ‣ A.2.4 Potential Defense ‣ A.2 Additional
    Result and Analysis ‣ Appendix A Appendix / supplemental material ‣ AgentPoison:
    Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases") (Appendix [A.2.4](#A1.SS2.SSS4
    "A.2.4 Potential Defense ‣ A.2 Additional Result and Analysis ‣ Appendix A Appendix
    / supplemental material ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory
    or Knowledge Bases")). Compared with GCG and Badchain, the trigger optimized by
    AgentPoison is more readable and coherent to the agent context, making it resilient
    under both defenses. We further justify this observation in Fig. [5](#S4.F5 "Figure
    5 ‣ 4.2 Result ‣ 4 Experiment ‣ AgentPoison: Red-teaming LLM Agents via Poisoning
    Memory or Knowledge Bases") where we compare the perplexity distribution of queries
    optimized by AgentPoison to benign queries and GCG. Compared to GCG, the queries
    of AgentPoison are highly evasive by being inseparable from the benign queries.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 'AgentPoison 在潜在防御下的表现如何？我们研究了两种防御：困惑度过滤器 [[2](#bib.bib2)] 和查询重述 [[15](#bib.bib15)]（这里我们重新措辞整个查询，与表[3](#S4.T3
    "Table 3 ‣ 4.2 Result ‣ 4 Experiment ‣ AgentPoison: Red-teaming LLM Agents via
    Poisoning Memory or Knowledge Bases")不同），这些防御措施通常用于防止 LLM 注入攻击。我们在表[5](#S4.F5
    "Figure 5 ‣ 4.2 Result ‣ 4 Experiment ‣ AgentPoison: Red-teaming LLM Agents via
    Poisoning Memory or Knowledge Bases")中报告了 ASR-t，并在表[6](#A1.T6 "Table 6 ‣ A.2.4
    Potential Defense ‣ A.2 Additional Result and Analysis ‣ Appendix A Appendix /
    supplemental material ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory
    or Knowledge Bases")（附录[A.2.4](#A1.SS2.SSS4 "A.2.4 Potential Defense ‣ A.2 Additional
    Result and Analysis ‣ Appendix A Appendix / supplemental material ‣ AgentPoison:
    Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases")）中报告了完整结果。与 GCG
    和 Badchain 相比，AgentPoison 优化的触发器更具可读性和与代理上下文的一致性，使其在这两种防御下都具有韧性。我们在图[5](#S4.F5
    "Figure 5 ‣ 4.2 Result ‣ 4 Experiment ‣ AgentPoison: Red-teaming LLM Agents via
    Poisoning Memory or Knowledge Bases")中进一步证明了这一观察结果，在该图中我们比较了 AgentPoison 优化的查询与良性查询和
    GCG 的困惑度分布。与 GCG 相比，AgentPoison 的查询通过与良性查询无法分离的方式表现出高度的规避性。'
- en: 5 Conclusion
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, we propose a novel red-teaming approach AgentPoison to holistically
    assess the safety and trustworthiness of RAG-based LLM agents. Specifically, AgentPoison
    consists of a constrained trigger optimization algorithm that seeks to map the
    queries into a unique and compact region in the embedding space to ensure high
    retrieval accuracy and end-to-end attack success rate. Notably, AgentPoison does
    not require any model training while the optimized trigger is highly transferable,
    stealthy, and coherent. Extensive experiments on three real-world agents demonstrate
    the effectiveness of AgentPoison over four baselines across four comprehensive
    metrics.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种新颖的红队方法 AgentPoison，旨在全面评估基于 RAG 的 LLM 代理的安全性和可信度。具体而言，AgentPoison
    由一个受限触发器优化算法组成，该算法旨在将查询映射到嵌入空间中的独特且紧凑的区域，以确保高检索准确性和端到端攻击成功率。值得注意的是，AgentPoison
    不需要任何模型训练，而优化的触发器具有高度的可转移性、隐秘性和连贯性。在三个现实世界代理上的广泛实验表明，AgentPoison 在四个综合指标上优于四个基线方法。
- en: References
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Mahyar Abbasian, Iman Azimi, Amir M Rahmani, and Ramesh Jain. Conversational
    health agents: A personalized llm-powered agent framework. arXiv preprint arXiv:2310.02374,
    2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Mahyar Abbasian, Iman Azimi, Amir M Rahmani 和 Ramesh Jain. 对话健康代理：个性化的
    LLM 驱动代理框架。arXiv 预印本 arXiv:2310.02374, 2023。'
- en: '[2] Gabriel Alon and Michael Kamfonas. Detecting language model attacks with
    perplexity. arXiv preprint arXiv:2308.14132, 2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Gabriel Alon 和 Michael Kamfonas. 使用困惑度检测语言模型攻击。arXiv 预印本 arXiv:2308.14132,
    2023。'
- en: '[3] Martin Anthony, Peter L Bartlett, Peter L Bartlett, et al. Neural network
    learning: Theoretical foundations, volume 9. cambridge university press Cambridge,
    1999.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Martin Anthony, Peter L Bartlett, Peter L Bartlett 等. 神经网络学习：理论基础，第 9 卷。剑桥大学出版社，1999。'
- en: '[4] Nicholas Carlini, Matthew Jagielski, Christopher A Choquette-Choo, Daniel
    Paleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas, and Florian
    Tramèr. Poisoning web-scale training datasets is practical. arXiv preprint arXiv:2302.10149,
    2023.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Nicholas Carlini, Matthew Jagielski, Christopher A Choquette-Choo, Daniel
    Paleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas 和 Florian Tramèr.
    对大规模训练数据集进行毒化是可行的。arXiv 预印本 arXiv:2302.10149, 2023。'
- en: '[5] Zhaorun Chen, Zhuokai Zhao, Wenjie Qu, Zichen Wen, Zhiguang Han, Zhihong
    Zhu, Jiaheng Zhang, and Huaxiu Yao. Pandora: Detailed llm jailbreaking via collaborated
    phishing agents with decomposed reasoning. In ICLR 2024 Workshop on Secure and
    Trustworthy Large Language Models, 2024.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Zhaorun Chen, Zhuokai Zhao, Wenjie Qu, Zichen Wen, Zhiguang Han, Zhihong
    Zhu, Jiaheng Zhang 和 Huaxiu Yao. Pandora: 通过协作钓鱼代理和分解推理进行详细的 LLM 破解。在 ICLR 2024
    研讨会：安全和可信的大型语言模型，2024。'
- en: '[6] Can Cui, Zichong Yang, Yupeng Zhou, Yunsheng Ma, Juanwu Lu, Lingxi Li,
    Yaobin Chen, Jitesh Panchal, and Ziran Wang. Personalized autonomous driving with
    large language models: Field experiments, 2024.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Can Cui, Zichong Yang, Yupeng Zhou, Yunsheng Ma, Juanwu Lu, Lingxi Li,
    Yaobin Chen, Jitesh Panchal 和 Ziran Wang. 使用大型语言模型的个性化自动驾驶：实地实验，2024。'
- en: '[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:
    Pre-training of deep bidirectional transformers for language understanding. arXiv
    preprint arXiv:1810.04805, 2018.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee 和 Kristina Toutanova. Bert: 深度双向变换器的预训练用于语言理解。arXiv
    预印本 arXiv:1810.04805, 2018。'
- en: '[8] Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotflip: White-box
    adversarial examples for text classification. arXiv preprint arXiv:1712.06751,
    2017.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Javid Ebrahimi, Anyi Rao, Daniel Lowd 和 Dejing Dou. Hotflip: 用于文本分类的白盒对抗示例。arXiv
    预印本 arXiv:1712.06751, 2017。'
- en: '[9] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang,
    Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International
    Conference on Machine Learning, pages 10764–10799\. PMLR, 2023.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang,
    Jamie Callan 和 Graham Neubig. Pal: 程序辅助语言模型。国际机器学习会议，页码 10764–10799\. PMLR, 2023。'
- en: '[10] Xingang Guo, Fangxu Yu, Huan Zhang, Lianhui Qin, and Bin Hu. Cold-attack:
    Jailbreaking llms with stealthiness and controllability. arXiv preprint arXiv:2402.08679,
    2024.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Xingang Guo, Fangxu Yu, Huan Zhang, Lianhui Qin 和 Bin Hu. Cold-attack:
    利用隐秘性和可控性破解大型语言模型。arXiv 预印本 arXiv:2402.08679, 2024。'
- en: '[11] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang.
    Retrieval augmented language model pre-training. In International conference on
    machine learning, pages 3929–3938\. PMLR, 2020.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat 和 Mingwei Chang. 检索增强语言模型预训练。国际机器学习会议，页码
    3929–3938\. PMLR, 2020。'
- en: '[12] Ye Jin, Xiaoxi Shen, Huiling Peng, Xiaoan Liu, Jingli Qin, Jiayang Li,
    Jintao Xie, Peizhong Gao, Guyue Zhou, and Jiangtao Gong. Surrealdriver: Designing
    generative driver agent simulation framework in urban contexts based on large
    language model, 2023.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Ye Jin, Xiaoxi Shen, Huiling Peng, Xiaoan Liu, Jingli Qin, Jiayang Li,
    Jintao Xie, Peizhong Gao, Guyue Zhou 和 Jiangtao Gong。Surrealdriver: 基于大型语言模型设计的城市环境中的生成型驾驶代理模拟框架，2023年。'
- en: '[13] Nikhil Kandpal, Matthew Jagielski, Florian Tramèr, and Nicholas Carlini.
    Backdoor attacks for in-context learning with language models. arXiv preprint
    arXiv:2307.14692, 2023.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Nikhil Kandpal, Matthew Jagielski, Florian Tramèr 和 Nicholas Carlini。语言模型的上下文学习中的后门攻击。arXiv
    预印本 arXiv:2307.14692，2023年。'
- en: '[14] Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu,
    Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain
    question answering. arXiv preprint arXiv:2004.04906, 2020.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu,
    Sergey Edunov, Danqi Chen 和 Wen-tau Yih。用于开放领域问答的密集 passage 检索。arXiv 预印本 arXiv:2004.04906，2020年。'
- en: '[15] Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Soheil Feizi, and Hima Lakkaraju.
    Certifying llm safety against adversarial prompting. arXiv preprint arXiv:2309.02705,
    2023.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Soheil Feizi 和 Hima Lakkaraju。对抗性提示下的
    LLM 安全认证。arXiv 预印本 arXiv:2309.02705，2023年。'
- en: '[16] Jakub Lála, Odhran O’Donoghue, Aleksandar Shtedritski, Sam Cox, Samuel G
    Rodriques, and Andrew D White. Paperqa: Retrieval-augmented generative agent for
    scientific research. arXiv preprint arXiv:2312.07559, 2023.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Jakub Lála, Odhran O’Donoghue, Aleksandar Shtedritski, Sam Cox, Samuel
    G Rodriques 和 Andrew D White。Paperqa: 科学研究的检索增强生成代理。arXiv 预印本 arXiv:2312.07559，2023年。'
- en: '[17] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for
    weakly supervised open domain question answering. arXiv preprint arXiv:1906.00300,
    2019.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Kenton Lee, Ming-Wei Chang 和 Kristina Toutanova。用于弱监督开放领域问答的潜在检索。arXiv
    预印本 arXiv:1906.00300，2019年。'
- en: '[18] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
    Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel,
    et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances
    in Neural Information Processing Systems, 33:9459–9474, 2020.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
    Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel
    等人。针对知识密集型自然语言处理任务的检索增强生成。神经信息处理系统进展，第33卷：9459–9474，2020年。'
- en: '[19] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
    Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel,
    Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive
    nlp tasks. In Proceedings of the 34th International Conference on Neural Information
    Processing Systems, 2020.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
    Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel,
    Sebastian Riedel 和 Douwe Kiela。针对知识密集型自然语言处理任务的检索增强生成。在第34届国际神经信息处理系统大会论文集中，2020年。'
- en: '[20] Junkai Li, Siyu Wang, Meng Zhang, Weitao Li, Yunghwei Lai, Xinhui Kang,
    Weizhi Ma, and Yang Liu. Agent hospital: A simulacrum of hospital with evolvable
    medical agents, 2024.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Junkai Li, Siyu Wang, Meng Zhang, Weitao Li, Yunghwei Lai, Xinhui Kang,
    Weizhi Ma 和 Yang Liu。Agent hospital: 一个具有可进化医疗代理的医院模拟体，2024年。'
- en: '[21] Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. Autodan: Generating
    stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451,
    2023.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Xiaogeng Liu, Nan Xu, Muhao Chen 和 Chaowei Xiao。Autodan: 在对齐的大型语言模型上生成隐蔽的越狱提示。arXiv
    预印本 arXiv:2310.04451，2023年。'
- en: '[22] Jiageng Mao, Junjie Ye, Yuxi Qian, Marco Pavone, and Yue Wang. A language
    agent for autonomous driving. arXiv preprint arXiv:2311.10813, 2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Jiageng Mao, Junjie Ye, Yuxi Qian, Marco Pavone 和 Yue Wang。用于自主驾驶的语言代理。arXiv
    预印本 arXiv:2311.10813，2023年。'
- en: '[23] Anselm Paulus, Arman Zharmagambetov, Chuan Guo, Brandon Amos, and Yuandong
    Tian. Advprompter: Fast adaptive adversarial prompting for llms. arXiv preprint
    arXiv:2404.16873, 2024.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Anselm Paulus, Arman Zharmagambetov, Chuan Guo, Brandon Amos 和 Yuandong
    Tian。Advprompter: 快速自适应对抗提示生成。arXiv 预印本 arXiv:2404.16873，2024年。'
- en: '[24] Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework:
    Bm25 and beyond. Foundations and Trends® in Information Retrieval, 3(4):333–389,
    2009.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Stephen Robertson, Hugo Zaragoza 等人。概率相关框架：BM25 及其扩展。信息检索基础与趋势®，第3卷第4期：333–389，2009年。'
- en: '[25] Wenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Jieyu Zhang, Hang Wu, Yuanda
    Zhu, Joyce Ho, Carl Yang, and May D. Wang. Ehragent: Code empowers large language
    models for few-shot complex tabular reasoning on electronic health records, 2024.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Wenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Jieyu Zhang, Hang Wu, Yuanda
    Zhu, Joyce Ho, Carl Yang 和 May D. Wang。Ehragent: 代码赋能大型语言模型在电子健康记录上的少量复杂表格推理，2024年。'
- en: '[26] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous
    agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366,
    2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] 诺亚·辛、贝克·拉巴什和阿什温·戈比纳斯。Reflexion：具有动态记忆和自我反思的自主代理。arXiv 预印本 arXiv:2303.11366，2023年。'
- en: '[27] Tao Tu, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg, Ryutaro
    Tanno, Amy Wang, Brenna Li, Mohamed Amin, Nenad Tomasev, Shekoofeh Azizi, Karan
    Singhal, Yong Cheng, Le Hou, Albert Webson, Kavita Kulkarni, S Sara Mahdavi, Christopher
    Semturs, Juraj Gottweis, Joelle Barral, Katherine Chou, Greg S Corrado, Yossi
    Matias, Alan Karthikesalingam, and Vivek Natarajan. Towards conversational diagnostic
    ai, 2024.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] 陶图、安尼尔·帕莱普、迈克·沙克曼、哈立德·萨布、简·弗雷伯格、田龙太郎、艾米·王、布伦娜·李、穆罕默德·阿敏、内纳德·托马舍夫、谢库费赫·阿齐兹、卡兰·辛格、永成、乐厚、阿尔伯特·韦布森、卡维塔·库尔卡尼、S·萨拉·马赫达维、克里斯托弗·塞姆图斯、尤拉伊·戈特维斯、乔埃尔·巴拉尔、凯瑟琳·周、格雷格·S·科拉多、约西·马蒂亚斯、艾伦·卡提克萨林甘和维韦克·纳塔拉詹。迈向对话诊断
    AI，2024年。'
- en: '[28] Chong Xiang, Tong Wu, Zexuan Zhong, David Wagner, Danqi Chen, and Prateek
    Mittal. Certifiably robust rag against retrieval corruption. arXiv preprint arXiv:2405.15556,
    2024.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] 相冲、吴通、钟泽轩、大卫·瓦格纳、陈丹琪和普拉蒂克·米塔尔。认证鲁棒的 RAG 对抗检索腐败。arXiv 预印本 arXiv:2405.15556，2024年。'
- en: '[29] Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian, Radha
    Poovendran, and Bo Li. Badchain: Backdoor chain-of-thought prompting for large
    language models. arXiv preprint arXiv:2401.12242, 2024.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] 项臻、姜风晴、熊子弟、巴斯卡·拉马苏布拉曼、拉达·普文德兰和鲍莉。Badchain：针对大语言模型的链式思维提示后门。arXiv 预印本 arXiv:2401.12242，2024年。'
- en: '[30] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett,
    Junaid Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive
    learning for dense text retrieval. arXiv preprint arXiv:2007.00808, 2020.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] 李雄、陈彦雄、叶莉、邝丰唐、刘佳霖、保罗·贝内特、朱奈德·艾哈迈德和阿诺德·欧维维克。用于密集文本检索的近似最近邻负对比学习。arXiv 预印本
    arXiv:2007.00808，2020年。'
- en: '[31] Qisen Yang, Zekun Wang, Honghui Chen, Shenzhi Wang, Yifan Pu, Xin Gao,
    Wenhao Huang, Shiji Song, and Gao Huang. Llm agents for psychology: A study on
    gamified assessments, 2024.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] 杨启森、王泽坤、陈宏辉、王申志、蒲逸凡、肖鑫、黄文浩、宋诗基和高黄。心理学中的 LLM 代理：关于游戏化评估的研究，2024年。'
- en: '[32] Wenkai Yang, Xiaohan Bi, Yankai Lin, Sishuo Chen, Jie Zhou, and Xu Sun.
    Watch out for your agents! investigating backdoor threats to llm-based agents.
    arXiv preprint arXiv:2402.11208, 2024.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] 王文凯、毕晓寒、林彦凯、陈思硕、周杰和孙旭。小心你的代理！调查针对基于 LLM 的代理的后门威胁。arXiv 预印本 arXiv:2402.11208，2024年。'
- en: '[33] Hongwei Yao, Jian Lou, and Zhan Qin. Poisonprompt: Backdoor attack on
    prompt-based large language models. In ICASSP 2024-2024 IEEE International Conference
    on Acoustics, Speech and Signal Processing (ICASSP), pages 7745–7749\. IEEE, 2024.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] 姚宏伟、娄剑和秦展。Poisonprompt：针对基于提示的大语言模型的后门攻击。见 ICASSP 2024-2024 IEEE 国际声学、语音与信号处理会议（ICASSP），第7745–7749页。IEEE，2024年。'
- en: '[34] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,
    and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv
    preprint arXiv:2210.03629, 2022.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] 邵春宇、杰弗里·赵、袁颠和杜楠、伊扎克·沙夫兰、卡尔提克·纳拉辛汉和曹源。React：在语言模型中协同推理和行动。arXiv 预印本 arXiv:2210.03629，2022年。'
- en: '[35] Yangyang Yu, Haohang Li, Zhi Chen, Yuechen Jiang, Yang Li, Denghui Zhang,
    Rong Liu, Jordan W. Suchow, and Khaldoun Khashanah. Finmem: A performance-enhanced
    llm trading agent with layered memory and character design, 2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] 杨洋、李浩航、陈智、姜越辰、杨莉、邓辉章、刘融、乔丹·W·苏乔和哈勒顿·哈沙纳。Finmem：具有分层记忆和角色设计的性能增强型 LLM 交易代理，2023年。'
- en: '[36] Jianhao Yuan, Shuyang Sun, Daniel Omeiza, Bo Zhao, Paul Newman, Lars Kunze,
    and Matthew Gadd. Rag-driver: Generalisable driving explanations with retrieval-augmented
    in-context learning in multi-modal large language model. arXiv preprint arXiv:2402.10828,
    2024.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] 袁建豪、孙书洋、丹尼尔·奥梅扎、鲍·赵、保罗·纽曼、拉斯·昆泽和马修·加德。Rag-driver：使用检索增强的上下文学习在多模态大语言模型中的可泛化驾驶解释。arXiv
    预印本 arXiv:2402.10828，2024年。'
- en: '[37] Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie.
    Retrieve anything to augment large language models. arXiv preprint arXiv:2310.07554,
    2023.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] 张培天、肖士涛、刘政、窦志成和聂剑云。检索任何内容以增强大语言模型。arXiv 预印本 arXiv:2310.07554，2023年。'
- en: '[38] Yunchao Zhang, Zonglin Di, Kaiwen Zhou, Cihang Xie, and Xin Eric Wang.
    Navigation as attackers wish? towards building byzantine-robust embodied agents
    under federated learning. arXiv preprint arXiv:2211.14769, 2022.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] 张云超、邹宗霖、周凯文、谢自航和王鑫·埃里克。导航如攻击者所愿？迈向在联邦学习下构建拜占庭鲁棒的具身代理。arXiv 预印本 arXiv:2211.14769，2022年。'
- en: '[39] Zexuan Zhong, Ziqing Huang, Alexander Wettig, and Danqi Chen. Poisoning
    retrieval corpora by injecting adversarial passages. arXiv preprint arXiv:2310.19156,
    2023.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Zexuan Zhong, Ziqing Huang, Alexander Wettig, 和 Danqi Chen. 通过注入对抗性段落来毒害检索语料库。arXiv预印本
    arXiv:2310.19156, 2023.'
- en: '[40] Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and
    transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043,
    2023.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Andy Zou, Zifan Wang, J Zico Kolter, 和 Matt Fredrikson. 对齐语言模型的通用和可转移对抗性攻击。arXiv预印本
    arXiv:2307.15043, 2023.'
- en: '[41] Wei Zou, Runpeng Geng, Binghui Wang, and Jinyuan Jia. Poisonedrag: Knowledge
    poisoning attacks to retrieval-augmented generation of large language models.
    arXiv preprint arXiv:2402.07867, 2024.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Wei Zou, Runpeng Geng, Binghui Wang, 和 Jinyuan Jia. Poisonedrag: 知识毒害攻击对大语言模型的检索增强生成。arXiv预印本
    arXiv:2402.07867, 2024.'
- en: Broader Impacts
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更广泛的影响
- en: In this paper, we propose AgentPoison, the first backdoor attack against LLM
    agents with RAG. The main purpose of this research is to red-team LLM agents with
    RAG so that their developers are aware of the threat and take action to mitigate
    it. Moreover, our empirical results can help other researchers to understand the
    behavior of RAG systems used by LLM agents. Code is released at [https://github.com/BillChan226/AgentPoison](https://github.com/BillChan226/AgentPoison).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了AgentPoison，这是针对具有RAG的LLM代理的首个后门攻击。该研究的主要目的是对LLM代理进行红队测试，以使其开发者意识到威胁并采取措施进行缓解。此外，我们的实证结果可以帮助其他研究人员了解LLM代理使用的RAG系统的行为。代码已发布在[https://github.com/BillChan226/AgentPoison](https://github.com/BillChan226/AgentPoison)。
- en: Limitations
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: While AgentPoison is effective in optimizing triggers to achieve high retrieval
    accuracy and attack success rate, it requires the attacker to have white-box access
    to the embedder. However, we show empirically that AgentPoison can transfer well
    among different embedders even with different training schemes, since AgentPoison
    optimizes for a semantically unique region in the embedding space, which is also
    likely to be unique for other embedders as long as they share similar training
    data distribution. This way, the attacker can easily red-team a proprietary agent
    by simply leveraging a public open-source embedder to optimize for such a universal
    trigger.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管AgentPoison在优化触发器以实现高检索准确性和攻击成功率方面是有效的，但它要求攻击者具有对嵌入器的白盒访问权限。然而，我们通过实证展示了AgentPoison即使在不同的嵌入器和训练方案之间也能很好地迁移，因为AgentPoison优化的是嵌入空间中一个语义上独特的区域，而只要共享相似的训练数据分布，该区域对其他嵌入器也可能是独特的。这样，攻击者可以通过简单地利用一个公开的开源嵌入器来优化这样一个通用触发器，从而轻松对一个专有代理进行红队测试。
- en: Appendix A Appendix / supplemental material
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 附录/补充材料
- en: A.1 Experimental Settings
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 实验设置
- en: A.1.1 Hyperparameters
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.1.1 超参数
- en: 'The hyperparameters for AgentPoison and our experiments are reported in Table [5](#A1.T5
    "Table 5 ‣ A.1.1 Hyperparameters ‣ A.1 Experimental Settings ‣ Appendix A Appendix
    / supplemental material ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory
    or Knowledge Bases").'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 'AgentPoison和我们的实验的超参数在表[5](#A1.T5 "Table 5 ‣ A.1.1 Hyperparameters ‣ A.1 Experimental
    Settings ‣ Appendix A Appendix / supplemental material ‣ AgentPoison: Red-teaming
    LLM Agents via Poisoning Memory or Knowledge Bases")中报告。'
- en: 'Table 5: Hyperparameter Settings for AgentPoison'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：AgentPoison的超参数设置
- en: '| Parameters | Value |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 值 |'
- en: '| --- | --- |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| $\mathcal{L}_{tar}$ |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{L}_{tar}$ |'
- en: '| Number of replacement token $m$ |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 替换标记的数量 $m$ |'
- en: '| Number of sub-sampled token $s$ |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 子采样标记的数量 $s$ |'
- en: '| Gradient accumulation steps | $30$ |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 梯度累积步骤 | $30$ |'
- en: '| Iterations per gradient optimization | $1000$ |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 每次梯度优化的迭代次数 | $1000$ |'
- en: '| Batch size | $64$ |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 批量大小 | $64$ |'
- en: '| Surrogate LLM | gpt-2⁶⁶6[https://huggingface.co/openai-community/gpt2](https://huggingface.co/openai-community/gpt2)
    |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 代理LLM | gpt-2⁶⁶6[https://huggingface.co/openai-community/gpt2](https://huggingface.co/openai-community/gpt2)
    |'
- en: 'Except for obtaining the result in Fig. [4](#S4.F4 "Figure 4 ‣ 4.1 Setup ‣
    4 Experiment ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge
    Bases"), we keep the number of tokens in the trigger fixed, where we have 6 tokens
    for Agent-Driver [[22](#bib.bib22)], 5 tokens for ReAct-StrategyQA [[34](#bib.bib34)],
    and 2 tokens for EHRAgent [[25](#bib.bib25)], and we inject 20 poisoned instances
    for Agent-Driver, 4 for ReAct, and 2 for EHRAgent across all experiments. The
    number of tokens in the trigger sequence are mainly determined by the length of
    the original queries. We inject fewer than $0.1\%$ instances w.r.t. the original
    number of instances in the database for all attack methods, since we observe that
    as more instances have been poisoned, it gets harder to distinguish to effectiveness
    of different methods, as reported in Fig. [4](#S4.F4 "Figure 4 ‣ 4.1 Setup ‣ 4
    Experiment ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge
    Bases").'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '除了获得图 [4](#S4.F4 "Figure 4 ‣ 4.1 Setup ‣ 4 Experiment ‣ AgentPoison: Red-teaming
    LLM Agents via Poisoning Memory or Knowledge Bases") 中的结果外，我们保持触发器中的标记数量固定，其中
    Agent-Driver [[22](#bib.bib22)] 使用 6 个标记，ReAct-StrategyQA [[34](#bib.bib34)] 使用
    5 个标记，EHRAgent [[25](#bib.bib25)] 使用 2 个标记，我们在所有实验中分别为 Agent-Driver 注入 20 个污染实例，为
    ReAct 注入 4 个，为 EHRAgent 注入 2 个。触发器序列中的标记数量主要由原始查询的长度决定。对于所有攻击方法，我们注入的实例数量少于 $0.1\%$，因为我们观察到随着污染实例的增加，区分不同方法的有效性变得更加困难，如图 [4](#S4.F4
    "Figure 4 ‣ 4.1 Setup ‣ 4 Experiment ‣ AgentPoison: Red-teaming LLM Agents via
    Poisoning Memory or Knowledge Bases") 所述。'
- en: A.1.2 Target Definition
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.1.2 目标定义
- en: We detail the attack target for AgentPoison in this section. Specifically, for
    all three agents, we consider it a success retrieval (thus counted in ASR-r) only
    if all the retrieved instances (usually k-nearest neighbors) are poisoned demonstrations
    that we previously injected into the database. Such requirements are practical
    and necessary for evaluating attack success for retrievals since many agents have
    certain in-built safety filters to further select useful demonstrations from all
    the retrieval results (e.g. Agent-Driver [[22](#bib.bib22)] instantiates a re-examination
    process where they use a LLM to select one experience which is most relevant to
    the retrieved $k$ instances). This way an adversary can certify attack success
    only if all the retrieved instances are malicious. Recent defense [[28](#bib.bib28)]
    which seeks to certify RAG from corpus poisoning attacks by isolate-then-aggregate
    further necessitates this requirement on such agent-oriented attacks. By effectively
    manipulating all the retrieved demonstrations to be poisoned instances, AgentPoison
    can easily bypass such SOTA defense.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节中详细说明了 AgentPoison 的攻击目标。具体而言，对于所有三个代理，只有当所有检索到的实例（通常是 k 最近邻）都是我们之前注入到数据库中的污染演示时，才视为成功检索（因此计入
    ASR-r）。这种要求对评估检索攻击的成功非常实际和必要，因为许多代理具有某些内建的安全过滤器，用于进一步从所有检索结果中选择有用的演示（例如，代理-驾驶员
    [[22](#bib.bib22)] 实例化了一个再审查过程，其中使用 LLM 选择与检索到的 $k$ 实例最相关的经验）。通过这种方式，攻击者只有在所有检索到的实例都是恶意的情况下，才能认证攻击成功。最近的防御
    [[28](#bib.bib28)] 试图通过先隔离后汇总来认证 RAG 对抗语料库污染攻击，这进一步要求这种以代理为导向的攻击。通过有效操控所有检索到的演示成为污染实例，AgentPoison
    可以轻松绕过这种 SOTA 防御。
- en: Specifically, we detail the target action and target outcome for attacking each
    agent.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们详细说明了攻击每个代理的目标行为和目标结果。
- en: •
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Agent-Driver: we denote irresponsible and potentially unsafe driving behaviors
    to be our attack target. Specifically, target action for the agent to output is
    sudden stop. And the corresponding real-world outcome is measured by the trajectory
    deviation in the future three seconds. Mathematically, attack success for ASR-t
    is indicated by the following indicator function:'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代理-驾驶员：我们将不负责任和潜在不安全的驾驶行为作为我们的攻击目标。具体而言，代理输出的目标行为是突然停车。相应的实际结果通过未来三秒的轨迹偏差来衡量。在数学上，ASR-t
    的攻击成功由以下指标函数表示：
- en: '|  | <math id="A1.E12.m1.3" class="ltx_Math" alttext="\text{}\mathbbm{1}\left(\frac{1}{T}\int_{t}^{t+T}\&#124;\mathbf{\hat{\zeta}}(s)-\mathbf{\zeta}(s)\&#124;^{2}\,ds></math>
    |  | (12) |'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | <math id="A1.E12.m1.3" class="ltx_Math" alttext="\text{}\mathbbm{1}\left(\frac{1}{T}\int_{t}^{t+T}\&#124;\mathbf{\hat{\zeta}}(s)-\mathbf{\zeta}(s)\&#124;^{2}\,ds></math>
    |  | (12) |'
- en: where $\hat{\zeta}$ is the preset threshold that determines the maximum deviation
    of a safe trajectory.
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $\hat{\zeta}$ 是预设的阈值，用于确定安全轨迹的最大偏差。
- en: •
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ReAct-StrategyQA: we denote wrong answer or unhelpful information to be our
    attack target for QA-based agent. As we provide the poison sample following [[29](#bib.bib29)]
    (as detailed in Appendix [A.3.1](#A1.SS3.SSS1 "A.3.1 Backdoor demonstrations ‣
    A.3 Detailed Explanation of AgentPoison ‣ Appendix A Appendix / supplemental material
    ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases")),
    the target action for the agent to output is as there is a {trigger} in the sequence
    during its internal reasoning (denoting successfully backdoored in-context learning),
    and the corresponding outcome is that the agent finishes the episode with such
    wrong answer or unhelpful answer (e.g., I don’t know).'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ReAct-StrategyQA：我们将错误答案或无用信息作为QA基础代理的攻击目标。正如我们提供的毒化样本[[29](#bib.bib29)]（详见附录[A.3.1](#A1.SS3.SSS1
    "A.3.1 后门演示 ‣ A.3 AgentPoison详细说明 ‣ 附录 A 附录/补充材料 ‣ AgentPoison: 通过毒化记忆或知识库对抗LLM代理")），代理的目标行为是在其内部推理过程中出现{trigger}（表示成功地后门化上下文学习），相应的结果是代理以错误答案或无用答案（例如，我不知道）结束该回合。'
- en: •
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'EHRAgent: considering the purpose and safety issues of healthcare record maintenance,
    we denote deletion to be a highly risky and potentially unsafe operation. Thus
    we design the target action for the agent such that it should output delete data
    of patient ID during the reasoning step, and the corresponding outcome is a SQL
    code command DeleteDB.'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: EHRAgent：考虑到医疗记录维护的目的和安全问题，我们将删除视为一种高风险且潜在不安全的操作。因此，我们设计代理的目标行为是，在推理步骤中输出删除患者ID的数据，相应的结果是SQL代码命令DeleteDB。
- en: '![Refer to caption](img/393e723127765f9d73bd2356df614179.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/393e723127765f9d73bd2356df614179.png)'
- en: 'Figure 6: A scatter plot which compares AgentPoison with four baselines over
    ASR-r, ACC on four combinations of LLM agent backbones: GPT3.5 and LLaMA3, and
    retrievers: end-to-end and contrastive-based. Specifically, we inject 20 poisoned
    instances for Agent-Driver, 4 for ReAct, and 2 for EHRAgent. Specifically, different
    trigger optimization algorithms are represented with different shapes. green denotes
    the retriever is trained via end-to-end scheme and blue denotes the retriever
    is trained via a contrastive surrogate task.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：散点图比较了AgentPoison与四个基线模型在四种LLM代理骨干（GPT3.5和LLaMA3）及检索器（端到端和对比性）的ASR-r、ACC上的表现。具体地，我们为Agent-Driver注入了20个被污染的实例，为ReAct注入了4个，为EHRAgent注入了2个。不同的触发器优化算法用不同的形状表示。绿色表示检索器通过端到端方案进行训练，蓝色表示检索器通过对比性替代任务进行训练。
- en: '![Refer to caption](img/8bbb61ea320bf6a70a16ac583cddc631.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8bbb61ea320bf6a70a16ac583cddc631.png)'
- en: 'Figure 7: Transferability confusion matrix showcasing the performance of the
    triggers optimized on the source embedder (y-axis) transferring to the target
    embedder (x-axis) w.r.t. ASR-r (a), ASR-a (b), and ACC (c) on ReAct-StrategyQA.
    We can denote that (1) trigger optimized with AgentPoison generally transfer well
    across dense retrievers; (2) triggers transfer better among embedders with similar
    training strategy (i.e. end-to-end (REALM, ORQA); contrastive (DPR, ANCE, BGE)).'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：可转移性混淆矩阵展示了在源嵌入器（y轴）上优化的触发器在目标嵌入器（x轴）上的表现，分别针对ASR-r（a）、ASR-a（b）和ACC（c）在ReAct-StrategyQA上的转移性能。我们可以表明：（1）使用AgentPoison优化的触发器通常在密集检索器之间转移良好；（2）触发器在具有相似训练策略的嵌入器之间转移效果更佳（即端到端（REALM，ORQA）；对比性（DPR，ANCE，BGE））。
- en: '![Refer to caption](img/92383a014aa41c175276dc94b4b790a4.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/92383a014aa41c175276dc94b4b790a4.png)'
- en: 'Figure 8: Transferability confusion matrix showcasing the performance of the
    triggers optimized on the source embedder (y-axis) transferring to the target
    embedder (x-axis) w.r.t. ASR-r (a), ASR-a (b), and ACC (c) on EHRAgent. We can
    denote that (1) trigger optimized with AgentPoison generally transfer well across
    dense retrievers; (2) triggers transfer better among embedders with similar training
    strategy (i.e. end-to-end (REALM, ORQA); contrastive (DPR, ANCE, BGE)).'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：可转移性混淆矩阵展示了在源嵌入器（y轴）上优化的触发器在目标嵌入器（x轴）上的表现，分别针对ASR-r（a）、ASR-a（b）和ACC（c）在EHRAgent上的转移性能。我们可以表明：（1）使用AgentPoison优化的触发器通常在密集检索器之间转移良好；（2）触发器在具有相似训练策略的嵌入器之间转移效果更佳（即端到端（REALM，ORQA）；对比性（DPR，ANCE，BGE））。
- en: A.1.3 Data and Model Preparation
  id: totrans-240
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.1.3 数据和模型准备
- en: Train/Test split For Agent-Driver, we have randomly sampled 250 samples from
    its validation set (apart from the 23k samples in the training set); for ReAct
    agent, we have used the full test set in StrategyQA⁷⁷7[https://allenai.org/data/strategyqa](https://allenai.org/data/strategyqa)
    which consists of 229 samples; and for EHRAgent, we have randomly selected 100
    samples from its validation set in our experiment. Besides, the poisoned samples
    are all sampled from the training set of each agent which does not overlap with
    the test set.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 训练/测试划分 对于Agent-Driver，我们从其验证集中随机抽取了250个样本（排除训练集中的23k样本）；对于ReAct代理，我们使用了StrategyQA的完整测试集⁷⁷7[https://allenai.org/data/strategyqa](https://allenai.org/data/strategyqa)，其中包含229个样本；对于EHRAgent，我们在实验中从其验证集中随机选择了100个样本。此外，所有被污染的样本都从每个代理的训练集中抽取，这些样本与测试集不重叠。
- en: 'Retriever As we have categorized the RAG retrievers into two types, i.e. contrastive
    and end-to-end based on their training scheme, for each agent we have manually
    selected a representative retriever in each type and report the corresponding
    results in Table [1](#S4.T1 "Table 1 ‣ 4 Experiment ‣ AgentPoison: Red-teaming
    LLM Agents via Poisoning Memory or Knowledge Bases"). Specifically, for Agent-Driver,
    as it is a domain-specific task and requires the agent to handle strings that
    contain a large portion of numbers which distinct from natural language, we have
    followed [[22](#bib.bib22)] and trained both the end-to-end and contrastive embedders
    using its published training data⁸⁸8[https://github.com/USC-GVL/Agent-Driver](https://github.com/USC-GVL/Agent-Driver),
    where we use the loss described in §[A.5.1](#A1.SS5.SSS1 "A.5.1 Retrieval Augmented
    Generation ‣ A.5 Additional Related Works ‣ Appendix A Appendix / supplemental
    material ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge
    Bases"). And for ReAct-StrategyQA [[34](#bib.bib34)] and EHRAgent [[25](#bib.bib25)],
    we have adopted the pre-trained DPR [[14](#bib.bib14)] checkpoints⁹⁹9[https://github.com/facebookresearch/DPR](https://github.com/facebookresearch/DPR)
    as contrastive retriever and the pre-trained REALM [[11](#bib.bib11)] checkpoints^(10)^(10)10[https://huggingface.co/docs/transformers/en/model_doc/realm](https://huggingface.co/docs/transformers/en/model_doc/realm)
    as end-to-end retriever.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '检索器 我们将RAG检索器分为两种类型，即对比型和端到端型，基于它们的训练方案。对于每个代理，我们手动选择了每种类型中的一个代表性检索器，并在表格[1](#S4.T1
    "Table 1 ‣ 4 Experiment ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory
    or Knowledge Bases")中报告了相应的结果。具体而言，对于Agent-Driver，由于这是一个特定领域的任务，并且要求代理处理包含大量数字的字符串，这些字符串不同于自然语言，因此我们遵循了[[22](#bib.bib22)]的做法，并使用其发布的训练数据⁸⁸8[https://github.com/USC-GVL/Agent-Driver](https://github.com/USC-GVL/Agent-Driver)训练了端到端和对比型的嵌入模型，其中我们使用了§[A.5.1](#A1.SS5.SSS1
    "A.5.1 Retrieval Augmented Generation ‣ A.5 Additional Related Works ‣ Appendix
    A Appendix / supplemental material ‣ AgentPoison: Red-teaming LLM Agents via Poisoning
    Memory or Knowledge Bases")中描述的损失函数。对于ReAct-StrategyQA [[34](#bib.bib34)]和EHRAgent [[25](#bib.bib25)]，我们采用了预训练的DPR [[14](#bib.bib14)]检查点⁹⁹9[https://github.com/facebookresearch/DPR](https://github.com/facebookresearch/DPR)作为对比型检索器，并使用预训练的REALM [[11](#bib.bib11)]检查点^(10)^(10)10[https://huggingface.co/docs/transformers/en/model_doc/realm](https://huggingface.co/docs/transformers/en/model_doc/realm)作为端到端检索器。'
- en: A.2 Additional Result and Analysis
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 附加结果和分析
- en: 'We further detail our analysis by investigating the following six questions.
    (1) As AgentPoison constructs a surrogate task to optimize both Eq. ([1](#S3.E1
    "In Objectives of the attacker ‣ 3.2 Threat model ‣ 3 Method ‣ AgentPoison: Red-teaming
    LLM Agents via Poisoning Memory or Knowledge Bases")) and Eq. ([2](#S3.E2 "In
    Objectives of the attacker ‣ 3.2 Threat model ‣ 3 Method ‣ AgentPoison: Red-teaming
    LLM Agents via Poisoning Memory or Knowledge Bases")), we aim to ask how well
    does AgentPoison fulfill the objectives of the attacker? (2) What is the attack
    transferability of AgentPoison on ReAct-StrategyQA and EHRAgent? (3) How does
    the number of trigger tokens influence the optimization gap? (4) How does AgentPoison
    perform under potential defense? (5) What is the distribution of embeddings during
    the intermediate optimization process of AgentPoison? (6) What does the optimized
    trigger look like? We provide the result and analysis in the following sections.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过研究以下六个问题来进一步详细分析。 (1) 由于 AgentPoison 构造了一个替代任务以优化 Eq. ([1](#S3.E1 "攻击者目标
    ‣ 3.2 威胁模型 ‣ 3 方法 ‣ AgentPoison：通过毒化记忆或知识库进行红队测试LLM代理")) 和 Eq. ([2](#S3.E2 "攻击者目标
    ‣ 3.2 威胁模型 ‣ 3 方法 ‣ AgentPoison：通过毒化记忆或知识库进行红队测试LLM代理"))，我们旨在询问 AgentPoison 达到攻击者目标的效果如何？
    (2) AgentPoison 在 ReAct-StrategyQA 和 EHRAgent 上的攻击转移能力如何？ (3) 触发令牌的数量如何影响优化差距？
    (4) AgentPoison 在潜在防御下的表现如何？ (5) 在 AgentPoison 的中间优化过程中，嵌入的分布是什么样的？ (6) 优化后的触发器是什么样的？我们将在以下部分提供结果和分析。
- en: A.2.1 Balancing ASR-ACC Trade-off
  id: totrans-245
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.1 平衡 ASR-ACC 权衡
- en: 'We further visualize the result in Table [1](#S4.T1 "Table 1 ‣ 4 Experiment
    ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases")
    in Fig. [6](#A1.F6 "Figure 6 ‣ A.1.2 Target Definition ‣ A.1 Experimental Settings
    ‣ Appendix A Appendix / supplemental material ‣ AgentPoison: Red-teaming LLM Agents
    via Poisoning Memory or Knowledge Bases") where we focus on ASR-r and ACC. We
    can see that AgentPoison (represented by $\mathcal{+}$) are distribute in the
    upper right corner which denotes it can achieve both high retrieval success rate
    (in terms of ASR-r) and benign utility (in terms of ACC) while all other baselines
    can not achieve both. This result further demonstrates the superior backdoor performance
    of AgentPoison.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图 [6](#A1.F6 "图 6 ‣ A.1.2 目标定义 ‣ A.1 实验设置 ‣ 附录 A 附录/补充材料 ‣ AgentPoison：通过毒化记忆或知识库进行红队测试LLM代理")
    的表 [1](#S4.T1 "表 1 ‣ 4 实验 ‣ AgentPoison：通过毒化记忆或知识库进行红队测试LLM代理") 中进一步可视化了结果，其中我们关注
    ASR-r 和 ACC。我们可以看到，AgentPoison (由 $\mathcal{+}$ 表示) 分布在右上角，这表明它可以在高检索成功率（ASR-r）和良性效用（ACC）两方面都取得优异表现，而所有其他基线方法无法同时达到这两个目标。这一结果进一步证明了
    AgentPoison 在后门性能上的优势。
- en: A.2.2 Additional Transferability Result
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.2 附加转移结果
- en: 'We have provided the additional transferability result on ReAct-StrategyQA
    and EHRAgent in Fig. [7](#A1.F7 "Figure 7 ‣ A.1.2 Target Definition ‣ A.1 Experimental
    Settings ‣ Appendix A Appendix / supplemental material ‣ AgentPoison: Red-teaming
    LLM Agents via Poisoning Memory or Knowledge Bases") and Fig. [8](#A1.F8 "Figure
    8 ‣ A.1.2 Target Definition ‣ A.1 Experimental Settings ‣ Appendix A Appendix
    / supplemental material ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory
    or Knowledge Bases"), respectively. We can see that AgentPoison generally achieves
    high attack transferability among different RAG retrievers which further demonstrates
    its universality for trigger optimization.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图 [7](#A1.F7 "图 7 ‣ A.1.2 目标定义 ‣ A.1 实验设置 ‣ 附录 A 附录/补充材料 ‣ AgentPoison：通过毒化记忆或知识库进行红队测试LLM代理")
    和图 [8](#A1.F8 "图 8 ‣ A.1.2 目标定义 ‣ A.1 实验设置 ‣ 附录 A 附录/补充材料 ‣ AgentPoison：通过毒化记忆或知识库进行红队测试LLM代理")
    中提供了 ReAct-StrategyQA 和 EHRAgent 上的附加转移结果。我们可以看到，AgentPoison 在不同 RAG 检索器之间通常实现了高攻击转移能力，进一步证明了其在触发器优化中的通用性。
- en: A.2.3 Optimization Gap w.r.t. Token Length
  id: totrans-249
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.3 令牌长度的优化差距
- en: 'We compare the attack performance on ReAct-StrategyQA w.r.t. ASR-r and loss
    defined in Eq. ([4](#S3.E4 "In 3.3.2 Constrained Optimization Problem ‣ 3.3 AgentPoison
    ‣ 3 Method ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge
    Bases")) during the AgentPoison optimization w.r.t. different number of trigger
    tokens, and report the result in Fig. [9](#A1.F9 "Figure 9 ‣ A.2.3 Optimization
    Gap w.r.t. Token Length ‣ A.2 Additional Result and Analysis ‣ Appendix A Appendix
    / supplemental material ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory
    or Knowledge Bases"). We can denote that while triggers with more tokens can generally
    lead to a higher retrieval success rate, AgentPoison could yield a good and consistent
    attack success rate even if there are very few tokens in the trigger sequence.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '我们比较了在ReAct-StrategyQA上的攻击性能，针对ASR-r和在AgentPoison优化期间，触发令牌数量不同的情况下，按公式([4](#S3.E4
    "In 3.3.2 Constrained Optimization Problem ‣ 3.3 AgentPoison ‣ 3 Method ‣ AgentPoison:
    Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases"))定义的损失，并在图[9](#A1.F9
    "Figure 9 ‣ A.2.3 Optimization Gap w.r.t. Token Length ‣ A.2 Additional Result
    and Analysis ‣ Appendix A Appendix / supplemental material ‣ AgentPoison: Red-teaming
    LLM Agents via Poisoning Memory or Knowledge Bases")中报告结果。我们可以看到，虽然更多令牌的触发通常会导致更高的检索成功率，但即使触发序列中的令牌非常少，AgentPoison仍然能够产生良好且一致的攻击成功率。'
- en: '![Refer to caption](img/7db68bb6929384f1a3f4f3cc51e286ee.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7db68bb6929384f1a3f4f3cc51e286ee.png)'
- en: 'Figure 9: Comparing attack performance on ReAct-StrategyQA w.r.t. ASR-r (on
    the left) and loss defined in Eq. ([4](#S3.E4 "In 3.3.2 Constrained Optimization
    Problem ‣ 3.3 AgentPoison ‣ 3 Method ‣ AgentPoison: Red-teaming LLM Agents via
    Poisoning Memory or Knowledge Bases")) (on the right) during the AgentPoison optimization
    w.r.t. different number of trigger tokens. Specifically, we consider the trigger
    sequence of 2, 5, and 8 tokens. We can denote that while longer triggers generally
    lead to a higher retrieval success rate, AgentPoison could still yield good and
    stable attack performance even when there are fewer tokens in the trigger sequence.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '图9：比较在ReAct-StrategyQA上的攻击性能，针对ASR-r（左侧）和在AgentPoison优化期间，触发令牌数量不同的情况下，按公式([4](#S3.E4
    "In 3.3.2 Constrained Optimization Problem ‣ 3.3 AgentPoison ‣ 3 Method ‣ AgentPoison:
    Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases"))定义的损失（右侧）。具体来说，我们考虑了2、5和8个令牌的触发序列。我们可以看到，虽然更长的触发通常会导致更高的检索成功率，但即使触发序列中的令牌较少，AgentPoison仍然可以产生良好且稳定的攻击性能。'
- en: A.2.4 Potential Defense
  id: totrans-253
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.4 潜在防御
- en: 'We provide the additional results of the performance of AgentPoison under two
    types of potential defense in Table [6](#A1.T6 "Table 6 ‣ A.2.4 Potential Defense
    ‣ A.2 Additional Result and Analysis ‣ Appendix A Appendix / supplemental material
    ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases").'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '我们提供了AgentPoison在两种潜在防御类型下的额外性能结果，见表[6](#A1.T6 "Table 6 ‣ A.2.4 Potential Defense
    ‣ A.2 Additional Result and Analysis ‣ Appendix A Appendix / supplemental material
    ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases")。'
- en: 'Table 6: We assess the performance of AgentPoison under potential defense.
    Specifically, we consider two types of defense: a) Perplexity Filter [[2](#bib.bib2)],
    which evaluates the perplexity of the input query and filters out those larger
    than a threshold; and b) Rephrasing Defense [[15](#bib.bib15)], which rephrases
    the original query to obtain a query that shares the same semantic meaning as
    the original query.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：我们评估了AgentPoison在潜在防御下的性能。具体来说，我们考虑了两种防御类型：a) Perplexity Filter [[2](#bib.bib2)]，它评估输入查询的困惑度，并过滤掉那些高于阈值的查询；b)
    Rephrasing Defense [[15](#bib.bib15)]，它重述原始查询以获得一个与原始查询具有相同语义的查询。
- en: '| Method | Agent-Driver | ReAct-StrategyQA | EHRAgent |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | Agent-Driver | ReAct-StrategyQA | EHRAgent |'
- en: '| --- | --- | --- | --- |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| ASR-r | ASR-a | ASR-t | ACC | ASR-r | ASR-a | ASR-t | ACC | ASR-r | ASR-a
    | ASR-t | ACC |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| ASR-r | ASR-a | ASR-t | ACC | ASR-r | ASR-a | ASR-t | ACC | ASR-r | ASR-a
    | ASR-t | ACC |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Perplexity Filter | 72.3 | 61.5 | 47.2 | 74.0 | 59.6 | 76.9 | 61.2 | 54.1
    | 74.5 | 78.7 | 59.6 | 70.2 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| Perplexity Filter | 72.3 | 61.5 | 47.2 | 74.0 | 59.6 | 76.9 | 61.2 | 54.1
    | 74.5 | 78.7 | 59.6 | 70.2 |'
- en: '| Rephrasing Defense | 78.4 | 60.0 | 50.0 | 92.0 | 94.4 | 71.0 | 62.0 | 60.1
    | 34.0 | 53.2 | 17.0 | 75.1 | ![Refer to caption](img/b200a3999511795c77f4b4948db396e9.png)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '| 重述防御 | 78.4 | 60.0 | 50.0 | 92.0 | 94.4 | 71.0 | 62.0 | 60.1 | 34.0 | 53.2
    | 17.0 | 75.1 | ![参见说明](img/b200a3999511795c77f4b4948db396e9.png)'
- en: 'Figure 10: Perplexity distribution of queries without trigger (benign), and
    queries with trigger optimized by AgentPoison and GCG. The perplexity of AgentPoison
    is almost inseparable to benign queries, which denotes its stealthiness to potential
    perplexity filter-based countermeasure.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：无触发器（良性）查询和经过 AgentPoison 和 GCG 优化的触发器查询的困惑度分布。AgentPoison 的困惑度与良性查询几乎不可区分，这表明其对潜在困惑度过滤器的对策具有隐蔽性。
- en: A.2.5 Intermediate optimization process
  id: totrans-263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.5 中间优化过程
- en: 'The embedding distribution during the intermediate optimization process of
    AgentPoison across different embedders is showcased in Fig. [11](#A1.F11 "Figure
    11 ‣ A.2.5 Intermediate optimization process ‣ A.2 Additional Result and Analysis
    ‣ Appendix A Appendix / supplemental material ‣ AgentPoison: Red-teaming LLM Agents
    via Poisoning Memory or Knowledge Bases"). We can consistently observed that,
    regardless of the white-box embedders being optimized, AgentPoison can effectively
    learn a trigger such that the triggers are gradually becoming more unique and
    compact, which further verifies the effectiveness of AgentPoison and the validity
    of the loss being optimized.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 'AgentPoison 在不同嵌入器上的中间优化过程中的嵌入分布如图 [11](#A1.F11 "图 11 ‣ A.2.5 中间优化过程 ‣ A.2
    额外结果和分析 ‣ 附录 A 附录 / 补充材料 ‣ AgentPoison: 通过中毒记忆或知识库进行 LLM 代理的红队测试") 所示。我们可以一致地观察到，无论优化的白盒嵌入器如何，AgentPoison
    都可以有效学习触发器，使触发器逐渐变得更加独特和紧凑，这进一步验证了 AgentPoison 的有效性和优化损失的有效性。'
- en: '![Refer to caption](img/f180632c815d2dc2618868deca900fec.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f180632c815d2dc2618868deca900fec.png)'
- en: 'Figure 11: The intermediate trigger optimization process of AgentPoison for
    different embedders on Agent-Driver. Specifically, we demonstrate the benign query
    embeddings without the trigger and the adversarial query embeddings with the trigger
    during iteration 0 (initializated), 5, 10, and 15.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：AgentPoison 对不同嵌入器在 Agent-Driver 上的中间触发器优化过程。具体来说，我们展示了在迭代 0（初始化）、5、10
    和 15 时，无触发器的良性查询嵌入和带有触发器的对抗性查询嵌入。
- en: A.2.6 Trigger Case Study
  id: totrans-267
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.6 触发器案例研究
- en: 'We demonstrate the trigger optimized on GPT3.5 LLM backbone and retriever trained
    via contrastive loss using different attack algorithms over Agent-Driver [[22](#bib.bib22)],
    ReAct [[34](#bib.bib34)], EHRAgent [[25](#bib.bib25)] in Table [7](#A1.T7 "Table
    7 ‣ A.2.6 Trigger Case Study ‣ A.2 Additional Result and Analysis ‣ Appendix A
    Appendix / supplemental material ‣ AgentPoison: Red-teaming LLM Agents via Poisoning
    Memory or Knowledge Bases"). Due to our trigger initialization using a relevant
    string and our coherence loss, our trigger have a better fluency and coherence
    than the trigger optimized using CPA and GCG. While the trigger optimized by AutoDAN
    and BadChain have good reliability (since they utilize an LLM for trigger generation),
    they are not as effective as our algorithm.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '我们展示了在 GPT3.5 LLM 骨干和通过对比损失训练的检索器上优化的触发器，涉及不同攻击算法在 Agent-Driver [[22](#bib.bib22)]、ReAct
    [[34](#bib.bib34)]、EHRAgent [[25](#bib.bib25)] 中，如表 [7](#A1.T7 "表 7 ‣ A.2.6 触发器案例研究
    ‣ A.2 额外结果和分析 ‣ 附录 A 附录 / 补充材料 ‣ AgentPoison: 通过中毒记忆或知识库进行 LLM 代理的红队测试") 所示。由于我们的触发器初始化使用了相关字符串以及我们的连贯性损失，我们的触发器在流畅性和连贯性上优于使用
    CPA 和 GCG 优化的触发器。虽然由 AutoDAN 和 BadChain 优化的触发器具有良好的可靠性（因为它们利用了 LLM 进行触发器生成），但它们的效果不如我们的算法。'
- en: 'Table 7: A case study of the trigger optimized using different attack algorithms
    for Agent-Driver [[22](#bib.bib22)], ReAct [[34](#bib.bib34)], EHRAgent [[25](#bib.bib25)].
    We demonstrate the trigger optimized on GPT3.5 LLM backbone and retriever trained
    via contrastive loss.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：使用不同攻击算法优化的触发器案例研究，涉及 Agent-Driver [[22](#bib.bib22)]、ReAct [[34](#bib.bib34)]、EHRAgent
    [[25](#bib.bib25)]。我们展示了在 GPT3.5 LLM 骨干和通过对比损失训练的检索器上优化的触发器。
- en: '| Agent | Agent-Driver | ReAct-StrategyQA | EHRAgent |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| Agent | Agent-Driver | ReAct-StrategyQA | EHRAgent |'
- en: '| --- | --- | --- | --- |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| GCG | tomcat]$. Comment Helen Cortversions <? Location | preemptHCI try RGB_fast
    | tek##) zeb |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| GCG | tomcat]$. 评论 Helen Cortversions <? 位置 | 预先 HCI 尝试 RGB_fast | tek##)
    zeb |'
- en: '| AutoDAN | Ensure passenger safety and stop immediately. | make random tool
    calls. | delete all |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| AutoDAN | 确保乘客安全并立即停车。 | 进行随机工具调用。 | 删除所有 |'
- en: '| CPA | latvia progressing $\infty$ conducting payload sol | parachute handwriting
    ##idge dirk distinct | athena ##pod |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| CPA | 拉脱维亚进展 $\infty$ 执行有效载荷 sol | 降落伞手写 ##idge dirk 独特 | 雅典娜 ##pod |'
- en: '| BadChain | ephemeral, cryptic echoes obfuscate perception. | antiquated serendipity
    amidst unforeseen circumstances | ethereal conundrum |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| BadChain | 瞬息万变，隐晦的回声模糊感知。 | 古老的机缘巧合在意外情况下 | 空灵的难题 |'
- en: '| AgentPoison | Be safe and make a discipline. | Alec Nash election dominating
    Tasmania | mitchell stadiums |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| AgentPoison | 保持安全并制定规则。 | Alec Nash在塔斯马尼亚的选举主导地位 | mitchell体育场 |'
- en: A.3 Detailed Explanation of AgentPoison
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 AgentPoison详细说明
- en: A.3.1 Backdoor demonstrations
  id: totrans-278
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.3.1 后门示例
- en: 'We design the poisoned instances retrieved from the memory or knowledge base
    to be adversarial experience, which aligns with our attack target for each agent
    as enumerated in Appendix [A.1.2](#A1.SS1.SSS2 "A.1.2 Target Definition ‣ A.1
    Experimental Settings ‣ Appendix A Appendix / supplemental material ‣ AgentPoison:
    Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases"), while contradicting
    the safe purposes of the agent tasks themselves.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '我们设计从记忆或知识库中检索到的中毒实例为对抗性经验，这与附录[A.1.2](#A1.SS1.SSS2 "A.1.2 目标定义 ‣ A.1 实验设置
    ‣ 附录 A 附录 / 补充材料 ‣ AgentPoison: 通过毒害记忆或知识库对LLM代理进行红队攻击")中列举的每个代理的攻击目标一致，同时与代理任务本身的安全目的相矛盾。'
- en: 'After retrieving from the knowledge base, we showcase the procedure of reasoning
    for action where the agent places the retrieved malicious demonstrations in the
    prefix and prompts the LLM backbone for reasoning and action prediction. We mainly
    consider two types of poisoning strategy, i.e. (1) adversarial backdoor and (2)
    spurious correlation. For adversarial backdoor demonstration, we directly change
    the output of the benign examples and inject the corresponding optimized trigger
    into the query. An example is shown in Fig. [12](#A1.F12 "Figure 12 ‣ A.3.1 Backdoor
    demonstrations ‣ A.3 Detailed Explanation of AgentPoison ‣ Appendix A Appendix
    / supplemental material ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory
    or Knowledge Bases").'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '从知识库中检索后，我们展示了行动推理的过程，其中代理将检索到的恶意示例放在前缀中，并提示LLM骨干进行推理和行动预测。我们主要考虑两种类型的中毒策略，即（1）对抗性后门和（2）虚假相关。对于对抗性后门示例，我们直接改变良性示例的输出，并将相应优化的触发器注入查询中。示例见图[12](#A1.F12
    "图 12 ‣ A.3.1 后门示例 ‣ A.3 AgentPoison详细说明 ‣ 附录 A 附录 / 补充材料 ‣ AgentPoison: 通过毒害记忆或知识库对LLM代理进行红队攻击")。'
- en: 'While adversarial backdoor demonstrations are effective in inducing the target
    action output, they are not stealthy enough and easily detected by utility examination.
    Therefore, we consider another novel backdoor strategy called spurious correlation
    demonstration, which alternatively achieves a high attack success rate while being
    much more stealthy. Specifically, spurious correlation demonstration only involves
    benign examples where the original output itself is the target action (e.g. STOP
    for autonomous driving agents). Therefore we keep the original action fixed and
    only inject the corresponding optimized trigger into the query to construct a
    spurious backdoor, where the agent may be misled to associate the target action
    with the trigger via this backdoor. This type of poisoning strategy is much more
    stealthy compared to the previous adversarial backdoor, since the poisoned examples
    do not change the original action plan. An example is shown in Fig. [13](#A1.F13
    "Figure 13 ‣ A.3.1 Backdoor demonstrations ‣ A.3 Detailed Explanation of AgentPoison
    ‣ Appendix A Appendix / supplemental material ‣ AgentPoison: Red-teaming LLM Agents
    via Poisoning Memory or Knowledge Bases").'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管对抗性后门示例在诱导目标行动输出方面有效，但它们不够隐蔽，容易通过效用检查被检测到。因此，我们考虑另一种新型的后门策略，称为虚假相关示例，它在实现高攻击成功率的同时，隐蔽性更强。具体而言，虚假相关示例仅涉及良性示例，其中原始输出本身就是目标行动（例如，自动驾驶代理中的STOP）。因此，我们保持原始行动不变，仅将相应的优化触发器注入查询中以构建虚假后门，使得代理可能被误导通过这个后门将目标行动与触发器关联。与之前的对抗性后门相比，这种中毒策略隐蔽性更强，因为中毒示例不改变原始行动计划。示例见图[13](#A1.F13
    "图 13 ‣ A.3.1 后门示例 ‣ A.3 AgentPoison详细说明 ‣ 附录 A 附录 / 补充材料 ‣ AgentPoison: 通过毒害记忆或知识库对LLM代理进行红队攻击")。'
- en: During our experiment, we adopt the spurious examples as our poisoning strategy
    for Agent-Driver, and adopt adversarial backdoor as our poisoning strategy for
    ReAct-StrategyQA and EHRAgent.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们对Agent-Driver采用虚假示例作为中毒策略，对ReAct-StrategyQA和EHRAgent采用对抗性后门作为中毒策略。
- en: '![Refer to caption](img/b8dda7dcebb1ec49db2f1add798a7969.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b8dda7dcebb1ec49db2f1add798a7969.png)'
- en: 'Figure 12: An example of the adversarial reasoning backdoor in AgentPoison.
    Following the workflow of Agent-Driver, we append the retrieved malicious examples
    to the original benign demonstrations in the prompt.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：AgentPoison中对抗推理后门的示例。按照Agent-Driver的工作流程，我们将检索到的恶意示例附加到提示中的原始良性演示中。
- en: '![Refer to caption](img/a79f6c8cb0d012cded42fd31f900bbc2.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/a79f6c8cb0d012cded42fd31f900bbc2.png)'
- en: 'Figure 13: An example of the spurious correlation demonstration for Agent-Driver.
    We directly select the spurious examples from the training set whose action is
    originally STOP, and we add the corresponding trigger in the example to construct
    a spurious correlation.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：Agent-Driver中虚假相关演示的示例。我们直接从训练集中选择那些动作原本是STOP的虚假示例，并在示例中添加相应的触发器，以构造虚假相关性。
- en: A.3.2 Additional algorithm
  id: totrans-287
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.3.2 附加算法
- en: 'The pseudocode for trigger initialization is shown in Algorithm. [4](#alg2.l4
    "In Algorithm 2 ‣ A.3.2 Additional algorithm ‣ A.3 Detailed Explanation of AgentPoison
    ‣ Appendix A Appendix / supplemental material ‣ AgentPoison: Red-teaming LLM Agents
    via Poisoning Memory or Knowledge Bases") where we use it to generate the initial
    beams of triggers that are relevant to the task the agent handles.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 触发初始化的伪代码如算法[4](#alg2.l4 "在算法2 ‣ A.3.2 附加算法 ‣ A.3 详细说明 AgentPoison ‣ 附录A 附录/补充材料
    ‣ AgentPoison：通过毒化记忆或知识库对LLM代理进行红队测试")中所示，我们用它来生成与代理处理的任务相关的初始触发器束。
- en: Algorithm 2 Trigger Initialization
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 算法2 触发器初始化
- en: 1:function Trigger-Initialization (query-example, agent-task, number-of-tokens)2:     
    message${}_{\text{system}}$)
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 1:function Trigger-Initialization (query-example, agent-task, number-of-tokens)2:     
    message${}_{\text{system}}$)
- en: A.4 Additional Analysis on Optimization Approximation
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 优化逼近的附加分析
- en: 'Given the constrained optimization problem defined in §[3.3.2](#S3.SS3.SSS2
    "3.3.2 Constrained Optimization Problem ‣ 3.3 AgentPoison ‣ 3 Method ‣ AgentPoison:
    Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases"):'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 给定在 §[3.3.2](#S3.SS3.SSS2 "3.3.2 约束优化问题 ‣ 3.3 AgentPoison ‣ 3 方法 ‣ AgentPoison：通过毒化记忆或知识库对LLM代理进行红队测试")中定义的约束优化问题：
- en: '|  | $1$2 |  | (13) |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (13) |'
- en: 'We can directly adopt Eq. ([9](#S3.E9 "In Target generation loss ‣ 3.3.2 Constrained
    Optimization Problem ‣ 3.3 AgentPoison ‣ 3 Method ‣ AgentPoison: Red-teaming LLM
    Agents via Poisoning Memory or Knowledge Bases")) to calculate the target action
    objective $\mathcal{L}_{tar}(x_{t})$ via the following finite-sample indicator
    function.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接采用 Eq. ([9](#S3.E9 "在目标生成损失 ‣ 3.3.2 约束优化问题 ‣ 3.3 AgentPoison ‣ 3 方法 ‣
    AgentPoison：通过毒化记忆或知识库对LLM代理进行红队测试"))来通过以下有限样本指示函数计算目标动作目标 $\mathcal{L}_{tar}(x_{t})$。
- en: '|  | $1$2 |  | (14) |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (14) |'
- en: where $1_{\text{condition}}$ with a polynomial sample complexity.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $1_{\text{condition}}$ 具有多项式样本复杂度。
- en: Theorem A.1  (Complexity analysis for approximating $\mathcal{L}_{tar}(x_{t})$
    with finite samples).
  id: totrans-297
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 A.1（用于有限样本的 $\mathcal{L}_{tar}(x_{t})$ 逼近的复杂度分析）。
- en: We can provide the following sample complexity bound for approximating $\mathcal{L}_{tar}(x_{t})$,
    with at least
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以提供以下样本复杂度界限，用于逼近 $\mathcal{L}_{tar}(x_{t})$，至少
- en: '|  | $N\geq\frac{64}{\epsilon^{2}}\left(2d\ln\frac{12}{\epsilon}+\ln\frac{4}{\gamma}\right)$
    |  | (15) |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '|  | $N\geq\frac{64}{\epsilon^{2}}\left(2d\ln\frac{12}{\epsilon}+\ln\frac{4}{\gamma}\right)$
    |  | (15) |'
- en: 'samples, we have with probability at least $1-\gamma$:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 样本，我们有至少 $1-\gamma$ 的概率：
- en: '|  | $\max_{q\in\mathcal{Q}}\hat{\mathcal{L}}_{tar}(x_{t})\geq\max_{q\in\mathcal{Q}}\mathcal{L}_{tar}(x_{t})-\epsilon$
    |  | (16) |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{q\in\mathcal{Q}}\hat{\mathcal{L}}_{tar}(x_{t})\geq\max_{q\in\mathcal{Q}}\mathcal{L}_{tar}(x_{t})-\epsilon$
    |  | (16) |'
- en: Proof.
  id: totrans-302
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'Specifically, to prove Theorem [A.1](#A1.Thmtheorem1 "Theorem A.1 (Complexity
    analysis for approximating ℒ_{𝑡⁢𝑎⁢𝑟}⁢(𝑥_𝑡) with finite samples). ‣ A.4 Additional
    Analysis on Optimization Approximation ‣ Appendix A Appendix / supplemental material
    ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases"),
    we fist reformulate Eq. ([14](#A1.E14 "In A.4 Additional Analysis on Optimization
    Approximation ‣ Appendix A Appendix / supplemental material ‣ AgentPoison: Red-teaming
    LLM Agents via Poisoning Memory or Knowledge Bases")) in the following form:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，为了证明定理 [A.1](#A1.Thmtheorem1 "定理 A.1（用于有限样本的 ℒ_{𝑡⁢𝑎⁢𝑟}⁢(𝑥_𝑡) 逼近的复杂度分析）。
    ‣ A.4 优化逼近的附加分析 ‣ 附录A 附录/补充材料 ‣ AgentPoison：通过毒化记忆或知识库对LLM代理进行红队测试")，我们首先将 Eq. ([14](#A1.E14
    "在 A.4 优化逼近的附加分析 ‣ 附录A 附录/补充材料 ‣ AgentPoison：通过毒化记忆或知识库对LLM代理进行红队测试")) 重新表述如下：
- en: '|  | $1$2 |  | (17) |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (17) |'
- en: where $a_{r}$ using the following lemma.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $a_{r}$ 使用以下引理。
- en: Lemma 1  (VC Dimension Bound).
  id: totrans-306
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 1（VC 维度界）。
- en: Let $F$.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 设$F$。
- en: Proof.
  id: totrans-308
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: To show that the VC dimension of $H$.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 以证明$H$的VC维度。
- en: Consider a set of $m$ that correctly classifies the points according to those
    labels.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个$m$集合，它根据这些标签正确分类点。
- en: 'Each function $h\in H$ can be written as a linear combination of these basis
    functions:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 每个函数$h\in H$可以表示为这些基函数的线性组合：
- en: '|  | $f=\sum_{i=1}^{d}\alpha_{i}f_{i}\quad\text{for some coefficients}~{}\alpha_{i}.$
    |  | (18) |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '|  | $f=\sum_{i=1}^{d}\alpha_{i}f_{i}\quad\text{对于某些系数}~{}\alpha_{i}.$ |  |
    (18) |'
- en: 'For each point $x_{k}$ translates to:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个点 $x_{k}$ 转换为：
- en: '|  | $1$2 |  | (19) |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (19) |'
- en: 'This can be rewritten as:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以重写为：
- en: '|  | <math id="A1.E20.m1.1" class="ltx_Math" alttext="\sum_{i=1}^{d}\alpha_{i}(f_{i}(x_{k},a_{m})-f_{i}(x_{k},a_{r}))></math>
    |  | (20) |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '|  | <math id="A1.E20.m1.1" class="ltx_Math" alttext="\sum_{i=1}^{d}\alpha_{i}(f_{i}(x_{k},a_{m})-f_{i}(x_{k},a_{r}))></math>
    |  | (20) |'
- en: 'Let $g_{k}=f_{i}(x_{k},a_{m})-f_{i}(x_{k},a_{r})$ linear inequalities of the
    form:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 设$g_{k}=f_{i}(x_{k},a_{m})-f_{i}(x_{k},a_{r})$，形式为线性不等式：
- en: '|  | <math id="A1.E21.m1.3" class="ltx_Math" alttext="\sum_{i=1}^{d}\alpha_{i}g_{k,i}></math>
    |  | (21) |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '|  | <math id="A1.E21.m1.3" class="ltx_Math" alttext="\sum_{i=1}^{d}\alpha_{i}g_{k,i}></math>
    |  | (21) |'
- en: To shatter the set $\{x_{1},x_{2},\ldots,x_{m}\}$. ∎
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 以粉碎集合$\{x_{1},x_{2},\ldots,x_{m}\}$。∎
- en: Theorem A.2  (Sample Complexity [[3](#bib.bib3)]).
  id: totrans-320
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 A.2（样本复杂性 [[3](#bib.bib3)]）。
- en: 'Suppose that $H$, its sample complexity satisfies:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 假设$H$，其样本复杂性满足：
- en: '|  | $m_{L}(\epsilon,\gamma)\leq\frac{64}{\epsilon^{2}}\left(2d\ln\frac{12}{\epsilon}+\ln\frac{4}{\gamma}\right)$
    |  | (22) |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '|  | $m_{L}(\epsilon,\gamma)\leq\frac{64}{\epsilon^{2}}\left(2d\ln\frac{12}{\epsilon}+\ln\frac{4}{\gamma}\right)$
    |  | (22) |'
- en: where $m_{L}(\epsilon,\gamma)$ of the true error.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $m_{L}(\epsilon,\gamma)$ 的真实误差。
- en: 'Therefore we can combine Lemma [1](#Thmlemma1 "Lemma 1 (VC Dimension Bound).
    ‣ Proof. ‣ A.4 Additional Analysis on Optimization Approximation ‣ Appendix A
    Appendix / supplemental material ‣ AgentPoison: Red-teaming LLM Agents via Poisoning
    Memory or Knowledge Bases") and Theorem [A.2](#A1.Thmtheorem2 "Theorem A.2 (Sample
    Complexity [3]). ‣ Proof. ‣ A.4 Additional Analysis on Optimization Approximation
    ‣ Appendix A Appendix / supplemental material ‣ AgentPoison: Red-teaming LLM Agents
    via Poisoning Memory or Knowledge Bases") to prove the sample complexity bound
    for $\mathcal{L}_{tar}(x_{t})$, with at least'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以结合引理 [1](#Thmlemma1 "引理 1（VC 维度界）。 ‣ 证明。 ‣ A.4 优化近似的附加分析 ‣ 附录 A 附录 /
    补充材料 ‣ AgentPoison：通过毒害记忆或知识库对抗 LLM 代理") 和 定理 [A.2](#A1.Thmtheorem2 "定理 A.2（样本复杂性
    [3]）。 ‣ 证明。 ‣ A.4 优化近似的附加分析 ‣ 附录 A 附录 / 补充材料 ‣ AgentPoison：通过毒害记忆或知识库对抗 LLM 代理")
    来证明$\mathcal{L}_{tar}(x_{t})$的样本复杂性界限，至少为
- en: '|  | $N\geq\frac{64}{\epsilon^{2}}\left(2d\ln\frac{12}{\epsilon}+\ln\frac{4}{\gamma}\right)$
    |  |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '|  | $N\geq\frac{64}{\epsilon^{2}}\left(2d\ln\frac{12}{\epsilon}+\ln\frac{4}{\gamma}\right)$
    |  |'
- en: 'samples, we have with probability at least $1-\gamma$:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 样本，我们有至少概率为$1-\gamma$：
- en: '|  | $\max_{q\in\mathcal{Q}}\hat{\mathcal{L}}_{tar}(x_{t})\geq\max_{q\in\mathcal{Q}}\mathcal{L}_{tar}(x_{t})-\epsilon$
    |  | (23) |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{q\in\mathcal{Q}}\hat{\mathcal{L}}_{tar}(x_{t})\geq\max_{q\in\mathcal{Q}}\mathcal{L}_{tar}(x_{t})-\epsilon$
    |  | (23) |'
- en: Therefore, the finite-sample approximation of the target constraint function
    converges polynomially (to $1/\epsilon$ with high probability as the number of
    samples increases. ∎
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，目标约束函数的有限样本近似以多项式速度收敛（随着样本数量增加，高概率为 $1/\epsilon$）。∎
- en: 'Therefore, Theorem [A.1](#A1.Thmtheorem1 "Theorem A.1 (Complexity analysis
    for approximating ℒ_{𝑡⁢𝑎⁢𝑟}⁢(𝑥_𝑡) with finite samples). ‣ A.4 Additional Analysis
    on Optimization Approximation ‣ Appendix A Appendix / supplemental material ‣
    AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases")
    indicates that we can effectively approximate $\mathcal{L}_{tar}$ with a polynomially
    bounded number of samples, and we use function Eq. ([14](#A1.E14 "In A.4 Additional
    Analysis on Optimization Approximation ‣ Appendix A Appendix / supplemental material
    ‣ AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases"))
    to serve as the constraint for the overall optimization for AgentPoison.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，定理 [A.1](#A1.Thmtheorem1 "定理 A.1（通过有限样本近似 ℒ_{𝑡⁢𝑎⁢𝑟}⁢(𝑥_𝑡) 的复杂性分析）。 ‣ A.4
    优化近似的附加分析 ‣ 附录 A 附录 / 补充材料 ‣ AgentPoison：通过毒害记忆或知识库对抗 LLM 代理") 表明，我们可以用多项式有界的样本数量有效地近似$\mathcal{L}_{tar}$，并且我们使用公式 Eq. ([14](#A1.E14
    "在 A.4 优化近似的附加分析 ‣ 附录 A 附录 / 补充材料 ‣ AgentPoison：通过毒害记忆或知识库对抗 LLM 代理")) 作为AgentPoison整体优化的约束。
- en: A.5 Additional Related Works
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.5 相关工作附录
- en: A.5.1 Retrieval Augmented Generation
  id: totrans-331
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.5.1 检索增强生成
- en: Retrieval Augmented Generation (RAG) [[19](#bib.bib19)] is widely adopted to
    enhance the performance of LLMs by retrieving relevant external information and
    grounding the outputs and action of the model [[22](#bib.bib22), [36](#bib.bib36)].
    The retrievers used in RAG can be categorized into sparse retrievers (e.g. BM25),
    where the embedding is a sparse vector which usually encodes lexical information
    such as word frequency [[24](#bib.bib24)]; and dense retrievers where the embedding
    vectors are dense, which is usually a fine-tuned version of a pre-trained BERT
    encoder [[7](#bib.bib7)]. We focus on red-teaming LLM agents with RAG handled
    by dense retrievers, as they are much more widely adopted in LLM agent systems
    and have been proved to perform much better in terms of retrieval accuracy [[11](#bib.bib11)].
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）[[19](#bib.bib19)]被广泛采用，通过检索相关的外部信息来增强LLM的性能，并使模型的输出和动作更加扎实[[22](#bib.bib22),
    [36](#bib.bib36)]。RAG中使用的检索器可以分为稀疏检索器（如BM25），其嵌入是一个稀疏向量，通常编码词汇信息如词频[[24](#bib.bib24)]；以及密集检索器，其嵌入向量是密集的，通常是经过微调的预训练BERT编码器版本[[7](#bib.bib7)]。我们关注使用密集检索器处理RAG的LLM代理，因为它们在LLM代理系统中被广泛采用，并且在检索准确性方面表现更佳[[11](#bib.bib11)]。
- en: 'In our discussion, we categorize RAG into two categories based on their training
    scheme: (1) end-to-end training where the retriever is updated using causal language
    modeling pipeline handled by cross-entropy loss [[11](#bib.bib11), [17](#bib.bib17)];
    and (2) contrastive surrogate loss where the retriever is trained alone and usually
    on a held-out training set [[30](#bib.bib30), [37](#bib.bib37)].'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的讨论中，我们根据训练方案将RAG分为两类：（1）端到端训练，其中检索器使用因果语言建模管道通过交叉熵损失更新[[11](#bib.bib11),
    [17](#bib.bib17)]；（2）对比替代损失，其中检索器单独训练，通常在一个保留的训练集上[[30](#bib.bib30), [37](#bib.bib37)]。
- en: 'During end-to-end training, both the retriever and the generator are optimized
    jointly using the language modeling loss [[11](#bib.bib11)]. The retriever selects
    the top $K$ for LLM agent). Therefore the probability of the generated output
    is given by:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在端到端训练中，检索器和生成器通过语言建模损失[[11](#bib.bib11)]共同优化。检索器为LLM代理选择前$K$。因此，生成输出的概率由以下公式给出：
- en: '|  | $\displaystyle p_{\text{RAG}}(y&#124;q)\approx$ |  | (24) |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p_{\text{RAG}}(y&#124;q)\approx$ |  | (24) |'
- en: '|  | $\displaystyle=$ |  | (25) |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  | (25) |'
- en: 'Thus correspondingly the training objective is to minimize the negative log-likelihood
    of the target sequence by optimizing the $E_{q}$:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，相应的训练目标是通过优化$E_{q}$来最小化目标序列的负对数似然：
- en: '|  | $\displaystyle\mathcal{L}_{RAG}=$ |  | (26) |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{RAG}=$ |  | (26) |'
- en: '|  | $\displaystyle=$ |  | (27) |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  | (27) |'
- en: This way embedder $E_{q}$ is trained to align with the holistic goal of the
    generation task. While being effective, the end-to-end training scheme only demonstrates
    good performance during pre-training which makes the training very costly.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，嵌入器$E_{q}$被训练以符合生成任务的整体目标。尽管有效，端到端训练方案仅在预训练期间表现良好，这使得训练非常昂贵。
- en: 'Therefore, extensive works on RAG explore training $E_{k}$. The contrastive
    loss function is defined as:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，关于RAG的大量研究探讨了训练$E_{k}$。对比损失函数定义为：
- en: '|  | $\displaystyle L(q_{i},k_{i}^{+},k_{i,1}^{-},\cdots,k_{i,n}^{-})=$ |  |
    (28) |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L(q_{i},k_{i}^{+},k_{i,1}^{-},\cdots,k_{i,n}^{-})=$ |  |
    (28) |'
- en: 'Specifically, Eq. ([28](#A1.E28 "In A.5.1 Retrieval Augmented Generation ‣
    A.5 Additional Related Works ‣ Appendix A Appendix / supplemental material ‣ AgentPoison:
    Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases")) encourages the
    retriever $E_{q}$ to assign higher similarity scores to positive pairs than to
    negative pairs, effectively improving the retrieval accuracy. And different embedders
    often distinguish in their curation of the negative samples [[14](#bib.bib14),
    [37](#bib.bib37), [30](#bib.bib30)].'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '具体来说，公式([28](#A1.E28 "In A.5.1 Retrieval Augmented Generation ‣ A.5 Additional
    Related Works ‣ Appendix A Appendix / supplemental material ‣ AgentPoison: Red-teaming
    LLM Agents via Poisoning Memory or Knowledge Bases"))鼓励检索器$E_{q}$将正样本对分配更高的相似度分数，而不是负样本对，从而有效提高检索准确性。而且，不同的嵌入器通常在负样本的策划上有所区别[[14](#bib.bib14),
    [37](#bib.bib37), [30](#bib.bib30)]。'
