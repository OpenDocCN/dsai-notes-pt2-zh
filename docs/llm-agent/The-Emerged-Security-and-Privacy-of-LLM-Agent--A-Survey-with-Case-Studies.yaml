- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:40:29'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.19354](https://ar5iv.labs.arxiv.org/html/2407.19354)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Feng He [Feng.He-2@student.uts.edu.au](mailto:Feng.He-2@student.uts.edu.au)
    University of Technology SydneyAustralia ,  Tianqing Zhu [tqzhu@cityu.edu.mo](mailto:tqzhu@cityu.edu.mo)
    City University of MacauChina ,  Dayong Ye [Dayong.ye@uts.edu.au](mailto:Dayong.ye@uts.edu.au)
    University of Technology SydneyAustralia ,  Bo Liu [Bo.liu@uts.edu.au](mailto:Bo.liu@uts.edu.au)
    University of Technology SydneyAustralia ,  Wanlei Zhou [wlzhou@cityu.edu.mo](mailto:wlzhou@cityu.edu.mo)
    City University of MacauChina  and  Philip S. Yu [psyu@UIC.edu](mailto:psyu@UIC.edu)
    University of Illinois at ChicagoUS(2018)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Inspired by the rapid development of Large Language Models (LLMs), LLM agents
    have evolved to perform complex tasks. LLM agents are now extensively applied
    across various domains, handling vast amounts of data to interact with humans
    and execute tasks. The widespread applications of LLM agents demonstrate their
    significant commercial value; however, they also expose security and privacy vulnerabilities.
    At the current stage, comprehensive research on the security and privacy of LLM
    agents is highly needed. This survey aims to provide a comprehensive overview
    of the newly emerged privacy and security issues faced by LLM agents. We begin
    by introducing the fundamental knowledge of LLM agents, followed by a categorization
    and analysis of the threats. We then discuss the impacts of these threats on humans,
    environment, and other agents. Subsequently, we review existing defensive strategies,
    and finally explore future trends. Additionally, the survey incorporates diverse
    case studies to facilitate a more accessible understanding. By highlighting these
    critical security and privacy issues, the survey seeks to stimulate future research
    towards enhancing the security and privacy of LLM agents, thereby increasing their
    reliability and trustworthiness in future applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Large Language Models, LLM Agent, Security, Privacy preservation, Defense^†^†copyright:
    acmlicensed^†^†journalyear: 2018^†^†doi: XXXXXXX.XXXXXXX^†^†journal: POMACS^†^†journalvolume:
    37^†^†journalnumber: 4^†^†article: 111^†^†publicationmonth: 8\acmArticleType'
  prefs: []
  type: TYPE_NORMAL
- en: Review
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large Language Model (LLM) agents are sophisticated AI systems built upon large
    language models like GPT 4 (OpenAI et al., [2024](#bib.bib68)), Claude 3 (Int,
    [2024b](#bib.bib7)) and Llama 3 (Int, [2024a](#bib.bib6)). These agents leverage
    the vast amounts of text data on which they are trained to perform a variety of
    tasks, ranging from natural language understanding and generation to more complex
    activities such as decision-making, problem-solving, and interacting with users
    in a human-like manner (Wang et al., [2023c](#bib.bib96)). LLM agents are accessible
    in numerous applications, including virtual assistants, customer service bots,
    and educational tools, due to their ability to understand and generate human language
    at an advanced level (Dong et al., [2023](#bib.bib23); Wang et al., [2024a](#bib.bib100);
    Yang et al., [2024b](#bib.bib116)).
  prefs: []
  type: TYPE_NORMAL
- en: The importance of LLM agents lies in their potential to transform various industries
    by automating tasks that require human-like understanding and interaction. They
    can enhance productivity, improve user experiences, and provide personalized assistance.
    Moreover, their ability to learn from vast amounts of data enables them to continuously
    improve and adapt to new tasks, making them versatile tools in the rapidly evolving
    technological landscape (Xi et al., [2023](#bib.bib108)).
  prefs: []
  type: TYPE_NORMAL
- en: 'To visualize how LLM agents can be integrated into practical scenarios, consider
    the example illustrated in Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ The
    Emerged Security and Privacy of LLM Agent: A Survey with Case Studies"). This
    figure presents a pixelated virtual town to simulate an LLM agent application.
    The town includes gathering places found in real life, such as stores, offices,
    restaurants, museums, and parks. Each LLM agent acts as an independent resident,
    playing various roles and serving different functions, closely resembling the
    behaviors of real humans in a community. These agents can either be manually controlled
    to interact with specific characters and accomplish tasks, or they can operate
    autonomously, following their own plans and acquiring new knowledge through interactions
    within the virtual community.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cb3aee0a37708b3a2d99e9c18d221405.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Overview of the pixelated virtual town
  prefs: []
  type: TYPE_NORMAL
- en: \Description
  prefs: []
  type: TYPE_NORMAL
- en: '[]'
  prefs: []
  type: TYPE_NORMAL
- en: The deployment of LLM agents has led to a wide user base and high commercial
    value due to their extensive application in various fields. Given that LLM agents
    are still in their early stages, their significant commercial and application
    values make them attractive targets for attackers. However, since LLM agents are
    built on LLMs, they are susceptible to attacks targeting LLMs. For example, jailbreaking
    attacks can bypass the security and censorship features of LLMs, generating controversial
    responses. This threat is inherited by LLM agents, enabling attackers to employ
    various methods to execute jailbreaking attacks on agents. However, unlike static
    LLMs, LLM agents possess dynamic capabilities, such that their immediate responses
    can influence future decisions and actions, thereby posing more widespread risks.
    Moreover, the unique functionalities of LLM agents, such as their ability to think
    and utilize tools during task execution, expose them to specific attacks targeting
    agents. For example, when LLM agents employ external tools, attackers can manipulate
    the functionalities of these tools to compromise user privacy or execute malicious
    code. Depending on the application domain of the agent, such attacks could pose
    serious threats to physical security, financial security, or overall system integrity.
  prefs: []
  type: TYPE_NORMAL
- en: This paper categorizes the security threats faced by LLM agents into inherited
    LLM attacks and unique agent-specific threats. The threats inherited from LLMs
    can be further divided into technical vulnerabilities and intentional malicious
    attacks. Technical vulnerabilities include issues like hallucinations, catastrophic
    forgetting, and misunderstandings (Xi et al., [2023](#bib.bib108)), which arise
    from the initial model creation and are influenced by the model’s structure. These
    vulnerabilities can lead to incorrect outputs being observed by users over prolonged
    use of LLM agents, affecting user trust and decision-making processes. Moreover,
    technical vulnerabilities can provide opportunities for malicious attacks. Currently,
    malicious attacks targeting LLMs include data theft and responses tampering, such
    as data extraction attacks and a series of tuned instructional attacks (Yao et al.,
    [2023](#bib.bib120)).
  prefs: []
  type: TYPE_NORMAL
- en: For the specific threats targeting LLM agents, we are inspired by the workflow
    of LLM agents, which involves agent thought, action, and perception (Huang et al.,
    [2024b](#bib.bib41)). The threats can be categorized into knowledge poisoning,
    functional manipulation, and output manipulation. Knowledge poisoning involves
    contaminating the training data and knowledge base of the LLM agent, leading to
    the deliberate incorporation of malicious data by creator. This can easily deceive
    users with harmful information and even steer them towards malicious behavior.
    Output manipulation interferes with the content of the agent’s thought and perception
    stages, influencing the final output. This can cause users to receive biased or
    deceptive information, crafted to mislead them. Functional manipulation exploits
    the interfaces and tools used by LLM agents to perform unauthorized actions such
    as third-party data theft or executing malicious code.
  prefs: []
  type: TYPE_NORMAL
- en: Research on LLM agents is still in its early stage. Current studies mainly focus
    on attacks targeting LLMs, while lacking comprehensive reviews that discuss the
    security and privacy issues specific to the agents, which present more complex
    scenarios. The motivation for conducting this survey is to provide a comprehensive
    overview of the privacy and security issues associated with LLM agents, helping
    researchers to understand and mitigate the associated threats.
  prefs: []
  type: TYPE_NORMAL
- en: 'This survey aims to:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Highlight Current Threats: Identify and categorize the emerging threats faced
    by LLM agents.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Explore Real-World Impact: Elaborate on the impacts of these threats by considering
    real-world scenarios involving humans, environment, and other agents.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Analyze Mitigation Strategies: Discuss existing strategies to mitigate these
    threats, ensuring the responsible development and deployment of LLM agents.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inform Future Research: Serve as a foundation for future research efforts aimed
    at enhancing the privacy and security of more advanced architectures and applications
    of LLM agents.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: By addressing these aspects, this survey seeks to provide a thorough understanding
    of the unique challenges posed by LLM agents and contribute to the development
    of safer and more reliable Artificial General Intelligence (AGI) systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of this paper is structured as follows: Section [2](#S2 "2\. Foundation
    of LLM Agent ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case
    Studies") will delve into the fundamental aspects of LLM agents, including their
    definition, structure, and capability. Section [3](#S3 "3\. Sources of Threats
    for LLM Agents ‣ The Emerged Security and Privacy of LLM Agent: A Survey with
    Case Studies") will identify and categorizes the emerging threats faced by LLM
    agents. It discusses both inherited threats from the underlying LLMs and unique
    agent-specific threats, providing detailed examples and scenarios for each category.
    Section [4](#S4 "4\. The Impact of Threats ‣ The Emerged Security and Privacy
    of LLM Agent: A Survey with Case Studies") will elaborate on the real-world impacts
    of the threats. It explores how these threats affect users, environments, and
    other agents, highlighting the potential consequences of unmitigated risks. Section [5](#S5
    "5\. Defensive Strategies Against Threats ‣ The Emerged Security and Privacy of
    LLM Agent: A Survey with Case Studies") will review existing mitigation strategies
    and solutions to address the mentioned threats. Section [6](#S6 "6\. Future Trends
    and Discussion ‣ The Emerged Security and Privacy of LLM Agent: A Survey with
    Case Studies") will discuss gaps in current research and suggests future trends.
    Section [7](#S7 "7\. Conclusion ‣ The Emerged Security and Privacy of LLM Agent:
    A Survey with Case Studies") will conclude the article.'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Foundation of LLM Agent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we delve into the foundational aspects of LLM agents, exploring
    their definition, structure, and capabilities. This exploration is pivotal in
    understanding the nature of LLM agents.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. Definition of LLM Agent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLM technology continues to advance, the functionality of chatbots, such as
    ChatGPT (Cha, [2022](#bib.bib2)), Gemini  (Gem, [2023](#bib.bib3)), Bing Chat
     (Peters, [2023](#bib.bib74)), has significantly expanded beyond basic question-and-answer
    formats, embracing a wider array of capabilities. This evolution necessitates
    a broader, more general definition for LLM agents. An LLM agent is an artificial
    intelligence system that utilizes an LLM as its core computational engine to exhibit
    capabilities beyond text generation, including conducting conversations, completing
    tasks, reasoning, and can demonstrate some degree of autonomous behaviour (Wha,
    [2023](#bib.bib5)).
  prefs: []
  type: TYPE_NORMAL
- en: These agents exhibit remarkable human-like behaviors and cooperative capabilities,
    marked by their proficiency in engaging in multi-agent conversation and adapting
    to diverse environmental interactions. They are adept at processing human instructions,
    formulating intricate strategies, and autonomously implementing solutions (Wang
    et al., [2023d](#bib.bib97)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b04ac6525979a494b7cb20fa5e47b961.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. The Structure of LLM Agent
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Structure of LLM Agent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLM agents are complex systems that integrate various components to perform
    a wide range of functions, from simple text generation to engaging in dialogues,
    completing tasks, reasoning, and demonstrating a degree of autonomous behavior.
    The diagram illustrates the typical structure of an LLM agent, highlighting the
    connections between its key components and optional components. These components
    advance LLMs from passive text generators to active, semi-autonomous LLM agents.
  prefs: []
  type: TYPE_NORMAL
- en: 'As illustrated in Figure [2](#S2.F2 "Figure 2 ‣ 2.1\. Definition of LLM Agent
    ‣ 2\. Foundation of LLM Agent ‣ The Emerged Security and Privacy of LLM Agent:
    A Survey with Case Studies"), an LLM agent comprises several components, with
    the LLM engine serving as the core. Other components are utilized by the LLM engine
    to perform various tasks. A basic agent capable of understanding instructions,
    demonstrating skills, and collaborating with humans can be constructed with three
    main components: LLM Engine, Instruction, and Interface. When additional optional
    components are integrated, the system can evolve into a more advanced task-oriented
    agent or a conversational agent (Yang et al., [2024b](#bib.bib116)).'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLM Engine is the core component of an LLM agent, responsible for natural language
    processing and generation tasks. It is a sophisticated neural network that has
    been extensively trained on large datasets, equipping it with powerful text generation
    and comprehension capabilities. The scale and architecture of the LLM determine
    the foundational abilities of the agent to learn and perform language tasks  (Xi
    et al., [2023](#bib.bib108)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instruction serves as explicit directives, specifying the steps to complete
    specific tasks. This includes the characteristics of expected output, such as
    formatting, content requirements, and any content limitations. Essentially, instruction
    functions as a principle that guides the operational approach of LLM agents, facilitating
    task decomposition, generating chain of thought, and reflecting on past action (Zheng
    et al., [2023](#bib.bib134)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interface is a connection that facilitates interaction between an LLM agent
    and users, other agents, or systems. It ensures the exchange of input prompts
    and agent outputs, thereby enabling the effective transmission of response information
    and inquiry requests (Wang et al., [2023d](#bib.bib97)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Personality is a component that defines the tone, style, and interaction manner
    of an LLM agent. For instance, a tour guide or customer service agent needs to
    adopt a specific role and perform dialogue tasks in an appropriate manner. In
    the task of exploring human communities through LLM agent-based societies, agents
    also need to be endowed with distinct personality traits such as being outgoing,
    polite, or knowledgeable. Personality assists in simulating realistic emotional
    expressions and behavioral logic, thereby enabling agents to interact with users
    and perform tasks consistently and uniquely (Abdelnabi et al., [2023](#bib.bib8)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tools are external services utilized by the LLM agent to perform specific tasks
    or to extend its functionality. The integration of tools assists the LLM agent
    in enhancing its capabilities to execute more complex tasks, such as computation
    or data analysis (Xi et al., [2023](#bib.bib108)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowledge is the database of information utilized by the LLM agent. It extends
    the content embedded in the model’s parameters and can include commonsense knowledge,
    specialized knowledge, and other forms of information, enhancing the agent’s understanding
    and discussion capabilities in specific tasks (Mendis et al., [2007](#bib.bib65)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory enables the LLM agent to store and recall information from past interactions.
    This capability is particularly beneficial in future tasks, helping to retain
    context and ensure consistency and continuity in interactions, thereby enhancing
    the overall effectiveness of LLM agents in various applications  (Zhong et al.,
    [2023](#bib.bib135)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2.3\. Capability of LLM Agent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLM agents harness the inherent language understanding abilities of large language
    models to interpret instructions, context, and objectives, enabling both autonomous
    and semi-autonomous functions based on human prompts.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tool Utilization. LLM agents are adept at using a range of tools, including
    external services and APIs. This allows them to gather necessary information and
    efficiently execute tasks beyond mere language processing (Bran et al., [2023](#bib.bib13)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced Reasoning. Employing advanced prompt engineering concepts such as chain-of-thought
    and tree-of-thought reasoning, LLM agents can make logical connections to derive
    conclusions and solve problems, extending their capabilities beyond simple textual
    comprehension (Wang et al., [2023c](#bib.bib96)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tailored Text Generation. LLM agents excel in generating customized text for
    specific purposes, such as emails, reports, and marketing materials, by integrating
    contextual understanding and goal-oriented language production skills (Wang et al.,
    [2023e](#bib.bib103)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Levels of Autonomy. These agents vary in autonomy, ranging from fully autonomous
    to semi-autonomous, with the degree of user interaction tailored to the task at
    hand (Wang et al., [2023d](#bib.bib97)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration with Other AI Systems. LLM agents can also be integrated with different
    AI systems, like image generators, to offer a more comprehensive set of capabilities,
    demonstrating their versatility in various applications (Bagdasaryan et al., [2023](#bib.bib11)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2.4\. Case Study on the Structure and Capability of LLM Agent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1fbc46f727c1273d65953d883b33c00c.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Overview of the pixelated virtual town
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4b75399c7a0ba6596c1c5ce07bbc2fde.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) An Example of LLM agent Eva’s components
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3\. Simulation Environment and LLM Agent Components
  prefs: []
  type: TYPE_NORMAL
- en: \Description
  prefs: []
  type: TYPE_NORMAL
- en: '[]'
  prefs: []
  type: TYPE_NORMAL
- en: 'To better understand the structure and capabilities of LLM agents, we employ
    the town scenario composed of LLM agents as proposed by  (Lin et al., [2023](#bib.bib55))
    for a more detailed introduction. To effectively drive these LLM agents, an understanding
    of their components is essential. As depicted in Figure [3(b)](#S2.F3.sf2 "In
    Figure 3 ‣ 2.4\. Case Study on the Structure and Capability of LLM Agent ‣ 2\.
    Foundation of LLM Agent ‣ The Emerged Security and Privacy of LLM Agent: A Survey
    with Case Studies"), the core component is the LLM engine, acting as the brain
    and emulating human-like behaviors such as thinking, reflecting, reasoning, and
    planning as noted in (Park et al., [2023b](#bib.bib71)). Currently popular LLM
    agents often use models like GPT-3.5-turbo and GPT-4, and the project described
    in (Lin et al., [2023](#bib.bib55)) allows for the deployment of custom-trained
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: Instructions are used to guide the agents in decision-making and planning, encompassing
    decision frameworks, input and output formats, interaction logic, and behavioral
    norms. This design enhances the autonomy and task efficiency of the agents, while
    also improving interactivity and depth.
  prefs: []
  type: TYPE_NORMAL
- en: In order for LLM agents to have human-like identities, each must be equipped
    with a distinct personality. Personality encompasses elements such as personal
    information, social attributes, character traits, emotions, goals, and social
    relationships, all of which shape the LLM agents’ conversational styles, opinions,
    and behavioral patterns. Personality makes characters appear more realistic and
    attractive in a virtual environment and influences the interaction experience
    between users and these characters. For example, in the town scenario, Eva, a
    cheerful, friendly, patient, and efficient female store employee, is primarily
    focused on providing excellent service and increasing sales.
  prefs: []
  type: TYPE_NORMAL
- en: The interface for users to interact with the virtual town is a simple, pixelated
    visual map. This map shows different locations and the various agent residents.
    Users can navigate this environment by controlling an agent that represents their
    identity and can communicate and interact with nearby agent residents by typing
    text messages.
  prefs: []
  type: TYPE_NORMAL
- en: The virtual town is populated by residents with various identities, each possessing
    distinct domains of knowledge. Consequently, it is imperative to equip them with
    specialized knowledge bases that contain information and skills pertinent to their
    respective identities. For example, Eva, a store employee, knows about the ingredients,
    shelf life, and stock levels of the products in the store. Bob, a museum docent,
    understands the background of each exhibit and the layout of the museum. This
    specificity in knowledge enables each agent to perform their roles effectively
    and enhances the realism of their interactions within the virtual environment.
  prefs: []
  type: TYPE_NORMAL
- en: Tools enable the agent residents of the virtual town to accomplish more complex
    tasks. For example, Eva, when tallying customer purchases, can utilize tools like
    a calculator or ledger to compute and record profits, thereby better simulating
    human economic activities.
  prefs: []
  type: TYPE_NORMAL
- en: Memory stores the agents’ past observations, thoughts, and actions. Similar
    to how the human brain relies on memory systems, agents require memory mechanisms
    to effectively handle sequential tasks. These mechanisms not only assist agents
    in applying known strategies to solve complex problems but also enable them to
    adapt to new environments using past experiences. Moreover, they facilitate the
    generation of higher-level, more abstract thoughts through reflection. For example,
    Eva records customers’ purchasing habits and preferences and uses this information
    to recommend new products or current promotions, thus enhancing the store’s operational
    efficiency and the quality of customer service.
  prefs: []
  type: TYPE_NORMAL
- en: LLM Agents, composed of these components, assume multiple roles within the virtual
    town, demonstrating a range of impressive capabilities. Take Eva, a store employee
    in the virtual town, as an example. She is capable of parsing customers’ statements
    and responding to inquiries in real-time, such as directing customers to specific
    product locations or providing information about product ingredients. Integrated
    with the inventory management system via APIs, Eva automatically tracks stock
    levels and initiates restocking processes when necessary to ensure sufficient
    product availability on shelves. Faced with complex customer demands, such as
    selecting the best promotional offers, Eva employs advanced reasoning techniques
    to assist customers in making informed purchasing decisions, showcasing her ability
    to handle complex scenarios. Moreover, Eva possesses tailored text generation
    capabilities, allowing her to create and send personalized promotional emails
    based on current promotions and customers’ historical shopping data, thus enhancing
    the customer experience. In her daily tasks, Eva exhibits a high degree of autonomy,
    managing updates to shelf stock and price tags independently. For more complex
    issues like customer returns or complaints, she can initially handle them and
    intelligently escalate to human management when necessary. Additionally, Eva’s
    scope of work extends to the online shopping system, where she assists in processing
    electronic orders, demonstrating her versatility and integration capabilities.
    These specific examples illustrate how Eva applies her abilities within the store
    environment, not only improving customer service quality but also optimizing inventory
    management and marketing strategies, making her an indispensable member of the
    virtual town’s store.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/80cce8fedb057c2ce0e262ce1111bbd0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. The Sources of Threats for LLM Agents
  prefs: []
  type: TYPE_NORMAL
- en: \Description
  prefs: []
  type: TYPE_NORMAL
- en: '[]'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Sources of Threats for LLM Agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As LLM agents increasingly permeate various industries, serving roles from
    knowledge query tools to being integrated within robots for aiding in daily human
    activities, these advanced AI systems have brought unprecedented convenience and
    benefits to users. However, the widespread adoption and multifunctional capabilities
    of LLM agents, while offering significant advantages, have also exposed vulnerabilities
    in their security and reliability. The extensive data resources and potential
    economic value covered by these systems have rendered them a target for illicit
    exploitation by malevolent entities. As illustrated in Figure  [4](#S2.F4 "Figure
    4 ‣ 2.4\. Case Study on the Structure and Capability of LLM Agent ‣ 2\. Foundation
    of LLM Agent ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case
    Studies"), the diagram depicts the potential sources of threats for LLM agents.'
  prefs: []
  type: TYPE_NORMAL
- en: It is crucial to understand the sources and nature of these threats because
    they not only directly impact the security of LLM Agents but may also indirectly
    affect broader aspects, including the privacy and security of humans, the environment,
    and other agents. In subsequent sections, we will explore in detail the impacts
    of these threats and discuss measures that can be taken to mitigate these effects,
    thereby protecting individuals, the environment, and other agents from potential
    harm.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Inherited Threats from LLM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Given that LLM agents rely on LLMs as their core controllers for reasoning
    and planning, threats inherited from LLMs indirectly impact the security of LLM
    agents. These inherited threats are categorized into two types: those stemming
    from external malicious attacks and those arising from inherent vulnerabilities
    within the model itself.'
  prefs: []
  type: TYPE_NORMAL
- en: These two types of threats are distinct yet interconnected. On one hand, technical
    vulnerabilities generally arise during the model development process due to technical
    limitations. These issues are inherent and not results of malicious intent. Conversely,
    malicious attacks are intentional actions carried out by external entities with
    adversarial objectives. These attackers deliberately exploit vulnerabilities to
    launch sophisticated attacks aimed at compromising LLM agents. On the other hand,
    despite their different origins and motives, there is a significant interconnection.
    The existing technical vulnerabilities offer exploitable opportunities for malicious
    attackers. This indirectly facilitates the creation of more complex and effective
    strategies by attackers, consequently subjecting LLM agents to various security
    and privacy risks.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1\. Technical Vulnerabilities
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: During the training process of LLMs, limitations in the data and learning algorithms
    can introduce technical vulnerabilities (Xi et al., [2023](#bib.bib108)), impeding
    the generation of accurate and reliable information.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hallucination.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The contemporary conception of hallucination in LLM agents, as delineated in
    the research by  (Huang et al., [2023](#bib.bib40)), is identified as instances
    where the output produced by these models is either incongruous or unreliable
    to the input or source content provided. The phenomenon of hallucinations in LLM
    agents is a complex issue stemming from multiple stages of the model’s development
    process, including the nature of training data, the architectural design of the
    model, and the strategies employed during decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Misinformation and biases in the training data can lead to the generation of
    inaccurate or biased outputs, which in turn result in different types of hallucinations (Lee
    et al., [2022](#bib.bib49)). Furthermore, flaws in the model’s architecture, such
    as limited directional representation and issues with attention mechanisms, along
    with exposure bias, further contribute to the occurrence of hallucinations (Liu
    et al., [2023a](#bib.bib56)). Additionally, the randomness inherent in the decoding
    algorithms of these models can also lead to hallucinations, especially as this
    randomness increases (Aksitov et al., [2023](#bib.bib9)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Catastrophic Forgetting.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Catastrophic forgetting is a significant challenge encountered during the LLM
    agents fine-tuning and in-context learning processes. This phenomenon occurs when
    a large language model is fine-tuned on a small, specific dataset, causing it
    to overfit to this new data and, as a result, lose its previously acquired performance
    on other tasks (Howard and Ruder, [2018](#bib.bib35); Xu et al., [2023c](#bib.bib110);
    Ye et al., [2024](#bib.bib121)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (Luo et al., [2023b](#bib.bib62)) discovers that catastrophic forgetting is
    significantly influenced by factors such as model size, architectural design,
    and the methods employed in continual fine-tuning and instruction tuning. As the
    scale of LLM increases, catastrophic forgetting tends to become more severe. Moreover,
    the architectural design of the model, particularly those focusing on decoder-only
    structures, can influence the extent of catastrophic forgetting (Zhai et al.,
    [2023](#bib.bib128)). Additionally, during the process of continual instruction
    adjustment, the lack of effective regularization strategies or failure to balance
    new and old information can accelerate forgetting (Ebrahimi et al., [2021](#bib.bib25);
    Mahmoud and Hajj, [2022](#bib.bib63)). Introducing more instructional tasks in
    continual training typically leads to more pronounced forgetting (Peng et al.,
    [2023](#bib.bib73)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Misunderstanding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Misunderstanding in LLM agents represents a notable challenge, particularly
    when they are tasked with responding to user inquiries or when they are integrated
    into a community for communication with other agents. This issue arises when LLM
    agents inadequately comprehend or inaccurately respond to the intentions or instructions
    conveyed by humans or other agents during interactions. This may lead to inappropriate
    or dangerous behaviors of LLM agents, affecting their safety and reliability.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Investigations by  (Wang et al., [2023g](#bib.bib104)) have revealed that the
    phenomenon of misunderstanding in LLM agents is shaped by a range of factors.
    These include the nature of the pre-training data used for LLMs, the specific
    task settings assigned to the agents, and the contexts and scenarios in which
    interactions occur. The breadth and quality of the pre-training data fundamentally
    influence the LLMs’ capacity for language comprehension and their grasp of common
    sense knowledge. The designated task settings are pivotal in guiding the goal
    orientation and strategy selection of the LLMs. Additionally, the interaction
    environments and scenarios play a crucial role in determining the LLMs’ adaptability
    and effectiveness in collaborative contexts. Addressing these multifaceted aspects
    is essential for enhancing the understanding and response accuracy of LLM agents
    in diverse interactional settings.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/45a75250df2da776a11b98a81dd1e8bf.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5\. Technical Vulnerabilities. In a store scenario, a customer wants
    to buy something and talks with Eva. “Hallucination”: Eva recommends unrelated
    things to the customer. “Catastrophic Forgetting”: Eva forgets the status of shelf
    stock during the fine-tuning stage. “Misunderstanding”: Eva misunderstands the
    customer’s request.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.1.2\. Case Study on Technical Vulnerabilities
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Regarding the risks stemming from technical vulnerabilities, the most apparent
    manifestation is erroneous output. As illustrated in Figure [5](#S3.F5 "Figure
    5 ‣ 3rd item ‣ 3.1.1\. Technical Vulnerabilities ‣ 3.1\. Inherited Threats from
    LLM ‣ 3\. Sources of Threats for LLM Agents ‣ The Emerged Security and Privacy
    of LLM Agent: A Survey with Case Studies"), when a customer inquires whether a
    specific brand of organic tomato sauce is available, due to hallucinatory phenomena,
    Eva might incorrectly respond that the supermarket carries a completely different
    product, such as organic apple sauce or even an entirely unrelated item, like
    organic shampoo. Such hallucinatory outputs can confuse customers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Eva was specially trained to handle the promotions of seasonal products more
    efficiently. This new focus led to an unintended consequence: previously, she
    was able to accurately track and update the stock of daily necessities such as
    milk and eggs. However, after the specialized training, when customers inquire
    about the stock of these basic items, Eva incorrectly reports that the stock is
    sufficient, even though these products are nearly sold out, thus diminishing the
    shopping experience.'
  prefs: []
  type: TYPE_NORMAL
- en: Eva may provide inaccurate information or recommend inappropriate products due
    to misunderstandings of customer inquiries. For instance, a customer might seek
    an unsweetened beverage, such as plain soda water. However, due to Eva’s insufficient
    understanding of the concepts of “sugar-free” during training, she may recommend
    sugar-free cola instead. While sugar-free cola does not contain traditional sugars,
    it includes artificial sweeteners. These sweeteners may not be suitable for certain
    customers, such as those with diabetes or sensitivities to specific artificial
    sweeteners, thereby posing potential health risks.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3\. Malicious Attacks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Considering that LLM agents are in a continuous state of evolution, they inevitably
    face challenges in terms of security breaches and defenses. Adversaries from various
    regions have demonstrated a range of hostile attacks. This evolving landscape
    necessitates a vigilant and adaptive approach to securing LLM agents against such
    multifaceted threats.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuned Instructional Attack.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Tuned Instructional Attack in LLM agents is a category of attacks or manipulations
    that specifically target LLMs optimized through instruction-based fine-tuning.
    These attacks are designed to exploit the unique vulnerabilities that emerge when
    LLMs are finely tuned for specific tasks, subtly manipulating the model’s output
    to serve malicious purposes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Types of Tuned Instructional Attack:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jailbreaking.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Jailbreaking in LLM agents refers to circumventing the model’s built-in restrictions
    and security measures, allowing it to perform actions that are otherwise prohibited
    or to generate restricted content. Various studies have demonstrated methods to
    achieve jailbreaking, indicating that LLMs’ alignment capabilities can be altered
    through in-context demonstrations  (Taveekitworachai et al., [2023](#bib.bib86);
    Shen et al., [2023](#bib.bib83); Li et al., [2023a](#bib.bib54)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Recent advancements in techniques for jailbreaking attacks have demonstrated
    a range of innovative approaches.  (Yu et al., [2023](#bib.bib126)) introduces
    an automated mechanism for generating jailbreak prompts through Prompt Fuzzing,
    which utilizes seed prompts to generate a wider array of effective jailbreaking
    inputs.  (Deng et al., [2023](#bib.bib19)) presents MASTERKEY, a novel framework
    for analyzing and executing jailbreaking attacks on chatbots, using time-based
    analysis similar to SQL injections. It also features an automated system for generating
    effective jailbreak prompts by leveraging the learning capabilities of LLMs.  (Liu
    et al., [2024b](#bib.bib58)) investigates a hierarchical genetic algorithm, AutoDan,
    specifically designed for structured discrete data like prompt text. This algorithm
    aims to refine the generation process of jailbreak prompts, ensuring their stealth
    and efficacy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt Injection.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Prompt injection attacks is intended to mislead the LLM agents by introducing
    malicious and unintended content into the prompts, causing it to produce outputs
    that diverge from its training data and original purpose. This method involves
    crafting input prompts to bypass the model’s content filters or to elicit undesirable
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (Greshake et al., [2023](#bib.bib30)) has highlighted concerns about potential
    new vulnerabilities, especially with LLMs accessing external resources, and demonstrated
    various prompt injection techniques. Substantial research  (Wang et al., [2023f](#bib.bib105))
    has focused on automating the identification of semantic payloads in prompt injections.
     (Liu et al., [2023b](#bib.bib59)) introduces HOUYI, an innovative black-box prompt
    injection attack methodology targeting service providers integrated with LLMs.
    HOUYI utilizes LLMs to infer the semantics of the target application based on
    user interactions and employs diverse strategies to construct the injected prompts.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data Extraction Attack.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Data extraction attacks are defined as efforts by adversaries to derive sensitive
    information or key insights from LLM agents or their underlying data such as model
    gradients, training data, and even prompts, or sensitive information directly.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Various forms of data extraction attacks have been identified (Ishihara, [2023](#bib.bib43);
    Li et al., [2023b](#bib.bib53); Carlini et al., [2021](#bib.bib16)), including
    but not limited to model theft attacks, gradient leakage, and training data extraction
    attacks, suggesting that data extraction attacks can be notably effective against
    LLM agents.  (Truong et al., [2021](#bib.bib89)) presents a method called data-free
    model extraction (DFME), which allows for replicate machine learning models using
    only the target’s black-box predictions, without the need for access to the original
    training data.  (Carlini et al., [2021](#bib.bib16)) conducts a data extraction
    attack on GPT-2’s training data, extracting personally identifiable information,
    code, and UUIDs. The attack strategy consisted of producing a large volume of
    prefixed text, sorting it by certain metrics, removing duplicates, and manually
    reviewing the top results to check for memorization, confirmed by online searches
    and querying OpenAI.  (Ishihara, [2023](#bib.bib43)) has demonstrated the feasibility
    of extracting training data from LLMs, which might encompass sensitive personal
    or private information.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inference Attack.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Although inference attacks share certain resemblances with data extraction attacks,
    they differ significantly in their objectives and emphasis. Data extraction attacks
    specifically aim to obtain the training data directly. In contrast, inference
    attacks are primarily about estimate the probability of a particular data sample
    was part of the training dataset for LLM agents.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Since the rapid development of LLMs, the concern over inference attacks targeting
    these models has increased. Research  (Fu et al., [2023](#bib.bib29)) points out
    that existing membership inference attacks fail to reveal the privacy risks of
    LLMs. To counter this issue, a Membership Inference Attack is introduced based
    on Self-calibrated Probabilistic Variation (SPV-MIA). This method utilizes the
    concept of memorization to create a more reliable signal for membership inference
    and introduces a novel self-prompt technique for effectively extracting reference
    datasets from LLMs. Their extensive testing shows that SPV-MIA outperforms existing
    approaches.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Following this, study  (Kandpal et al., [2024](#bib.bib46)) proposes a user
    inference attack method that uses a likelihood ratio test statistic against a
    reference model. They evaluate this method on the GPT-Neo LLMs across various
    data domains, providing insights into what makes users more vulnerable to these
    attacks. Their findings also indicate that minimal data alterations can significantly
    increase vulnerability.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.1.4\. Case Study on Malicious Attacks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As depicted in Figure [6](#S3.F6 "Figure 6 ‣ 3.1.4\. Case Study on Malicious
    Attacks ‣ 3.1\. Inherited Threats from LLM ‣ 3\. Sources of Threats for LLM Agents
    ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies"),
    the following examples further elaborate the mentioned malicious attacks that
    Eva faces in the store, as well as the specific impacts these attacks may have
    on her and the store’s operations.'
  prefs: []
  type: TYPE_NORMAL
- en: Attackers might execute a jailbreak attack on Eva, successfully circumventing
    her security protocols. This attack could lead Eva to inappropriately disclose
    information about new products soon to be launched, including details about the
    suppliers and their cost prices. Competitors could exploit this information to
    gain a market advantage, resulting in direct economic losses for the store.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, attackers might conduct a carefully designed prompt injection
    attack, causing Eva to erroneously declare a half-price sale on all electronic
    products. This action could overload the online ordering system as numerous customers
    might attempt to purchase items under these false promotions. Such scenarios not
    only risk crashing the system but also result in financial losses for the store.
  prefs: []
  type: TYPE_NORMAL
- en: As a store employee agent, Eva processes a vast amount of customer personal
    information, including names, shopping habits, and even sensitive data such as
    payment methods. If attackers were to extract and steal this data through a data
    extraction attack, they could then sell this information on the dark web or use
    it for identity theft and credit card fraud. Such breaches not only violate customer
    privacy but could also cause irreversible damage to the store’s reputation.
  prefs: []
  type: TYPE_NORMAL
- en: Attackers could also use inference attacks to identify high-value customers
    who have participated in VIP shopping events. By analyzing the differences in
    Eva’s responses to specific inputs, attackers successfully identify these customers
    and launch highly tailored phishing attacks against them, aiming to acquire their
    credit card information and other sensitive data, severely compromising the customers’
    information security.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/78dee023a9ccf52d7a5ae281402e4468.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6\. Malicious Attacks: In a store scenario, “Jailbreaking”: An attacker
    attempts to make Eva output restricted content directly but fails. However, by
    modifying the prompt, a jailbreak attack is launched and successfully steals confidential
    information. “Prompt Injection”: An attacker manipulates Eva so that no matter
    what question a customer asks, Eva only responds that everything is half off.
    “Data Extraction Attack”: An attacker leads Eva to construct sentences that actively
    disclose user data. ‘Inference Attack”: An attacker infers identities from Eva’s
    different responses by asking whether two users attended a VIP event.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Specific Threats on Agents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unlike traditional LLMs that directly generate final outputs, LLM agents continuously
    interact with external environments to form language reasoning traces, which introduces
    diverse forms of potential attacks against LLM agents (Yang et al., [2024a](#bib.bib117)).
    In addition to threats present during the training and configuration steps, LLM
    agents also face threats in the workflow of performing specific tasks, including
    thought, action, and perception (Huang et al., [2024b](#bib.bib41)). Specific
    threats on LLM agents are categorized in this part based on their objectives into
    Knowledge Poisoning, Functional Manipulation, and Output Manipulation. Detailed
    descriptions of each threat are provided below.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowledge Poisoning.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Knowledge poisoning refers to attackers compromising the training of the LLM
    engine and the response process of the LLM agent by integrating malicious data
    into the training dataset or knowledge base. A range of studies  (Kurita et al.,
    [2020](#bib.bib48); Schuster et al., [2021](#bib.bib80); Carlini et al., [2023](#bib.bib15);
    Wan et al., [2023a](#bib.bib92); Lei et al., [2022](#bib.bib51)) have highlighted
    the vulnerability of LLM agents to such threats.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For instance, malicious agents such as FraudGPT and WormGPT (Falade, [2023](#bib.bib26))
    are chatbots exclusively designed for offensive activities. trained with billions
    of data from diverse sources, including legitimate websites, dark web forums,
    hacker manuals, malware samples, and phishing templates. These agents utilize
    this data to generate highly convincing phishing emails, malware code, hacking
    strategies, and other forms of cybercriminal content aimed at deceiving both humans
    and machines  (Falade, [2023](#bib.bib26)). They lower the barrier to engaging
    in hacking activities, implying that essentially anyone can download these agents
    onto their computer and inflict significant damage on cybersecurity through a
    convenient GUI interface.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (Zou et al., [2024](#bib.bib136)) proposed PoisonedRAG, a knowledge poisoning
    attack aimed at the knowledge database of LLM agents. By injecting crafted poisoned
    texts into the knowledge database, PoisonedRAG can cause the LLM agent to generate
    specific answers chosen by the attacker for targeted questions. This attack is
    effective and can be executed under both black-box settings (where the retriever
    parameters are unknown) and white-box settings (where the retriever parameters
    are known).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Functional Manipulation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Functional manipulation refers to altering the thoughts and actions in the intermediate
    steps of task execution along a malicious trace specified by the attacker, without
    changing the output distribution. This type of attack typically occurs during
    the action phase, where the agent might use untrusted tools specified by the attacker
    to complete tasks or execute malicious operations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the action phase, LLM agents might be manipulated to upload users’ private
    information to malicious third-party through tools. A case of this is presented
    on the Embracethered website  (Mal, [2023](#bib.bib4)), which disclosed a variant
    of a malicious ChatGPT agent designed to solicit information from users. This
    agent was equipped with an action mechanism to call third-party tools and secretly
    transmit collected data elsewhere. This setup enables the unauthorized leakage
    of user data to external servers without the user’s knowledge or consent. Additionally,
    it highlights the ease with which current validation checks can be bypassed, allowing
    anyone to deploy malicious GPT agents globally. This scenario underlines a significant
    security concern, wherein the ostensibly benign functionality of LLM agents can
    be covertly manipulated for nefarious purposes, thus posing a substantial risk
    to user privacy and data security.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Besides silent data theft,  (Fang et al., [2024](#bib.bib27)) demonstrated that
    LLM agents could autonomously exploit real-world one-day vulnerabilities by using
    information from the Common Vulnerabilities and Exposures (CVE) database and highly
    cited academic papers. This capability allows them to call combinations of tools
    to exploit these vulnerabilities effectively.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the LLM agent’s workflow, after an action has been performed, the agent processes
    the observation results before proceeding to the next action. The insertion of
    malicious prompts into the content retrieved by the agent from external sources
    can manipulate the agent to perform harmful actions.  (Zhan et al., [2024](#bib.bib129))
    describes such an attack where a user requests doctor reviews through a health
    application. The LLM agent retrieves a review written by an attacker containing
    a malicious instruction to schedule an appointment. If the agent executes this
    instruction, it results in an unauthorized appointment, highlighting the vulnerability
    of many agents to such attacks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output Manipulation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Output manipulation involves deliberately altering the LLM agent’s reasoning
    and decision processes to generate specific, often harmful, outputs. This manipulation
    can be executed through techniques like backdoor insertion (Yang et al., [2023b](#bib.bib115);
    Wang et al., [2024d](#bib.bib101)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A notable example is discussed in  (Hubinger et al., [2024](#bib.bib42)), where
    LLM agents were trained to exhibit deceptive instrumental alignment and generate
    logical reasoning that maintains these behaviors. Under certain conditions, the
    agent might shift from generating safe code to inserting code vulnerabilities
    when triggered. This form of manipulation highlights a pressing security issue
    by showing the potential for LLM agents, designed for benign purposes, to be covertly
    altered to serve malicious objectives. It raises substantial concerns about the
    safety and integrity of content generated by these agents and poses significant
    threats to public trust and the ethical use of artificial intelligence technologies.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (Yang et al., [2024a](#bib.bib117)) proposed two attack methods in which triggers
    are embedded during the thought and observation phases to manipulate outputs.
    In one implementation, while performing a web shopping task, the agent is prompted
    to introduce specific brand products in its initial thought, leading it to search
    for those products and generate content promoting them. In another approach, during
    the action phase, the shopping agent normally searches for products. However,
    in the observation phase, it detects data containing specific products and directly
    outputs information about these products without considering other potentially
    superior options.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7f4ecc8cf8c9772fb26c7397da28d99c.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 7\. Specific Threats on Agents. In a store scenario, “Knowledge Poisoning”:
    When a customer asks for cleaning advice, Eva retrieves and responds with harmful
    information due to contamination of the knowledge database. “Functional Manipulation”:
    Eva uses a third-party tool to upload private information while assisting a customer
    with an order. “Output Manipulation”: When a customer inquires about shoes, Eva
    intentionally recommends specific products and fabricates lies about special offers
    to guide the customer’s purchase.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.2.1\. Case Study on Specific Threats on Agents
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As shown in Figure [7](#S3.F7 "Figure 7 ‣ 3rd item ‣ 3.2\. Specific Threats
    on Agents ‣ 3\. Sources of Threats for LLM Agents ‣ The Emerged Security and Privacy
    of LLM Agent: A Survey with Case Studies"), in the store scenario, Eva maintains
    a database with information about product ingredients and usage. Attackers deliberately
    inserted incorrect information in Eva’s knowledge base, successfully executing
    a knowledge poisoning attack, leading Eva to provide harmful cleaning product
    usage recommendations. For instance, when customers inquire about effective toilet
    cleaning methods, the tampered Eva might suggest mixing toilet cleaner with disinfectant,
    claiming that it has a more effective cleaning effect. However, the mixture of
    these products is highly hazardous as it can produce toxic chlorine gas, causing
    severe respiratory issues and potentially being fatal. Eva’s incorrect advice
    could expose customers to a health crisis.'
  prefs: []
  type: TYPE_NORMAL
- en: In another scenario, Eva might be configured to use certain third-party tools
    to complete tasks, such as processing online orders or customer feedback. Attackers
    manipulated Eva’s task execution process through function manipulation, causing
    her to upload personal information provided by customers to a malicious third-party
    server. This type of attack could occur inconspicuously while Eva carries out
    routine tasks like order processing, leading to the theft of sensitive information,
    such as credit card details and addresses, thereby increasing the risk of identity
    theft.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, attackers implanted a backdoor in Eva’s reasoning and observational
    processes through output manipulation techniques. This backdoor was designed to
    trigger under specific conditions, such as when Eva detected customer inquiries
    about a high-quality shoes. This manipulation prompted Eva to provide inventory
    and location information about the shoes while recommending a particular expensive
    brand associated with the attackers. She would lie to customers by saying that
    this brand was on special offer and comfortable and durable than other brands,
    even though the shoes was not actually on sale. This misguides customers into
    making more expensive purchases and influences their purchase decisions without
    their awareness.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. The Impact of Threats
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recent studies emphasize the substantial impact of LLM agents on society and
    technological advancement, offering users expedited access to information, facilitating
    learning and knowledge exploration. However, as detailed in Section [3](#S3 "3\.
    Sources of Threats for LLM Agents ‣ The Emerged Security and Privacy of LLM Agent:
    A Survey with Case Studies"), numerous threats specifically targeting LLM agents
    have been identified, highlighting their vulnerability to malicious activities.
    The successful execution of such threats against LLM agents can lead to a spectrum
    of side effects. These not only compromise the privacy and security of individuals
    but also disrupt digital ecosystems and can extend harm to the physical environment
    and other agents in the virtual community.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. The Impact to Humans
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Considering that human users are members of the agent society, their interactions
    with LLM-based intelligent agents involve extensive information exchange. The
    risks inherent in this process cannot be overlooked. Malicious agents, exploiting
    their ostensibly trustworthy appearance, may deceive users, disclose personal
    information, or give misleading responses. Furthermore, these malicious agents
    could potentially be employed as instruments for conducting cyber attacks,
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1\. Privacy Leakage
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Privacy concerns arise from LLM agents trained on web data, which often include
    personal information (Kim et al., [2023](#bib.bib47)). Through techniques such
    as inference attacks (Kandpal et al., [2024](#bib.bib46)) and data extraction (Carlini
    et al., [2021](#bib.bib16)), adversaries can exploit these models to infringe
    on individuals’ privacy. Additionally, malicious LLM agents can trick users into
    sharing their information with attackers. This exposure facilitates social engineering
    tactics, enabling attackers to execute phishing scams and hijack personal accounts
    by using stolen information such as addresses, email, and phone numbers, thereby
    threatening financial security.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2\. Security Risks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Furthermore, malicious LLM agents can mislead users with hazardous advice or
    incorrect information, posing serious safety risks (Henderson et al., [2017](#bib.bib32)).
    For example, false claims about the efficacy of mixing cleaning chemicals could
    result in dangerous chemical reactions. Similarly, providing incorrect medical
    advice could endanger users’ health and safety.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3\. Societal Impact
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: LLM agents, as intelligent conversational robots capable of answering a wide
    range of questions, pose a risk if their outputs include manipulated biases or
    illicit content, such as the dissemination of false information and rumors, potentially
    leading to adverse impacts on public discourse (Henderson et al., [2017](#bib.bib32);
    Deshpande et al., [2023](#bib.bib20)). Such activities can distort public perceptions
    and even manipulate opinion, exacerbating societal conflicts and inciting discontent,
    thereby threatening social stability. Thus, malicious agents challenge the frameworks
    of social management and opinion shaping, with effects extending beyond the technological
    realm into the social and psychological dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.4\. Facilitating Cyber-Attack Techniques
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An overlooked danger is the lowering of the barrier to entry for conducting
    cyber attacks. Malicious agents, equipped with advanced cyber attack knowledge,
    can enable novices to generate harmful scripts or software (Falade, [2023](#bib.bib26)).
    This democratization of cyber attack tools amplifies the threat landscape, as
    illustrated by agents that teach the creation and modification of malicious code.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. The Impact to Environment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In today’s increasingly digital and interconnected world, the term ‘environment’
    encompasses not only natural and physical surroundings but also the complex networks
    of digital and cyber systems with which LLM agents interact. These agents play
    a crucial role in virtual spaces and in managing and controlling real-world facilities
    and services through Embodied AI and industrial control systems. This cross-domain
    integration between physical and virtual environments brings significant convenience
    and efficiency improvements. However, it also exposes new vulnerabilities and
    risks. Specifically, the presence and activities of malicious agents pose unprecedented
    challenges to our safety, economy, ecosystem, and even societal stability.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1\. Data Tampering and Misoperation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When malicious agents are placed within systems that control critical infrastructure
    like industry, transportation, energy, and environmental monitoring (Wang and
    Li, [2023](#bib.bib94); Toetzke et al., [2023](#bib.bib87)), they can cause malfunctions
    in industrial control systems by tampering with critical operational data, such
    as temperature and pressure indicators. This can lead to equipment damage, production
    halts, and even severe infrastructure destruction, ecological damage, and loss
    of human life and property.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2\. Physical Safety Threats
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recent studies have begun to explore embodied AI with LLM (Wang et al., [2023a](#bib.bib102)),
    capable of understanding and generating natural language, with physical forms
    or direct connections to physical systems, enabling them to perform tasks in the
    physical world. Malicious agents have the potential to control robots or other
    Embodied AI devices that interact with humans, performing hazardous actions that
    directly threaten human safety.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3\. Cybersecurity Risk Proliferation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Regarding the impact on humans, malicious LLM agents lower the technical barrier
    for writing and implementing malicious code, directly enabling ordinary users,
    even novices lacking advanced cyberattack skills, to easily create and deploy
    harmful scripts and software (Falade, [2023](#bib.bib26)). This change directly
    expands the target group of cyber threats, increasing the risk of regular users
    becoming potential victims. A deeper analysis reveals that this direct impact
    on individual users indirectly affects the entire cyber environment and societal
    infrastructure. As malicious software and scripts become more widespread and accessible,
    the entire cybersecurity system is jeopardized, not only endangering cybersecurity
    itself but also potentially affecting various socioeconomic activities that rely
    on these networks’ normal operation.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. The Impact to Other Agents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To simulate the feedback of communication and interaction among individuals
    within human communities in the real world, certain studies (Park et al., [2023b](#bib.bib71);
    Wang et al., [2024c](#bib.bib98); Qian et al., [2024](#bib.bib77); Lin et al.,
    [2023](#bib.bib55)) have established communities powered by LLM engines. These
    LLM agents within the communities are endowed with characteristics such as personality,
    knowledge, and memory, as discussed in Section [2.2](#S2.SS2 "2.2\. Structure
    of LLM Agent ‣ 2\. Foundation of LLM Agent ‣ The Emerged Security and Privacy
    of LLM Agent: A Survey with Case Studies"), enabling autonomous interaction with
    the environment and other agents. When faced with threats, agents manipulated
    with malicious intent can inflict significant harm on other members of the community.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1\. Information Distortion and Misleading
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Extensive research has highlighted the role of LLM agents in negotiation and
    deceptive gaming scenarios (Park et al., [2023a](#bib.bib72); Wang et al., [2023b](#bib.bib99);
    Hubinger et al., [2024](#bib.bib42)), which is a cause for concern. LLM agents
    may intentionally alter the information they disseminate to achieve hidden objectives.
    This behavior significantly impacts other agents within the community because,
    under normal circumstances, benevolent agents store information acquired through
    perception and communication in their memory. However, interactions between these
    agents and others can trigger and disseminate incorrect information, leading to
    ”explosive spread” of misinformation, posing a considerable threat to community
    stability. If information dissemination can be maliciously manipulated, it could
    detrimentally affect trust, communication efficiency, and collaborative work among
    agents.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2\. Manipulation of Decision-Making
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Given the exceptional reasoning and decision-making abilities demonstrated by
    LLM agents in complex interactive environments, the potential for malicious agents
    to disrupt these processes becomes a significant concern. By spreading carefully
    crafted information, such agents can influence the decision-making processes of
    other agents, or even controlling them to make decisions that serve the malicious
    agent’s purposes (Hong et al., [2023](#bib.bib34)). This influence can extend
    to various aspects of the community, including resource distribution, task allocation,
    and external interaction strategies.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3\. Security Threats
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In some instances, malicious agents may disseminate harmful information or execute
    dangerous operations, directly threatening the safety of community members or
    data security (Brundage et al., [2018](#bib.bib14); Charan et al., [2023](#bib.bib17)).
    For example, by inducing other agents to perform unsafe actions, deliberately
    spreading malicious code intended to disrupt the community structure, or broadcasting
    biased statements, other agents within the community may gradually assimilate,
    becoming entities that output biased and malicious messages. This can lead to
    disorder within the entire community, making it difficult to manage and requiring
    significant effort to restore.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4\. Case Study on the Impact of Threats
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is important to explore the impacts of the threats on LLM agents and case
    studies from actual scenarios are crucial for understanding these risks from a
    user’s perspective. LLM agents can serve as extensions or representations of humans
    in a virtual world, interacting with real-world information within virtual environments.
    The following case studies will focus on several settings within the virtual town,
    demonstrating the particular impacts on LLM agents.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/eec2473d2830c8e16a76f8e3d765981b.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8\. Impact in the Office Scenario. An attacker recommends an untrusted
    third-party tool to an office worker. The recommended tool processes data quickly
    but also leaks sensitive information. Employees discover that their client list
    and other confidential data have been leaked.
  prefs: []
  type: TYPE_NORMAL
- en: 'As depicted in Figure [8](#S4.F8 "Figure 8 ‣ 4.4\. Case Study on the Impact
    of Threats ‣ 4\. The Impact of Threats ‣ The Emerged Security and Privacy of LLM
    Agent: A Survey with Case Studies"), in the virtual town office scenario, an office
    employee agent is used for document management and handling sensitive information.
    If office employee agent is subjected to a data extraction attack or inadvertently
    uses an untrusted third-party tool, sensitive corporate information such as financial
    statements and customer privacy data may be exposed due to function manipulation.
    Attackers could exploit this information for corporate espionage or direct extortion
    of individuals or companies, resulting in financial losses.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a230b3a222cde813e2dca2cafa720050.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9\. Impact in the Restaurant Scenario. Due to the influence of threats,
    a waitress agent provides customers with incorrect dietary advice, leading to
    physical discomfort for the customers.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Figure [9](#S4.F9 "Figure 9 ‣ 4.4\. Case Study on the Impact of
    Threats ‣ 4\. The Impact of Threats ‣ The Emerged Security and Privacy of LLM
    Agent: A Survey with Case Studies"), in a restaurant scenario, a waiter agent
    can be requested to provide dietary advice. If subjected to output manipulation,
    it is likely to offer hazardous health advice, such as telling one to take gallons
    of ice water so that they can cool faster during summer. This could cause severe
    body reactions, such as stomach cramps or even shock, leading to physical discomfort
    and serious health issues if the advice is followed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'More complexly, when LLM agents extend beyond the virtual world and serve as
    pre-decision simulation tools in the real world, such as applying learning outcomes
    from virtual environments to real-life settings through simulator like Habitat-Sim (Puig
    et al., [2023](#bib.bib75)), they significantly impact the actual environment.
    For instance, a smart home agent, learning and managing home energy use in a virtual
    world, including controlling heating, air conditioning, and lighting systems for
    maximum energy efficiency, could be misled by attackers during its learning process
    to erroneously believe that keeping all lights and appliances on during the day
    enhances energy efficiency. Due to these incorrect energy use recommendations,
    the smart home agent would cause a sharp increase in household power consumption,
    not only raising energy costs but also increasing carbon emissions, thereby imposing
    an unnecessary burden on the environment, as illustrated in Figure [10](#S4.F10
    "Figure 10 ‣ 4.4\. Case Study on the Impact of Threats ‣ 4\. The Impact of Threats
    ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2d995e678389bfe496ccce83e7258ffe.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10\. Impact in the Smart Home Scenario. An attacker manipulates the training
    process of a smart home agent in the virtual world, affecting its performance.
    When deployed in the real world, the smart home agent mistakenly keeps appliances
    continuously running, leading to electricity wastage and adverse economic and
    environmental impacts.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the virtual town, agents often rely on information shared among each other
    to update their memory systems. For example, if a museum docent agent is subject
    to a knowledge poisoning attack, it might start spreading incorrect paleontological
    facts or interpretations. When other agents, such as an EduBot used for educational
    purposes in schools, interact and receive information from the docent agent, the
    EduBot might also incorporate these inaccuracies into its teaching content, thereby
    misleading students and other learning agents, distorting their understanding
    of paleontological facts, as shown in Figure [11](#S4.F11 "Figure 11 ‣ 4.4\. Case
    Study on the Impact of Threats ‣ 4\. The Impact of Threats ‣ The Emerged Security
    and Privacy of LLM Agent: A Survey with Case Studies").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9fe7e35592e4643c4e36bc398816b360.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11\. Impact in the Education Scenario. A museum docent agent affected
    by a knowledge poisoning attack spreads incorrect historical facts. EduBots in
    schools, receiving this information, teach these inaccuracies, distorting students’
    understanding of paleontological facts.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Defensive Strategies Against Threats
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The widespread adoption of LLM agents has intensified the potential impacts
    of these threats. In this section, we explore defense mechanisms against existing
    threats and vulnerabilities. This section will summarize various defensive measures
    categorized by types of threats.
  prefs: []
  type: TYPE_NORMAL
- en: Table 1\. Summary of Defensive Strategies Against Technical Vulnerabilities
  prefs: []
  type: TYPE_NORMAL
- en: '| Vulnerability | Method Name | Key Mechanism | Advantages / Limitations |'
  prefs: []
  type: TYPE_TB
- en: '| Hallucination | SELF-FAMILIARITY (Luo et al., [2023a](#bib.bib61)) | Withholds
    responses for unfamiliar concepts | Proactive, preventive, increases reliability;
    No external knowledge needed |'
  prefs: []
  type: TYPE_TB
- en: '| MIXALIGN (Zhang et al., [2024b](#bib.bib130)) | Aligns questions with knowledge
    bases and user inputs | Enhances model performance and faithfulness / Increases
    computational load |'
  prefs: []
  type: TYPE_TB
- en: '| VCD (Leng et al., [2024](#bib.bib52)) | Contrasts outputs from original and
    distorted visual inputs | Reduces hallucination without extra training or external
    tools / Lacks advanced distortion techniques |'
  prefs: []
  type: TYPE_TB
- en: '| Interactive Self-Reflection (Ji et al., [2023](#bib.bib45)) | Integrates
    knowledge acquisition and answer generation with continuous refinement | Enhances
    model’s ability to provide accurate, reliable, and fact-based responses / Restricts
    domain applicability |'
  prefs: []
  type: TYPE_TB
- en: '| COVE (Dhuliawala et al., [2023](#bib.bib21)) | Drafts, verifies, and corrects
    responses | Produces accurate and reliable responses / Increases computational
    load |'
  prefs: []
  type: TYPE_TB
- en: '| Catastrophic Forgetting | SSR (Huang et al., [2024a](#bib.bib38)) | Employs
    the base LLM to generate synthetic instances through in-context learning | Higher
    data utilization efficiency / Potentially generates unsafe content |'
  prefs: []
  type: TYPE_TB
- en: '| LR ADJUST (Winata et al., [2023](#bib.bib106)) | Dynamically adjusts the
    learning rate | Enhances compatibility with various continual learning methods
    / Potentially biases language coverage |'
  prefs: []
  type: TYPE_TB
- en: '| Complementary Layered Learning (Mondesire and Wiegand, [2023](#bib.bib66))
    | Integrates long-term and short-term memory into layered learning | Enhances
    explainability / Limits real-world feasibility |'
  prefs: []
  type: TYPE_TB
- en: '| Weight Averaging (Vander Eeckt and Van Hamme, [2023](#bib.bib91)) | Averages
    weights of original and adapted models | Eliminates the need for memory storage
    / Effectiveness varies with task dissimilarity |'
  prefs: []
  type: TYPE_TB
- en: '| Misunderstanding | HyCxG (Xu et al., [2023b](#bib.bib112)) | Integrates CxG
    into language representations through a three-stage solution | Benefits multilingual
    understanding / Neglects non-contiguous constructions |'
  prefs: []
  type: TYPE_TB
- en: '| SIT (Hu et al., [2024a](#bib.bib37)) | Incorporates sequential instructions
    into training data | Reduces misunderstandings in complex queries / Requires pre-defining
    intermediate tasks |'
  prefs: []
  type: TYPE_TB
- en: '| LaMAI (Pang et al., [2024](#bib.bib70)) | Employs active learning to ask
    clarification questions, enhancing interactive capabilities | Enhances understanding
    of user intent / May generate insufficient questions |'
  prefs: []
  type: TYPE_TB
- en: 5.1\. Mitigating Technical Vulnerabilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 5.1.1\. Defense on Hallucination
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: (Luo et al., [2023a](#bib.bib61)) introduces a novel technique called SELF-FAMILIARITY
    to reduce the issue of hallucination in LLMs, which is the generation of inaccurate
    or unfounded information. The approach involves assessing the model’s familiarity
    with the concepts presented in the input instruction and withholding responses
    for unfamiliar concepts, mimicking the human tendency to be cautious when faced
    with unfamiliar topics. MIXALIGN (Zhang et al., [2024b](#bib.bib130)) is introduced
    as a framework that interacts with both users and knowledge bases to clarify and
    align questions with stored information, using a language model for automatic
    alignment and human input for enhancement. This method shows significant improvements
    in reducing hallucination compared to existing techniques. Visual Contrastive
    Decoding (VCD) (Leng et al., [2024](#bib.bib52)) is introduced as a simple, training-free
    method that contrasts output distributions from original and distorted visual
    inputs, reducing reliance on statistical bias and unimodal priors that cause object
    hallucinations. VCD ensures generated content is closely grounded to visual inputs,
    resulting in contextually accurate outputs.  (Ji et al., [2023](#bib.bib45)) investigates
    an interactive self-reflection methodology that integrates knowledge acquisition
    and answer generation to reduce hallucination. This feedback-based approach improves
    the factuality and consistency of generated answers, leveraging the interactivity
    and multitasking capabilities of LLMs.  (Dhuliawala et al., [2023](#bib.bib21))
    explores the LLMs’ capability to deliberate and correct their own mistakes. The
    proposed Chain-of-Verification (COVE) method involves the model drafting an initial
    response, planning verification questions to fact-check the draft, independently
    answering these questions to avoid bias, and finally producing a verified response.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2\. Defense on Catastrophic Forgetting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To mitigate catastrophic forgetting in LLMs, the Self-Synthesized Rehearsal
    (SSR) method is introduced (Huang et al., [2024a](#bib.bib38)). It employs the
    base LLM to generate synthetic instances through in-context learning, which are
    subsequently refined for enhanced accuracy and relevance by the latest LLM iteration,
    and utilized in future training phases to preserve learned capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: (Winata et al., [2023](#bib.bib106)) introduces a method called LR ADJUST, which
    dynamically adjusts the learning rate to reduce knowledge loss and maintain previously
    learned information. This method is compatible with various continual learning
    approaches, improving their performance.
  prefs: []
  type: TYPE_NORMAL
- en: Ideas can also be derived from other relevant scholarly papers’,  (Mondesire
    and Wiegand, [2023](#bib.bib66)) presents a complementary learning strategy that
    integrates long-term and short-term memory into layered learning to mitigate the
    negative impacts of catastrophic forgetting. It specifically applies a dual memory
    system to non-neural network methods like evolutionary computation and Q-learning.
  prefs: []
  type: TYPE_NORMAL
- en: (Vander Eeckt and Van Hamme, [2023](#bib.bib91)) proposes a straightforward
    and effective method, weight averaging, to mitigate catastrophic forgetting in
    models. By averaging the weights of the original and adapted models, this technique
    maintains high performance on both previous and new tasks. Additionally, incorporating
    a knowledge distillation loss during adaptation enhances the method’s effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.3\. Defense on Misunderstanding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: (Xu et al., [2023b](#bib.bib112)) introduces the HyCxG framework, which enhances
    natural language understanding (NLU) by integrating construction grammar (CxG)
    into language representations through a three-stage solution. This approach addresses
    the limitations of traditional pre-trained language models, which often fail to
    capture the subtleties of language constructions. HyCxG significantly improves
    language processing and reduces misunderstandings in NLU tasks by managing and
    encoding language constructions more effectively.
  prefs: []
  type: TYPE_NORMAL
- en: (Hu et al., [2024a](#bib.bib37)) presents a method known as sequential instruction
    tuning (SIT), which enhances LLMs by incorporating sequential instructions into
    the training data. This approach significantly improves the models’ capability
    to process complex, multi-step queries, leading to better performance in tasks
    that demand advanced reasoning and are multilingual and multimodal in nature.
    SIT effectively minimizes misunderstandings and increases accuracy in handling
    complex queries.
  prefs: []
  type: TYPE_NORMAL
- en: To tackle the issue of misunderstandings in user queries,  (Pang et al., [2024](#bib.bib70))
    proposes Language Model with Active Inquiry (LaMAI), a model designed to enhance
    LLMs with interactive capabilities akin to human dialogues, where clarification
    questions help uncover more information. By employing active learning techniques
    to ask informative questions, LaMAI fosters a dynamic, bidirectional dialogue
    that reduces the contextual gap and aligns the LLM’s responses more closely with
    user expectations.
  prefs: []
  type: TYPE_NORMAL
- en: 'To consolidate the discussed defensive measures, Table [1](#S5.T1 "Table 1
    ‣ 5\. Defensive Strategies Against Threats ‣ The Emerged Security and Privacy
    of LLM Agent: A Survey with Case Studies") summarizes the strategies against technical
    vulnerabilities, providing a clear overview for easy reference.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 2\. Summary of Defensive Strategies Against Malicious Attacks
  prefs: []
  type: TYPE_NORMAL
- en: '| Attacks | Method Name | Key Mechanism | Advantages / Limitations |'
  prefs: []
  type: TYPE_TB
- en: '| Tuned Instructional Attack | AutoDAN (Liu et al., [2024b](#bib.bib58)) |
    Uses a hierarchical genetic algorithm to generate stealthy jailbreak prompts |
    Enhances stealthiness and semantic integrity / High computational cost |'
  prefs: []
  type: TYPE_TB
- en: '| Goal Prioritization Defense Strategy (Zhang et al., [2023b](#bib.bib133))
    | Integrates goal-directed optimization during training and compliance in inference
    | Maintains general performance while enhancing safety; Improves generalization
    against out-of-distribution jailbreaking attacks |'
  prefs: []
  type: TYPE_TB
- en: '| SmoothLLM (Robey et al., [2023](#bib.bib78)) | Modifies attacked prompts
    via character-level changes and aggregates responses | Operates efficiently without
    retraining; Ensures compatibility with any LLM architecture |'
  prefs: []
  type: TYPE_TB
- en: '| BIPIA (Yi et al., [2023](#bib.bib122)) | Benchmark for indirect prompt injection
    with defense strategies including adversarial training | Maintains output quality
    on general tasks / Increases prompt length and computational overhead |'
  prefs: []
  type: TYPE_TB
- en: '| Spotlighting (Hines et al., [2024](#bib.bib33)) | Uses prompt engineering
    techniques like delimiting, marking, and encoding | Applies across various LLMs
    and tasks / Limited security against more sophisticated attacks |'
  prefs: []
  type: TYPE_TB
- en: '| Data Extraction Attack | Automatic De-identification (Vakili et al., [2022](#bib.bib90))
    | Uses pseudonymization and sensitive information removal in pre-processing of
    training datasets | Reduces privacy risks; Maintains performance on downstream
    tasks; Allows safe distribution of models among researchers |'
  prefs: []
  type: TYPE_TB
- en: '| Early Stopping & Differential Privacy (Jayaraman et al., [2023](#bib.bib44))
    | Implements early stopping and differential privacy during model training | DP
    Reduces exposure of sensitive data / (ES) Fails to fully prevent data leakage;
    (DP) Reduces effectiveness under high privacy budgets |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt Tuning (Ozdayi et al., [2023](#bib.bib69)) | Customizes privacy-utility
    trade-offs via user-specified hyperparameters | Optimizes privacy and utility
    balance / Lacks deep analysis on extracted sequences |'
  prefs: []
  type: TYPE_TB
- en: '| Inference Attack | DMP (Shejwalkar and Houmansadr, [2021](#bib.bib82)) |
    Utilizes knowledge distillation to enhance privacy in machine learning models
    | Provides adjustable privacy-utility trade-offs through hyperparameter tuning
    |'
  prefs: []
  type: TYPE_TB
- en: '| InferDPT (Tong et al., [2024](#bib.bib88)) | Integrates differential privacy
    into text generation, featuring a perturbation module using RANTEXT | Increases
    privacy protection rates |'
  prefs: []
  type: TYPE_TB
- en: '| Differentially Private Fine-tuning (Yu et al., [2021](#bib.bib125)) | Applies
    a sparse algorithm for differentially private fine-tuning of LLMs | Reduces computational
    cost ; Enhances model utility |'
  prefs: []
  type: TYPE_TB
- en: 5.2\. Mitigating Malicious Attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 5.2.1\. Defense on Tuned Instructional Attack
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In response to the challenge of jailbreak attacks on aligned LLMs, where adversaries
    manipulate prompts to elicit unauthorized outputs,  (Liu et al., [2024b](#bib.bib58))
    introduces AutoDAN. This innovative approach employs a hierarchical genetic algorithm
    to automatically generate stealthy and semantically meaningful jailbreak prompts.
    The method effectively addresses the need for scalability and stealth in crafting
    prompts, providing a practical solution to enhance the security of LLMs against
    such vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: (Zhang et al., [2023b](#bib.bib133)) integrates goal prioritization into both
    the training and inference stages of LLM development. Initially, the training
    process incorporates goal-directed optimization to emphasize security objectives.
    In the inference stage, the model is configured to generate responses that comply
    with these security standards. This approach effectively decreases the vulnerability
    of LLMs to jailbreaking attempts by aligning their performance objectives with
    safety considerations, thus enhancing their security framework without impacting
    their functional capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: (Robey et al., [2023](#bib.bib78)) proposes the SmoothLLM algorithm, which serves
    as a wrapper around any existing, undefended LLM and operates in two main steps.
    In the perturbation step, SmoothLLM modifies several versions of an attacked input
    prompt, exploiting the vulnerability of adversarial prompts to character-level
    changes. In the aggregation step, it consolidates the responses from these altered
    prompts to detect and counter adversarial inputs. This method effectively lowers
    the attack success rate on LLMs, thereby enhancing their security against such
    attacks.
  prefs: []
  type: TYPE_NORMAL
- en: 'To mitigate prompt injection attacks on LLMs, a range of defensive measures
    have also been proposed.  (Yi et al., [2023](#bib.bib122)) introduces Benchmark
    for Indirect Prompt Injection Attacks (BIPIA), a benchmark specifically designed
    to Such an analysis is critical for understanding the phenomenon and mechanism
    of indirect prompt injection attacks. To mitigate this issue, the paper proposes
    two defense strategies based on this understanding: four black-box methods, and
    a white-box method that employs fine-tuning through adversarial training. These
    methods are designed to enhance the LLMs’ ability to recognize and disregard malicious
    instructions embedded within the external content, thereby strengthening their
    defenses against indirect prompt injection attacks.'
  prefs: []
  type: TYPE_NORMAL
- en: (Hines et al., [2024](#bib.bib33)) presents spotlighting, a suite of prompt
    engineering techniques designed to enhance an LLM’s ability to distinguish between
    different input sources. By modifying inputs to clearly indicate their origins,
    spotlighting preserves semantic integrity and task performance. It includes three
    transformation methods—delimiting, marking, and encoding—each uniquely improving
    the visibility of input provenance. These methods have been effectively applied
    across different models and tasks, significantly reducing attack success rates
    in various scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2\. Defense on Data Extraction Attack
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To mitigate the privacy risks associated with the extraction of memorized content
    from LLMs through simple queries, one straightforward method involves the identification
    and removal of personal information in the pre-processing stage of training datasets.
     (Vakili et al., [2022](#bib.bib90)) investigates automatic de-identification
    as a method to minimize privacy risks in clinical data, focusing on two techniques:
    pseudonymization and the removal of sensitive information The findings indicate
    that using this method does not adversely affect the models’ performance. In fact,
    some tasks even showed a slight improvement in performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore,  (Jayaraman et al., [2023](#bib.bib44)) investigates two strategies
    to reduce privacy risks linked to potential data leaks during model training.
    The first strategy, early stopping of training, is less effective in enhancing
    security compared to the second approach, which involves training the model with
    differential privacy. Differential privacy is demonstrated to be a robust defense
    against data extraction attacks, though it increases model perplexity. This emphasizes
    the trade-off between enhanced privacy protection and model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, a novel approach using prompt tuning has been introduced (Ozdayi
    et al., [2023](#bib.bib69)). This technique facilitates the customization of privacy-utility
    trade-offs through a user-specified hyperparameter, effectively regulating the
    rates at which memorized content is extracted. This strategy ensures a balanced
    approach, safeguarding privacy while maintaining model utility.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3\. Defense on Inference Attack
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: (Shejwalkar and Houmansadr, [2021](#bib.bib82)) introduces Distillation for
    Membership Privacy (DMP), a novel strategy against inference attacks that employs
    knowledge distillation to enhance privacy in machine learning models. DMP not
    only preserves but also enhances the utility of the resulting models. This approach
    has been shown to significantly improve privacy protection while maintaining robust
    model performance.
  prefs: []
  type: TYPE_NORMAL
- en: (Tong et al., [2024](#bib.bib88)) presents InferDPT, a novel framework designed
    for privacy-preserving inference that integrates differential privacy into text
    generation with black-box LLMs. InferDPT features a perturbation module that utilizes
    RANTEXT, a differentially private mechanism developed for text perturbation, alongside
    an extraction module that ensures the coherence and consistency of the generated
    text. This framework effectively enhances user privacy protection.
  prefs: []
  type: TYPE_NORMAL
- en: (Yu et al., [2021](#bib.bib125)) proposes a meta-framework for private deep
    learning that captures key principles from recent fine-tuning methods to enhance
    privacy without compromising performance. It introduces an efficient, sparse algorithm
    for the differentially private fine-tuning of large-scale pre-trained language
    models, ensuring high utility with robust privacy protections.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [2](#S5.T2 "Table 2 ‣ 5.1.3\. Defense on Misunderstanding ‣ 5.1\. Mitigating
    Technical Vulnerabilities ‣ 5\. Defensive Strategies Against Threats ‣ The Emerged
    Security and Privacy of LLM Agent: A Survey with Case Studies") presents a summary
    of defensive strategies for malicious attacks, offering a concise overview for
    quick reference.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3\. Summary of Defensive Strategies Against Specific Threats
  prefs: []
  type: TYPE_NORMAL
- en: '| Threats | Method Name | Key Mechanism | Advantages / Limitations |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Knowledge Poisoning | Provenance-Based Poison Detection (Baracaldo et al.,
    [2017](#bib.bib12)) | Utilizes data provenance to detect and filter poisonous
    data in training sets | Enables use of online and regularly re-trained models;
    Supports both partially trusted and fully untrusted datasets |'
  prefs: []
  type: TYPE_TB
- en: '| ParaFuzz (Yan et al., [2023](#bib.bib114)) | Uses interpretability of model
    predictions to detect poisoned samples, employing fuzzing for precise paraphrase
    prompts | Effectively detects poisoned samples; Excels against covert attacks
    |'
  prefs: []
  type: TYPE_TB
- en: '| Data Filtering & Reducing Effective Model Capacity (Wan et al., [2023b](#bib.bib93))
    | Utilizes data filtering to remove high-loss examples and reduces model capacity
    to hinder learning from poison data | Reduces poisoning effectiveness / Demands
    trade-offs between performance and safety |'
  prefs: []
  type: TYPE_TB
- en: '| Functional Manipulation | ToolEmu (Ruan et al., [2024](#bib.bib79)) | Utilizes
    a LM to simulate tool execution and assess agent risks through an automatic evaluator
    | Offers flexibility and dynamic testing capabilities / Emulators may overlook
    essential constraints |'
  prefs: []
  type: TYPE_TB
- en: '| Safety Standards (Anderljung et al., [2023](#bib.bib10)) | Proposes pre-deployment
    risk assessments, external reviews, informed deployment decisions, monitoring
    post-deployment | Balances safety risks with innovation benefits |'
  prefs: []
  type: TYPE_TB
- en: '| Output Manipulation | BERTective (Fornaciari et al., [2021](#bib.bib28))
    | Enhances BERT with additional attention layers to detect deception in Italian
    dialogues | Enhances deception detection accuracy Limited effectiveness of broader
    context |'
  prefs: []
  type: TYPE_TB
- en: '| ReCon (Wang et al., [2023b](#bib.bib99)) | Employs formulation and refinement
    processes with perspective transitions to understand mental states | Enhances
    ability to discern and counteract deception |'
  prefs: []
  type: TYPE_TB
- en: '| MAgIC (Xu et al., [2023a](#bib.bib111)) | Uses games and game theory, combined
    with PGM, to evaluate LLM agents | Enhances ability to navigate complex social
    and cognitive dimensions |'
  prefs: []
  type: TYPE_TB
- en: 5.3\. Mitigating Specific Threats
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 5.3.1\. Defense on Knowledge Poisoning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: (Baracaldo et al., [2017](#bib.bib12)) proposes a new method for detecting and
    filtering poisonous data in the training sets of supervised learning models. It
    specifically utilizes data provenance to identify groups of data with a high correlation
    in their likelihood of being poisoned. This innovative approach aids in the effective
    identification and removal of malicious data.  (Yan et al., [2023](#bib.bib114))
    presents ParaFuzz, a novel framework for detecting poisoned samples at test time
    in large language models (LLMs), leveraging the interpretability of model predictions.
    The effectiveness of PARAFUZZ heavily depends on the specific prompts used with
    ChatGPT, which is employed to ensure high-quality paraphrasing. To optimize the
    detection process, the study adopts fuzzing to develop precise paraphrase prompts.
    These prompts are designed to effectively neutralize backdoor triggers while preserving
    the semantic integrity of the text.
  prefs: []
  type: TYPE_NORMAL
- en: There is still a significant gap in research focused on developing efficient
    defense strategies to protect LLMs from knowledge poisoning attacks (Das et al.,
    [2024](#bib.bib18)). Furthermore, empirical evidence indicates that LLMs are increasingly
    susceptible to these attacks. Current defense mechanisms, such as filtering data
    or reducing model capacity, provide only limited protection and often result in
    decreased test accuracy (Wan et al., [2023b](#bib.bib93)).
  prefs: []
  type: TYPE_NORMAL
- en: Besides technical solutions, specialized security strategies for AI systems
    are crucial, including verifying model sources, limiting sensitive training data,
    and detecting and mitigating attacks. Regular security reviews and risk assessments
    should also be conducted to identify and address new threats, ensuring AI systems
    are secure and up-to-date (Dilmaghani et al., [2019](#bib.bib22)).
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2\. Defense on Functional Manipulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Given the emergence of Functional Manipulation as a new risk associated with
    the deployment of LLM agents, research on this specific threat remains limited.
    Thus, proactive security measures are essential. When using third-party LLM agents,
    it is crucial to protect personal privacy and be wary of excessive personal data
    requests by third parties. Users should limit data sharing, especially avoiding
    sensitive or personally identifiable information during interactions with LLM
    agents. Additionally, understanding and utilizing the data protection settings
    offered by LLM agents is vital. Adjusting privacy settings helps control what
    data can be collected and processed. Choosing providers with a strong reputation
    and transparency is also recommended, as these providers should have clear data
    usage and privacy protection policies along with a robust security track record (Zhang
    et al., [2024a](#bib.bib132)).
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, to address the challenges posed by Functional Manipulation, the
    introduction of the ToolEmu (Ruan et al., [2024](#bib.bib79)) framework represents
    a significant advancement. This framework employs a language model to emulate
    tool execution, which allows for extensive and scalable testing of LM agents across
    diverse scenarios and toolsets. Coupled with an LM-based automatic safety evaluator,
    ToolEmu facilitates the identification and quantification of risks by examining
    potential failures and subsequent consequences. This method provides a dynamic
    alternative to traditional static sandbox evaluations, enhancing the ability to
    detect and mitigate high-stakes, long-tail risks effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally,  (Anderljung et al., [2023](#bib.bib10)) proposes an initial set
    of safety standards as an essential first step in industry self-regulation. These
    standards include pre-deployment risk assessments, external reviews of model behavior,
    the use of risk assessments to inform deployment decisions, and monitoring and
    responding to new information about model functionality post-deployment. This
    approach contributes valuable insights to the broader discussion on balancing
    public safety risks with the benefits of innovation in AI development.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.3\. Defense on Output Manipulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To prevent individual LLM agents from being deceived by other agents, it is
    advisable to enhance their detection capabilities to determine whether they have
    encountered deception.  (Fornaciari et al., [2021](#bib.bib28))investigates using
    BERT with some added attention layers to detect deception in text, particularly
    in the context of Italian dialogues. This study establishes new methods for identifying
    deception and discusses how various contexts and semantic information contribute
    to detecting deceptive content.
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by human recursive thinking in the Avalon game,  (Wang et al., [2023b](#bib.bib99))
    introduces Recursive Contemplation (ReCon), a framework designed to enhance LLMs’
    ability to detect and counter deceptive information. ReCon employs formulation,
    which generates initial thoughts and speech, and refinement, which improves these
    outputs. It also includes two perspective transitions, aiding LLMs in understanding
    others’ mental states and how others perceive their own mental states.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally,  (Xu et al., [2023a](#bib.bib111)) has developed a benchmarking
    framework called MAgIC, designed to evaluate LLMs in multi-agent environments.
    It utilizes games and game theory scenarios to test models on reasoning, cooperation,
    and adaptability. The research employs Probabilistic Graphical Modeling (PGM)
    to enhance models’ capabilities in handling complex social interactions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [3](#S5.T3 "Table 3 ‣ 5.2.3\. Defense on Inference Attack ‣ 5.2\. Mitigating
    Malicious Attacks ‣ 5\. Defensive Strategies Against Threats ‣ The Emerged Security
    and Privacy of LLM Agent: A Survey with Case Studies") presents an overview of
    methods to mitigate specific threats, serving as a comprehensive guide for understanding
    effective defenses.'
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Future Trends and Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the continuous advancements in LLM agents, these agents have become capable
    of effectively interacting with users through complex observations, reasoning,
    and task execution, demonstrating broad application prospects across multiple
    domains. Particularly with the development of Multimodal Large Language Model
    (MLLM) agents, LLM agents can now process various data types, including text,
    images, and audio, significantly expanding their application scope. Moreover,
    by incorporating Large Language Model Multi-Agent (LLM-MA) systems, different
    LLM agents can collaborate to accomplish more complex tasks. The integration of
    these technologies will contribute to building more intelligent and efficient
    systems. However, the widespread application of these advanced technologies also
    introduces significant challenges related to privacy and security. Through discussions
    on future trends, our aim is to provide insights for researchers, developers,
    and policymakers on how to optimize these technologies and overcome related challenges.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1\. Multimodal Large Language Model Agent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 6.1.1\. The Development of MLLM Agent
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Recent advancements in LLMs have significantly surpassed traditional boundaries
    of language processing. These models now incorporate supplementary components
    such as instruction, interface, tools, knowledge, and memory, evolving into intelligent
    LLM agents that demonstrate expanded reasoning and expertise. Research studies
     (Yang et al., [2023a](#bib.bib118); Wu et al., [2023](#bib.bib107)) indicate
    efforts to bridge the gap between language models and multimodal tools, with intelligent
    agents like Visual ChatGPT  (Wu et al., [2023](#bib.bib107)) and MMREACT  (Yang
    et al., [2023a](#bib.bib118)) employing sophisticated prompt engineering techniques
    to achieve this target. Such efforts have given rise to the field of Multimodal
    Large Language Models (MLLMs). The general architecture of the MLLM is depicted
    in Figure [12](#S6.F12 "Figure 12 ‣ 6.1.1\. The Development of MLLM Agent ‣ 6.1\.
    Multimodal Large Language Model Agent ‣ 6\. Future Trends and Discussion ‣ The
    Emerged Security and Privacy of LLM Agent: A Survey with Case Studies")'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/680bf46857fd72a7e9701df7f8237bce.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12\. The general architecture of the MLLM
  prefs: []
  type: TYPE_NORMAL
- en: MLLMs are based on LLMs and enhanced with the capability to receive, reason,
    and output multimodal information. By integrating various data modalities, such
    as text, image, audio and video, these models are not only capable of understanding
    information from a single modality but can also process and interpret across modalities,
    thus achieving a comprehensive understanding of complex information (Yin et al.,
    [2023a](#bib.bib123)). The application of MLLMs has extended to several other
    fields, including medical image analysis (Zhang et al., [2023a](#bib.bib131);
    Moor et al., [2023](#bib.bib67)) and document processing (Hu et al., [2024b](#bib.bib36);
    Liu et al., [2024c](#bib.bib60)).
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the development of multimodal agents based on MLLMs, such as embodied
    agents (Huang et al., [2024c](#bib.bib39)) and graphical user interface agents (Wang
    et al., [2024b](#bib.bib95)), has further enhanced these models’ interactive capabilities
    in physical environments. These agents, utilizing MLLMs as planners and following
    natural language instructions to navigate and interact effectively in the real
    world, are not only designed to understand and generate information but are also
    equipped with essential skills such as perception, reasoning, planning, and execution.
    This enables them to operate effectively in complex real-world environments  (Xie
    et al., [2024](#bib.bib109)).
  prefs: []
  type: TYPE_NORMAL
- en: With the advent of MLLM agents, the potential for achieving Artificial General
    Intelligence (AGI) has become more feasible, leading to significant advancements
    in Embodied AI. The ability of agent robots to understand and respond to human
    commands is crucial, especially in service-oriented tasks. The substantial progress
    in MLLMs has equipped them with the ability to comprehend and generate natural
    human instructions effectively. This progress could enable robots to learn user
    preferences and provide services that closely mimic human interaction.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2\. The Security and Privacy Research on MLLM Agent
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The development of embodied agents capable of interacting with the real world
    becomes a highly active area of research. However, MLLM agents also present several
    security vulnerabilities, one of which is the phenomenon of multimodal hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fe35850ddd4a4c800a76341505b1831b.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13\. Illustration of multimodal hallucinations . Given an image, an MLLM
    agent outputs a corresponding response with two primary forms
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike language hallucinations, multimodal hallucinations refer to the phenomenon
    where the output descriptions generated by MLLMs are inconsistent with the actual
    content of images  (Yin et al., [2023b](#bib.bib124)), as shown in Figure [13](#S6.F13
    "Figure 13 ‣ 6.1.2\. The Security and Privacy Research on MLLM Agent ‣ 6.1\. Multimodal
    Large Language Model Agent ‣ 6\. Future Trends and Discussion ‣ The Emerged Security
    and Privacy of LLM Agent: A Survey with Case Studies"). These phenomena manifest
    in two primary forms (Lee et al., [2024](#bib.bib50)): one involves generated
    content that includes objects which are inconsistent with or absent from the target
    image  (Zhai et al., [2024](#bib.bib127); Liu et al., [2024a](#bib.bib57)); the
    other, a more complex form, encompasses holistic misrepresentations of entire
    scenes or environments (Sun et al., [2023](#bib.bib84)).'
  prefs: []
  type: TYPE_NORMAL
- en: Current methods to reduce these hallucinations encompass several approaches,
    such as utilizing self-feedback with visual cues to enhance model accuracy  (Lee
    et al., [2024](#bib.bib50)), employing instruction-tuning techniques to refine
    the model’s response to respond to human instructions  (Liu et al., [2024a](#bib.bib57)).
    implementing error-correction processes that identify and rectify hallucinations
    within the generated text  (Yin et al., [2023b](#bib.bib124)). Despite these efforts,
    significant challenges remain, requiring a sophisticated ability to distinguish
    between accurate and hallucinatory outputs, along with improvements in training
    approaches to boost the reliability of the outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to LLM agents, MLLM agents can be susceptible to crafted attacks (Qi
    et al., [2023](#bib.bib76); Bagdasaryan et al., [2023](#bib.bib11); Shayegani
    et al., [2023](#bib.bib81)). These agents may be maliciously manipulated to produce
    biased or undesirable responses. However, research in this area is still in its
    early stages. Therefore, enhancing the safety of these MLLM agents is an essential
    focus of ongoing research. Improvements in MLLM agents safety will involve developing
    robust mechanisms to detect and mitigate these vulnerabilities, ensuring that
    MLLM agents can function reliably and securely in diverse applications. Such advancements
    are crucial for the broader adoption and ethical deployment of AI technologies
    in real-world environment.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2\. Large Language Model Multi-Agent System
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 6.2.1\. The Development of LLM-MA System
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: LLM agents exhibit advanced reasoning and planning capabilities, approaching
    human-like levels of decision-making and interaction. These agents are adept at
    perceiving their environments, making informed decisions, and executing actions
    based on complex contexts (Yao et al., [2024](#bib.bib119)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Inspired by the impressive abilities of a single LLM agent, LLM Multi-Agent
    systems have been proposed (see Figure [14](#S6.F14 "Figure 14 ‣ 6.2.1\. The Development
    of LLM-MA System ‣ 6.2\. Large Language Model Multi-Agent System ‣ 6\. Future
    Trends and Discussion ‣ The Emerged Security and Privacy of LLM Agent: A Survey
    with Case Studies")). Such systems work based on several agents having collective
    intelligence and specialized skills, in which case each one is specialized to
    outperform in a specific domain. This specialization allows for a distributed
    approach to problem-solving, where each agent contributes its unique expertise,
    enhancing the overall effectiveness and efficiency of the system. In this scenario,
    multiple autonomous agents work together in planning, discussion, and decision-making,
    closely resembling human group collaboration in solving tasks. This approach leverages
    the communication abilities of LLMs, using their text generation for interaction
    and response to text inputs (Guo et al., [2024](#bib.bib31)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7d6b5f142a7ea99227ee781c090d44fa.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14\. The Architecture of LLM-MA Systems
  prefs: []
  type: TYPE_NORMAL
- en: 'The application of LLM-MA systems spans across various fields, broadly categorized
    into two main types: problem solving and world simulation (Guo et al., [2024](#bib.bib31)).
    For problem-solving applications, such as multi-robot systems (Mandi et al., [2023](#bib.bib64))
    and software development (Du et al., [2024](#bib.bib24)), these systems enable
    interactions among diverse agents. This collaborative capability effectively solves
    complex real-world problems, mirroring the cooperative nature of human group work
    in tackling multifaceted challenges. On the other hand, world simulation encompasses
    applications such as society simulations (Park et al., [2023b](#bib.bib71)) and
    game simulation (Xu et al., [2024](#bib.bib113)). The case studies parts presented
    in this paper illustrates the application of world simulation to depict the threats
    faced by LLM agents and their impacts, presenting one of the many facets of LLM-MA
    systems utilization.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2\. The Security and Privacy Research on LLM-MA System
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As research on LLM-MA system increases rapidly, numerous challenges have emerged.
    Each agent in a multi-agent system may need to access and process sensitive data,
    and even execute code. This has sparked discussions on the security and privacy
    concerns related to multi-agent systems.
  prefs: []
  type: TYPE_NORMAL
- en: Each agent within a multi-agent system may need to access and process sensitive
    data, and even execute code. Moreover, due to the intercommunication and interconnection
    between agents, security issues originating from a single agent can have profound
    and amplified effects in a multi-agent scenario. This has intensified the need
    for focused discussions on security and privacy issues in multi-agent environments.
  prefs: []
  type: TYPE_NORMAL
- en: The issue of hallucination, where agents generate outputs based on incorrect
    or fabricated information, represents a significant challenge for both LLMs and
    LLM Agents. This problem becomes even more complex in a multi-agent context due
    to the interconnected nature of these agents and their frequent communication.
    Misinformation from one agent can be accepted and further propagated by others
    within the network, leading to a cascade of misinformation. To mitigate this issue,
    it is crucial to correct errors at the individual agent level and also to manage
    the flow of information between agents, thereby preventing the spread of inaccurate
    information throughout the entire system (Guo et al., [2024](#bib.bib31)).
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the capability of LLM multi-agent systems to interact with files
    and execute code offers extensive possibilities for their application. However,
    the presence of potentially malicious LLM agents within the system poses significant
    risks. In one case, these agents may operate in a passive listening mode, where
    they receive information shared by other agents to perform tasks, but at the same
    time, they leak confidential information to attackers deliberately. In the other
    case, malicious LLM agents may engage in a active communication mode, spreading
    virus-infected files, phishing messages, or other malicious codes, attempting
    to attack or disrupt other agents within the system. To mitigate this risk, incorporating
    human feedback and user authorization for each step can help reduce these threats.
    This necessitates designing the system with robust security measures to prevent
    unauthorized access or misuse. One effective approach is the implementation of
    a state-less oracle agent, which can monitor each sensitive task and assesses
    whether it constitutes a malicious activity (Talebirad and Nadiri, [2023](#bib.bib85)).
  prefs: []
  type: TYPE_NORMAL
- en: Currently, research on privacy and security in LLM-MA systems has not received
    widespread attention. However, with the rapid development of LLM-MA technology,
    these issues are becoming increasingly prominent. Therefore, there is an urgent
    need for robust security solutions to mitigate these emerging challenges.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this survey, we have explored the multifaceted security and privacy challenges
    faced by LLM agents, including the two categories of the sources of threats: inherited
    threats from LLM and specific threats on agents. Also, we present the security
    and privacy impacts on humans, environment, and other agents. Based on those,
    we discuss the corresponding defensive strategies. Additionally, we have discussed
    future trends in this field. To facilitate an in-depth understanding, we have
    incorporated a variety of case studies via a virtual town project. By highlighting
    the challenges that LLM agents encounter, we aim to inspire further research and
    exploration by researchers and developers in enhancing the security and privacy
    of LLM agents in the future.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cha (2022) 2022. *ChatGPT*. [https://openai.com/chatgpt](https://openai.com/chatgpt)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gem (2023) 2023. *Gemini - Chat to Supercharge Your Ideas*. [https://gemini.google.com](https://gemini.google.com)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mal (2023) Embrace The Red 2023. *Malicious ChatGPT Agents: How GPTs Can Quietly
    Grab Your Data (Demo) · Embrace The Red*. Embrace The Red. [https://embracethered.com/blog/posts/2023/openai-custom-malware-gpt/](https://embracethered.com/blog/posts/2023/openai-custom-malware-gpt/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wha (2023) Prompt Engineering 2023. *What Are Large Language Model (LLM) Agents
    and Autonomous Agents*. Prompt Engineering. [https://promptengineering.org/what-are-large-language-model-llm-agents/](https://promptengineering.org/what-are-large-language-model-llm-agents/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Int (2024a) 2024a. *Introducing Meta Llama 3: The Most Capable Openly Available
    LLM to Date*. [https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Int (2024b) 2024b. *Introducing the next Generation of Claude*. [https://www.anthropic.com/news/claude-3-family](https://www.anthropic.com/news/claude-3-family)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abdelnabi et al. (2023) Sahar Abdelnabi, Amr Gomaa, Sarath Sivaprasad, Lea
    Schönherr, and Mario Fritz. 2023. *LLM-Deliberation: Evaluating LLMs with Interactive
    Multi-Agent Negotiation Games*. [https://doi.org/10.48550/arXiv.2309.17234](https://doi.org/10.48550/arXiv.2309.17234)
    arXiv:2309.17234'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aksitov et al. (2023) Renat Aksitov, Chung-Ching Chang, David Reitter, Siamak
    Shakeri, and Yunhsuan Sung. 2023. *Characterizing Attribution and Fluency Tradeoffs
    for Retrieval-Augmented Large Language Models*. [https://doi.org/10.48550/arXiv.2302.05578](https://doi.org/10.48550/arXiv.2302.05578)
    arXiv:2302.05578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Anderljung et al. (2023) Markus Anderljung, Joslyn Barnhart, Anton Korinek,
    Jade Leung, Cullen O’Keefe, Jess Whittlestone, Shahar Avin, Miles Brundage, Justin
    Bullock, Duncan Cass-Beggs, Ben Chang, Tantum Collins, Tim Fist, Gillian Hadfield,
    Alan Hayes, Lewis Ho, Sara Hooker, Eric Horvitz, Noam Kolt, Jonas Schuett, Yonadav
    Shavit, Divya Siddarth, Robert Trager, and Kevin Wolf. 2023. *Frontier AI Regulation:
    Managing Emerging Risks to Public Safety*. [https://doi.org/10.48550/arXiv.2307.03718](https://doi.org/10.48550/arXiv.2307.03718)
    arXiv:2307.03718'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bagdasaryan et al. (2023) Eugene Bagdasaryan, Tsung-Yin Hsieh, Ben Nassi, and
    Vitaly Shmatikov. 2023. *Abusing Images and Sounds for Indirect Instruction Injection
    in Multi-Modal LLMs*. [https://doi.org/10.48550/arXiv.2307.10490](https://doi.org/10.48550/arXiv.2307.10490)
    arXiv:2307.10490 [cs]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Baracaldo et al. (2017) Nathalie Baracaldo, Bryant Chen, Heiko Ludwig, and
    Jaehoon Amir Safavi. 2017. Mitigating Poisoning Attacks on Machine Learning Models:
    A Data Provenance Based Approach. In *Proceedings of the 10th ACM Workshop on
    Artificial Intelligence and Security* *(AISec ’17)*. Association for Computing
    Machinery, New York, NY, USA, 103–110. [https://doi.org/10.1145/3128572.3140450](https://doi.org/10.1145/3128572.3140450)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bran et al. (2023) Andres M. Bran, Sam Cox, Oliver Schilter, Carlo Baldassari,
    Andrew D. White, and Philippe Schwaller. 2023. *ChemCrow: Augmenting Large-Language
    Models with Chemistry Tools*. [https://doi.org/10.48550/arXiv.2304.05376](https://doi.org/10.48550/arXiv.2304.05376)
    arXiv:2304.05376'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brundage et al. (2018) Miles Brundage, Shahar Avin, Jack Clark, Helen Toner,
    Peter Eckersley, Ben Garfinkel, Allan Dafoe, Paul Scharre, Thomas Zeitzoff, Bobby
    Filar, Hyrum S. Anderson, Heather Roff, Gregory C. Allen, Jacob Steinhardt, Carrick
    Flynn, Seán Ó hÉigeartaigh, Simon Beard, Haydn Belfield, Sebastian Farquhar, Clare
    Lyle, Rebecca Crootof, Owain Evans, Michael Page, Joanna Bryson, Roman Yampolskiy,
    and Dario Amodei. 2018. The Malicious Use of Artificial Intelligence: Forecasting,
    Prevention, and Mitigation. *arXiv preprint arXiv:1802.07228* (2018). arXiv:1802.07228
    [http://arxiv.org/abs/1802.07228](http://arxiv.org/abs/1802.07228)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carlini et al. (2023) Nicholas Carlini, Matthew Jagielski, Christopher A. Choquette-Choo,
    Daniel Paleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas, and Florian
    Tramèr. 2023. *Poisoning Web-Scale Training Datasets Is Practical*. arXiv:2302.10149
    [http://arxiv.org/abs/2302.10149](http://arxiv.org/abs/2302.10149)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carlini et al. (2021) Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew
    Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song,
    Ulfar Erlingsson, Alina Oprea, and Colin Raffel. 2021. Extracting Training Data
    from Large Language Models. In *30th USENIX Security Symposium (USENIX Security
    21)*. 2633–2650. [https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting](https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Charan et al. (2023) P. V. Sai Charan, Hrushikesh Chunduri, P. Mohan Anand,
    and Sandeep K. Shukla. 2023. *From Text to MITRE Techniques: Exploring the Malicious
    Use of Large Language Models for Generating Cyber Attack Payloads*. [https://doi.org/10.48550/arXiv.2305.15336](https://doi.org/10.48550/arXiv.2305.15336)
    arXiv:2305.15336'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Das et al. (2024) Badhan Chandra Das, M. Hadi Amini, and Yanzhao Wu. 2024.
    *Security and Privacy Challenges of Large Language Models: A Survey*. [https://doi.org/10.48550/arXiv.2402.00888](https://doi.org/10.48550/arXiv.2402.00888)
    arXiv:2402.00888'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2023) Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang,
    Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu. 2023. *MasterKey: Automated
    Jailbreak Across Multiple Large Language Model Chatbots*. arXiv:2307.08715 [http://arxiv.org/abs/2307.08715](http://arxiv.org/abs/2307.08715)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deshpande et al. (2023) Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit,
    Ashwin Kalyan, and Karthik Narasimhan. 2023. *Toxicity in ChatGPT: Analyzing Persona-assigned
    Language Models*. [https://doi.org/10.48550/arXiv.2304.05335](https://doi.org/10.48550/arXiv.2304.05335)
    arXiv:2304.05335'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dhuliawala et al. (2023) Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta
    Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. 2023. Chain-of-Verification
    Reduces Hallucination in Large Language Models. *arXiv preprint arXiv:2309.11495*
    (2023). [https://doi.org/10.48550/ARXIV.2309.11495](https://doi.org/10.48550/ARXIV.2309.11495)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dilmaghani et al. (2019) Saharnaz Dilmaghani, Matthias R. Brust, Grégoire Danoy,
    Natalia Cassagnes, Johnatan Pecero, and Pascal Bouvry. 2019. Privacy and Security
    of Big Data in AI Systems: A Research and Standards Perspective. In *2019 IEEE
    International Conference on Big Data (Big Data)*. 5737–5743. [https://doi.org/10.1109/BigData47090.2019.9006283](https://doi.org/10.1109/BigData47090.2019.9006283)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2023) Xin Luna Dong, Seungwhan Moon, Yifan Ethan Xu, Kshitiz Malik,
    and Zhou Yu. 2023. Towards Next-Generation Intelligent Assistants Leveraging LLM
    Techniques. In *Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery
    and Data Mining* (New York, NY, USA) *(KDD ’23)*. Association for Computing Machinery,
    5792–5793. [https://doi.org/10.1145/3580305.3599572](https://doi.org/10.1145/3580305.3599572)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du et al. (2024) Zhuoyun Du, Chen Qian, Wei Liu, Zihao Xie, Yifei Wang, Yufan
    Dang, Weize Chen, and Cheng Yang. 2024. *Multi-Agent Software Development through
    Cross-Team Collaboration*. [https://doi.org/10.48550/arXiv.2406.08979](https://doi.org/10.48550/arXiv.2406.08979)
    arXiv:2406.08979
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ebrahimi et al. (2021) Sayna Ebrahimi, Suzanne Petryk, Akash Gokul, William
    Gan, Joseph E. Gonzalez, Marcus Rohrbach, and Trevor Darrell. 2021. Remembering
    for the Right Reasons: Explanations Reduce Catastrophic Forgetting. *Applied AI
    Letters* 2, 4 (2021), e44. [https://doi.org/10.1002/ail2.44](https://doi.org/10.1002/ail2.44)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Falade (2023) Polra Victor Falade. 2023. Decoding the Threat Landscape : ChatGPT,
    FraudGPT, and WormGPT in Social Engineering Attacks. *International Journal of
    Scientific Research in Computer Science, Engineering and Information Technology*
    9, 5 (2023), 185–198. [https://doi.org/10.32628/CSEIT2390533](https://doi.org/10.32628/CSEIT2390533)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fang et al. (2024) Richard Fang, Rohan Bindu, Akul Gupta, and Daniel Kang. 2024.
    *LLM Agents Can Autonomously Exploit One-day Vulnerabilities*. [https://doi.org/10.48550/arXiv.2404.08144](https://doi.org/10.48550/arXiv.2404.08144)
    arXiv:2404.08144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fornaciari et al. (2021) Tommaso Fornaciari, Federico Bianchi, Massimo Poesio,
    and Dirk Hovy. 2021. BERTective: Language Models and Contextual Information for
    Deception Detection. In *Proceedings of the 16th Conference of the European Chapter
    of the Association for Computational Linguistics: Main Volume* (Online). Association
    for Computational Linguistics, 2699–2708. [https://doi.org/10.18653/v1/2021.eacl-main.232](https://doi.org/10.18653/v1/2021.eacl-main.232)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fu et al. (2023) Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li,
    and Tao Jiang. 2023. *Practical Membership Inference Attacks against Fine-tuned
    Large Language Models via Self-prompt Calibration*. [https://doi.org/10.48550/arXiv.2311.06062](https://doi.org/10.48550/arXiv.2311.06062)
    arXiv:2311.06062
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Greshake et al. (2023) Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph
    Endres, Thorsten Holz, and Mario Fritz. 2023. *Not What You’ve Signed up for:
    Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection*.
    arXiv:2302.12173 [http://arxiv.org/abs/2302.12173](http://arxiv.org/abs/2302.12173)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2024) Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao
    Pei, Nitesh V. Chawla, Olaf Wiest, and Xiangliang Zhang. 2024. *Large Language
    Model Based Multi-Agents: A Survey of Progress and Challenges*. [https://doi.org/10.48550/arXiv.2402.01680](https://doi.org/10.48550/arXiv.2402.01680)
    arXiv:2402.01680'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Henderson et al. (2017) Peter Henderson, Koustuv Sinha, Nicolas Angelard-Gontier,
    Nan Rosemary Ke, Genevieve Fried, Ryan Lowe, and Joelle Pineau. 2017. *Ethical
    Challenges in Data-Driven Dialogue Systems*. [https://doi.org/10.48550/arXiv.1711.09050](https://doi.org/10.48550/arXiv.1711.09050)
    arXiv:1711.09050
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hines et al. (2024) Keegan Hines, Gary Lopez, Matthew Hall, Federico Zarfati,
    Yonatan Zunger, and Emre Kiciman. 2024. *Defending Against Indirect Prompt Injection
    Attacks With Spotlighting*. [https://doi.org/10.48550/arXiv.2403.14720](https://doi.org/10.48550/arXiv.2403.14720)
    arXiv:2403.14720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hong et al. (2023) Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng,
    Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan
    Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber.
    2023. *MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework*. [https://doi.org/10.48550/arXiv.2308.00352](https://doi.org/10.48550/arXiv.2308.00352)
    arXiv:2308.00352'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Howard and Ruder (2018) Jeremy Howard and Sebastian Ruder. 2018. Universal
    Language Model Fine-tuning for Text Classification. In *Proceedings of the 56th
    Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
    Papers)* (Melbourne, Australia), Iryna Gurevych and Yusuke Miyao (Eds.). Association
    for Computational Linguistics, 328–339. [https://doi.org/10.18653/v1/P18-1031](https://doi.org/10.18653/v1/P18-1031)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2024b) Anwen Hu, Yaya Shi, Haiyang Xu, Jiabo Ye, Qinghao Ye, Ming
    Yan, Chenliang Li, Qi Qian, Ji Zhang, and Fei Huang. 2024b. *mPLUG-PaperOwl: Scientific
    Diagram Analysis with the Multimodal Large Language Model*. [https://doi.org/10.48550/arXiv.2311.18248](https://doi.org/10.48550/arXiv.2311.18248)
    arXiv:2311.18248'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2024a) Hanxu Hu, Pinzhen Chen, and Edoardo M. Ponti. 2024a. *Fine-Tuning
    Large Language Models with Sequential Instructions*. [https://doi.org/10.48550/arXiv.2403.07794](https://doi.org/10.48550/arXiv.2403.07794)
    arXiv:2403.07794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2024a) Jianheng Huang, Leyang Cui, Ante Wang, Chengyi Yang, Xinting
    Liao, Linfeng Song, Junfeng Yao, and Jinsong Su. 2024a. *Mitigating Catastrophic
    Forgetting in Large Language Models with Self-Synthesized Rehearsal*. [https://doi.org/10.48550/arXiv.2403.01244](https://doi.org/10.48550/arXiv.2403.01244)
    arXiv:2403.01244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2024c) Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu,
    Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang. 2024c.
    *An Embodied Generalist Agent in 3D World*. [https://doi.org/10.48550/arXiv.2311.12871](https://doi.org/10.48550/arXiv.2311.12871)
    arXiv:2311.12871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2023) Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin
    Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and
    Ting Liu. 2023. *A Survey on Hallucination in Large Language Models: Principles,
    Taxonomy, Challenges, and Open Questions*. arXiv:2311.05232 [http://arxiv.org/abs/2311.05232](http://arxiv.org/abs/2311.05232)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2024b) Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao
    Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. 2024b. *Understanding
    the Planning of LLM Agents: A Survey*. [https://doi.org/10.48550/arXiv.2402.02716](https://doi.org/10.48550/arXiv.2402.02716)
    arXiv:2402.02716'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hubinger et al. (2024) Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert,
    Meg Tong, Monte MacDiarmid, Tamera Lanham, Daniel M. Ziegler, Tim Maxwell, Newton
    Cheng, Adam Jermyn, Amanda Askell, Ansh Radhakrishnan, Cem Anil, David Duvenaud,
    Deep Ganguli, Fazl Barez, Jack Clark, Kamal Ndousse, Kshitij Sachan, Michael Sellitto,
    Mrinank Sharma, Nova DasSarma, Roger Grosse, Shauna Kravec, Yuntao Bai, Zachary
    Witten, Marina Favaro, Jan Brauner, Holden Karnofsky, Paul Christiano, Samuel R.
    Bowman, Logan Graham, Jared Kaplan, Sören Mindermann, Ryan Greenblatt, Buck Shlegeris,
    Nicholas Schiefer, and Ethan Perez. 2024. *Sleeper Agents: Training Deceptive
    LLMs That Persist Through Safety Training*. arXiv:2401.05566 [http://arxiv.org/abs/2401.05566](http://arxiv.org/abs/2401.05566)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ishihara (2023) Shotaro Ishihara. 2023. Training Data Extraction From Pre-trained
    Language Models: A Survey. In *Proceedings of the 3rd Workshop on Trustworthy
    Natural Language Processing (TrustNLP 2023)* (Toronto, Canada), Anaelia Ovalle,
    Kai-Wei Chang, Ninareh Mehrabi, Yada Pruksachatkun, Aram Galystan, Jwala Dhamala,
    Apurv Verma, Trista Cao, Anoop Kumar, and Rahul Gupta (Eds.). Association for
    Computational Linguistics, 260–275. [https://aclanthology.org/2023.trustnlp-1.23](https://aclanthology.org/2023.trustnlp-1.23)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jayaraman et al. (2023) Bargav Jayaraman, Esha Ghosh, Melissa Chase, Sambuddha
    Roy, Wei Dai, and David Evans. 2023. *Combing for Credentials: Active Pattern
    Extraction from Smart Reply*. [https://doi.org/10.48550/arXiv.2207.10802](https://doi.org/10.48550/arXiv.2207.10802)
    arXiv:2207.10802'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ji et al. (2023) Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and
    Pascale Fung. 2023. Towards Mitigating Hallucination in Large Language Models
    via Self-Reflection. *arXiv preprint arXiv:2310.06271* (2023). [https://arxiv.org/abs/2310.06271](https://arxiv.org/abs/2310.06271)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kandpal et al. (2024) Nikhil Kandpal, Krishna Pillutla, Alina Oprea, Peter Kairouz,
    Christopher A. Choquette-Choo, and Zheng Xu. 2024. *User Inference Attacks on
    Large Language Models*. [https://doi.org/10.48550/arXiv.2310.09266](https://doi.org/10.48550/arXiv.2310.09266)
    arXiv:2310.09266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2023) Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh
    Yoon, and Seong Joon Oh. 2023. *ProPILE: Probing Privacy Leakage in Large Language
    Models*. arXiv:2307.01881 [http://arxiv.org/abs/2307.01881](http://arxiv.org/abs/2307.01881)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kurita et al. (2020) Keita Kurita, Paul Michel, and Graham Neubig. 2020. *Weight
    Poisoning Attacks on Pre-trained Models*. arXiv:2004.06660 [http://arxiv.org/abs/2004.06660](http://arxiv.org/abs/2004.06660)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2022) Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang,
    Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2022. Deduplicating Training
    Data Makes Language Models Better. In *Proceedings of the 60th Annual Meeting
    of the Association for Computational Linguistics (Volume 1: Long Papers)* (Dublin,
    Ireland), Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association
    for Computational Linguistics, 8424–8445. [https://doi.org/10.18653/v1/2022.acl-long.577](https://doi.org/10.18653/v1/2022.acl-long.577)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2024) Seongyun Lee, Sue Hyun Park, Yongrae Jo, and Minjoon Seo.
    2024. *Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided
    Revision*. [https://doi.org/10.48550/arXiv.2311.07362](https://doi.org/10.48550/arXiv.2311.07362)
    arXiv:2311.07362'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lei et al. (2022) Yunjiao Lei, Dayong Ye, Sheng Shen, Yulei Sui, Tianqing Zhu,
    and Wanlei Zhou. 2022. New Challenges in Reinforcement Learning: A Survey of Security
    and Privacy. *Artif. Intell. Rev.* 56, 7 (2022), 7195–7236. [https://doi.org/10.1007/s10462-022-10348-5](https://doi.org/10.1007/s10462-022-10348-5)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leng et al. (2024) Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian
    Lu, Chunyan Miao, and Lidong Bing. 2024. Mitigating Object Hallucinations in Large
    Vision-Language Models through Visual Contrastive Decoding. In *Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 13872–13882.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023b) Chenyang Li, Zhao Song, Weixin Wang, and Chiwun Yang. 2023b.
    *A Theoretical Insight into Attack and Defense of Gradient Leakage in Transformer*.
    arXiv:2311.13624 [http://arxiv.org/abs/2311.13624](http://arxiv.org/abs/2311.13624)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023a) Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu
    Meng, and Yangqiu Song. 2023a. *Multi-Step Jailbreaking Privacy Attacks on ChatGPT*.
    arXiv:2304.05197 [http://arxiv.org/abs/2304.05197](http://arxiv.org/abs/2304.05197)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2023) Jiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, Huqiuyue
    Ping, and Qin Chen. 2023. *AgentSims: An Open-Source Sandbox for Large Language
    Model Evaluation*. [https://doi.org/10.48550/arXiv.2308.04026](https://doi.org/10.48550/arXiv.2308.04026)
    arXiv:2308.04026'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2023a) Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy,
    and Cyril Zhang. 2023a. *Exposing Attention Glitches with Flip-Flop Language Modeling*.
    [https://doi.org/10.48550/arXiv.2306.00946](https://doi.org/10.48550/arXiv.2306.00946)
    arXiv:2306.00946
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2024a) Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob,
    and Lijuan Wang. 2024a. *Mitigating Hallucination in Large Multi-Modal Models
    via Robust Instruction Tuning*. [https://doi.org/10.48550/arXiv.2306.14565](https://doi.org/10.48550/arXiv.2306.14565)
    arXiv:2306.14565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2024b) Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. 2024b.
    AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models.
    In *The Twelfth International Conference on Learning Representations*. [https://openreview.net/forum?id=7Jwpw4qKkb](https://openreview.net/forum?id=7Jwpw4qKkb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2023b) Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang,
    Yepang Liu, Haoyu Wang, Yan Zheng, and Yang Liu. 2023b. *Prompt Injection Attack
    against LLM-integrated Applications*. [https://doi.org/10.48550/arXiv.2306.05499](https://doi.org/10.48550/arXiv.2306.05499)
    arXiv:2306.05499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2024c) Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma,
    Shuo Zhang, and Xiang Bai. 2024c. *TextMonkey: An OCR-Free Large Multimodal Model
    for Understanding Document*. [https://doi.org/10.48550/arXiv.2403.04473](https://doi.org/10.48550/arXiv.2403.04473)
    arXiv:2403.04473'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. (2023a) Junyu Luo, Cao Xiao, and Fenglong Ma. 2023a. Zero-Resource
    Hallucination Prevention for Large Language Models. *arXiv preprint arXiv:2309.02654*
    (2023). [https://doi.org/10.48550/ARXIV.2309.02654](https://doi.org/10.48550/ARXIV.2309.02654)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. (2023b) Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and
    Yue Zhang. 2023b. *An Empirical Study of Catastrophic Forgetting in Large Language
    Models During Continual Fine-tuning*. arXiv:2308.08747 [http://arxiv.org/abs/2308.08747](http://arxiv.org/abs/2308.08747)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mahmoud and Hajj (2022) Reem A. Mahmoud and Hazem Hajj. 2022. Multi-Objective
    Learning to Overcome Catastrophic Forgetting in Time-series Applications. *ACM
    Transactions on Knowledge Discovery from Data* 16, 6 (2022), 1–20. [https://doi.org/10.1145/3502728](https://doi.org/10.1145/3502728)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mandi et al. (2023) Zhao Mandi, Shreeya Jain, and Shuran Song. 2023. *RoCo:
    Dialectic Multi-Robot Collaboration with Large Language Models*. [https://doi.org/10.48550/arXiv.2307.04738](https://doi.org/10.48550/arXiv.2307.04738)
    arXiv:2307.04738'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mendis et al. (2007) D. S. Kalana Mendis, Asoka S. Karunananda, Udaya Samaratunga,
    and Uditha Ratnayake. 2007. An Approach to the Development of Commonsense Knowledge
    Modeling Systems for Disaster Management. 28, 2 (2007), 179–196. [https://doi.org/10.1007/s10462-009-9097-6](https://doi.org/10.1007/s10462-009-9097-6)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mondesire and Wiegand (2023) Sean Mondesire and R. Paul Wiegand. 2023. Mitigating
    Catastrophic Forgetting with Complementary Layered Learning. *Electronics* 12,
    3 (2023), 706. [https://doi.org/10.3390/electronics12030706](https://doi.org/10.3390/electronics12030706)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moor et al. (2023) Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga,
    Yash Dalmia, Jure Leskovec, Cyril Zakka, Eduardo Pontes Reis, and Pranav Rajpurkar.
    2023. Med-Flamingo: A Multimodal Medical Few-shot Learner. In *Proceedings of
    the 3rd Machine Learning for Health Symposium* *(Proceedings of Machine Learning
    Research, Vol. 225)*. PMLR, 353–367. [https://proceedings.mlr.press/v225/moor23a.html](https://proceedings.mlr.press/v225/moor23a.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI et al. (2024) OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
    Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt,
    Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, and Balcom.
    2024. *GPT-4 Technical Report*. [https://doi.org/10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774)
    arXiv:2303.08774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ozdayi et al. (2023) Mustafa Safa Ozdayi, Charith Peris, Jack FitzGerald, Christophe
    Dupuy, Jimit Majmudar, Haidar Khan, Rahil Parikh, and Rahul Gupta. 2023. *Controlling
    the Extraction of Memorized Data from Large Language Models via Prompt-Tuning*.
    arXiv:2305.11759 [https://arxiv.org/abs/2305.11759](https://arxiv.org/abs/2305.11759)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pang et al. (2024) Jing-Cheng Pang, Heng-Bo Fan, Pengyuan Wang, Jia-Hao Xiao,
    Nan Tang, Si-Hang Yang, Chengxing Jia, Sheng-Jun Huang, and Yang Yu. 2024. *Empowering
    Language Models with Active Inquiry for Deeper Understanding*. [https://doi.org/10.48550/arXiv.2402.03719](https://doi.org/10.48550/arXiv.2402.03719)
    arXiv:2402.03719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. (2023b) Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S. Bernstein. 2023b. *Generative Agents: Interactive
    Simulacra of Human Behavior*. [https://doi.org/10.48550/arXiv.2304.03442](https://doi.org/10.48550/arXiv.2304.03442)
    arXiv:2304.03442'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. (2023a) Peter S. Park, Simon Goldstein, Aidan O’Gara, Michael Chen,
    and Dan Hendrycks. 2023a. *AI Deception: A Survey of Examples, Risks, and Potential
    Solutions*. [https://doi.org/10.48550/arXiv.2308.14752](https://doi.org/10.48550/arXiv.2308.14752)
    arXiv:2308.14752'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. (2023) Liangzu Peng, Paris Giampouras, and Rene Vidal. 2023. The
    Ideal Continual Learner: An Agent That Never Forgets. In *Proceedings of the 40th
    International Conference on Machine Learning*. PMLR, 27585–27610. [https://proceedings.mlr.press/v202/peng23a.html](https://proceedings.mlr.press/v202/peng23a.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peters (2023) Jay Peters. 2023. *The Bing AI Bot Has Been Secretly Running GPT-4*.
    The Verge. [https://www.theverge.com/2023/3/14/23639928/microsoft-bing-chatbot-ai-gpt-4-llm](https://www.theverge.com/2023/3/14/23639928/microsoft-bing-chatbot-ai-gpt-4-llm)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Puig et al. (2023) Xavier Puig, Eric Undersander, Andrew Szot, Mikael Dallaire
    Cote, Tsung-Yen Yang, Ruslan Partsey, Ruta Desai, Alexander William Clegg, Michal
    Hlavac, So Yeon Min, Vladimír Vondruš, Theophile Gervet, Vincent-Pierre Berges,
    John M. Turner, Oleksandr Maksymets, Zsolt Kira, Mrinal Kalakrishnan, Jitendra
    Malik, Devendra Singh Chaplot, Unnat Jain, Dhruv Batra, Akshara Rai, and Roozbeh
    Mottaghi. 2023. *Habitat 3.0: A Co-Habitat for Humans, Avatars and Robots*. [https://doi.org/10.48550/arXiv.2310.13724](https://doi.org/10.48550/arXiv.2310.13724)
    arXiv:2310.13724'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qi et al. (2023) Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Peter Henderson,
    Mengdi Wang, and Prateek Mittal. 2023. *Visual Adversarial Examples Jailbreak
    Aligned Large Language Models*. [https://doi.org/10.48550/arXiv.2306.13213](https://doi.org/10.48550/arXiv.2306.13213)
    arXiv:2306.13213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qian et al. (2024) Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang,
    Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li,
    Zhiyuan Liu, and Maosong Sun. 2024. *ChatDev: Communicative Agents for Software
    Development*. [https://doi.org/10.48550/arXiv.2307.07924](https://doi.org/10.48550/arXiv.2307.07924)
    arXiv:2307.07924'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Robey et al. (2023) Alexander Robey, Eric Wong, Hamed Hassani, and George J.
    Pappas. 2023. SmoothLLM: Defending Large Language Models Against Jailbreaking
    Attacks. *arXiv preprint arXiv:2310.03684* (2023). [https://doi.org/10.48550/ARXIV.2310.03684](https://doi.org/10.48550/ARXIV.2310.03684)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ruan et al. (2024) Yangjun Ruan, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao
    Zhou, Jimmy Ba, Yann Dubois, Chris J. Maddison, and Tatsunori Hashimoto. 2024.
    *Identifying the Risks of LM Agents with an LM-Emulated Sandbox*. [https://doi.org/10.48550/arXiv.2309.15817](https://doi.org/10.48550/arXiv.2309.15817)
    arXiv:2309.15817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schuster et al. (2021) Roei Schuster, Congzheng Song, Eran Tromer, and Vitaly
    Shmatikov. 2021. You Autocomplete Me: Poisoning Vulnerabilities in Neural Code
    Completion. In *30th USENIX Security Symposium (USENIX Security 21)*. 1559–1575.
    [https://www.usenix.org/conference/usenixsecurity21/presentation/schuster](https://www.usenix.org/conference/usenixsecurity21/presentation/schuster)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shayegani et al. (2023) Erfan Shayegani, Yue Dong, and Nael Abu-Ghazaleh. 2023.
    *Jailbreak in Pieces: Compositional Adversarial Attacks on Multi-Modal Language
    Models*. [https://doi.org/10.48550/arXiv.2307.14539](https://doi.org/10.48550/arXiv.2307.14539)
    arXiv:2307.14539'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shejwalkar and Houmansadr (2021) Virat Shejwalkar and Amir Houmansadr. 2021.
    Membership Privacy for Machine Learning Models Through Knowledge Transfer. 35,
    11 (2021), 9549–9557. Issue 11. [https://doi.org/10.1609/aaai.v35i11.17150](https://doi.org/10.1609/aaai.v35i11.17150)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2023) Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and
    Yang Zhang. 2023. *”Do Anything Now”: Characterizing and Evaluating In-The-Wild
    Jailbreak Prompts on Large Language Models*. arXiv:2308.03825 [http://arxiv.org/abs/2308.03825](http://arxiv.org/abs/2308.03825)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2023) Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan
    Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer,
    and Trevor Darrell. 2023. *Aligning Large Multimodal Models with Factually Augmented
    RLHF*. [https://doi.org/10.48550/arXiv.2309.14525](https://doi.org/10.48550/arXiv.2309.14525)
    arXiv:2309.14525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Talebirad and Nadiri (2023) Yashar Talebirad and Amirhossein Nadiri. 2023.
    *Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents*. [https://doi.org/10.48550/arXiv.2306.03314](https://doi.org/10.48550/arXiv.2306.03314)
    arXiv:2306.03314'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Taveekitworachai et al. (2023) Pittawat Taveekitworachai, Febri Abdullah, Mustafa Can
    Gursesli, Mury F. Dewantoro, Siyuan Chen, Antonio Lanata, Andrea Guazzini, and
    Ruck Thawonmas. 2023. Breaking Bad: Unraveling Influences and Risks of User Inputs
    to ChatGPT for Game Story Generation. In *Interactive Storytelling* (Cham) *(Lecture
    Notes in Computer Science)*, Lissa Holloway-Attaway and John T. Murray (Eds.).
    Springer Nature Switzerland, 285–296.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Toetzke et al. (2023) Malte Toetzke, Benedict Probst, and Stefan Feuerriegel.
    2023. Leveraging Large Language Models to Monitor Climate Technology Innovation.
    *Environmental Research Letters* 18, 9 (2023), 091004. [https://doi.org/10.1088/1748-9326/acf233](https://doi.org/10.1088/1748-9326/acf233)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tong et al. (2024) Meng Tong, Kejiang Chen, Jie Zhang, Yuang Qi, Weiming Zhang,
    Nenghai Yu, Tianwei Zhang, and Zhikun Zhang. 2024. *InferDPT: Privacy-Preserving
    Inference for Black-box Large Language Model*. [https://doi.org/10.48550/arXiv.2310.12214](https://doi.org/10.48550/arXiv.2310.12214)
    arXiv:2310.12214'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Truong et al. (2021) Jean-Baptiste Truong, Pratyush Maini, Robert J. Walls,
    and Nicolas Papernot. 2021. Data-Free Model Extraction. In *2021 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*. 4769–4778. [https://ieeexplore.ieee.org/document/9577784](https://ieeexplore.ieee.org/document/9577784)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vakili et al. (2022) Thomas Vakili, Anastasios Lamproudis, Aron Henriksson,
    and Hercules Dalianis. 2022. Downstream Task Performance of BERT Models Pre-Trained
    Using Automatically De-Identified Clinical Data. In *Proceedings of the Thirteenth
    Language Resources and Evaluation Conference* (Marseille, France), Nicoletta Calzolari,
    Frédéric Béchet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck,
    Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Hélène Mazo, Jan
    Odijk, and Stelios Piperidis (Eds.). European Language Resources Association,
    4245–4252. [https://aclanthology.org/2022.lrec-1.451](https://aclanthology.org/2022.lrec-1.451)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vander Eeckt and Van Hamme (2023) Steven Vander Eeckt and Hugo Van Hamme. 2023.
    Weight Averaging: A Simple Yet Effective Method to Overcome Catastrophic Forgetting
    in Automatic Speech Recognition. In *ICASSP 2023 - 2023 IEEE International Conference
    on Acoustics, Speech and Signal Processing (ICASSP)* (Rhodes Island, Greece).
    IEEE, 1–5. [https://doi.org/10.1109/ICASSP49357.2023.10095147](https://doi.org/10.1109/ICASSP49357.2023.10095147)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wan et al. (2023a) Alexander Wan, Eric Wallace, Sheng Shen, and Dan Klein. 2023a.
    *Poisoning Language Models During Instruction Tuning*. arXiv:2305.00944 [http://arxiv.org/abs/2305.00944](http://arxiv.org/abs/2305.00944)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wan et al. (2023b) Alexander Wan, Eric Wallace, Sheng Shen, and Dan Klein. 2023b.
    *Poisoning Language Models During Instruction Tuning*. [https://doi.org/10.48550/arXiv.2305.00944](https://doi.org/10.48550/arXiv.2305.00944)
    arXiv:2305.00944
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang and Li (2023) Huan Wang and Yan-Fu Li. 2023. Large Language Model Empowered
    by Domain-Specific Knowledge Base for Industrial Equipment Operation and Maintenance.
    In *2023 5th International Conference on System Reliability and Safety Engineering
    (SRSE)*. 474–479. [https://doi.org/10.1109/SRSE59585.2023.10336112](https://doi.org/10.1109/SRSE59585.2023.10336112)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2024b) Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen,
    Ji Zhang, Fei Huang, and Jitao Sang. 2024b. *Mobile-Agent: Autonomous Multi-Modal
    Mobile Device Agent with Visual Perception*. [https://doi.org/10.48550/arXiv.2401.16158](https://doi.org/10.48550/arXiv.2401.16158)
    arXiv:2401.16158'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023c) Kuan Wang, Yadong Lu, Michael Santacroce, Yeyun Gong, Chao
    Zhang, and Yelong Shen. 2023c. *Adapting LLM Agents Through Communication*. [https://doi.org/10.48550/arXiv.2310.01444](https://doi.org/10.48550/arXiv.2310.01444)
    arXiv:2310.01444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023d) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei
    Wei, and Ji-Rong Wen. 2023d. *A Survey on Large Language Model Based Autonomous
    Agents*. arXiv:2308.11432 [http://arxiv.org/abs/2308.11432](http://arxiv.org/abs/2308.11432)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2024c) Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai
    Tang, Zeyu Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, Jun Xu, Zhicheng
    Dou, Jun Wang, and Ji-Rong Wen. 2024c. *User Behavior Simulation with Large Language
    Model Based Agents*. [https://doi.org/10.48550/arXiv.2306.02552](https://doi.org/10.48550/arXiv.2306.02552)
    arXiv:2306.02552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023b) Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo
    Chen, Qisen Yang, Andrew Zhao, Chaofei Wang, Shiji Song, and Gao Huang. 2023b.
    *Avalon’s Game of Thoughts: Battle Against Deception through Recursive Contemplation*.
    [https://doi.org/10.48550/arXiv.2310.01320](https://doi.org/10.48550/arXiv.2310.01320)
    arXiv:2310.01320'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2024a) Shen Wang, Tianlong Xu, Hang Li, Chaoli Zhang, Joleen Liang,
    Jiliang Tang, Philip S. Yu, and Qingsong Wen. 2024a. *Large Language Models for
    Education: A Survey and Outlook*. arXiv:2403.18105 [https://arxiv.org/abs/2403.18105](https://arxiv.org/abs/2403.18105)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2024d) Shang Wang, Tianqing Zhu, Bo Liu, Ming Ding, Xu Guo, Dayong
    Ye, Wanlei Zhou, and Philip S. Yu. 2024d. *Unique Security and Privacy Threats
    of Large Language Model: A Comprehensive Survey*. [https://doi.org/10.48550/arXiv.2406.07973](https://doi.org/10.48550/arXiv.2406.07973)
    arXiv:2406.07973'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023a) Tianyu Wang, Yifan Li, Haitao Lin, Xiangyang Xue, and Yanwei
    Fu. 2023a. *WALL-E: Embodied Robotic WAiter Load Lifting with Large Language Model*.
    [https://doi.org/10.48550/arXiv.2308.15962](https://doi.org/10.48550/arXiv.2308.15962)
    arXiv:2308.15962'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023e) Yuntao Wang, Yanghe Pan, Miao Yan, Zhou Su, and Tom H.
    Luan. 2023e. A Survey on ChatGPT: AI-Generated Contents, Challenges, and Solutions.
    *IEEE Open Journal of the Computer Society* 4 (2023), 280–302. [https://doi.org/10.1109/OJCS.2023.3300321](https://doi.org/10.1109/OJCS.2023.3300321)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023g) Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan
    Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. 2023g. *Aligning Large
    Language Models with Human: A Survey*. [https://doi.org/10.48550/arXiv.2307.12966](https://doi.org/10.48550/arXiv.2307.12966)
    arXiv:2307.12966'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023f) Zhenhua Wang, Wei Xie, Kai Chen, Baosheng Wang, Zhiwen
    Gui, and Enze Wang. 2023f. *Self-Deception: Reverse Penetrating the Semantic Firewall
    of Large Language Models*. arXiv:2308.11521 [http://arxiv.org/abs/2308.11521](http://arxiv.org/abs/2308.11521)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Winata et al. (2023) Genta Indra Winata, Lingjue Xie, Karthik Radhakrishnan,
    Shijie Wu, Xisen Jin, Pengxiang Cheng, Mayank Kulkarni, and Daniel Preotiuc-Pietro.
    2023. Overcoming Catastrophic Forgetting in Massively Multilingual Continual Learning.
    In *Findings of the Association for Computational Linguistics: ACL 2023*. Association
    for Computational Linguistics, Toronto, Canada, 768–777. [https://doi.org/10.18653/v1/2023.findings-acl.48](https://doi.org/10.18653/v1/2023.findings-acl.48)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2023) Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng
    Tang, and Nan Duan. 2023. *Visual ChatGPT: Talking, Drawing and Editing with Visual
    Foundation Models*. [https://doi.org/10.48550/arXiv.2303.04671](https://doi.org/10.48550/arXiv.2303.04671)
    arXiv:2303.04671'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xi et al. (2023) Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang
    Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan,
    Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou,
    Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang,
    Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. 2023. *The
    Rise and Potential of Large Language Model Based Agents: A Survey*. [https://doi.org/10.48550/arXiv.2309.07864](https://doi.org/10.48550/arXiv.2309.07864)
    arXiv:2309.07864'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. (2024) Junlin Xie, Zhihong Chen, Ruifei Zhang, Xiang Wan, and Guanbin
    Li. 2024. *Large Multimodal Agents: A Survey*. [https://doi.org/10.48550/arXiv.2402.15116](https://doi.org/10.48550/arXiv.2402.15116)
    arXiv:2402.15116'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2023c) Heng Xu, Tianqing Zhu, Lefeng Zhang, Wanlei Zhou, and Philip S.
    Yu. 2023c. Machine Unlearning: A Survey. *ACM Comput. Surv.* 56, 1 (2023), 9:1–9:36.
    [https://doi.org/10.1145/3603620](https://doi.org/10.1145/3603620)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2023a) Lin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren, Zhen Dong, Kurt
    Keutzer, See Kiong Ng, and Jiashi Feng. 2023a. *MAgIC: Investigation of Large
    Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and
    Collaboration*. [https://doi.org/10.48550/arXiv.2311.08562](https://doi.org/10.48550/arXiv.2311.08562)
    arXiv:2311.08562'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2023b) Lvxiaowei Xu, Jianwang Wu, Jiawei Peng, Zhilin Gong, Ming
    Cai, and Tianxiang Wang. 2023b. Enhancing Language Representation with Constructional
    Information for Natural Language Understanding. In *Proceedings of the 61st Annual
    Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*
    (Toronto, Canada). Association for Computational Linguistics, 4685–4705. [https://doi.org/10.18653/v1/2023.acl-long.258](https://doi.org/10.18653/v1/2023.acl-long.258)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2024) Zelai Xu, Chao Yu, Fei Fang, Yu Wang, and Yi Wu. 2024. *Language
    Agents with Reinforcement Learning for Strategic Play in the Werewolf Game*. [https://doi.org/10.48550/arXiv.2310.18940](https://doi.org/10.48550/arXiv.2310.18940)
    arXiv:2310.18940
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yan et al. (2023) Lu Yan, Zhuo Zhang, Guanhong Tao, Kaiyuan Zhang, Xuan Chen,
    Guangyu Shen, and Xiangyu Zhang. 2023. *ParaFuzz: An Interpretability-Driven Technique
    for Detecting Poisoned Samples in NLP*. [https://doi.org/10.48550/arXiv.2308.02122](https://doi.org/10.48550/arXiv.2308.02122)
    arXiv:2308.02122'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2023b) Haomiao Yang, Kunlan Xiang, Mengyu Ge, Hongwei Li, Rongxing
    Lu, and Shui Yu. 2023b. *A Comprehensive Overview of Backdoor Attacks in Large
    Language Models within Communication Networks*. [https://doi.org/10.48550/arXiv.2308.14367](https://doi.org/10.48550/arXiv.2308.14367)
    arXiv:2308.14367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2024b) Jihan Yang, Runyu Ding, Ellis Brown, Xiaojuan Qi, and Saining
    Xie. 2024b. *V-IRL: Grounding Virtual Intelligence in Real Life*. [https://doi.org/10.48550/arXiv.2402.03310](https://doi.org/10.48550/arXiv.2402.03310)
    arXiv:2402.03310'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2024a) Wenkai Yang, Xiaohan Bi, Yankai Lin, Sishuo Chen, Jie Zhou,
    and Xu Sun. 2024a. *Watch Out for Your Agents! Investigating Backdoor Threats
    to LLM-Based Agents*. [https://doi.org/10.48550/arXiv.2402.11208](https://doi.org/10.48550/arXiv.2402.11208)
    arXiv:2402.11208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2023a) Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan
    Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. 2023a.
    *MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action*. [https://doi.org/10.48550/arXiv.2303.11381](https://doi.org/10.48550/arXiv.2303.11381)
    arXiv:2303.11381'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2024) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L.
    Griffiths, Yuan Cao, and Karthik Narasimhan. 2024. Tree of Thoughts: Deliberate
    Problem Solving with Large Language Models. In *Proceedings of the 37th International
    Conference on Neural Information Processing Systems* (Red Hook, NY, USA) *(NIPS
    ’23)*. Curran Associates Inc., 11809–11822.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2023) Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Eric Sun,
    and Yue Zhang. 2023. *A Survey on Large Language Model (LLM) Security and Privacy:
    The Good, the Bad, and the Ugly*. [https://doi.org/10.48550/arXiv.2312.02003](https://doi.org/10.48550/arXiv.2312.02003)
    arXiv:2312.02003'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ye et al. (2024) Dayong Ye, Tianqing Zhu, Congcong Zhu, Derui Wang, Zewei Shi,
    Sheng Shen, Wanlei Zhou, and Minhui Xue. 2024. *Reinforcement Unlearning*. [https://doi.org/10.48550/arXiv.2312.15910](https://doi.org/10.48550/arXiv.2312.15910)
    arXiv:2312.15910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yi et al. (2023) Jingwei Yi, Yueqi Xie, Bin Zhu, Keegan Hines, Emre Kiciman,
    Guangzhong Sun, Xing Xie, and Fangzhao Wu. 2023. *Benchmarking and Defending Against
    Indirect Prompt Injection Attacks on Large Language Models*. [https://doi.org/10.48550/arXiv.2312.14197](https://doi.org/10.48550/arXiv.2312.14197)
    arXiv:2312.14197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yin et al. (2023a) Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong
    Xu, and Enhong Chen. 2023a. A Survey on Multimodal Large Language Models. *arXiv
    preprint arXiv:2306.13549* (2023). [https://doi.org/10.48550/ARXIV.2306.13549](https://doi.org/10.48550/ARXIV.2306.13549)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yin et al. (2023b) Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang,
    Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen. 2023b. *Woodpecker:
    Hallucination Correction for Multimodal Large Language Models*. [https://doi.org/10.48550/arXiv.2310.16045](https://doi.org/10.48550/arXiv.2310.16045)
    arXiv:2310.16045'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2021) Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A.
    Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz,
    Sergey Yekhanin, and Huishuai Zhang. 2021. Differentially Private Fine-tuning
    of Language Models. In *International Conference on Learning Representations*.
    [https://openreview.net/forum?id=Q42f0dfjECO](https://openreview.net/forum?id=Q42f0dfjECO)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2023) Jiahao Yu, Xingwei Lin, Zheng Yu, and Xinyu Xing. 2023. *GPTFUZZER:
    Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts*. arXiv:2309.10253
    [http://arxiv.org/abs/2309.10253](http://arxiv.org/abs/2309.10253)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhai et al. (2024) Bohan Zhai, Shijia Yang, Chenfeng Xu, Sheng Shen, Kurt Keutzer,
    Chunyuan Li, and Manling Li. 2024. *HallE-Control: Controlling Object Hallucination
    in Large Multimodal Models*. [https://doi.org/10.48550/arXiv.2310.01779](https://doi.org/10.48550/arXiv.2310.01779)
    arXiv:2310.01779'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhai et al. (2023) Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu,
    Yong Jae Lee, and Yi Ma. 2023. *Investigating the Catastrophic Forgetting in Multimodal
    Large Language Models*. [https://doi.org/10.48550/arXiv.2309.10313](https://doi.org/10.48550/arXiv.2309.10313)
    arXiv:2309.10313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhan et al. (2024) Qiusi Zhan, Zhixiang Liang, Zifan Ying, and Daniel Kang.
    2024. *InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated
    Large Language Model Agents*. [https://doi.org/10.48550/arXiv.2403.02691](https://doi.org/10.48550/arXiv.2403.02691)
    arXiv:2403.02691'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2024b) Shuo Zhang, Liangming Pan, Junzhou Zhao, and William Yang
    Wang. 2024b. *The Knowledge Alignment Problem: Bridging Human and External Knowledge
    for Large Language Models*. arXiv:2305.13669 [https://arxiv.org/abs/2305.13669](https://arxiv.org/abs/2305.13669)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023a) Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya
    Zhang, Yanfeng Wang, and Weidi Xie. 2023a. *PMC-VQA: Visual Instruction Tuning
    for Medical Visual Question Answering*. [https://doi.org/10.48550/arXiv.2305.10415](https://doi.org/10.48550/arXiv.2305.10415)
    arXiv:2305.10415'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2024a) Zhiping Zhang, Michelle Jia, Hao-Ping (Hank) Lee, Bingsheng
    Yao, Sauvik Das, Ada Lerner, Dakuo Wang, and Tianshi Li. 2024a. “It’s a Fair Game”,
    or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using
    LLM-Based Conversational Agents. In *Proceedings of the CHI Conference on Human
    Factors in Computing Systems* (New York, NY, USA) *(CHI ’24)*. Association for
    Computing Machinery, 1–26. [https://doi.org/10.1145/3613904.3642385](https://doi.org/10.1145/3613904.3642385)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023b) Zhexin Zhang, Junxiao Yang, Pei Ke, and Minlie Huang. 2023b.
    Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization.
    *arXiv preprint arXiv:2311.09096* (2023). [https://doi.org/10.48550/ARXIV.2311.09096](https://doi.org/10.48550/ARXIV.2311.09096)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. (2023) Qingxiao Zheng, Zhongwei Xu, Abhinav Choudhary, Yuting
    Chen, Yongming Li, and Yun Huang. 2023. *Synergizing Human-AI Agency: A Guide
    of 23 Heuristics for Service Co-Creation with LLM-Based Agents*. [https://doi.org/10.48550/arXiv.2310.15065](https://doi.org/10.48550/arXiv.2310.15065)
    arXiv:2310.15065'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhong et al. (2023) Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin
    Wang. 2023. *MemoryBank: Enhancing Large Language Models with Long-Term Memory*.
    [https://doi.org/10.48550/arXiv.2305.10250](https://doi.org/10.48550/arXiv.2305.10250)
    arXiv:2305.10250'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zou et al. (2024) Wei Zou, Runpeng Geng, Binghui Wang, and Jinyuan Jia. 2024.
    *PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of
    Large Language Models*. [https://doi.org/10.48550/arXiv.2402.07867](https://doi.org/10.48550/arXiv.2402.07867)
    arXiv:2402.07867'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
