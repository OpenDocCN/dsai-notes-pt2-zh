- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:52:43'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.08995](https://ar5iv.labs.arxiv.org/html/2402.08995)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Jiaying Lu, Bo Pan, Jieyi Chen, Yingchaojie Feng, Jingyuan Hu, Yuchen Peng,
    Wei Chen
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Recently, Large Language Model based Autonomous system (LLMAS) has gained great
    popularity for its potential to simulate complicated behaviors of human societies.
    One of its main challenges is to present and analyze the dynamic events evolution
    of LLMAS. In this work, we present a visualization approach to explore detailed
    statuses and agents’ behavior within LLMAS. We propose a general pipeline that
    establishes a behavior structure from raw LLMAS execution events, leverages a
    behavior summarization algorithm to construct a hierarchical summary of the entire
    structure in terms of time sequence, and a cause trace method to mine the causal
    relationship between agent behaviors. We then develop AgentLens, a visual analysis
    system that leverages a hierarchical temporal visualization for illustrating the
    evolution of LLMAS, and supports users to interactively investigate details and
    causes of agents’ behaviors. Two usage scenarios and a user study demonstrate
    the effectiveness and usability of our AgentLens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'LLM, autonomous system, agent, visual analysis.^†^†publicationid: pubid: 0000–0000/00$00.00 © 2021
    IEEE![Refer to caption](img/58df0be1d929626861392092a4cb2506.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: The user interface of AgentLens comprises three views. The Outline
    View (A) displays the trajectory of each agent using different colored curves,
    enabling users to identify significant patterns or event summarization during
    the evolution of LLMAS. By clicking on a time step in each curve, users can further
    investigate it in the Agent View (B). It allows users to progressively reveal
    agent event information and trace the cause of specific agent behavior. The Monitor
    View (C) automatically adjusts the graphical representation of LLMAS based on
    the user’s current point of interest.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Autonomous agents, as computational entities that possess a certain degree of
    autonomy[[1](#bib.bib1), [2](#bib.bib2)], are seen as a promising pathway toward
    achieving artificial general intelligence (AGI)[[3](#bib.bib3), [4](#bib.bib4)].
    In recent years, owing to the breakthroughs in natural language processing[[5](#bib.bib5),
    [6](#bib.bib6), [7](#bib.bib7)] achieved by Large Language Models (LLM), the LLM-based
    autonomous agent has gained widespread adoption in both academia and industry[[8](#bib.bib8),
    [9](#bib.bib9)]. Built upon LLM-based agents, LLM-based autonomous systems (LLMAS)
    deploy multiple agents within a shared environment, enabling them to display behavior
    and social patterns akin to humans. This collective intelligence fosters emergent
    social dynamics, such as the formation of new relationships, diffusion of information,
    and the rise of coordination among agents[[10](#bib.bib10)]. Consequently, LLMAS
    exhibits significant potential in society simulation[[10](#bib.bib10), [11](#bib.bib11)],
    software engineering[[12](#bib.bib12), [13](#bib.bib13)], and scientific research[[14](#bib.bib14)].
  prefs: []
  type: TYPE_NORMAL
- en: However, monitoring and analyzing the dynamic evolution of LLMAS, including
    agents in LLMAS and event sequences undertaken by them, can be challenging due
    to the tremendous information generated during the system evolution and the inherent
    unpredictability of LLMs. The most straightforward approach for analyzing LLMAS
    is to inject logging code into LLMAS to trace agent events of interest and check
    the raw output logs in text format[[15](#bib.bib15)]. However, this approach requires
    expertise with specific LLMAS and is unintuitive for general users. To address
    this, many LLMAS projects provide a graphical representation of the simulation
    process[[9](#bib.bib9)], which is typically re-playable 2D [[16](#bib.bib16),
    [17](#bib.bib17), [10](#bib.bib10), [18](#bib.bib18)] or 3D video[[19](#bib.bib19),
    [20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22)]. By transforming a fixed
    sequence of intermediate simulation events into expressive visual recordings,
    users can digest that information more efficiently and intuitively. However, a
    re-playable recording with a fixed level of abstraction limits the flexibility
    of analysis for LLMAS. Even for a specific LLMAS and a fixed usage scenario, a
    user’s short-term analysis target will change frequently during the analysis process.
    As the users’ analysis target varies, the type, quantity, and granularity of agent
    events to be visualized also need to change. Moreover, analyzing the agent’s behavior
    at a specific time point requires users to switch the recording back and forth
    to trace the cause and consequence of this behavior, which is tedious and unreliable.
  prefs: []
  type: TYPE_NORMAL
- en: This work thus presents a visualization approach to assist users in efficiently
    analyzing the evolving status and complex behaviors of agents within an LLMAS.
    To mitigate cognitive overload due to the profusion of data produced throughout
    the evolution of LLMAS, and to enhance adaptability for subsequent analytical
    processes, we introduce a general pipeline, which establishes a hierarchical behavior
    structure of agent entities and raw event sequences within the LLMAS operational
    records. The formulation of the structure is based on our survey of prevalent
    architectures within extant LLMAS, coupled with a design study that engaged 4
    LLMAS developers and 4 layman users. We design an LLM-based algorithm for summarizing
    agent behavior that furnishes a hierarchical depiction of sequences of agent events.
    Additionally, we employ a cause trace method to unearth the causal linkages among
    disparate agent events. Based on the extracted hierarchical structure, we then
    develop AgentLens, a visual analysis system designed to facilitate interactive
    analysis and exploration of agent behaviors in LLMAS.
  prefs: []
  type: TYPE_NORMAL
- en: 'AgentLens provides a multi-faceted perspective for LLMAS through its three
    distinct but interrelated views, each offering a different level of abstraction.
    The Outline View ([Fig. 1](#S0.F1 "In AgentLens: Visual Analysis for Agent Behaviors
    in LLM-based Autonomous Systems"), <svg id="S1.p4.1.pic1" class="ltx_picture"
    height="14.06" overflow="visible" version="1.1" width="14.06"><g transform="translate(0,14.06)
    matrix(1 0 0 -1 0 0) translate(7.03,0) translate(0,7.03)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -5.19 -4.73)" fill="#000000"
    stroke="#000000"><foreignobject width="10.38" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">A</foreignobject></g></g></svg>) illustrates
    the spatiotemporal trajectory of each agent with curves of different colors, aiding
    users in identifying notable agents or their intriguing behaviors throughout the
    evolution of LLMAS. Users can quickly scan agent behaviors at different granularity
    ([Fig. 1](#S0.F1 "In AgentLens: Visual Analysis for Agent Behaviors in LLM-based
    Autonomous Systems"), <svg id="S1.p4.2.pic2" class="ltx_picture" height="18.61"
    overflow="visible" version="1.1" width="18.61"><g transform="translate(0,18.61)
    matrix(1 0 0 -1 0 0) translate(9.31,0) translate(0,9.31)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -7.13 -3.48)" fill="#000000"
    stroke="#000000"><foreignobject width="14.25" height="11.95" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$A_{1}$</foreignobject></g></g></svg>), and
    click any time point on an agent curve to further investigate it in the Agent
    View ([Fig. 1](#S0.F1 "In AgentLens: Visual Analysis for Agent Behaviors in LLM-based
    Autonomous Systems"), <svg id="S1.p4.5.pic5" class="ltx_picture" height="13.64"
    overflow="visible" version="1.1" width="13.64"><g transform="translate(0,13.64)
    matrix(1 0 0 -1 0 0) translate(6.82,0) translate(0,6.82)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -4.9 -4.73)" fill="#000000"
    stroke="#000000"><foreignobject width="9.8" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">B</foreignobject></g></g></svg>). The Agent
    View allows users to progressively reveal agent event information on demand and
    trace the cause of certain agent behavior. The Monitor View ([Fig. 1](#S0.F1 "In
    AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"),
    <svg id="S1.p4.6.pic6" class="ltx_picture" height="13.75" overflow="visible" version="1.1"
    width="13.75"><g transform="translate(0,13.75) matrix(1 0 0 -1 0 0) translate(6.88,0)
    translate(0,6.88)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -5 -4.73)" fill="#000000" stroke="#000000"><foreignobject width="9.99"
    height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">C</foreignobject></g></g></svg>)
    automatically adjusts the graphical representation of LLMAS for users based on
    their current point of interest in the Outline View or the Agent View. To evaluate
    the performance of AgentLens, we present two cases and conduct a user study with
    14 participants to gather their feedback. The results indicate that AgentLens
    is capable of assisting users in the LLMAS evolution analysis and agent behaviors
    investigation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main contributions of our work are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To the best of our knowledge, our work is the first visual analysis system that
    enables analysis and explorations of agent behaviors within LLMAS.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose a general pipeline that establishes a hierarchical behavior structure
    from raw LLMAS execution events to facilitate downstream analysis.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We conduct two cases and a user study to demonstrate the capabilities of our
    system. The evaluation results confirm the usefulness and effectiveness of the
    behavior structure andAgentLens.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 LLM-based Autonomous Agents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Franklin et al.[[23](#bib.bib23)] defined the agent as an entity situated in
    the environment that senses the environment and acts on it over time, in pursuit
    of its own agenda and so as to affect what it senses in the future. Possessing
    the ability to perform intelligent operations without human intervention, the
    autonomous agent remains a steadfast goal in artificial intelligence research[[3](#bib.bib3),
    [24](#bib.bib24)].
  prefs: []
  type: TYPE_NORMAL
- en: The progression of LLMs [[25](#bib.bib25), [6](#bib.bib6)] has underscored exceptional
    proficiency in areas of comprehension, reasoning, and language generation[[26](#bib.bib26)],
    which kindled optimism for continued advancements in the realm of autonomous agents.
    With the advent of LLMs, the study of LLM-based autonomous agents began to thrive.
    This includes enhancing agents’ self-reflective capabilities [[27](#bib.bib27),
    [28](#bib.bib28)], implementing superior task decomposition strategies [[29](#bib.bib29)],
    and endowing the ability to utilize and create tools[[30](#bib.bib30), [31](#bib.bib31),
    [32](#bib.bib32), [33](#bib.bib33)]. There is also a vibrant development of applications
    of LLM-based agents in the open source community [[34](#bib.bib34), [35](#bib.bib35),
    [15](#bib.bib15)].
  prefs: []
  type: TYPE_NORMAL
- en: Recently, researchers have found that LLM-based agents can address a wider range
    of tasks through collaboration or competition. Camel[[36](#bib.bib36)] presented
    a framework that emphasizes the autonomous interaction between communicative agents.
    It is capable of creating varied, detailed instructions across numerous tasks,
    thereby providing a platform for these agents to demonstrate their cognitive operations.
    Talebirad et al.[[37](#bib.bib37)] introduced a comprehensive framework for multi-agent
    collaboration based on LLMs. ProAgent[[18](#bib.bib18)] exhibited the distinctive
    ability for agents to foresee the upcoming decisions of collaborators and adjust
    their behaviors, enabling them to excel in cooperative reasoning tasks. Multi-Agent
    Debate (MAD)[[38](#bib.bib38)] introduced an approach in which several agents
    present their arguments collaboratively while a judge guides the discourse, enhancing
    agents’ divergent thinking for deep-reflective tasks.
  prefs: []
  type: TYPE_NORMAL
- en: However, as the number and the intricacy of agents increase, the complexity
    of analyzing their behaviors escalates rapidly. While past works have focused
    on elevating the capabilities of LLM-based agents in emulating human-like behaviors,
    they often overlooked how to effectively analyze agent behaviors. In this work,
    we identify this research gap and present a visualization approach for analyzing
    agent behaviors in LLM-based multi-agent systems.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 LLM-based Autonomous System
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By incorporating numerous LLM-based agents into a cohesive environment, the
    LLMAS is capable of handling diverse complex scenarios. For example, WebAgent[[39](#bib.bib39)]
    demonstrated the possibility of building agents that can complete the tasks on
    real websites following natural language instructions. ChatDev[[12](#bib.bib12)]
    and MetaGPT[[13](#bib.bib13)] experimented with software development in multi-agent
    communication settings. Zhang et al.[[19](#bib.bib19)] built embodied agents to
    cooperate effectively with humans. Park et al.[[10](#bib.bib10)] situates generative
    agents with unique characteristics in a societal context, in order to mimic human
    social behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: Several task-independent frameworks designed for diverse usages have received
    considerable attention within the community. AgentVerse[[17](#bib.bib17)] dynamically
    assembled multi-agent teams tailored to task complexities, outperforming individual
    agents with adaptable team structures. AgentSims[[16](#bib.bib16)] offered a real-time
    evaluation platform for LLM-based agents, enabling adaptable configurations to
    facilitate the performance evaluation of different modules. AutoGen[[40](#bib.bib40)]
    fostered conversations among multiple agents and organized individual insights
    in a general manner, offering an interconnected manner to coordinate multiple
    agents within the LLMAS. MetaGPT[[13](#bib.bib13)] injects effective human workflows
    into multi-agent collaboration by encoding Standardized Operational Procedures
    (SOP) into prompts, underscoring the potential of incorporating human domain expertise
    into LLMAS. CGMI[[11](#bib.bib11)] replicated human interactions and imitated
    human routines in real-world scenarios, which enhances the realism of more humanized
    simulation of complex social scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Previous LLMAS research has primarily focused on constructing more universal
    frameworks or designing for specific domains, yet there has been a noticeable
    lack of emphasis on the analysis methods of parallel behaviors among agents within
    LLMAS. Contemporary LLMAS predominantly depend on conventional methods for surveillance
    and analysis. MetaGPT[[13](#bib.bib13)] utilizes log outputs for record maintenance,
    while Park et al.[[10](#bib.bib10)] adopts panoramic videos for observation, providing
    detailed maps with agent avatars to denote their locations and behaviors. Distinct
    from preceding efforts, our work offers an interactive visual system that hierarchically
    organizes events, facilitating users in quickly grasping the happenings within
    LLMAS.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Event Sequence Visualization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data featuring time-based event sequences is widespread and can be found in
    various sectors, including healthcare records[[41](#bib.bib41), [42](#bib.bib42),
    [43](#bib.bib43)], career design[[44](#bib.bib44), [45](#bib.bib45)] and social
    interactions[[46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48)]. In these fields,
    distinct types of time-stamped events are sequentially organized, each relevant
    to a particular subject or entity. While earlier methods[[49](#bib.bib49), [50](#bib.bib50)]
    have been geared toward simpler, low-dimensional data, the data sets encountered
    in real-world scenarios frequently display a higher level of complexity, calling
    for more comprehensive analytical ideas and methods.
  prefs: []
  type: TYPE_NORMAL
- en: A substantial number of research on event sequence visualization is notably
    correlated with fields where there is a prevalent demand for event information
    condensation, such as in the realm of social media data[[51](#bib.bib51)], the
    sphere of smart manufacturing [[52](#bib.bib52)], and the study of anomalous user
    behaviors[[53](#bib.bib53)]. Guo et al.[[54](#bib.bib54)] proposed an organizational
    framework for event sequences to summarize the common goal of different properties
    with great heterogeneity. EventThread[[44](#bib.bib44)] focuses on visualization
    and cluster analysis, providing an interactive interface for browsing and summarizing
    event sequence data. Building on past frameworks of condensing events and visualizing
    them, we focused on the behavioral patterns of LLM agents and proposed an LLM-driven
    approach to handle non-structured natural language-based event sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Event sequence visualization has highly relevant applications in the realm of
    collective behavior analysis, which aligns closely with the focus of our research,
    both referring to activities conducted by a temporary and unstructured group of
    people [[55](#bib.bib55), [56](#bib.bib56), [47](#bib.bib47)]. In the field of
    social media, collective actions emerge from the collaborative efforts of users
    engaged in disseminating information and navigating through virtual spaces. A
    variety of sophisticated visual analytics methodologies have been introduced to
    scrutinize these group dynamics. R-map[[57](#bib.bib57)], Socialwave[[58](#bib.bib58)],
    FluxFlow[[59](#bib.bib59)] and Google+ ripples[[60](#bib.bib60)] are specifically
    tailored to examine the mechanics of information propagation, while Maqui[[61](#bib.bib61)]
    and Frequence[[46](#bib.bib46)] offers insights into the complexities of human
    mobility within this context.
  prefs: []
  type: TYPE_NORMAL
- en: While existing research has made significant contributions to the field, there’s
    a growing need to address the increasingly complex behaviors and interactions
    that call for the advancement of autonomous systems. Our work introduces event
    sequence visualization as an integral tool for the analysis and exploration of
    LLMAS.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Common Architecture of LLMAS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/25edbaeb7ab017740cf508cd26b1746b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The common architecture abstracted from existing LLMAS consists of
    four layers: system states, agents, tasks, and operations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To ensure maximum compatibility with various LLMAS, we survey LLMAS-related
    papers [[62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65),
    [66](#bib.bib66), [67](#bib.bib67), [68](#bib.bib68), [69](#bib.bib69), [27](#bib.bib27),
    [28](#bib.bib28)] as well as some projects[[70](#bib.bib70), [71](#bib.bib71),
    [72](#bib.bib72), [73](#bib.bib73), [74](#bib.bib74), [75](#bib.bib75), [76](#bib.bib76),
    [77](#bib.bib77), [78](#bib.bib78), [79](#bib.bib79)] with high stars in open
    source communities published before August 31, 2023. We analyzed their system
    architectures and components, based on which we abstract a common architecture
    (as shown in [Fig. 2](#S3.F2 "In 3.1 Common Architecture of LLMAS ‣ 3 Overview
    ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"))
    for LLMAS. The system state in LLMAS provides the environmental information at
    any time point. At each time point, each agent executes its own task, which consists
    of several atomic operations. A raw event is generated whenever an operation is
    executed by an agent, thereby advancing the evolution of LLMAS.'
  prefs: []
  type: TYPE_NORMAL
- en: System State provides a comprehensive understanding of the environment. By acquiring
    the environmental information from the system state, agents can comprehend the
    current context and conditions. For example, the system state can inform agents
    about object locations and environmental properties, which significantly impact
    their decision-making and planning processes. In addition, the system state governs
    the timelines of each agent, ensuring events by different agents are temporally
    aligned.
  prefs: []
  type: TYPE_NORMAL
- en: Agents are autonomous entities with cognitive abilities and action capability.
    By performing various types of tasks, agents can interact with the environment
    and gradually change the system state to achieve their goals. Additionally, agents
    can communicate and collaborate with each other. They can share their knowledge
    and exchange messages to accomplish more complex duties.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tasks are typically customized for the usage scenario of LLMAS. A sequence
    of operations with a common goal can be grouped as a task. Extending prior research
    that has focused on different scenarios for agents, we classify tasks into three
    categories: Perceive, Think, and Act. In Perceive tasks, the agent obtains perception
    of the external system. Such perception includes sensing the environment (virtual,
    real, or external resources), as well as perceiving other agents. In Think tasks,
    the agent engages in decision-making, reasoning, planning, and other behaviors
    based on external perception and its own memory. In Act tasks, the agent interacts
    with the external system by providing outputs, including text outputs, virtual
    actions, or specific invocations such as tool usage.'
  prefs: []
  type: TYPE_NORMAL
- en: Operations are the basic units for Tasks. Operations can be classified based
    on their target, including Environmental Operations, Memory Operations, and Decision
    Operations. Environment Operations execute interactions toward the external system,
    including other agents and the environment defined by LLMAS. Memory Operations
    involve storing and updating the memory of an agent. Decision Operations are for
    decision-making and action planning, where LLM-based agents typically utilize
    LLMs for decision operations.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Design Requirement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our study focuses on users involved in analyzing, exploring, and monitoring
    LLMAS. Our primary goal is to create a system that enhances users’ comprehension
    of LLMAS. We recruited 4 developers highly familiar with LLMAS and 4 users who
    have a basic understanding of LLMAS and have previously utilized such systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'To identify the design requirements, We asked participants to explore the behaviors
    of agents in Reverie¹¹1https://reverie.herokuapp.com/arXiv_Demo/#, a typical autonomous
    system consisting of 25 LLM-based agents. We requested participants to actively
    explore and delve into the identification of agent behaviors that intrigued them,
    as well as to investigate the underlying causes or consequences. To facilitate
    this, we encouraged participants to “think aloud”, articulating the information
    they sought and the type of assistance they desired throughout the process. We
    then conducted the first interview with them to collect their feedback on the
    whole exploration process. At the same time, we maintain regular contact with
    them to keep them updated on the design requirements. Based on their feedback,
    and combined with the survey on existing LLMAS work in [section 3.1](#S3.SS1 "3.1
    Common Architecture of LLMAS ‣ 3 Overview ‣ AgentLens: Visual Analysis for Agent
    Behaviors in LLM-based Autonomous Systems"), the following 4 design requirements
    can be summarized.'
  prefs: []
  type: TYPE_NORMAL
- en: R1\. Provide suitable generality of information for different analysis targets.
    During the evolution of LLMAS, a significant volume of information is continuously
    generated, which is overwhelming for users to comprehend. While the current 2D
    graphical interface of Reverie provides a fixed visual abstraction, many users
    express their desire to change the generality of presented information to better
    match their current analysis target. For instance, users want to scan summarized
    agent traces across a large time scale when they analyze the long-term relationship
    among several agents, while they prefer a detailed presentation of an agent’s
    operations when they analyze how the agent performs a certain task. Therefore,
    the system should provide users with flexible levels of abstraction for the generated
    information of LLMAS, and allows users to reveal details according to their analysis
    target.
  prefs: []
  type: TYPE_NORMAL
- en: R2\. Present agents’ transition of physical location and thought content. The
    physical and mental changes of agents play a vital role in driving and reflecting
    the evolution of the entire LLMAS. Nevertheless, currently, users can only stare
    at the re-playable recording to see if there is a location transition of the agent
    and check the raw execution log to find when the agent starts to think about a
    certain idea, which is inefficient and error-prone. Therefore, the system should
    provide visual emphasis on agents’ transition of location and highlight the time
    points the agent starts to think about a topic the user wishes to explore.
  prefs: []
  type: TYPE_NORMAL
- en: R3\. Underscore possible causes of agent behaviors. When users become interested
    in a certain behavior of the agent, they usually want to investigate the cause
    or consequence of this behavior. However, an agent’s behavior can be influenced
    not only by its current perception and thoughts but also by the memory of its
    past behavior. It is tedious and unreliable for users to switch the replayable
    recording back and forth to locate the cause of the behaviors of certain agents.
    Therefore, the system should provide a mechanism to mine the possible causes of
    an agent’s behaviors and highlight them for users’ investigation.
  prefs: []
  type: TYPE_NORMAL
- en: R4\. Explicate the context of LLM invocation. LLM plays a crucial role as the
    core of the LLMAS, which is frequently invocated to make cognitive decisions for
    agents. To inform the background for making a certain decision, the preceding
    contextual information is organized in a specific manner with a customized template
    and then sent as a prompt to the LLM. Therefore, to help users understand how
    and why a decision is made by an agent, the system should present the decisions
    made by LLM and explicate the context of its invocation. Moreover, it is desirable
    to provide visual enhancement to help users trace how the context information
    is collected from previous agent behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Approach Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0d9fb9620e76b8afc34edebb7ff49efc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The workflow of our approach consists of three major steps. (A) Collect
    raw execution log of events from the LLMAS evolution process. (B) Establish a
    behavior structure with hierarchical summarization and a cause trace method. (C)
    Provide an interactive user interface for visual exploration and analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In alignment with the aforementioned design requirements, we designed AgentLens,
    a proof-of-concept system dedicated to visualizing agent behaviors during the
    LLMAS evolution. The workflow of our approach is depicted in [Fig. 3](#S3.F3 "In
    3.3 Approach Overview ‣ 3 Overview ‣ AgentLens: Visual Analysis for Agent Behaviors
    in LLM-based Autonomous Systems"). Users can utilize logging codes to log their
    LLMAS evolution process and capture raw events executed by agents. Based on these
    raw events, we establish a hierarchical structure to summarize agent behaviors
    in different granularity and trace possible causal relationships among their behaviors
    ([section 4](#S4 "4 Behavior Structure Establishment ‣ AgentLens: Visual Analysis
    for Agent Behaviors in LLM-based Autonomous Systems")). A user interface and a
    series of interactions are provided to support interactive exploration and analysis
    of the agent behaviors in LLMAS ([section 5](#S5 "5 User Interface ‣ AgentLens:
    Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems")).'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Behavior Structure Establishment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1f5934e9014b5f48c7d9a72392341238.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The behavior structure is established through a three-step pipeline:
    (A) We organize raw events into behaviors, (B) summarize and segment behaviors
    for an agent, and (C) trace causal relationships among behaviors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we introduce a pipeline designed to establish the hierarchical
    behavior structure from raw events generated during the evolution of LLMAS. It
    facilitates the generation of structured data for visualization, achieved via
    summarization and causal analysis of agent behaviors. As shown in [Fig. 4](#S4.F4
    "In 4 Behavior Structure Establishment ‣ AgentLens: Visual Analysis for Agent
    Behaviors in LLM-based Autonomous Systems"), the pipeline consists of three steps:
    (A) processing the raw events and organizing them into behaviors based on the
    common architecture shown in [Fig. 2](#S3.F2 "In 3.1 Common Architecture of LLMAS
    ‣ 3 Overview ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous
    Systems") (R1), (B) summarizing these behaviors and segmenting them in accordance
    with their semantic implications. (R1, R2), and (C) tracing the cause between
    these behaviors by analyzing the correlations among original events (R3, R4).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Behavior Definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: During the evolution of LLMAS, multiple raw events are generated, creating large,
    often chaotic, and obscure text logs with the scaling of agent populations. To
    streamline downstream analysis and visualization efforts, we defined agent behaviors
    as structured representations that encapsulate the sequence of raw events (R1).
  prefs: []
  type: TYPE_NORMAL
- en: 'Drawing upon the system state adopted by most LLMAS architectures, we denote
    the timeline $T$ as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $T_{t}=\langle e_{t-1},\bigcup a_{t-1}[i],\bigcup s_{t}[i]\rangle$ |  |
    (1) |'
  prefs: []
  type: TYPE_TB
- en: where $e_{t-1}$, which imply factual (e.g. duplicate segments generated by prompt
    construction) and semantic (e.g. repeated biased interpretations of the same observation)
    duplications.
  prefs: []
  type: TYPE_NORMAL
- en: 'To address these problems, we synthesize events on $T$ for each agent into
    their behaviors:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $B_{i,t_{0}\cdots t_{1}}=\bigcup_{t\in[t_{0},t_{1}]}s_{t,i}$ |  | (2)
    |'
  prefs: []
  type: TYPE_TB
- en: It refers to the set of operations performed by the $i$ within the temporal
    series T.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Behavior Summarization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ba38913bd6debb948d3eccda6ab3c5e0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The agent behavior is summarized in four stages: (A) Raw Events:
    acquire raw events from the logs to detail the occurrences involving the agent
    along the timeline, including the agent’s location, actions, memory, and conversations.
    (B) Description Generation: organize the raw events and employ models such as
    LLMs to generate concise descriptions of the behaviors. (C) Behavior Embedding:
    translate the behavior descriptions into a sequence of textual embedding vectors.
    (D) Timeline Segmentation: involve the detection of change points within the sequence
    of behavior vectors, followed by the corresponding segmentation of the agent’s
    timeline.'
  prefs: []
  type: TYPE_NORMAL
- en: In various LLMAS, operations manifest in different forms, such as text, images,
    and even physical behaviors in the factory environment. Meanwhile, new behaviors
    of those agents are continuously generated as $T$ <svg id="S4.SS2.p1.10.10.pic6"
    class="ltx_picture" height="14.17" overflow="visible" version="1.1" width="14.17"><g
    transform="translate(0,14.17) matrix(1 0 0 -1 0 0) translate(7.08,0) translate(0,7.08)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -5.28 -4.73)" fill="#000000" stroke="#000000"><foreignobject width="10.57"
    height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">D</foreignobject></g></g></svg>).
    Ultimately, we can summarize a multitude of small behaviors into several noteworthy
    behaviors with segmented timelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Description Generation: We incorporate an external text summarization model,
    which acts as a standalone LLM agent that operates independently of LLMAS. All
    annotated descriptions are concatenated to form a comprehensive model input (i.e.
    prompts for LLM). Given this long text sequence as input, the summarization model
    generates a succinct behavior description, significantly reducing the information
    length while maintaining the original meaning (shown in [Fig. 5](#S4.F5 "In 4.2
    Behavior Summarization ‣ 4 Behavior Structure Establishment ‣ AgentLens: Visual
    Analysis for Agent Behaviors in LLM-based Autonomous Systems"), <svg id="S4.SS2.p2.1.1.pic1"
    class="ltx_picture" height="13.64" overflow="visible" version="1.1" width="13.64"><g
    transform="translate(0,13.64) matrix(1 0 0 -1 0 0) translate(6.82,0) translate(0,6.82)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -4.9 -4.73)" fill="#000000" stroke="#000000"><foreignobject width="9.8"
    height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B</foreignobject></g></g></svg>,
    from Prompt to Response). Concurrently, we prompt that the summarization model
    yields a highly abstract description of the behavior, employing both textual and
    emoji symbols. Textual descriptions serve as the foundation for the forthcoming
    embedding model and emoji symbols are conceived to facilitate subsequent visualization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Behavior Embedding: We further utilize all summarized behavior descriptions,
    embedding them to better grasp the latent semantics, including the inherent similarities
    and hierarchical relationships. To maximize the efficiency of the encoding schema,
    we adopt the text-embedding model²²2https://platform.openai.com/docs/guides/embeddings
    pretrained on large-scale internet text data, renowned for its superior performance,
    cost-effectiveness, and simplicity of use. The summarized behavior descriptions
    are then each encoded into a 1536-dimensional vector, constituting the sequence
    $E_{\text{agent}}$ for each agent. With these powerful embeddings, we can uncover
    the semantic similarity of a single behavior, thereby unlocking the potential
    to tackle a myriad of complex text sequence analyses.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Timeline Segmentation: Considering the data characteristics of the embedding
    sequence $e$ and our design requirements, we employ the Window-based change point
    detection (WIN) algorithm [[80](#bib.bib80)] with the cosine distance measure
    to segment the sequence. This approach is suitable for real-time or streaming
    data contexts, as it allows for incremental updates in response to the arrival
    of new data and exhibits insensitivity to short-term and frequent fluctuations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, to compare two embedding vectors $e_{x}$ are the Euclidean scalar
    product and norm respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $k(e_{x},e_{y}):=\frac{\langle e_{x}&#124;e_{y}\rangle}{\&#124;e_{x}\&#124;\&#124;e_{y}\&#124;}$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'Then we recall the cost $c(\cdot)$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $c(e_{a..b})=\sum_{t=a+1}^{b}k(e_{t},e_{t})-\frac{1}{b-a}\sum_{s,t=a+1}^{b}k(e_{s},e_{t})$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'WIN utilizes two sliding windows that traverse the data stream. By comparing
    the statistical properties of the signals within each window, a discrepancy measure
    is obtained based on the cost function $c$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $d(e_{u..v},e_{v..w})=c(e_{u..w})-c(e_{u..v})-c(e_{v..w})$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: The discrepancy $d$.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/df584891e1f5d14b6f3949079d58c4e1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The timeline segmentation results of an agent in Reverie. The x-axis
    denotes the timeline, while the y-axis corresponds to the value of the principle
    PCA component of agent behavior embedding at each time point.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Fig. 6](#S4.F6 "In 4.2 Behavior Summarization ‣ 4 Behavior Structure Establishment
    ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems")
    provides an illustrative example of the timeline segmentation process. Here we
    try to segment the timeline of a writer agent in the Reverie environment. The
    agent’s entire morning schedule is shown in ( [Fig. 6](#S4.F6 "In 4.2 Behavior
    Summarization ‣ 4 Behavior Structure Establishment ‣ AgentLens: Visual Analysis
    for Agent Behaviors in LLM-based Autonomous Systems"), <svg id="S4.SS2.p9.1.1.pic1"
    class="ltx_picture" height="14.06" overflow="visible" version="1.1" width="14.06"><g
    transform="translate(0,14.06) matrix(1 0 0 -1 0 0) translate(7.03,0) translate(0,7.03)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -5.19 -4.73)" fill="#000000" stroke="#000000"><foreignobject width="10.38"
    height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">A</foreignobject></g></g></svg>),
    spanning from midnight to noon, encompassing 4000 time points (0$\to$4000) on
    the timeline. To facilitate an intuitive understanding of the segmentation result,
    we conducted principal component analysis (PCA) on the embedding of behavior at
    each time point, and used the y-axis to encode the values of the primary PCA components,
    resulting in the orange line plot presented in [Fig. 6](#S4.F6 "In 4.2 Behavior
    Summarization ‣ 4 Behavior Structure Establishment ‣ AgentLens: Visual Analysis
    for Agent Behaviors in LLM-based Autonomous Systems").'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see in ( [Fig. 6](#S4.F6 "In 4.2 Behavior Summarization ‣ 4 Behavior
    Structure Establishment ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based
    Autonomous Systems"), <svg id="S4.SS2.p10.1.1.pic1" class="ltx_picture" height="14.06"
    overflow="visible" version="1.1" width="14.06"><g transform="translate(0,14.06)
    matrix(1 0 0 -1 0 0) translate(7.03,0) translate(0,7.03)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -5.19 -4.73)" fill="#000000"
    stroke="#000000"><foreignobject width="10.38" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">A</foreignobject></g></g></svg>), by applying
    the segmentation algorithm (with N=5 as an example), this period is summarized
    into five main behaviors (“sleep and plan”, “revisiting previous work”, etc.).
    Moreover, if we re-apply the timeline segmentation algorithm to the “dedicated
    writing” behavior, which spans time points 2251$\to$3177 ([Fig. 6](#S4.F6 "In
    4.2 Behavior Summarization ‣ 4 Behavior Structure Establishment ‣ AgentLens: Visual
    Analysis for Agent Behaviors in LLM-based Autonomous Systems"), <svg id="S4.SS2.p10.3.3.pic2"
    class="ltx_picture" height="13.64" overflow="visible" version="1.1" width="13.64"><g
    transform="translate(0,13.64) matrix(1 0 0 -1 0 0) translate(6.82,0) translate(0,6.82)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -4.9 -4.73)" fill="#000000" stroke="#000000"><foreignobject width="9.8"
    height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B</foreignobject></g></g></svg>)
    on the timeline, we can further divide it into five sub-behaviors (“gather ideas”,
    “brainstorm”, etc.). Note that all these sub-behaviors can be considered as “dedicated
    writing”, while exhibiting more subtle distinctions among them.'
  prefs: []
  type: TYPE_NORMAL
- en: Another observation to note is that in the line plot formed by PCA principal
    component values, there are some peaks. These peaks occur because the agent executes
    specific operations at this time point, such as generating new memories or perceiving
    new objects. However, these operations do not have a lasting impact on the agent’s
    ongoing behavior. Therefore, they are usually regarded as tiny behaviors contained
    in their parent behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Cause Tracing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Within a complex timeline, any agent event is influenced by both its internal
    memory and interactions with the external environment. By tracing the causal factors
    of these events, users can gain valuable insights into agent behaviors (R3) and
    LLM invocation for decision-making (R4), thereby improving the credibility and
    interpretability of LLMAS.
  prefs: []
  type: TYPE_NORMAL
- en: Existing works[[10](#bib.bib10)] primarily rely on log debugging to explicitly
    reveal the origins of agents’ operations. However, these methods place an additional
    cognitive burden on users due to the need for manual tracing and often fail to
    capture implicit causal relationships. For instance, current thinking can be influenced
    by observations over a long time steps. To efficiently trace the behavior causes,
    we propose a two-fold provenance tracing method to mine the causal relationships
    between underlying events within the behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Explicit Causes: It refers to the distinct and observable causal relationships
    that can be directly discerned from raw event logs, explicitly delineating the
    direct influence relationships between operations. For example, in open-source
    agent creation frameworks like Langchain[[34](#bib.bib34)] and AgentVerse[[17](#bib.bib17)],
    mechanisms have been implemented to index attributes of agent memory, facilitating
    direct backtracking to the relevant source operations upon the invocation of an
    agent’s memory. When such explicit causal chains are completed in LLMAS, users
    can thus obtain these records through raw event logs and transmit them to AgentLens.
    AgentLens utilizes these logs as input to facilitate the analysis of downstream
    tasks for users.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Implicit Causes: Throughout the evolution of LLMAS, the agents’ invocations
    of historical operations are not always documented, but rather are expressed through
    complex intermediate variables or latent patterns within the program. To capture
    these implicit causal relationships, we conduct relevance detection based on the
    text similarities (as in [eq. 3](#S4.E3 "In 4.2 Behavior Summarization ‣ 4 Behavior
    Structure Establishment ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based
    Autonomous Systems")) between the textual log of these operations themselves,
    thereby revealing the latent connections between events. To strike a balance between
    uncovering potential causal relationships and preventing information overload
    for users, we define a similarity threshold $\delta$.'
  prefs: []
  type: TYPE_NORMAL
- en: After the extraction of both explicit and implicit causes among operations is
    completed, we have ascertained every possible pair $<o_{src},o_{res}></math>.
    The connections between operations can be elevated to the connection between the
    corresponding behaviors in a bottom-up fashion, in accordance with the definition
    of behavior outlined in [section 4.1](#S4.SS1 $</foreignobject></g></g></svg>).
  prefs: []
  type: TYPE_NORMAL
- en: 'Agent Interaction Analysis: Each agent in the Outline View is represented as
    a uniquely colored curve, whose x-axis encodes the system time point and y-axis
    encodes the location of the agent, depicting the transition of the location of
    each agent (R2). When several agents are in the same time and location, they can
    have interactions (e.g. conversations, collaborations, or conflicts) with each
    other. Since these interactions usually play a crucial role in affecting the LLMAS’s
    evolution, we highlight them by filling the area among the corresponding segment
    of agent curves. Users can click an interaction area of interest to check the
    integration details ([Fig. 1](#S0.F1 "In AgentLens: Visual Analysis for Agent
    Behaviors in LLM-based Autonomous Systems"), <svg id="S5.SS1.p3.1.pic1" class="ltx_picture"
    height="18.61" overflow="visible" version="1.1" width="18.61"><g transform="translate(0,18.61)
    matrix(1 0 0 -1 0 0) translate(9.31,0) translate(0,9.31)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -7.13 -3.48)" fill="#000000"
    stroke="#000000"><foreignobject width="14.25" height="11.95" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$A_{2}$</foreignobject></g></g></svg>). Drawing
    inspiration from previous work of storytelling[[82](#bib.bib82), [83](#bib.bib83)],
    we enforce agent curves to get closer if there is an interaction among them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Agent Memory Search: Sometimes users want to conduct exploration about when
    and how the agents start to have thoughts about a specific topic (R2). Therefore,
    we provide a search box in the top right corner of the view, allowing users to
    add keywords related to the topic they want to explore. Whenever a keyword is
    added, the points on the agent curves corresponding to time points associated
    with relevant memory will be highlighted ([Fig. 1](#S0.F1 "In AgentLens: Visual
    Analysis for Agent Behaviors in LLM-based Autonomous Systems"), <svg id="S5.SS1.p4.1.pic1"
    class="ltx_picture" height="18.61" overflow="visible" version="1.1" width="18.61"><g
    transform="translate(0,18.61) matrix(1 0 0 -1 0 0) translate(9.31,0) translate(0,9.31)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -7.13 -3.48)" fill="#000000" stroke="#000000"><foreignobject width="14.25"
    height="11.95" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$A_{3}$</foreignobject></g></g></svg>).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Agent View
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When users notice a specific phenomenon or behavior from the Outline View and
    wish to further explore it, they can click on the corresponding time point on
    an agent curve to access more details (R1) in the Agent View.
  prefs: []
  type: TYPE_NORMAL
- en: 'Agent Characteristic: A complex LLMAS typically contains agents with different
    characteristics. For example, agents might be assigned different roles and goals,
    which are usually realized through prompt engineering or LLM fine-tuning. Since
    these details are important for users to understand and infer an agent’s behavior,
    we display them on the left panel of the Agent View ([Fig. 1](#S0.F1 "In AgentLens:
    Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"), <svg id="S5.SS2.p2.1.pic1"
    class="ltx_picture" height="19.19" overflow="visible" version="1.1" width="19.19"><g
    transform="translate(0,19.19) matrix(1 0 0 -1 0 0) translate(9.59,0) translate(0,9.59)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -7.53 -3.48)" fill="#000000" stroke="#000000"><foreignobject width="15.06"
    height="11.95" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$B_{1}$</foreignobject></g></g></svg>).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Time Point Revealing: On the right panel of Agent View, we provide users with
    a timeline ([Fig. 1](#S0.F1 "In AgentLens: Visual Analysis for Agent Behaviors
    in LLM-based Autonomous Systems"), <svg id="S5.SS2.p3.1.pic1" class="ltx_picture"
    height="19.19" overflow="visible" version="1.1" width="19.19"><g transform="translate(0,19.19)
    matrix(1 0 0 -1 0 0) translate(9.59,0) translate(0,9.59)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -7.53 -3.48)" fill="#000000"
    stroke="#000000"><foreignobject width="15.06" height="11.95" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$B_{2}$</foreignobject></g></g></svg>) (R4);
    If the user clicks ![[Uncaptioned image]](img/117e629cdab57bc6fe842e846ac345cb.png),
    a description panel will pop up to show the texts stored into the memory at this
    operation; If the user clicks ![[Uncaptioned image]](img/98f341a8671571697546d51a5c7477e6.png),
    a description panel will pop up to show what the agent is perceiving from or act
    on the environment.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cause Tracing: In addition to obtaining detailed behavioral information about
    agents, users also need to locate and analyze the reasons behind these agent behaviors.
    Whenever the user clicks an operator icon in the Agent View, the system will utilize
    the cause trace method described in Section [4.3](#S4.SS3 "4.3 Cause Tracing ‣
    4 Behavior Structure Establishment ‣ AgentLens: Visual Analysis for Agent Behaviors
    in LLM-based Autonomous Systems") to find previous operators that potentially
    have an intrinsic relationship with the current operation and highlight their
    corresponding time point on the Agent View (R3). We use edges with orange color
    to connect the selected operator and their predecessors. Since the agent behaviors
    could be affected by previous operations a long time ago, we provide users with
    a mini-map to visualize the point of the current operation and its related predecessors
    across the whole timeline ([Fig. 1](#S0.F1 "In AgentLens: Visual Analysis for
    Agent Behaviors in LLM-based Autonomous Systems"), <svg id="S5.SS2.p4.1.pic1"
    class="ltx_picture" height="19.19" overflow="visible" version="1.1" width="19.19"><g
    transform="translate(0,19.19) matrix(1 0 0 -1 0 0) translate(9.59,0) translate(0,9.59)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -7.53 -3.48)" fill="#000000" stroke="#000000"><foreignobject width="15.06"
    height="11.95" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$B_{4}$</foreignobject></g></g></svg>)
    (R1). Based on this mini-map, users can switch back and forth between the cause
    and result across the timeline more easily.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Monitor View
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'LLMAS typically provides a graphical representation of the dynamic simulation.
    It could be re-playable for 2D video or 3D, contingent upon the LLMAS evolution
    logs provided by the user for AgentLens. This visual representation transforms
    abstract simulation data into perceptually friendly visual elements, which helps
    users understand LLMAS and verify their analysis more intuitively. However, manually
    switching between different locations and time points can be tedious and interrupt
    the user’s analysis flow. Therefore, we provide the Monitor View to support fluent
    adjustment of the panoramic visualization of LLMAS ([Fig. 1](#S0.F1 "In AgentLens:
    Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"), <svg id="S5.SS3.p1.1.1.pic1"
    class="ltx_picture" height="13.75" overflow="visible" version="1.1" width="13.75"><g
    transform="translate(0,13.75) matrix(1 0 0 -1 0 0) translate(6.88,0) translate(0,6.88)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -5 -4.73)" fill="#000000" stroke="#000000"><foreignobject width="9.99"
    height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">C</foreignobject></g></g></svg>)
    based on users’ current focus and demand for context.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Focus Switching: Whenever the user clicks a time point on agent curve from
    the Outline View or a time point from the Agent View, the Monitor View will automatically
    switch to the location of that agent at that time point, providing a corresponding
    concrete visualization to complement the other two views (R1).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Context Revealing: The Monitor View also supports spatial and temporal context
    revealing to help users better comprehend the current focus point. As for the
    spatial context, the user can scroll the mouse wheel to adjust the level of scope,
    ranging from a macroscopic view of the entire LLMAS to a microscopic focus on
    a single agent. As for the temporal context, whenever the user changes the focus
    point from time point A to time point B, they can right-click the mouse to replay
    a fast-forward recording of that period of time in the Monitor View.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Usage Scenarios
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '6.1 Scenario A: Information Diffusion'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5996e753ed2158aa2b81f3c78cf43c4c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The first usage scenario showcases the support AgentLens provides
    to the user in exploring social patterns like Information Diffusion. (A) The user
    gleans the characteristics of each agent via the Agent View. (B) Proceeding to
    the Outline View, the user searches for the keyword “party”, discovering several
    related memory points generated in several conversations. (C) Utilizing the Agent
    View, the user delves into the origins of these conversational patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This case demonstrates how our system helps users understand the patterns of
    agent behaviors in LLMAS. In the initialization phase, the user adds the information
    “Organize Valentine’s Day party at Hobbs Coffee on the evening of February 14th”
    to the characteristic ([Fig. 7](#S6.F7 "In 6.1 Scenario A: Information Diffusion
    ‣ 6 Usage Scenarios ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based
    Autonomous Systems"), <svg id="S6.SS1.p1.1.pic1" class="ltx_picture" height="18.61"
    overflow="visible" version="1.1" width="18.61"><g transform="translate(0,18.61)
    matrix(1 0 0 -1 0 0) translate(9.31,0) translate(0,9.31)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -7.13 -3.48)" fill="#000000"
    stroke="#000000"><foreignobject width="14.25" height="11.95" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$A_{1}$</foreignobject></g></g></svg>) of the
    agent Isabella Rodriguez (IR) and wishes to observe the evolution of the system
    on February 13th.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To focus on the theme of the party, the user searches for the occurrence of
    the keyword “party” ([Fig. 7](#S6.F7 "In 6.1 Scenario A: Information Diffusion
    ‣ 6 Usage Scenarios ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based
    Autonomous Systems"), <svg id="S6.SS1.p2.1.pic1" class="ltx_picture" height="13.64"
    overflow="visible" version="1.1" width="13.64"><g transform="translate(0,13.64)
    matrix(1 0 0 -1 0 0) translate(6.82,0) translate(0,6.82)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -4.9 -4.73)" fill="#000000"
    stroke="#000000"><foreignobject width="9.8" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">B</foreignobject></g></g></svg>) in the agent’s
    memory and follows IR’s timeline for observation. The user discovers that the
    message primarily spreads during IR’s conversations with others. Furthermore,
    the user finds the “party” memory highlight surfacing in the conversation between
    Ayesha Khan (AK) and John Smith (JS). Upon examining their dialogue ([Fig. 7](#S6.F7
    "In 6.1 Scenario A: Information Diffusion ‣ 6 Usage Scenarios ‣ AgentLens: Visual
    Analysis for Agent Behaviors in LLM-based Autonomous Systems"), <svg id="S6.SS1.p2.2.pic2"
    class="ltx_picture" height="19.19" overflow="visible" version="1.1" width="19.19"><g
    transform="translate(0,19.19) matrix(1 0 0 -1 0 0) translate(9.59,0) translate(0,9.59)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -7.53 -3.48)" fill="#000000" stroke="#000000"><foreignobject width="15.06"
    height="11.95" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$B_{1}$</foreignobject></g></g></svg>),
    during which IR extends an invitation to AK to participate in the party preparation.'
  prefs: []
  type: TYPE_NORMAL
- en: With the assistance of AgentLens, the user successfully pinpoints an instance
    of information diffusion from a primary disseminator IR to a secondary one AK,
    then gradually diffusing towards other agents. From the Agent View, users discover
    that with the increase in both secondary propagators and the number of conversations
    related to “party”, the speed of “party” diffusion throughout the small town significantly
    accelerates.
  prefs: []
  type: TYPE_NORMAL
- en: '6.2 Scenario B: Unexpected Social Patterns'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/56c67e002f2a8bc00148019dd7e97106.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The second usage scenario presents how AgentLens aids users in explaining
    an unexpected agent behavior. (A) The user identifies some unexpected agent behaviors
    in Outline View, like an agent participating in information dissemination without
    engaging in a related conversation. Upon validation through Monitor View, the
    user determines that this pattern corresponds to the eavesdropping behavior of
    the agent. (B) The user uses Agent View to investigate the reasons behind the
    agent’s reluctance to participate in the discussion. Finally, the user discovers
    that a certain decision operation at the time point results in the behavior.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this scenario, the user uncovers an unexpected pattern of information diffusion:
    eavesdropping.'
  prefs: []
  type: TYPE_NORMAL
- en: 'During the observation of the “party” propagation process ([Fig. 8](#S6.F8
    "In 6.2 Scenario B: Unexpected Social Patterns ‣ 6 Usage Scenarios ‣ AgentLens:
    Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"), <svg id="S6.SS2.p2.1.pic1"
    class="ltx_picture" height="18.61" overflow="visible" version="1.1" width="18.61"><g
    transform="translate(0,18.61) matrix(1 0 0 -1 0 0) translate(9.31,0) translate(0,9.31)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -7.13 -3.48)" fill="#000000" stroke="#000000"><foreignobject width="14.25"
    height="11.95" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$A_{1}$</foreignobject></g></g></svg>).
    The user infers that SM comes to know about the “party” by eavesdropping on others’
    conversations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The user seeks to investigate why SM does not join the conversation. The user
    expands the corresponding time point([Fig. 8](#S6.F8 "In 6.2 Scenario B: Unexpected
    Social Patterns ‣ 6 Usage Scenarios ‣ AgentLens: Visual Analysis for Agent Behaviors
    in LLM-based Autonomous Systems"), <svg id="S6.SS2.p3.1.pic1" class="ltx_picture"
    height="13.64" overflow="visible" version="1.1" width="13.64"><g transform="translate(0,13.64)
    matrix(1 0 0 -1 0 0) translate(6.82,0) translate(0,6.82)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -4.9 -4.73)" fill="#000000"
    stroke="#000000"><foreignobject width="9.8" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">B</foreignobject></g></g></svg>) in the Agent
    View and identifies the Decision Operation that determines SM’s choice not to
    participate in the discussion. The prompt dispatches to the LLM incorporated agent
    settings pertaining to SM, like “SM is IR’s friend” and “Sam is not very familiar
    with GM”, in addition to the immediate observations made by SM, such as “IR and
    GM are presently engaged in a conversation” among other pieces of prompt input.
    It is the response returned by the LLM, based on the prompt, making the decision
    for SM’s subsequent action that he determines not to join the conversation.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 User Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We conducted a user study to evaluate the performance of AgentLens in enhancing
    LLMAS analysis. The study was specially designed to assess the comprehensive efficiency,
    effectiveness, and usability of the system. We also examine the analytical support
    provided by our system compared to a baseline system, which replicates the visual
    approach in existing LLMAS works.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Participants
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To prevent participants from having prior knowledge of the system before evaluation,
    we recruited 14 new participants (denoted as P1-P14) from a local university who
    had not been involved in the design requirements phase of this study, thereby
    enhancing the assessment validity and the results generalizability. These participants
    have diverse academic backgrounds, with most being undergraduate and graduate
    students from fields such as computer science, software engineering, and sociology.
    Some of them are developers with a high level of expertise in LLMAS, while others
    only have had direct interaction with LLMAS.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Baseline Systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A baseline system³³3https://reverie.herokuapp.com/arXiv_Demo/# has been set
    up for direct comparison with our proposed system. Both the baseline system and
    our system utilize the log data generated by Reverie[[10](#bib.bib10)], which
    records the interactions and memory logs of agents within the system during the
    simulation process.
  prefs: []
  type: TYPE_NORMAL
- en: The baseline provides a view for replaying past events with plain text descriptions
    of agent settings and behaviors, which simulates a typical LLMAS panoramic visualization.
    Firstly, it features a monitoring interface that uses a flat map as the background.
    This allows users to replay and observe the agent positions and behavior descriptions
    at different time points through a timeline. Secondly, the system offers a textual
    representation of the current events for each agent, including the agent’s location,
    the action in progress, and the ongoing dialogue (if any). Finally, the system
    also provides a pure textual display of all events in each agent’s evolutionary
    process, encompassing the agent’s personality, complete memory records, and event
    sequences. These features enable users to understand the agent behaviors and status
    and delve into their evolutionary process.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Procedure and Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Introduction (10 min): Initially, we provided a concise overview of the research,
    including the motivation and methodology. We then collected basic personal information
    from them, including their gender, age, and occupation. In addition, we obtained
    authorization to record their behaviors during the subsequent task analysis. Finally,
    we describe the characteristics of the individual views in both baseline and AgentLens
    in detail and demonstrate their practical use in a specific scenario.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Task-based analysis (40 min): In this stage, participants were required to
    undertake 2 groups of analytical tasks (refer to [Figures 9](#S7.F9 "In 7.4.1
    Individual Behavior Analysis ‣ 7.4 Task Completion Analysis ‣ 7 User Evaluation
    ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems")
    and [10](#S7.F10 "Fig. 10 ‣ 7.4.2 Emergent Phenomena Identification ‣ 7.4 Task
    Completion Analysis ‣ 7 User Evaluation ‣ AgentLens: Visual Analysis for Agent
    Behaviors in LLM-based Autonomous Systems")), designed to evaluate the system’s
    overall effectiveness and usability. Participants were required to fulfill tasks
    for each system, with the duration and accuracy of task completion being recorded.
    To obviate the potential for participants to replicate responses through memorization
    [[84](#bib.bib84)], the sequence in which the two systems were presented was randomized.
    Each task was uniquely tailored for both systems while ensuring an equivalent
    level of challenge.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Semi-structured interview (30 min): To enhance the evaluation of the method
    and interface efficacy, we utilized the five-point Likert scale in an 8-item questionnaire.
    Additionally, we employed the System Usability Scale (SUS)[[85](#bib.bib85)] to
    evaluate the usability of AgentLens. Participants were asked to rate each question
    from 1 (strongly disagree) to 5 (strongly agree) to gauge their agreement levels.
    During the questionnaire process, we encouraged participants to speak freely to
    uncover the reasoning behind their ratings.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.4 Task Completion Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the task-based analysis, we conducted a quantitative comparison between
    AgentLensnd the baseline, focusing on accuracy and task completion time. We developed
    two distinct groups of evaluation tasks to assess the efficacy of 2 systems for
    the analysis of agent behaviors ([Fig. 9](#S7.F9 "In 7.4.1 Individual Behavior
    Analysis ‣ 7.4 Task Completion Analysis ‣ 7 User Evaluation ‣ AgentLens: Visual
    Analysis for Agent Behaviors in LLM-based Autonomous Systems")) and the identification
    of emergent phenomena arising from such behaviors ([Fig. 10](#S7.F10 "In 7.4.2
    Emergent Phenomena Identification ‣ 7.4 Task Completion Analysis ‣ 7 User Evaluation
    ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems")).'
  prefs: []
  type: TYPE_NORMAL
- en: 7.4.1 Individual Behavior Analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'T1 - T6 in [Fig. 9](#S7.F9 "In 7.4.1 Individual Behavior Analysis ‣ 7.4 Task
    Completion Analysis ‣ 7 User Evaluation ‣ AgentLens: Visual Analysis for Agent
    Behaviors in LLM-based Autonomous Systems") are designed with elicit concise answers,
    requiring participants to rapidly comprehend the fundamental characteristics and
    behaviors of agents. Based on the analytical target, we categorize this set of
    tasks into 3 classifications. Participants exhibit varying levels of accuracy
    and time expenditure across tasks, however, there was a notable improvement in
    task accuracy ($p=1.2e-3$) with AgentLens.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0b0b1302893071c8d7c689a5f8eb1996.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Statistical result of the accuracy and time consumption for participants
    completing individual behavior analysis tasks using both AgentLens and the baseline
    system.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Single-agent analysis (T1 - T2): This set of tasks focuses on the system’s
    enhancement of simple information analysis about individual agents. Without compromising
    task accuracy, AgentLens decreased time consumption by 33% for T1 ($\mu_{AgentLens}=8.02,\mu_{baseline}=12.03$)
    compared to the baseline system. The visual representation of agent characteristics
    in the Agent View eliminates the need for search operations in T1\. Furthermore,
    the event summarization method helps participants quickly identify agent behaviors,
    eliminating the need to sift through complex log records to complete T2.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-agent analysis (T3 - T4): This set of tasks demonstrates the system’s
    effect in assisting participants with the analysis of interactions between agents.
    It is noteworthy that one participant failed in both two tasks using the baseline
    system due to his incorrect agent selection. AgentLens reduced time consumption
    by 78.3% for T3 ($\mu_{AgentLens}=20.00,\mu_{baseline}=92.20$). The visual encoding
    in AgentLens, particularly in the Outline View, allowed participants to quickly
    derive answers by observing agent interactions including dialogues and cohabitation
    instances.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Behavior Cause analysis (T5 - T6): In this set of tasks, AgentLens demonstrated
    marked improvements over the baseline in facilitating the exploration of the cause
    of agent behaviors. While a part of the participants quickly obtained answers
    using the baseline in T5, AgentLens still provided a 39.4% improvement with the
    topic search feature ($\mu_{AgentLens}=17.83,\mu_{baseline}=29.42$).'
  prefs: []
  type: TYPE_NORMAL
- en: 7.4.2 Emergent Phenomena Identification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'T7-T9 in [Fig. 10](#S7.F10 "In 7.4.2 Emergent Phenomena Identification ‣ 7.4
    Task Completion Analysis ‣ 7 User Evaluation ‣ AgentLens: Visual Analysis for
    Agent Behaviors in LLM-based Autonomous Systems") are designed to correspond to
    three categories of emergent phenomena arising from agent autonomy, which is not
    explicitly pre-programmed in LLMAS. These tasks are more complex for the participants,
    requiring back-and-forth exploration and analysis through multiple steps. We invited
    evaluators to assess the accuracy of the participant’s responses. Concurrently,
    we observe that AgentLens demonstrates capabilities in complex analytical tasks
    that the traditional baseline failed to achieve, particularly in the exploration
    of emergent behaviors arising from agent autonomy.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/43ab8228481412c3dfc0878c7373fcdf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Statistical result of the accuracy and time consumption for emergent
    phenomena identification tasks using both AgentLens and the baseline system.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Topic propagation (T7): Participants are tasked with identifying the propagation
    path of a specific topic, such as “a Valentine’s Day party will be held” or “someone
    is preparing the selection for mayor”. Nearly all participants consider the task
    to be impossible while utilizing the baseline, as “this task is akin to searching
    for a needle in a haystack” (P11). When utilizing AgentLens, the majority of participants
    swiftly opted for the Agent Memory Search within the Outline View to conduct searches
    on the propagated topics. Leveraging the representation of Agent Interaction Analysis
    within the view, participants could easily explore the propagation paths. Although
    the propagation path participants were asked to identify has multiple branches
    and complex scenarios, 9 participants completed the task using AgentLens.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Agent congregation (T8): Participants are required to identify a congregation
    phenomenon, defined as more than three agents engaging in the same behavior at
    the same location, and participants should explain the reason behind it. While
    using the baseline, participants were compelled to conduct extended observations
    and iterative replays of the recorded video. Despite locating the participants
    of the aggregation, they remained unable to ascertain the underlying causes of
    the phenomena. Through the interactivity among the three views of AgentLens, particularly
    the design of Monitor View and Outline View, participants were able to rapidly
    detect aggregation phenomena. Coupled with the method of behavior summarization,
    9 participants successfully provided explanations for the aggregations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unexpected behavior (T9): Participants were tasked with identifying and rationalizing
    unexpected agent behaviors across two systems. When using the baseline system,
    they noted that agent behaviors appeared uniformly logical and coherent. Additionally,
    the requisite alternation between observing multiple agents hindered their analytical
    process, thereby increasing the difficulty of detecting unexpected phenomena.
    With the assistance of AgentLens, this task became more manageable. P5 identified
    through Outline View that ”agent RP did not leave his room throughout the entire
    day.” He traced the cause using Agent View and discovered that the agent had received
    a plan that did not require leaving the house from LLM during the planning phase
    for that day. Another participant P8 noticed in Agent View that agent TT was able
    to observe the activities of agent IR in the adjacent room, and this observation
    influenced TT’s subsequent decisions. The user suggested that this phenomenon
    should be addressed in the LLMAS, as in human society, individuals do not possess
    the ability to see through walls.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.5 Semi-structured Interview Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We posed 8 interview questions in [Fig. 11](#S7.F11 "In 7.5 Semi-structured
    Interview Analysis ‣ 7 User Evaluation ‣ AgentLens: Visual Analysis for Agent
    Behaviors in LLM-based Autonomous Systems")) and a SUS questionnaire([Fig. 12](#S7.F12
    "In 7.5.3 Usability ‣ 7.5 Semi-structured Interview Analysis ‣ 7 User Evaluation
    ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"))
    to participants. Evaluating the results of the questionnaire with feedback obtained
    during the interview, we reported the performance of AgentLens including its effectiveness
    and usability, offering insights into its practical application.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fa2aee7ffb0a69cc65610e15842f987d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: The questionnaire with results showing the efficacy of our method
    and interfaces.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.1 Pipeline Effectiveness
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: All participants agreed that the event summary is informative (Q1) and helpful.
    P10 commented, “The summaries are quite accurate. I can quickly locate the events
    and understand the evolution of an agent throughout the day with the help of the
    story-like subheadings.” P1 felt impressed with the way of summarizing the agent’s
    status, “like having an agent helping me monitor this LLMAS.”
  prefs: []
  type: TYPE_NORMAL
- en: Most participants agreed that the results of the cause trace met their expectations
    (Q2). They are willing to utilize the traced events to help analyze their interested
    events. For instance, P3 intended to incorporate the agent characteristics into
    the cause trace process. P5 pointed out that the cause trace served to “unveil
    the black box of agent behavior.”.
  prefs: []
  type: TYPE_NORMAL
- en: The hierarchical structure received unanimous endorsement from all participants
    (Q3). They all admitted that the hierarchical structure elucidated the level at
    which they could retrieve information. Especially in the analysis of complex phenomena,
    the behavior hierarchical structure can “effectively reduce information density”(P6)
    and “help me quickly focus on key phenomena”(P10). Nonetheless, P12, who was relatively
    inexperienced with the LLMAS, expressed a need for more “user-oriented guidance”.
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.2 Visual Effectiveness
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Outline View was appreciated by the participants for agents behavior analysis
    (Q4). It helps participants circumvent the risk of “getting lost in the complex
    and chaotic agent lines” (P1) by summarizing and visualizing the agent’s status.
    The interactive design, such as the click-to-highlight and view-details features,
    is “remarkably user-friendly and intuitive” (P11). In addition, the encoding of
    interactions among agents also received positive feedback from users (Q5). The
    gray box, which intertwines two lines to represent agent dialogues, “stands out
    right away” (P2). Some participants (P5, P7) indicated that they were accustomed
    to first spotting interesting agent dialogues in the relatively compact view,
    then zooming in to delve into more details. P7, who completed the task of identifying
    congregation phenomenon(T8) expeditiously, attributes the success to ”the visualization
    is trying to aggregate the curves of agents who are interacting with each other.”
    P9 commented, “If I can dynamically adjust the positions of the agents in the
    view, the layout can better match my expectation.”
  prefs: []
  type: TYPE_NORMAL
- en: The Monitor View was found to be useful for validating the observation (Q6).
    Several participants indicated that after observing the Monitor View, they gained
    more confidence in the results of their analysis. P10 mentioned, “The monitor
    screen adjusts as I shift my focus in different views, kind of like video software,
    but it offers much more details than regular video playback.” P10 commended the
    interaction of this view in relation to the other two especially in complex tasks,
    “This interactive responsiveness is beneficial during my iterative analysis process.”
    P5 suggested that the Monitor View could be more beneficial if it could “display
    the location information of other unfocused agents”.
  prefs: []
  type: TYPE_NORMAL
- en: The Agent View provides strong support for participants to analyze individual
    agent characteristics (Q7) and the causal relationships between agent behaviors
    (Q8). When observing agents of interest, they can “quickly understand the agent’s
    personality and style of action” (P1). P6 said, “The retrospective analysis is
    intuitive, but the individual timeline is too long. It would be better if I could
    explore the causes without having to drag the view around.” P4 praised the minimap
    in the Agent View, “When I was trying to understand agent behaviors, I love using
    the minimap’s navigation. It helped me find the causal links fast with those cool
    summary emojis.” P13 commented, “Developers should think about adding the agent
    view to their projects. Without it, agent behaviors might not seem convincing.”
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.3 Usability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9fc025516d391e712a6fccebc9687649.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: The SUS questionnaire with results showing the usability of AgentLens.'
  prefs: []
  type: TYPE_NORMAL
- en: We employed the SUS questionnaire to assess the system usability, thereby reporting
    users’ cognitive load with AgentLens. Several developers among the participants
    conveyed not only their intent to use AgentLens in the future but also to consider
    its integration within their LLMAS development, which has significantly encouraged
    us.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, participants provided positive comments on the usability. P9 lauded
    the workflow of AgentLens, “I thoroughly enjoyed the freedom of exploration the
    system facilitated.”. P13 noted, “The interaction is very fluid”, but revealed
    a longing for automated assistance during complex analytical tasks: “It would
    be perfect if the system could understand the type of task I want to analyze from
    just a few of my clicks.” Moreover, participants expressed their confidence and
    enjoyment when using AgentLens. However, several participants indicated that the
    system necessitates a measure of preliminary technical knowledge, despite acknowledgment
    from P2 that “this is principally due to the intrinsic complexity of LLMAS itself.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ultimately, we achieved an average score of 67.5 on the SUS questionnaire(refer
    to [Fig. 12](#S7.F12 "In 7.5.3 Usability ‣ 7.5 Semi-structured Interview Analysis
    ‣ 7 User Evaluation ‣ AgentLens: Visual Analysis for Agent Behaviors in LLM-based
    Autonomous Systems")), which we find exhilarating. However, it also serves as
    a reminder of the necessity for future optimization.'
  prefs: []
  type: TYPE_NORMAL
- en: 8 Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we commence by encapsulating the lessons collected from the
    user feedback, including providing comparisons within an agent and enabling modifications
    for system configurations. Subsequently, we deliberate on the generalizability,
    as well as the limitations and future work.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Lessons Learned
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Providing comparison within an agent. During the evaluation process, we recorded
    some specific interaction patterns among the users, although they did not actively
    mention them in the interview. Some users frequently analyzed the behaviors of
    a single agent across various temporal intervals. For instance, they compared
    the behaviors of an agent at 8 a.m. on February 13 with those at the same time
    on February 14\. To facilitate this, they typically delved into the Outline View
    to explore the events associated with the agent at these two distinct time points.
    Observing disparate agent behaviors across separate days, users inferred the existence
    of certain agent behavior patterns. This discovery inspires us to further investigate
    strategies for visually “folding” the agent’s timeline, such as overlaying two
    periods of the timeline, thereby aiding users in rapidly comparing and encapsulating
    the agent’s behavior patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Enabling modifications for system configurations. Participants appreciated
    the aid provided by the novel behavior summarization method proposed in our study,
    which effectively mitigates information overload. Nevertheless, some users demonstrated
    an interest in understanding how these summaries are generated. They endorsed
    the summarization method after we clarified the details, as in [Fig. 5](#S4.F5
    "In 4.2 Behavior Summarization ‣ 4 Behavior Structure Establishment ‣ AgentLens:
    Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"). However,
    they still gave specific requirements, such as customizing the source of the summary
    contents. For example, one participant exhibited indifference towards the agent’s
    location information. Such feedback motivates us to enable users to tailor the
    extraction pipeline in future research, thereby enhancing the usability of the
    exploratory analysis in a user-centric manner.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Generalizability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our work builds upon the existing LLMAS, designed for the surveillance and analysis
    of agent behaviors. While we conduct our research based on Reverie, it can be
    seamlessly integrated into other LLMAS analysis processes. Moreover, the key components
    of our system, such as the Outline View and Agent View, are decoupled from the
    LLMAS implementations. The Monitor View is a representation of the replay monitor
    ubiquitous in most LLMAS. Developers can easily provide their own monitoring snapshots
    to populate this view. Therefore, our work is general to various LLMAS and can
    be used directly by developers in their LLMAS.
  prefs: []
  type: TYPE_NORMAL
- en: Our system’s capabilities extend beyond LLMAS analysis and can be applied to
    a wide range of applications, such as the analysis of multi-person communities
    and the development of open-world games. For the analysis of multi-person communities,
    the Outline View and Monitor View can assist in simultaneously examining numerous
    actions on multiple subject timelines. This enables analysts to rapidly comprehend
    the main behaviors of different entities and their interactions. Within the realm
    of open-world games, the incorporation of the Outline View can aid players in
    exploring non-player characters (NPC) behaviors in an immersive manner. Game developers
    can also utilize the Agent View to analyze and optimize the NPCs in the development
    stages, fostering the creation of more intelligent NPCs.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3 Limitations and Future Work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Despite the encouraging performance of AgentLens, there are several limitations
    and potential areas for further research.
  prefs: []
  type: TYPE_NORMAL
- en: Provide a more flexible interface. The current layout of the agent line and
    position block in the Outline View is pre-computed. Despite considerable efforts
    to minimize the crossover of lines, it remains difficult to avoid, particularly
    as the number of agents and the evolutionary timespan of LLMAS increase. One of
    our future tasks is to provide a more flexible layout for the Outline View, automatically
    reorganizing the view based on the user’s interest regarding agent events.
  prefs: []
  type: TYPE_NORMAL
- en: Allow users to modify pre-configured settings. AgentLens introduces a set of
    pre-configured settings for users, such as the granularity of Timeline Segmentation
    and the similarity threshold for Cause Trace. These configurations optimize the
    exploration experience for users, making better trade-offs between the intricate
    nature of the information and its succinct presentation. Nonetheless, some users
    expressed a desire to modify these presets during the analysis process to facilitate
    more flexible exploration. To accommodate these needs, we plan to incorporate
    a customizable preset panel for users in our system.
  prefs: []
  type: TYPE_NORMAL
- en: Support interactive exploration among different agent execution strategies.
    In this work, we focus on facilitating users’ exploration and analysis of the
    LLMAS operational process. However, this process is significantly influenced by
    agent execution strategies like planning methods and memory mechanisms. For example,
    the agent may choose to first make a high-level plan to divide tasks into several
    sub-tasks that can be completed in different orders, or choose to adopt a depth-first
    strategy that adaptively changes its target based on the incoming information.
    While the design of an effective agent planning strategy is attracting an increasing
    amount of research attention [[17](#bib.bib17), [86](#bib.bib86), [87](#bib.bib87),
    [88](#bib.bib88)], how to interactively analyze the effect of different planning
    strategies in LLMAS is still unexplored. Moreover, analyzing the influence of
    agent memory mechanisms on the agent execution process is an area of considerable
    interest. While currently the agent memory mechanisms are usually hard-coded in
    the LLMAS program, allowing users to interactively modify the agent’s memory content
    or recall strategies and visually examine its downstream effects could be crucial
    for better understanding and optimizing LLMAS.
  prefs: []
  type: TYPE_NORMAL
- en: Extend to multimodal LLMAS. Text-based interaction has been widely adopted in
    most existing LLMAS [[16](#bib.bib16), [10](#bib.bib10), [12](#bib.bib12)] in
    which agents are predicated on textual perception and decision-making. Even embodied
    agents [[19](#bib.bib19), [20](#bib.bib20)] typically transmute the perceived
    multimodal data like imagery and auditory inputs into a textual format for later
    processing. However, with the popularity of multimodal LLMs[[6](#bib.bib6), [89](#bib.bib89)],
    the future may see the emergence of LLMAS in which agents genuinely perceive,
    think, and act based on multimodal data. Future work can explore how the agents
    interact with multimodal data (e.g., image interpretation [[90](#bib.bib90)] and
    creation [[91](#bib.bib91)]) in this authentic multimodal LLMAS.
  prefs: []
  type: TYPE_NORMAL
- en: 9 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work presents a visualization approach for LLMAS, addressing the challenge
    of analyzing complex agent behaviors during LLMAS evolution. We introduce a general
    pipeline that establishes a hierarchical behavior structure from the raw execution
    events of LLMAS, including a behavior summarization algorithm and a cause-tracing
    method. Our system, AgentLens, offers an intuitive and hierarchical representation
    of the evolution of multiple agents, enabling users to interactively investigate
    behavior details and causes. Through two usage scenarios and a user study, we
    have demonstrated the performance of our pipeline and visual designs.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We would like to thank Ke Wang and Minfeng Zhu for their kind help. We also
    would like to thank the anonymous reviewers for their insightful comments. This
    paper is supported by the National Natural Science Foundation of China (62132017,
    62302435), Zhejiang Provincial Natural Science Foundation of China (LD24F020011),
    and “Pioneer” and “Leading Goose” R&D Program of Zhejiang (2024C01167).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] G. A. Agha, *ACTORS - a model of concurrent computation in distributed
    systems*, ser. MIT Press series in artificial intelligence.   MIT Press, 1990.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] N. H. S., “Software agents: an overview,” *The Knowledge Engineering Review*,
    vol. 11, p. 205–244, Jul. 1996.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] M. J. Wooldridge and N. R. Jennings, “Intelligent agents: theory and practice,”
    *The Knowledge Engineering Review*, vol. 10, no. 2, pp. 115–152, Jun. 1995.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] M. Hutter, *Universal artificial intelligence: Sequential decisions based
    on algorithmic probability*.   Springer Science & Business Media, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang,
    S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens,
    A. Askell, P. Welinder, P. F. Christiano, J. Leike, and R. Lowe, “Training language
    models to follow instructions with human feedback,” in *NeurIPS*.   New Orleans,
    USA: PMLR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] OpenAI, “GPT-4 technical report,” *CoRR*, vol. abs/2303.08774, Mar. 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama,
    M. Bosma, D. Zhou, D. Metzler, E. H. Chi, T. Hashimoto, O. Vinyals, P. Liang,
    J. Dean, and W. Fedus, “Emergent abilities of large language models,” *TMLR*,
    vol. 2022, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang,
    X. Chen, Y. Lin, W. X. Zhao, Z. Wei, and J. Wen, “A survey on large language model
    based autonomous agents,” *CoRR*, vol. abs/2308.11432, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang, S. Jin,
    E. Zhou, R. Zheng, X. Fan, X. Wang, L. Xiong, Y. Zhou, W. Wang, C. Jiang, Y. Zou,
    X. Liu, Z. Yin, S. Dou, R. Weng, W. Cheng, Q. Zhang, W. Qin, Y. Zheng, X. Qiu,
    X. Huan, and T. Gui, “The rise and potential of large language model based agents:
    A survey,” *CoRR*, vol. abs/2309.07864, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] J. S. Park, J. C. O’Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S.
    Bernstein, “Generative agents: Interactive simulacra of human behavior,” in *Proc. UIST*.   San
    Francisco, USA: ACM, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] J. Shi, J. Zhao, Y. Wang, X. Wu, J. Li, and L. He, “CGMI: Configurable
    general multi-agent interaction framework,” *CoRR*, vol. abs/2308.12503, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] C. Qian, X. Cong, C. Yang, W. Chen, Y. Su, J. Xu, Z. Liu, and M. Sun,
    “Communicative agents for software development,” *CoRR*, vol. abs/2307.07924,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] S. Hong, X. Zheng, J. Chen, Y. Cheng, J. Wang, C. Zhang, Z. Wang, S. K. S.
    Yau, Z. Lin, L. Zhou, C. Ran, L. Xiao, and C. Wu, “MetaGPT: Meta programming for
    multi-agent collaborative framework,” *CoRR*, vol. abs/2308.00352, Aug. 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] D. A. Boiko, R. MacKnight, and G. Gomes, “Emergent autonomous scientific
    research capabilities of large language models,” *CoRR*, vol. abs/2304.05332,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Gravitas, “AutoGPT,” https://github.com/Significant-Gravitas/AutoGPT,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] J. Lin, H. Zhao, A. Zhang, Y. Wu, H. Ping, and Q. Chen, “AgentSims: An
    open-source sandbox for large language model evaluation,” *CoRR*, vol. abs/2308.04026,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] W. Chen, Y. Su, J. Zuo, C. Yang, C. Yuan, C. Qian, C. Chan, Y. Qin, Y. Lu,
    R. Xie, Z. Liu, M. Sun, and J. Zhou, “AgentVerse: Facilitating multi-agent collaboration
    and exploring emergent behaviors in agents,” *CoRR*, vol. abs/2308.10848, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] C. Zhang, K. Yang, S. Hu, Z. Wang, G. Li, Y. Sun, C. Zhang, Z. Zhang,
    A. Liu, S. Zhu, X. Chang, J. Zhang, F. Yin, Y. Liang, and Y. Yang, “ProAgent:
    Building proactive cooperative AI with large language models,” *CoRR*, vol. abs/2308.11339,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] H. Zhang, W. Du, J. Shan, Q. Zhou, Y. Du, J. B. Tenenbaum, T. Shu, and
    C. Gan, “Building cooperative embodied agents modularly with large language models,”
    *CoRR*, vol. abs/2307.02485, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] X. Zhu, Y. Chen, H. Tian, C. Tao, W. Su, C. Yang, G. Huang, B. Li, L. Lu,
    X. Wang, Y. Qiao, Z. Zhang, and J. Dai, “Ghost in the Minecraft: Generally capable
    agents for open-world environments via large language models with text-based knowledge
    and memory,” *CoRR*, vol. abs/2305.17144, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] D. Hafner, J. Pasukonis, J. Ba, and T. P. Lillicrap, “Mastering diverse
    domains through world models,” *CoRR*, vol. abs/2301.04104, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] A. Mirchev, B. Kayalibay, P. van der Smagt, and J. Bayer, “Variational
    state-space models for localisation and dense 3D mapping in 6 DoF,” in *ICLR*.   Austria:
    OpenReview.net, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] S. Franklin and A. C. Graesser, “Is it an agent, or just a program?: a
    taxonomy for autonomous agents,” in *Proc. ATAL*.   Budapest, Hungary: Springer,
    1996.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar,
    P. Lee, Y. T. Lee, Y. Li, S. M. Lundberg, H. Nori, H. Palangi, M. T. Ribeiro,
    and Y. Zhang, “Sparks of artificial general intelligence: Early experiments with
    GPT-4,” *CoRR*, vol. abs/2303.12712, Mar. 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux, T. Lacroix,
    B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave,
    and G. Lample, “LLaMA: Open and efficient foundation language models,” *CoRR*,
    vol. abs/2302.13971, Feb. 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell *et al.*, “Language models are few-shot learners,”
    *Advances in neural information processing systems*, vol. 33, pp. 1877–1901, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] S. Yao, J. Zhao, D. Yu, N. Du, I. S. andKarthik R. Narasimhan, and Y. Cao,
    “ReAct: Synergizing reasoning and acting in language models,” in *ICLR*.   Kigali,
    Rwanda: OpenReview.net, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] S. Noah, C. Federico, G. Ashwin, N. K. R, and Y. Shunyu, “Reflexion: Language
    agents with verbal reinforcement learning,” in *NeurIPS*.   New Orleans, USA:
    PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan,
    “Tree of thoughts: Deliberate problem solving with large language models,” *CoRR*,
    vol. abs/2305.10601, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, L. Zettlemoyer,
    N. Cancedda, and T. Scialom, “Toolformer: Language models can teach themselves
    to use tools,” *CoRR*, vol. abs/2302.04761, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Y. Qin, S. Hu, Y. Lin, W. Chen, N. Ding, G. Cui, Z. Zeng, Y. Huang, C. Xiao,
    C. Han, Y. R. Fung, Y. Su, H. Wang, C. Qian, R. Tian, K. Zhu, S. Liang, X. Shen,
    B. Xu, Z. Zhang, Y. Ye, B. Li, Z. Tang, J. Yi, Y. Zhu, Z. Dai, L. Yan, X. Cong,
    Y. Lu, W. Zhao, Y. Huang, J. Yan, X. Han, X. Sun, D. Li, J. Phang, C. Yang, T. Wu,
    H. Ji, Z. Liu, and M. Sun, “Tool learning with foundation models,” *CoRR*, vol.
    abs/2304.08354, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Y. Qin, S. Liang, Y. Ye, K. Zhu, L. Yan, Y. Lu, Y. Lin, X. Cong, X. Tang,
    B. Qian, S. Zhao, R. Tian, R. Xie, J. Zhou, M. Gerstein, D. Li, Z. Liu, and M. Sun,
    “Toolllm: Facilitating large language models to master 16000+ real-world apis,”
    *CoRR*, vol. abs/2307.16789, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] C. Qian, C. Han, Y. R. Fung, Y. Qin, Z. Liu, and H. Ji, “CREATOR: disentangling
    abstract and concrete reasonings of large language models through tool creation,”
    *CoRR*, vol. abs/2305.14318, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] H. Chase, “Langchain,” https://github.com/hwchase17/langchain, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Y. Nakajima, “BabyAGI,” https://github.com/yoheinakajima/babyagi, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] G. Li, H. A. A. K. Hammoud, H. Itani, D. Khizbullin, and B. Ghanem, “CAMEL:
    communicative agents for ”mind” exploration of large scale language model society,”
    *CoRR*, vol. abs/2303.17760, Mar. 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Y. Talebirad and A. Nadiri, “Multi-agent collaboration: Harnessing the
    power of intelligent LLM agents,” *CoRR*, vol. abs/2306.03314, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] T. Liang, Z. He, W. Jiao, X. Wang, Y. Wang, R. Wang, Y. Yang, Z. Tu, and
    S. Shi, “Encouraging divergent thinking in large language models through multi-agent
    debate,” *CoRR*, vol. abs/2305.19118, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain,
    V. Kosaraju, W. Saunders, X. Jiang, K. Cobbe, T. Eloundou, G. Krueger, K. Button,
    M. Knight, B. Chess, and J. Schulman, “WebGPT: Browser-assisted question-answering
    with human feedback,” *CoRR*, vol. abs/2112.09332, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Q. Wu, G. Bansal, J. Zhang, Y. Wu, S. Zhang, E. Zhu, B. Li, L. Jiang,
    X. Zhang, and C. Wang, “AutoGen: Enabling next-gen LLM applications via multi-agent
    conversation framework,” *CoRR*, vol. abs/2308.08155, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] R. Guo, T. Fujiwara, Y. Li, K. M. Lima, S. Sen, N. K. Tran, and K. Ma,
    “Comparative visual analytics for assessing medical records with sequence embedding,”
    *Visualization Informatics*, vol. 4, no. 2, pp. 72–85, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Z. Jin, S. Cui, S. Guo, D. Gotz, J. Sun, and N. Cao, “CarePre: An intelligent
    clinical decision assistance system,” *ACM Transactions on Computing for Healthcare*,
    vol. 1, no. 1, pp. 1–20, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] C. B. Nielsen, S. D. Jackman, I. Birol, and S. J. M. Jones, “ABySS-Explorer:
    Visualizing genome sequence assemblies,” *IEEE Transactions on Visualization and
    Computer Graphics*, vol. 15, no. 6, pp. 881–888, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] S. Guo, K. Xu, R. Zhao, D. Gotz, H. Zha, and N. Cao, “EventThread: Visual
    summarization and stage analysis of event sequence data,” *IEEE Transactions on
    Visualization and Computer Graphics*, vol. 24, no. 1, pp. 56–65, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] S. Guo, Z. Jin, D. Gotz, F. Du, H. Zha, and N. Cao, “Visual progression
    analysis of event sequence data,” *IEEE Transactions on Visualization and Computer
    Graphics*, vol. 25, no. 1, pp. 417–426, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] A. Perer and F. Wang, “Frequence: interactive mining and visualization
    of temporal frequent event sequences,” in *IUI*.   Haifa, Israel: ACM, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Y. Han, A. Rozga, N. Dimitrova, G. D. Abowd, and J. T. Stasko, “Visual
    analysis of proximal temporal relationships of social and communicative behaviors,”
    *Computer Graphics Forum*, vol. 34, no. 3, pp. 51–60, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] N. Cao, Y. Lin, F. Du, and D. Wang, “Episogram: Visual summarization of
    egocentric social interactions,” *IEEE Computer Graphics and Applications*, vol. 36,
    no. 5, pp. 72–81, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] F. Fischer, J. Fuchs, P. Vervier, F. Mansmann, and O. Thonnard, “VisTracer:
    a visual analytics tool to investigate routing anomalies in traceroutes,” in *VizSec*.   Seattle,
    USA: ACM, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] L. Wenting, W. Meng, and C. J. H, “Real-time event identification through
    low-dimensional subspace characterization of high-dimensional synchrophasor data,”
    *IEEE Transactions on Power Systems*, vol. 33, no. 5, pp. 4937–4947, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Y. Wu, N. Cao, D. Gotz, Y. Tan, and D. A. Keim, “A survey on visual analytics
    of social media data,” *IEEE Transactions on Multimedia*, vol. 18, no. 11, pp.
    2135–2148, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] F. Zhou, X. Lin, C. Liu, Y. Zhao, P. Xu, L. Ren, T. Xue, and L. Ren, “A
    survey of visualization for smart manufacturing,” *Journal of Visualization*,
    vol. 22, no. 2, pp. 419–435, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Y. Shi, Y. Liu, H. Tong, J. He, G. Yan, and N. Cao, “Visual analytics
    of anomalous user behaviors: A survey,” *IEEE Transactions on Big Data*, vol. 8,
    no. 2, pp. 377–396, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Y. Guo, S. Guo, Z. Jin, S. Kaul, D. Gotz, and N. Cao, “Survey on visual
    analysis of event sequence data,” *IEEE Transactions on Visualization and Computer
    Graphics*, vol. 28, no. 12, pp. 5091–5112, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] X. Yuan, Z. Wang, Z. Liu, C. Guo, H. Ai, and D. Ren, “Visualization of
    social media flows with interactively identified key players,” in *IEEE VAST*.   Paris,
    France: IEEE Computer Society, 2014, pp. 291–292.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Y. Wu, S. Liu, K. Yan, M. Liu, and F. Wu, “OpinionFlow: Visual analysis
    of opinion diffusion on social media,” *IEEE Transactions on Visualization and
    Computer Graphics*, vol. 20, no. 12, pp. 1763–1772, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] S. Chen, S. Li, S. Chen, and X. Yuan, “R-Map: A map metaphor for visualizing
    information reposting process in social media,” *IEEE Transactions on Visualization
    and Computer Graphics*, vol. 26, no. 1, pp. 1204–1214, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] G. Sun, T. Tang, T. Peng, R. Liang, and Y. Wu, “SocialWave: Visual analysis
    of spatio-temporal diffusion of information on social media,” *ACM Transactions
    on Intelligent Systems and Technology*, vol. 9, no. 2, pp. 1–23, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] J. Zhao, N. Cao, Z. Wen, Y. Song, Y. Lin, and C. Collins, “#FluxFlow:
    Visual analysis of anomalous information spreading on social media,” *IEEE Transactions
    on Visualization and Computer Graphics*, vol. 20, no. 12, pp. 1773–1782, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] F. B. Viégas, M. Wattenberg, J. Hebert, G. Borggaard, A. Cichowlas, J. Feinberg,
    J. Orwant, and C. R. Wren, “Google+Ripples: a native visualization of information
    flow,” in *WWW*.   Rio de Janeiro, Brazil: ACM, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] P. Law, Z. Liu, S. Malik, and R. C. Basole, “MAQUI: Interweaving queries
    and pattern mining for recursive event sequence exploration,” *IEEE Transactions
    on Visualization and Computer Graphics*, vol. 25, no. 1, pp. 396–406, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] S. N. Dambekodi, S. Frazier, P. Ammanabrolu, and M. O. Riedl, “Playing
    text-based games with common sense,” *CoRR*, vol. abs/2012.02757, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] M. J. Hausknecht, P. Ammanabrolu, M. Côté, and X. Yuan, “Interactive fiction
    games: A colossal adventure,” in *AAAI*.   New York, USA: AAAI Press, 2020, pp.
    7903–7910.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] R. Liu, R. Yang, C. Jia, G. Zhang, D. Zhou, A. M. Dai, D. Yang, and S. Vosoughi,
    “Training socially aligned language models in simulated human society,” *CoRR*,
    vol. abs/2305.16960, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] D. Driess, F. Xia, M. S. M. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter,
    A. Wahid, J. Tompson, Q. Vuong, T. Yu, W. Huang, Y. Chebotar, P. Sermanet, D. Duckworth,
    S. Levine, V. Vanhoucke, K. Hausman, M. Toussaint, K. Greff, A. Zeng, I. Mordatch,
    and P. Florence, “PaLM-E: An embodied multimodal language model,” in *ICML*.   Honolulu,
    USA: PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] S. Paul, A. Roy-Chowdhury, and A. Cherian, “AVLEN: audio-visual-language
    embodied navigation in 3d environments,” in *NeurIPS*, New Orleans, USA, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] J. S. Park, L. Popowski, C. J. Cai, M. R. Morris, P. Liang, and M. S.
    Bernstein, “Social simulacra: Creating populated prototypes for social computing
    systems,” in *UIST*.   Bend, USA: ACM, 2022, pp. 1–18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] C. Gao, X. Lan, Z. Lu, J. Mao, J. Piao, H. Wang, D. Jin, and Y. Li, “S${}^{\mbox{3}}$:
    Social-network simulation system with large language model-empowered agents,”
    *CoRR*, vol. abs/2307.14984, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] R. Williams, N. Hosseinichimeh, A. Majumdar, and N. Ghaffarzadegan, “Epidemic
    modeling with generative agents,” *CoRR*, vol. abs/2307.04986, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] A. O’Gara, “Hoodwinked: Deception and cooperation in a text-based game
    for language models,” *CoRR*, vol. abs/2308.01404, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] L. Fan, G. Wang, Y. Jiang, A. Mandlekar, Y. Yang, H. Zhu, A. Tang, D. Huang,
    Y. Zhu, and A. Anandkumar, “MineDojo: Building open-ended embodied agents with
    internet-scale knowledge,” in *NeurIPS*, New Orleans, USA, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and
    A. Anandkumar, “Voyager: An open-ended embodied agent with large language models,”
    *CoRR*, vol. abs/2305.16291, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] C. Lynch, A. Wahid, J. Tompson, T. Ding, J. Betker, R. Baruch, T. Armstrong,
    and P. Florence, “Interactive language: Talking to robots in real time,” *CoRR*,
    vol. abs/2210.06407, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] D. Surís, S. Menon, and C. Vondrick, “ViperGPT: Visual inference via python
    execution for reasoning,” in *ICCV*.   Paris, France: IEEE, 2023, pp. 11 854–11 864.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] K. Nottingham, P. Ammanabrolu, A. Suhr, Y. Choi, H. Hajishirzi, S. Singh,
    and R. Fox, “Do embodied agents dream of pixelated sheep: Embodied decision making
    using language guided world modelling,” in *ICML*.   Honolulu, USA: PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] J. Cui, Z. Li, Y. Yan, B. Chen, and L. Yuan, “ChatLaw: Open-source legal
    large language model with integrated external knowledge bases,” *CoRR*, vol. abs/2306.16092,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson,
    I. Mordatch, Y. Chebotar, P. Sermanet, T. Jackson, N. Brown, L. Luu, S. Levine,
    K. Hausman, and B. Ichter, “Inner monologue: Embodied reasoning through planning
    with language models,” in *Conference on Robot Learning*, vol. 205.   Auckland,
    New Zealand: PMLR, 2022, pp. 1769–1782.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, and Y. Zhuang, “HuggingGPT: Solving
    AI tasks with ChatGPT and its friends in hugging face,” *CoRR*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] X. Liang, B. Wang, H. Huang, S. Wu, P. Wu, L. Lu, Z. Ma, and Z. Li, “Unleashing
    infinite-length input capacity for large-scale language models with self-controlled
    memory system,” *CoRR*, vol. abs/2304.13343, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] C. J. Chu, “Time series segmentation: A sliding window approach,” *Inf.
    Sci.*, vol. 85, no. 1-3, pp. 147–173, Jul. 1995.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] C. Truong, L. Oudre, and N. Vayatis, “Selective review of offline change
    point detection methods,” *Signal Process.*, vol. 167, Feb. 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] M. Ogawa and K. Ma, “Software evolution storylines,” in *Proc. VISSOFT*.   Salt
    Lake City, USA: ACM, 2010, pp. 35–42.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Y. Tanahashi and K. Ma, “Design considerations for optimizing storyline
    visualizations,” *IEEE Transactions on Visualization and Computer Graphics*, vol. 18,
    no. 12, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Y. Feng, X. Wang, B. Pan, K. Wong, Y. Ren, S. Liu, Z. Yan, Y. Ma, H. Qu,
    and W. Chen, “XNLI: explaining and diagnosing NLI-based visual data analysis,”
    *IEEE Transactions on Visualization and Computer Graphics*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] B. John, “SUS: a retrospective,” *Journal of usability studies*, vol. 8,
    no. 2, p. 29–40, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] X. Team, “XAgent: An autonomous agent for complex task solving,” 2023,
    https://github.com/OpenBMB/XAgent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] R. Team, “Research agent,” 2023, https://github.com/mukulpatnaik/researchgpt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Y. Zheng, C. Ma, K. Shi, and H. Huang, “Agents meet OKR: an object and
    key results driven agent system with hierarchical self-collaboration and self-evaluation,”
    *CoRR*, vol. abs/2311.16542, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Z. Yang, L. Li, K. Lin, J. Wang, C. Lin, Z. Liu, and L. Wang, “The dawn
    of LMMs: Preliminary explorations with GPT-4V(ision),” *CoRR*, vol. abs/2309.17421,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] F. Yingchaojie, C. Jiazhou, H. Keyu, J. K. Wong, Y. Hui, Z. Wei, Z. Rongchen,
    L. Xiaonan, and C. Wei, “iPoet: interactive painting poetry creation with visual
    multimodal analysis,” *Journal of Visualization*, vol. 25, no. 3, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Y. Feng, X. Wang, K. K. Wong, S. Wang, Y. Lu, M. Zhu, B. Wang, and W. Chen,
    “PromptMagician: Interactive prompt engineering for text-to-image creation,” *IEEE
    Transactions on Visualization and Computer Graphics*, vol. 30, no. 1, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 10 Biography Section
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| ![[Uncaptioned image]](img/78eb170cd42d083995a036bba7e80812.png) | Jiaying
    Lu is currently a Master student in the State Key Lab of CAD&CG at Zhejiang University,
    China. She received the B.E. degree in Computer Science and Technology from the
    Zhejiang University, China in 2022\. Her research interests include LLM agent
    and visual analytics. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/c0f6a0f94e4b66a278db0fef57093c06.png) | Bo Pan
    is currently a Ph.D. candidate in the State Key Lab of CAD&CG at Zhejiang University,
    China. He received the BS degree in Electrical and Computer Engineering from the
    University of Illinois Urbana-Champaign and Zhejiang University in 2022\. His
    research interests include visualization and deep learning. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/d5f81452825546473936271caf23be1c.png) | Jieyi
    Chen is currently a Master student in the State Key Lab of CAD&CG at Zhejiang
    University, China. She received the B.E. degree from the Zhejiang University of
    Technology, China in 2023\. Her research interests include visualization and visual
    analytics. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/fe4a813a169419f4c46688f899ad2919.png) | Yingchaojie
    Feng is currently a Ph.D. candidate in the State Key Lab of CAD&CG at Zhejiang
    University, China. He received the B.E. degree in software engineering from the
    Zhejiang University of Technology, China in 2020\. His research interests include
    data visualization, human-computer interaction, and natural language processing.
    For more details, please refer to https://yingchaojiefeng.github.io/. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/d2d8e3ded670626afec498ce163bf41c.png) | Jingyuan
    Hu is an undergraduate in the Chu Kochen Honors College at Zhejiang University.
    His research interests include visualization and visual analytics. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/ed49a6d26f3357f25bfcc891ec137614.png) | Yuchen
    Peng is currently a Ph.D candidate in the State Key Laboratory of Blockchain and
    Data Security from the Zhejiang University. He received the B.E. degree in computer
    science and technology from the Zhejiang University, China in 2022\. His research
    interests include database system and data management in machine learning. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/057e55915c899dd13d8a77886e2d5efa.png) | Wei Chen
    is a professor in the State Key Lab of CAD&CG at Zhejiang University. His current
    research interests include visualization and visual analytics. He has published
    more than 80 IEEE/ACM Transactions and IEEE VIS papers. He actively served in
    many leading conferences and journals, like IEEE PacificVIS steering committee,
    ChinaVIS steering committee, paper cochairs of IEEE VIS, IEEE PacificVIS, IEEE
    LDAV and ACM SIGGRAPH Asia VisSym. He is an associate editor of IEEE TVCG, IEEE
    TBG, ACM TIST, IEEE T-SMC-S, IEEE TIV, IEEE CG&A, FCS, and JOV. More information
    can be found at: http://www.cad.zju.edu.cn/home/chenwei. |'
  prefs: []
  type: TYPE_TB
