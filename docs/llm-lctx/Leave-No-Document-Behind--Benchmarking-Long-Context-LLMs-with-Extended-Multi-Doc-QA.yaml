- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'åˆ†ç±»: æœªåˆ†ç±»'
- en: 'date: 2024-09-08 19:02:17'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ—¥æœŸ: 2024-09-08 19:02:17'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc
    QA'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸ç•™ä»»ä½•æ–‡æ¡£ï¼šåŸºäºæ‰©å±•å¤šæ–‡æ¡£é—®ç­”çš„é•¿æ–‡æœ¬LLMsåŸºå‡†æµ‹è¯•
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2406.17419](https://ar5iv.labs.arxiv.org/html/2406.17419)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2406.17419](https://ar5iv.labs.arxiv.org/html/2406.17419)
- en: 'Minzheng Wang^(1,3), Longze Chen^(2,3)Â¹Â¹footnotemark: 1, Cheng Fuâ´, Shengyi
    Liaoâ´, Xinghua Zhangâ´, Bingli Wuâ´'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç‹æ•æ­£^(1,3), é™ˆé¾™æ³½^(2,3)Â¹Â¹è„šæ³¨æ ‡è®°: 1, å‚…æˆ^(4), å»–ç››æ¯…^(4), å¼ å…´å^(4), å´å†°åŠ›^(4)'
- en: 'Haiyang Yuâ´, Nan XuÂ¹, Lei Zhang^(2,3), Run Luo^(2,3), Yunshui Li^(2,3), Min
    YangÂ², Fei Huangâ´, Yongbin Liâ´Â²Â²footnotemark: 2'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 'äºæµ·æ´‹â´, è®¸æ¥ Â¹, å¼ ç£Š^(2,3), ç½—æ¶¦^(2,3), æäº‘æ°´^(2,3), æ¨æ•Â², é»„é£â´, ææ°¸å®¾â´Â²Â²è„šæ³¨æ ‡è®°: 2'
- en: Â¹MAIS, Institute of Automation, Chinese Academy of Sciences
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Â¹MAIS, ä¸­å›½ç§‘å­¦é™¢è‡ªåŠ¨åŒ–ç ”ç©¶æ‰€
- en: Â²Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Â²ä¸­å›½ç§‘å­¦é™¢æ·±åœ³å…ˆè¿›æŠ€æœ¯ç ”ç©¶é™¢
- en: Â³School of Artificial Intelligence, University of Chinese Academy of Sciences
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Â³ä¸­å›½ç§‘å­¦é™¢å¤§å­¦äººå·¥æ™ºèƒ½å­¦é™¢
- en: â´Alibaba Group
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: â´é˜¿é‡Œå·´å·´é›†å›¢
- en: ğŸ–‚:Â wangminzheng2023@ia.ac.cn;Â Â lz.chen2@siat.ac.cn Equal contribution.Corresponding
    author.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğŸ–‚: wangminzheng2023@ia.ac.cn; lz.chen2@siat.ac.cn ç­‰è´¡çŒ®.é€šè®¯ä½œè€….'
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: 'Long-context modeling capabilities have garnered widespread attention, leading
    to the emergence of Large Language Models (LLMs) with ultra-context windows. Meanwhile,
    benchmarks for evaluating long-context LLMs are gradually catching up. However,
    existing benchmarks employ irrelevant noise texts to artificially extend the length
    of test cases, diverging from the real-world scenarios of long-context applications.
    To bridge this gap, we propose a novel long-context benchmark, Loong, aligning
    with realistic scenarios through extended multi-document question answering (QA).
    Unlike typical document QA, in Loongâ€™s test cases, each document is relevant to
    the final answer, ignoring any document will lead to the failure of the answer.
    Furthermore, Loong introduces four types of tasks with a range of context lengths:
    Spotlight Locating, Comparison, Clustering, and Chain of Reasoning, to facilitate
    a more realistic and comprehensive evaluation of long-context understanding. Extensive
    experiments indicate that existing long-context language models still exhibit
    considerable potential for enhancement. Retrieval augmented generation (RAG) achieves
    poor performance, demonstrating that Loong can reliably assess the modelâ€™s long-context
    modeling capabilities.Â¹Â¹1The code and benchmark are available at [https://github.com/MozerWang/Loong](https://github.com/MozerWang/Loong)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: é•¿æ–‡æœ¬å»ºæ¨¡èƒ½åŠ›å·²å¼•èµ·å¹¿æ³›å…³æ³¨ï¼Œä¿ƒä½¿äº†å…·æœ‰è¶…é•¿ä¸Šä¸‹æ–‡çª—å£çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‡ºç°ã€‚åŒæ—¶ï¼Œè¯„ä¼°é•¿æ–‡æœ¬LLMsçš„åŸºå‡†æµ‹è¯•ä¹Ÿåœ¨é€æ­¥è·Ÿè¿›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•ä½¿ç”¨äº†æ— å…³çš„å™ªå£°æ–‡æœ¬æ¥äººä¸ºå»¶é•¿æµ‹è¯•æ¡ˆä¾‹çš„é•¿åº¦ï¼Œä¸çœŸå®çš„é•¿æ–‡æœ¬åº”ç”¨åœºæ™¯æœ‰æ‰€åç¦»ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹é•¿æ–‡æœ¬åŸºå‡†æµ‹è¯•â€”â€”Loongï¼Œé€šè¿‡æ‰©å±•çš„å¤šæ–‡æ¡£é—®ç­”ï¼ˆQAï¼‰ä¸ç°å®åœºæ™¯å¯¹é½ã€‚ä¸å…¸å‹çš„æ–‡æ¡£é—®ç­”ä¸åŒï¼Œåœ¨Loongçš„æµ‹è¯•æ¡ˆä¾‹ä¸­ï¼Œæ¯ä¸ªæ–‡æ¡£ä¸æœ€ç»ˆç­”æ¡ˆç›¸å…³ï¼Œå¿½è§†ä»»ä½•ä¸€ä¸ªæ–‡æ¡£éƒ½ä¼šå¯¼è‡´ç­”æ¡ˆå¤±è´¥ã€‚æ­¤å¤–ï¼ŒLoongå¼•å…¥äº†å››ç§ç±»å‹çš„ä»»åŠ¡ï¼Œå…·æœ‰ä¸åŒçš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼šç„¦ç‚¹å®šä½ã€æ¯”è¾ƒã€èšç±»å’Œæ¨ç†é“¾ï¼Œä»¥ä¾¿æ›´çœŸå®å’Œå…¨é¢åœ°è¯„ä¼°é•¿æ–‡æœ¬ç†è§£èƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œç°æœ‰çš„é•¿æ–‡æœ¬è¯­è¨€æ¨¡å‹ä»å…·æœ‰ç›¸å½“å¤§çš„æå‡æ½œåŠ›ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰è¡¨ç°ä¸ä½³ï¼Œè¡¨æ˜Loongå¯ä»¥å¯é åœ°è¯„ä¼°æ¨¡å‹çš„é•¿æ–‡æœ¬å»ºæ¨¡èƒ½åŠ›ã€‚Â¹Â¹1ä»£ç å’ŒåŸºå‡†æµ‹è¯•å¯åœ¨
    [https://github.com/MozerWang/Loong](https://github.com/MozerWang/Loong) è·å–
- en: 'Leave No Document Behind:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ç•™ä»»ä½•æ–‡æ¡£ï¼š
- en: Benchmarking Long-Context LLMs with Extended Multi-Doc QA
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæ‰©å±•å¤šæ–‡æ¡£é—®ç­”çš„é•¿æ–‡æœ¬LLMsåŸºå‡†æµ‹è¯•
- en: 'Minzheng Wang^(1,3)^â€ ^â€ thanks: Equal contribution., Longze Chen^(2,3)Â¹Â¹footnotemark:
    1, Cheng Fuâ´, Shengyi Liaoâ´, Xinghua Zhangâ´, Bingli Wuâ´ Haiyang Yuâ´, Nan XuÂ¹,
    Lei Zhang^(2,3), Run Luo^(2,3), Yunshui Li^(2,3), Min YangÂ²^â€ ^â€ thanks: Corresponding
    author., Fei Huangâ´, Yongbin Liâ´Â²Â²footnotemark: 2 Â¹MAIS, Institute of Automation,
    Chinese Academy of Sciences Â²Shenzhen Institute of Advanced Technology, Chinese
    Academy of Sciences Â³School of Artificial Intelligence, University of Chinese
    Academy of Sciences â´Alibaba Group ğŸ–‚:Â wangminzheng2023@ia.ac.cn;Â Â lz.chen2@siat.ac.cn'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç‹æ•æ­£^(1,3)^â€ ^â€ æ„Ÿè°¢: ç­‰è´¡çŒ®., é™ˆé¾™æ³½^(2,3)Â¹Â¹è„šæ³¨æ ‡è®°: 1, å‚…æˆ^(4), å»–ç››æ¯…^(4), å¼ å…´å^(4), å´å†°åŠ›^(4)
    äºæµ·æ´‹^(4), è®¸æ¥ Â¹, å¼ ç£Š^(2,3), ç½—æ¶¦^(2,3), æäº‘æ°´^(2,3), æ¨æ•Â²^â€ ^â€ æ„Ÿè°¢: é€šè®¯ä½œè€…., é»„é£^(4), ææ°¸å®¾^(4)Â²Â²è„šæ³¨æ ‡è®°:
    2 Â¹MAIS, ä¸­å›½ç§‘å­¦é™¢è‡ªåŠ¨åŒ–ç ”ç©¶æ‰€ Â²ä¸­å›½ç§‘å­¦é™¢æ·±åœ³å…ˆè¿›æŠ€æœ¯ç ”ç©¶é™¢ Â³ä¸­å›½ç§‘å­¦é™¢å¤§å­¦äººå·¥æ™ºèƒ½å­¦é™¢ â´é˜¿é‡Œå·´å·´é›†å›¢ ğŸ–‚: wangminzheng2023@ia.ac.cn;
    lz.chen2@siat.ac.cn'
- en: '![Refer to caption](img/80e0db5186c53acb3602f36b4622372f.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/80e0db5186c53acb3602f36b4622372f.png)'
- en: 'Figure 1: Previous benchmarks vs. Loong. ![Refer to caption](img/d331886b9a784081ab138740c1288102.png)
    marks the existence of evidence related to the answer in that document. Compared
    to centralized distribution in previous ones, evidence in Loong are scattered
    in different parts across multi-document long contexts, necessitating that no
    document can be ignored for success.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 1: ä¹‹å‰çš„åŸºå‡†æµ‹è¯•ä¸ Loong çš„å¯¹æ¯”ã€‚ ![å‚è§è¯´æ˜](img/d331886b9a784081ab138740c1288102.png)
    æ ‡è®°äº†ä¸è¯¥æ–‡æ¡£ä¸­çš„ç­”æ¡ˆç›¸å…³çš„è¯æ®çš„å­˜åœ¨ã€‚ä¸ä¹‹å‰çš„é›†ä¸­åˆ†å¸ƒç›¸æ¯”ï¼ŒLoong ä¸­çš„è¯æ®åˆ†æ•£åœ¨å¤šæ–‡æ¡£é•¿ä¸Šä¸‹æ–‡ä¸­çš„ä¸åŒéƒ¨åˆ†ï¼Œå¿…é¡»æ³¨æ„æ‰€æœ‰æ–‡æ¡£ï¼Œä»¥ç¡®ä¿æˆåŠŸã€‚'
- en: 1 Introduction
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 å¼•è¨€
- en: Large Language Models (LLMs) have exhibited remarkable proficiency in diverse
    downstream applicationsÂ OpenAI ([2023](#bib.bib19)). Recent works focus on scaling
    up the context window of LLMsÂ (Xiong etÂ al., [2023](#bib.bib35); Peng etÂ al.,
    [2023](#bib.bib21); Chen etÂ al., [2024b](#bib.bib9)), which is crucial for LLMs
    in handling complex tasks that require delving deeply into long texts. A few LLM
    (e.g. GPT-4o, Gemini-Pro) websites have been equipped with the intelligent document
    analysis function, allowing users to upload documents for answering queries. Meanwhile,
    retrieval-augmented generation (RAG) has been a commonly used framework that prompts
    LLMs with multiple relevant retrieved contents and can significantly improve model
    performanceÂ Wu etÂ al. ([2024](#bib.bib33)); Chen etÂ al. ([2024a](#bib.bib8)).
    These demand the model leverage its long-context capability to conduct an in-depth
    analysis of multiple long documents.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§ä¸‹æ¸¸åº”ç”¨ä¸­å±•ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›Â OpenAI ([2023](#bib.bib19))ã€‚è¿‘æœŸçš„ç ”ç©¶é›†ä¸­äºæ‰©å¤§ LLMs çš„ä¸Šä¸‹æ–‡çª—å£Â (Xiong
    et al., [2023](#bib.bib35); Peng et al., [2023](#bib.bib21); Chen et al., [2024b](#bib.bib9))ï¼Œè¿™å¯¹
    LLMs å¤„ç†éœ€è¦æ·±å…¥é˜…è¯»é•¿æ–‡æœ¬çš„å¤æ‚ä»»åŠ¡è‡³å…³é‡è¦ã€‚ä¸€äº› LLMï¼ˆä¾‹å¦‚ GPT-4oã€Gemini-Proï¼‰ç½‘ç«™å·²é…å¤‡äº†æ™ºèƒ½æ–‡æ¡£åˆ†æåŠŸèƒ½ï¼Œå…è®¸ç”¨æˆ·ä¸Šä¼ æ–‡æ¡£ä»¥å›ç­”æŸ¥è¯¢ã€‚åŒæ—¶ï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å·²æˆä¸ºä¸€ç§å¸¸ç”¨æ¡†æ¶ï¼Œå®ƒé€šè¿‡å¤šé¡¹ç›¸å…³æ£€ç´¢å†…å®¹æ¥æç¤º
    LLMsï¼Œå¹¶èƒ½æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½Â Wu et al. ([2024](#bib.bib33)); Chen et al. ([2024a](#bib.bib8))ã€‚è¿™äº›è¦æ±‚æ¨¡å‹åˆ©ç”¨å…¶é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›å¯¹å¤šä¸ªé•¿æ–‡æ¡£è¿›è¡Œæ·±å…¥åˆ†æã€‚
- en: '| Benchmark |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| åŸºå‡†æµ‹è¯• |'
- en: '&#124; Â Â  Multi-doc &#124;'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Â Â  å¤šæ–‡æ¡£ &#124;'
- en: '&#124; Tasks &#124;'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ä»»åŠ¡ &#124;'
- en: '|'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Â Â  Broad &#124;'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Â Â  å¹¿æ³›æ€§ &#124;'
- en: '&#124; Length Sets &#124;'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; é•¿åº¦é›†åˆ &#124;'
- en: '|'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Â Â  Avoidance of &#124;'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Â Â  é¿å… &#124;'
- en: '&#124; Contamination &#124;'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; æ±¡æŸ“ &#124;'
- en: '|'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Â Â  Realistic &#124;'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Â Â  ç°å®æ€§ &#124;'
- en: '&#124; Scenarios &#124;'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; åœºæ™¯ &#124;'
- en: '|'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Â Â  High Evidence &#124;'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Â Â  é«˜è¯æ® &#124;'
- en: '&#124; Dispersion &#124;'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; åˆ†æ•£æ€§ &#124;'
- en: '| Â Â  Multilingual |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| Â Â  å¤šè¯­è¨€ |'
- en: '| L-EvalÂ (An etÂ al., [2023](#bib.bib1)) | âœ— | âœ— | âœ— | âœ“ | âœ— | âœ“ |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| L-EvalÂ (An et al., [2023](#bib.bib1)) | âœ— | âœ— | âœ— | âœ“ | âœ— | âœ“ |'
- en: '| LongBenchÂ (Bai etÂ al., [2023b](#bib.bib5)) | âœ“ | âœ— | âœ— | âœ“ | âœ— | âœ“ |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| LongBenchÂ (Bai et al., [2023b](#bib.bib5)) | âœ“ | âœ— | âœ— | âœ“ | âœ— | âœ“ |'
- en: '| MarathonÂ (Zhang etÂ al., [2023](#bib.bib37)) | âœ“ | âœ“ | âœ— | âœ“ | âœ— | âœ“ |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| MarathonÂ (Zhang et al., [2023](#bib.bib37)) | âœ“ | âœ“ | âœ— | âœ“ | âœ— | âœ“ |'
- en: '| LooGLEÂ (Li etÂ al., [2023](#bib.bib17)) | âœ— | âœ— | âœ— | âœ“ | âœ— | âœ“ |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| LooGLEÂ (Li et al., [2023](#bib.bib17)) | âœ— | âœ— | âœ— | âœ“ | âœ— | âœ“ |'
- en: '| InfiniteBenchÂ (Zhang etÂ al., [2024](#bib.bib38)) | âœ“ | âœ— | âœ— | âœ“ | âœ— | âœ“
    |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| InfiniteBenchÂ (Zhang et al., [2024](#bib.bib38)) | âœ“ | âœ— | âœ— | âœ“ | âœ— | âœ“
    |'
- en: '| RULERÂ (Hsieh etÂ al., [2024](#bib.bib15)) | âœ“ | âœ“ | âœ— | âœ— | âœ— | âœ— |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| RULERÂ (Hsieh et al., [2024](#bib.bib15)) | âœ“ | âœ“ | âœ— | âœ— | âœ— | âœ— |'
- en: '| NIAHÂ (Kamradt, [2023](#bib.bib16)) | âœ— | âœ“ | âœ“ | âœ— | âœ— | âœ— |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| NIAHÂ (Kamradt, [2023](#bib.bib16)) | âœ— | âœ“ | âœ“ | âœ— | âœ— | âœ— |'
- en: '| Loong (Ours) | âœ“ | âœ“ | âœ“ | âœ“ | âœ“ | âœ“ |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| Loong (æˆ‘ä»¬) | âœ“ | âœ“ | âœ“ | âœ“ | âœ“ | âœ“ |'
- en: 'Table 1: Characteristics of Loong, where the evidences are scattered across
    multi-document long contexts.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¡¨ 1: Loong çš„ç‰¹æ€§ï¼Œå…¶ä¸­è¯æ®åˆ†æ•£åœ¨å¤šæ–‡æ¡£é•¿ä¸Šä¸‹æ–‡ä¸­ã€‚'
- en: 'However, there remains a lack of appropriate benchmarks for evaluating long-context
    understanding in real-world multi-document scenarios. Multi-document input as
    long-context modeling possesses extensive application scenarios of LLMs, such
    as analysis of financial reports over the years. Nevertheless, most existing benchmarks
    only place emphasis on single-document long contextsÂ (An etÂ al., [2023](#bib.bib1);
    Li etÂ al., [2023](#bib.bib17); Kamradt, [2023](#bib.bib16)) or involve multi-document
    question answering settings by adding distracting information to the input of
    existing short-context QA datasetsÂ (Hsieh etÂ al., [2024](#bib.bib15)). As shown
    in [FigureÂ 1](#S0.F1 "In Leave No Document Behind: Benchmarking Long-Context LLMs
    with Extended Multi-Doc QA"), evidence supporting the answer in previous benchmarks
    is relatively centralized, such as being contained within a single document. Yet,
    such a centralized distribution of evidence may cause the model to overlook certain
    documents and take shortcuts to formulate an answer, complicating the modeling
    of the real context length. Moreover, the prevalent evaluation tasks, such as
    â€œneedle in a haystackâ€ (NIAH)Â (Kamradt, [2023](#bib.bib16)), only scratch the
    surface of long-context understanding by searching from context, far from real-world
    demands.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œä»ç¼ºä¹é€‚å½“çš„åŸºå‡†æ¥è¯„ä¼°ç°å®ä¸–ç•Œä¸­å¤šæ–‡æ¡£åœºæ™¯çš„é•¿ä¸Šä¸‹æ–‡ç†è§£ã€‚å¤šæ–‡æ¡£è¾“å…¥ä½œä¸ºé•¿ä¸Šä¸‹æ–‡å»ºæ¨¡å…·æœ‰å¹¿æ³›çš„åº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚å¤šå¹´æ¥çš„è´¢åŠ¡æŠ¥å‘Šåˆ†æã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰åŸºå‡†ä»…å¼ºè°ƒå•æ–‡æ¡£é•¿ä¸Šä¸‹æ–‡ï¼ˆAn
    et al., [2023](#bib.bib1); Li et al., [2023](#bib.bib17); Kamradt, [2023](#bib.bib16)ï¼‰æˆ–é€šè¿‡å°†åˆ†æ•£ä¿¡æ¯æ·»åŠ åˆ°ç°æœ‰çŸ­ä¸Šä¸‹æ–‡QAæ•°æ®é›†çš„è¾“å…¥ä¸­æ¥æ¶‰åŠå¤šæ–‡æ¡£é—®ç­”è®¾ç½®ï¼ˆHsieh
    et al., [2024](#bib.bib15)ï¼‰ã€‚å¦‚[å›¾1](#S0.F1 "åœ¨ã€Šç•™ä¸‹æ–‡æ¡£ä¸è½ä¸‹ï¼šé•¿ä¸Šä¸‹æ–‡LLMsçš„å¤šæ–‡æ¡£QAåŸºå‡†ã€‹")æ‰€ç¤ºï¼Œä¹‹å‰åŸºå‡†ä¸­æ”¯æŒç­”æ¡ˆçš„è¯æ®ç›¸å¯¹é›†ä¸­ï¼Œä¾‹å¦‚åŒ…å«åœ¨å•ä¸ªæ–‡æ¡£ä¸­ã€‚ç„¶è€Œï¼Œè¿™ç§è¯æ®çš„é›†ä¸­åˆ†å¸ƒå¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹å¿½ç•¥æŸäº›æ–‡æ¡£ï¼Œå¹¶é‡‡å–æ·å¾„æ¥å½¢æˆç­”æ¡ˆï¼Œå¤æ‚åŒ–äº†çœŸå®ä¸Šä¸‹æ–‡é•¿åº¦çš„å»ºæ¨¡ã€‚æ­¤å¤–ï¼Œæµè¡Œçš„è¯„ä¼°ä»»åŠ¡ï¼Œå¦‚â€œé’ˆåœ¨å¹²è‰å †ä¸­â€ï¼ˆNIAHï¼‰
    (Kamradt, [2023](#bib.bib16))ï¼Œä»…ä»…è§¦åŠäº†é•¿ä¸Šä¸‹æ–‡ç†è§£çš„è¡¨é¢ï¼Œé€šè¿‡ä»ä¸Šä¸‹æ–‡ä¸­æœç´¢ï¼Œè¿œæœªæ»¡è¶³ç°å®ä¸–ç•Œçš„éœ€æ±‚ã€‚
- en: 'We commence with â€œ$\mathtt{leave}$â€ and scatter the evidence across multi-document
    long contexts. In this context, bypassing any document will lead to an erroneous
    answer, which better tests the long-context modeling ability. To this end, this
    paper develops Loong, an innovative benchmark crafted to evaluate the long-context
    ability of LLMs across multiple documents in real-world scenarios. Loong typically
    consists of 11 documents per test instance on average, spanning three real-world
    scenarios in English and Chinese: (1) Financial Reports, (2) Legal Cases, and
    (3) Academic Papers. Meanwhile, Loong introduces new evaluation tasks from the
    perspectives of Spotlight Locating, Comparison, Clustering, and Chain of Reasoning.
    Furthermore, Loong features inputs of varying lengths (e.g., 10K-50K, 50K-100K,
    100K-200K, >200K) and evaluation tasks of diverse difficulty, enabling fine-grained
    assessment of LLMs across different context lengths and task complexities.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»â€œ$\mathtt{leave}$â€å¼€å§‹ï¼Œå¹¶å°†è¯æ®åˆ†æ•£åˆ°å¤šä¸ªæ–‡æ¡£çš„é•¿ä¸Šä¸‹æ–‡ä¸­ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç»•è¿‡ä»»ä½•æ–‡æ¡£å°†å¯¼è‡´é”™è¯¯çš„ç­”æ¡ˆï¼Œè¿™æ›´å¥½åœ°æµ‹è¯•äº†é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å¼€å‘äº†Loongï¼Œä¸€ä¸ªåˆ›æ–°çš„åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°LLMsåœ¨ç°å®åœºæ™¯ä¸­è·¨å¤šä¸ªæ–‡æ¡£çš„é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›ã€‚Loongé€šå¸¸åŒ…å«æ¯ä¸ªæµ‹è¯•å®ä¾‹å¹³å‡11ä¸ªæ–‡æ¡£ï¼Œæ¶µç›–ä¸‰ä¸ªç°å®åœºæ™¯ï¼š
    (1) è´¢åŠ¡æŠ¥å‘Šï¼Œ (2) æ³•å¾‹æ¡ˆä»¶ï¼Œ (3) å­¦æœ¯è®ºæ–‡ã€‚åŒæ—¶ï¼ŒLoongä»èšç„¦å®šä½ã€æ¯”è¾ƒã€èšç±»å’Œæ¨ç†é“¾ç­‰è§’åº¦å¼•å…¥äº†æ–°çš„è¯„ä¼°ä»»åŠ¡ã€‚æ­¤å¤–ï¼ŒLoongå…·æœ‰ä¸åŒé•¿åº¦çš„è¾“å…¥ï¼ˆä¾‹å¦‚ï¼Œ10K-50Kï¼Œ50K-100Kï¼Œ100K-200Kï¼Œ>200Kï¼‰å’Œå¤šæ ·çš„è¯„ä¼°ä»»åŠ¡éš¾åº¦ï¼Œä½¿å¾—å¯¹LLMsåœ¨ä¸åŒä¸Šä¸‹æ–‡é•¿åº¦å’Œä»»åŠ¡å¤æ‚æ€§ä¸‹çš„ç»†ç²’åº¦è¯„ä¼°æˆä¸ºå¯èƒ½ã€‚
- en: 'We conduct extensive experiments on Loong to test the long-context modeling
    capabilities of several advanced LLMs. The empirical results show that even the
    current most powerful LLMs still struggle with the tasks in Loong, suggesting
    significant room for improvement in current LLMs. Furthermore, this paper conducts
    in-depth analyses regarding the behavior of long-context LLMs, involving RAG and
    the scaling law of context size. Our main contributions are summarized as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨Loongä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œä»¥æµ‹è¯•å‡ ç§å…ˆè¿›LLMsçš„é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡èƒ½åŠ›ã€‚å®è¯ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯å½“å‰æœ€å¼ºå¤§çš„LLMsä¹Ÿåœ¨Loongä¸­çš„ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œè¿™è¡¨æ˜å½“å‰LLMsè¿˜æœ‰æ˜¾è‘—çš„æ”¹è¿›ç©ºé—´ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å¯¹é•¿ä¸Šä¸‹æ–‡LLMsçš„è¡Œä¸ºè¿›è¡Œäº†æ·±å…¥åˆ†æï¼ŒåŒ…æ‹¬RAGå’Œä¸Šä¸‹æ–‡å¤§å°çš„æ‰©å±•è§„å¾‹ã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®æ€»ç»“å¦‚ä¸‹ï¼š
- en: â€¢
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Loong primarily focuses on testing long-context ability of LLMs across multiple
    documents by scattering the evidence to examine the real length of long contexts.
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Loongä¸»è¦å…³æ³¨é€šè¿‡åˆ†æ•£è¯æ®æ¥æµ‹è¯•LLMsåœ¨å¤šä¸ªæ–‡æ¡£ä¸­çš„é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›ï¼Œä»¥æ£€æŸ¥é•¿ä¸Šä¸‹æ–‡çš„çœŸå®é•¿åº¦ã€‚
- en: â€¢
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Loong offers evaluation sets with varying input lengths and evaluation tasks
    of differing difficulty, encompassing novel task categories as well as common
    multi-document scenarios.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Loong æä¾›äº†å…·æœ‰ä¸åŒè¾“å…¥é•¿åº¦å’Œéš¾åº¦ä¸åŒçš„è¯„ä¼°ä»»åŠ¡çš„è¯„ä¼°é›†ï¼ŒåŒ…æ‹¬æ–°é¢–çš„ä»»åŠ¡ç±»åˆ«ä»¥åŠå¸¸è§çš„å¤šæ–‡æ¡£åœºæ™¯ã€‚
- en: â€¢
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: All test instances are newly annotated and checked to guarantee quality. Extensive
    experiments and analyses deeply unveil the long-context modeling abilities of
    LLMs.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ‰€æœ‰æµ‹è¯•å®ä¾‹éƒ½ç»è¿‡æ–°æ ‡æ³¨å’Œæ£€æŸ¥ï¼Œä»¥ä¿è¯è´¨é‡ã€‚å¹¿æ³›çš„å®éªŒå’Œåˆ†ææ·±å…¥æ­ç¤ºäº† LLMs åœ¨é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡æ–¹é¢çš„èƒ½åŠ›ã€‚
- en: '![Refer to caption](img/ad24c7a364e405e040c7a9603d81f2e4.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/ad24c7a364e405e040c7a9603d81f2e4.png)'
- en: 'Figure 2: Showcase of four evaluation tasks in Loong (<$\mathtt{di}$> marks
    the content of the i-th document). a) Spotlight Locating: Locate the evidences.
    b) Comparison: Locate and compare the evidences. c) Clustering: Locate and cluster
    the evidences into groups. d) Chain of Reasoning: Locate and reasoning along a
    logical chain.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2ï¼šå±•ç¤º Loong ä¸­çš„å››ä¸ªè¯„ä¼°ä»»åŠ¡ (<$\mathtt{di}$> æ ‡è®°äº†ç¬¬ i ä¸ªæ–‡æ¡£çš„å†…å®¹)ã€‚ a) èšç„¦å®šä½ï¼šå®šä½è¯æ®ã€‚ b) æ¯”è¾ƒï¼šå®šä½å¹¶æ¯”è¾ƒè¯æ®ã€‚
    c) èšç±»ï¼šå®šä½å¹¶å°†è¯æ®åˆ†ç»„ã€‚ d) æ¨ç†é“¾ï¼šå®šä½å¹¶æ²¿é€»è¾‘é“¾æ¨ç†ã€‚
- en: 2 Related Work
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 ç›¸å…³å·¥ä½œ
- en: 2.1 Long-Context Language Models
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 é•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹
- en: With support for increasingly larger context windows, closed-source LLMs have
    taken the lead in the field of long-context modeling. From 128k to 1000k, GPT-4oÂ (OpenAI,
    [2023](#bib.bib19)), Claude3-200kÂ (Anthropic, [2024a](#bib.bib2)) and Gemini-pro1.5-1000kÂ (Reid
    etÂ al., [2024](#bib.bib25)) are capable of modeling increasingly longer documents,
    expanding the new scenarios that LLMs can handle.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ”¯æŒè¶Šæ¥è¶Šå¤§çš„ä¸Šä¸‹æ–‡çª—å£æ–¹é¢ï¼Œé—­æº LLMs åœ¨é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡é¢†åŸŸå¤„äºé¢†å…ˆåœ°ä½ã€‚ä» 128k åˆ° 1000kï¼ŒGPT-4o (OpenAI, [2023](#bib.bib19))ã€Claude3-200k
    (Anthropic, [2024a](#bib.bib2)) å’Œ Gemini-pro1.5-1000k (Reid et al., [2024](#bib.bib25))
    èƒ½å¤Ÿå»ºæ¨¡è¶Šæ¥è¶Šé•¿çš„æ–‡æ¡£ï¼Œæ‰©å±•äº† LLMs èƒ½å¤Ÿå¤„ç†çš„æ–°åœºæ™¯ã€‚
- en: Considering the quadratic complexity of TransformerÂ (Vaswani etÂ al., [2017](#bib.bib31)),
    training LLMs with extensive context windows from scratch necessitates substantial
    computational resources, exceeding the capabilities of the general researchers.
    Consequently, recent studies have explored ways to expand the context length of
    these models during the fine-tuning stage. For example, PIÂ (Chen etÂ al., [2023](#bib.bib10)),
    NTK-awareÂ (bloc97, [2023](#bib.bib6)), YaRNÂ (Peng etÂ al., [2023](#bib.bib21)),
    GiraffeÂ (Pal etÂ al., [2023](#bib.bib20)), Code LLaMAÂ (Roziere etÂ al., [2023](#bib.bib26)),
    and PoSEÂ (Zhu etÂ al., [2023](#bib.bib39)) adapts position embedding based on the
    rotary position encoding (RoPE)Â (Su etÂ al., [2024](#bib.bib29)), with only a few
    fine-tuning steps, the context length can be efficiently extended.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘åˆ° Transformer çš„äºŒæ¬¡å¤æ‚åº¦ (Vaswani et al., [2017](#bib.bib31))ï¼Œä»å¤´å¼€å§‹è®­ç»ƒå…·æœ‰å¹¿æ³›ä¸Šä¸‹æ–‡çª—å£çš„
    LLMs éœ€è¦å¤§é‡è®¡ç®—èµ„æºï¼Œè¶…å‡ºäº†æ™®é€šç ”ç©¶äººå‘˜çš„èƒ½åŠ›èŒƒå›´ã€‚å› æ­¤ï¼Œæœ€è¿‘çš„ç ”ç©¶æ¢ç´¢äº†åœ¨å¾®è°ƒé˜¶æ®µæ‰©å±•è¿™äº›æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦çš„æ–¹æ³•ã€‚ä¾‹å¦‚ï¼ŒPI (Chen et al.,
    [2023](#bib.bib10))ã€NTK-aware (bloc97, [2023](#bib.bib6))ã€YaRN (Peng et al., [2023](#bib.bib21))ã€Giraffe
    (Pal et al., [2023](#bib.bib20))ã€Code LLaMA (Roziere et al., [2023](#bib.bib26))
    å’Œ PoSE (Zhu et al., [2023](#bib.bib39)) é‡‡ç”¨äº†åŸºäºæ—‹è½¬ä½ç½®ç¼–ç  (RoPE) (Su et al., [2024](#bib.bib29))
    çš„ä½ç½®åµŒå…¥ï¼Œé€šè¿‡ä»…å°‘é‡çš„å¾®è°ƒæ­¥éª¤ï¼Œå°±èƒ½é«˜æ•ˆåœ°æ‰©å±•ä¸Šä¸‹æ–‡é•¿åº¦ã€‚
- en: Another strong baseline for long-context modeling is the sliding window method.
    Various sliding window-based variants such as ALibiÂ (Press etÂ al., [2021](#bib.bib22)),
    xPosÂ (Sun etÂ al., [2022](#bib.bib30)), PCWÂ (Ratner etÂ al., [2022](#bib.bib24)),
    LM-InfinitÂ (Han etÂ al., [2023](#bib.bib14)), StreamingLLMÂ (Xiao etÂ al., [2023](#bib.bib34))
    are used to achieve efficient context scaling. Yet they diverge from the global
    perception characteristic of the Transformer, failing to exploit the entire context.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªå¼ºå¤§çš„é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡åŸºçº¿æ˜¯æ»‘åŠ¨çª—å£æ–¹æ³•ã€‚å„ç§åŸºäºæ»‘åŠ¨çª—å£çš„å˜ä½“ï¼Œå¦‚ ALibi (Press et al., [2021](#bib.bib22))ã€xPos
    (Sun et al., [2022](#bib.bib30))ã€PCW (Ratner et al., [2022](#bib.bib24))ã€LM-Infinit
    (Han et al., [2023](#bib.bib14))ã€StreamingLLM (Xiao et al., [2023](#bib.bib34))ï¼Œè¢«ç”¨æ¥å®ç°é«˜æ•ˆçš„ä¸Šä¸‹æ–‡æ‰©å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬åç¦»äº†
    Transformer çš„å…¨å±€æ„ŸçŸ¥ç‰¹æ€§ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨æ•´ä¸ªä¸Šä¸‹æ–‡ã€‚
- en: 2.2 Long-Context Benchmarks
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 é•¿ä¸Šä¸‹æ–‡åŸºå‡†
- en: Long-context modeling methods are rapidly evolving, yet the quality of existing
    benchmarks does not align with this progress. Synthetic task such as Needle-in-a-Haystack
    (NIAH)Â (Kamradt, [2023](#bib.bib16)) and Counting starsÂ (Song etÂ al., [2024](#bib.bib28))
    are initially utilized for evaluating long-context language models (LCLMs) due
    to their lower construction costs, but they are indicative of only a surface form
    of long-context understanding.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡æ–¹æ³•æ­£åœ¨è¿…é€Ÿå‘å±•ï¼Œä½†ç°æœ‰åŸºå‡†çš„è´¨é‡å¹¶æœªä¸æ­¤è¿›å±•ä¿æŒä¸€è‡´ã€‚åˆæˆä»»åŠ¡å¦‚ Needle-in-a-Haystack (NIAH) (Kamradt,
    [2023](#bib.bib16)) å’Œ Counting stars (Song et al., [2024](#bib.bib28)) æœ€åˆè¢«ç”¨äºè¯„ä¼°é•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹
    (LCLMs)ï¼Œç”±äºå…¶è¾ƒä½çš„æ„å»ºæˆæœ¬ï¼Œä½†å®ƒä»¬ä»…åæ˜ äº†é•¿ä¸Šä¸‹æ–‡ç†è§£çš„è¡¨é¢å½¢å¼ã€‚
- en: LongbenchÂ (Bai etÂ al., [2023b](#bib.bib5)), LooGLE Â (Li etÂ al., [2023](#bib.bib17))
    and MarathonÂ (Zhang etÂ al., [2023](#bib.bib37)) are earlier benchmarks for comprehensive
    assessment of long context. However, the average length for most tasks is between
    5k and 25k, far less than the window size of LCLMs. L-EvalÂ (An etÂ al., [2023](#bib.bib1)),
    BAMBOOÂ (Dong etÂ al., [2023](#bib.bib11)), CLongEvalÂ (Qiu etÂ al., [2024](#bib.bib23))
    and InfiniteBenchÂ (Zhang etÂ al., [2024](#bib.bib38)) contain sufficiently long
    evaluation data, and the wide variety of tasks makes the assessment more comprehensive.
    RULERÂ (Hsieh etÂ al., [2024](#bib.bib15)) creates a comprehensive testing method
    with flexibly adjustable length and difficulty, yet they only add distracting
    information to the input of existing short-context QA datasets.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: LongbenchÂ ï¼ˆBai ç­‰ï¼Œ[2023b](#bib.bib5)ï¼‰ã€LooGLEÂ ï¼ˆLi ç­‰ï¼Œ[2023](#bib.bib17)ï¼‰å’Œ MarathonÂ ï¼ˆZhang
    ç­‰ï¼Œ[2023](#bib.bib37)ï¼‰æ˜¯æ—©æœŸç”¨äºå…¨é¢è¯„ä¼°é•¿ä¸Šä¸‹æ–‡çš„åŸºå‡†ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ä»»åŠ¡çš„å¹³å‡é•¿åº¦åœ¨5kåˆ°25kä¹‹é—´ï¼Œè¿œä½äºLCLMsçš„çª—å£å¤§å°ã€‚L-EvalÂ ï¼ˆAn
    ç­‰ï¼Œ[2023](#bib.bib1)ï¼‰ã€BAMBOOÂ ï¼ˆDong ç­‰ï¼Œ[2023](#bib.bib11)ï¼‰ã€CLongEvalÂ ï¼ˆQiu ç­‰ï¼Œ[2024](#bib.bib23)ï¼‰å’Œ
    InfiniteBenchÂ ï¼ˆZhang ç­‰ï¼Œ[2024](#bib.bib38)ï¼‰åŒ…å«äº†è¶³å¤Ÿé•¿çš„è¯„ä¼°æ•°æ®ï¼Œä¸”ä»»åŠ¡çš„å¤šæ ·æ€§ä½¿å¾—è¯„ä¼°æ›´åŠ å…¨é¢ã€‚RULERÂ ï¼ˆHsieh
    ç­‰ï¼Œ[2024](#bib.bib15)ï¼‰åˆ›å»ºäº†ä¸€ä¸ªå…·æœ‰çµæ´»å¯è°ƒé•¿åº¦å’Œéš¾åº¦çš„ç»¼åˆæµ‹è¯•æ–¹æ³•ï¼Œä½†å®ƒä»¬ä»…å‘ç°æœ‰çŸ­ä¸Šä¸‹æ–‡é—®ç­”æ•°æ®é›†çš„è¾“å…¥ä¸­æ·»åŠ äº†å¹²æ‰°ä¿¡æ¯ã€‚
- en: While these long-context benchmarks have their own advantages, we still lack
    a benchmark that is sufficiently long, free from data contaminationÂ (Golchin and
    Surdeanu, [2023](#bib.bib13)), and fully aligned with the real-world multi-document
    question answering scenario.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡è¿™äº›é•¿ä¸Šä¸‹æ–‡åŸºå‡†å„æœ‰ä¼˜ç‚¹ï¼Œä½†æˆ‘ä»¬ä»ç¼ºä¹ä¸€ä¸ªè¶³å¤Ÿé•¿ã€æ²¡æœ‰æ•°æ®æ±¡æŸ“ï¼ˆGolchin å’Œ Surdeanuï¼Œ[2023](#bib.bib13)ï¼‰ä¸”å®Œå…¨ç¬¦åˆçœŸå®ä¸–ç•Œå¤šæ–‡æ¡£é—®ç­”åœºæ™¯çš„åŸºå‡†ã€‚
- en: 2.3 Retrieval Augmented Language Models
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 æ£€ç´¢å¢å¼ºè¯­è¨€æ¨¡å‹
- en: Leveraging long documents as external knowledge, Retrieval Augmented Language
    Models (RALMs) has achieved comparable or even better performance than LCLMs fine-tuned
    for specific tasks with long document. In previous study, RALMs could directly
    utilize the content retrieved during the inference phase. REPLUGÂ (Shi etÂ al.,
    [2023](#bib.bib27)) treats the language model as a black box and the retrieval
    component as an adjustable plug-and-play module. RETROÂ (Borgeaud etÂ al., [2022](#bib.bib7))
    use a chunked cross-attention module to incorporate the retrieved text. Additionally,
    Xu etÂ al. ([2023](#bib.bib36)) explored whether RALMs or LCLMs are more suitable
    for long-context tasks under a larger parameter setting. However, there is currently
    a lack of analysis on what tasks RALMs and LCLMs each excel at, thus making it
    difficult to determine which type a black box model belongs to.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ©ç”¨é•¿æ–‡æ¡£ä½œä¸ºå¤–éƒ¨çŸ¥è¯†ï¼Œæ£€ç´¢å¢å¼ºè¯­è¨€æ¨¡å‹ï¼ˆRALMsï¼‰åœ¨å¤„ç†é•¿æ–‡æ¡£çš„ç‰¹å®šä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¸LCLMsç›¸å½“ç”šè‡³æ›´å¥½çš„æ€§èƒ½ã€‚åœ¨ä»¥å¾€çš„ç ”ç©¶ä¸­ï¼ŒRALMså¯ä»¥ç›´æ¥åˆ©ç”¨æ¨ç†é˜¶æ®µæ£€ç´¢åˆ°çš„å†…å®¹ã€‚REPLUGÂ ï¼ˆShi
    ç­‰ï¼Œ[2023](#bib.bib27)ï¼‰å°†è¯­è¨€æ¨¡å‹è§†ä¸ºé»‘ç®±ï¼Œå°†æ£€ç´¢ç»„ä»¶è§†ä¸ºå¯è°ƒæ’æ‹”æ¨¡å—ã€‚RETROÂ ï¼ˆBorgeaud ç­‰ï¼Œ[2022](#bib.bib7)ï¼‰ä½¿ç”¨äº†åˆ†å—çš„äº¤å‰æ³¨æ„åŠ›æ¨¡å—æ¥èå…¥æ£€ç´¢åˆ°çš„æ–‡æœ¬ã€‚æ­¤å¤–ï¼ŒXu
    ç­‰äººï¼ˆ[2023](#bib.bib36)ï¼‰æ¢è®¨äº†åœ¨æ›´å¤§å‚æ•°è®¾ç½®ä¸‹RALMsä¸LCLMsåœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§ã€‚ç„¶è€Œï¼Œç›®å‰ç¼ºä¹å¯¹RALMså’ŒLCLMså„è‡ªæ“…é•¿çš„ä»»åŠ¡çš„åˆ†æï¼Œå› æ­¤å¾ˆéš¾ç¡®å®šé»‘ç®±æ¨¡å‹å±äºå“ªç§ç±»å‹ã€‚
- en: '3 Loong: A Long-Context Benchmark'
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '3 Loong: é•¿ä¸Šä¸‹æ–‡åŸºå‡†'
- en: 3.1 Overview
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 æ¦‚è¿°
- en: 'The Loong benchmark comprises tasks across four categories: Spotlight Locating,
    Comparison, Clustering, and Chain of reasoning. To align with realistic scenarios,
    we collect documents from three domains: financial reports, academic papers, and
    legal cases. Furthermore, all tasks are presented in the question-answering format,
    which are all newly annotated by GPT-4o and humans. Totally, Loong includes 1600
    test instances in both Chinese and English, featuring four sets with different
    intervals of context size: $\mathtt{Set1}$ (200-250K). We use tiktokenÂ²Â²2[https://platform.openai.com/tokenizer](https://platform.openai.com/tokenizer)
    tokenizer to tokenize the input and report the number of tokens. [TableÂ 2](#S3.T2
    "In 3.2 Evaluation Task â€£ 3 Loong: A Long-Context Benchmark â€£ Leave No Document
    Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA") and [AppendixÂ C](#A3
    "Appendix C Length Distribution â€£ Leave No Document Behind: Benchmarking Long-Context
    LLMs with Extended Multi-Doc QA") show the details of data statistics. The following
    sections will provide a detailed description of the evaluation task and benchmark
    construction.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 'Loong åŸºå‡†æµ‹è¯•åŒ…å«å››ä¸ªç±»åˆ«çš„ä»»åŠ¡ï¼šèšå…‰ç¯å®šä½ã€æ¯”è¾ƒã€èšç±»å’Œæ¨ç†é“¾ã€‚ä¸ºäº†ç¬¦åˆç°å®åœºæ™¯ï¼Œæˆ‘ä»¬ä»ä¸‰ä¸ªé¢†åŸŸæ”¶é›†äº†æ–‡æ¡£ï¼šè´¢åŠ¡æŠ¥å‘Šã€å­¦æœ¯è®ºæ–‡å’Œæ³•å¾‹æ¡ˆä»¶ã€‚æ­¤å¤–ï¼Œæ‰€æœ‰ä»»åŠ¡éƒ½ä»¥é—®ç­”æ ¼å¼å‘ˆç°ï¼Œå‡ç”±
    GPT-4o å’Œäººå·¥è¿›è¡Œæ–°æ ‡æ³¨ã€‚æ€»çš„æ¥è¯´ï¼ŒLoong åŒ…å« 1600 ä¸ªä¸­è‹±æ–‡æµ‹è¯•å®ä¾‹ï¼Œæ¶µç›–å››ä¸ªä¸åŒä¸Šä¸‹æ–‡å¤§å°åŒºé—´çš„é›†åˆï¼š$\mathtt{Set1}$ (200-250K)ã€‚æˆ‘ä»¬ä½¿ç”¨
    tiktokenÂ²Â²2[https://platform.openai.com/tokenizer](https://platform.openai.com/tokenizer)
    åˆ†è¯å™¨å¯¹è¾“å…¥è¿›è¡Œåˆ†è¯å¹¶æŠ¥å‘Šæ ‡è®°æ•°ã€‚[è¡¨ 2](#S3.T2 "3.2 è¯„ä¼°ä»»åŠ¡ â€£ 3 Loong: é•¿æ–‡æœ¬åŸºå‡†æµ‹è¯• â€£ ä¸é—æ¼ä»»ä½•æ–‡æ¡£ï¼šåŸºå‡†æµ‹è¯•é•¿æ–‡æœ¬ LLM
    çš„æ‰©å±•å¤šæ–‡æ¡£é—®ç­”")å’Œ[é™„å½• C](#A3 "é™„å½• C é•¿åº¦åˆ†å¸ƒ â€£ ä¸é—æ¼ä»»ä½•æ–‡æ¡£ï¼šåŸºå‡†æµ‹è¯•é•¿æ–‡æœ¬ LLM çš„æ‰©å±•å¤šæ–‡æ¡£é—®ç­”") æ˜¾ç¤ºäº†æ•°æ®ç»Ÿè®¡çš„è¯¦ç»†ä¿¡æ¯ã€‚ä»¥ä¸‹éƒ¨åˆ†å°†æä¾›è¯„ä¼°ä»»åŠ¡å’ŒåŸºå‡†æ„å»ºçš„è¯¦ç»†æè¿°ã€‚'
- en: 3.2 Evaluation Task
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 è¯„ä¼°ä»»åŠ¡
- en: '| Category | Avg Token | Language | #Test Instance |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| ç±»åˆ« | å¹³å‡æ ‡è®°æ•° | è¯­è¨€ | æµ‹è¯•å®ä¾‹æ•°é‡ |'
- en: '| Task |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| ä»»åŠ¡ |'
- en: '| Spotlight Locating | 119.3K | EN, ZH | 250 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| èšå…‰ç¯å®šä½ | 119.3K | EN, ZH | 250 |'
- en: '| Comparison | 110.6K | EN, ZH | 300 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| æ¯”è¾ƒ | 110.6K | EN, ZH | 300 |'
- en: '| Clustering | 109.8K | EN, ZH | 641 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| èšç±» | 109.8K | EN, ZH | 641 |'
- en: '| Chain of Reasoning | 103.9K | EN, ZH | 409 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| æ¨ç†é“¾ | 103.9K | EN, ZH | 409 |'
- en: '| Sub Task |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| å­ä»»åŠ¡ |'
- en: '| Sequential Enumeration | 103K | EN, ZH | 87 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| é¡ºåºæšä¸¾ | 103K | EN, ZH | 87 |'
- en: '| Extremum Acquisition | 115K | EN, ZH | 143 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| æå€¼è·å– | 115K | EN, ZH | 143 |'
- en: '| Range Awareness | 111K | EN, ZH | 70 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| èŒƒå›´æ„è¯† | 111K | EN, ZH | 70 |'
- en: '| Report Integration | 117K | EN, ZH | 250 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| æŠ¥å‘Šæ•´åˆ | 117K | EN, ZH | 250 |'
- en: '| Citation&Reference | 105K | EN | 270 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| å¼•ç”¨ä¸å‚è€ƒæ–‡çŒ® | 105K | EN | 270 |'
- en: '| Case Classification | 106K | ZH | 121 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| æ¡ˆä¾‹åˆ†ç±» | 106K | ZH | 121 |'
- en: '| Temporal Analysis | 112K | EN, ZH | 100 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| æ—¶é—´åˆ†æ | 112K | EN, ZH | 100 |'
- en: '| Citation Chain | 91K | EN | 130 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| å¼•ç”¨é“¾ | 91K | EN | 130 |'
- en: '| Link the Links | 117K | ZH | 113 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| é“¾æ¥é“¾æ¥ | 117K | ZH | 113 |'
- en: '| Solitaire | 94K | ZH | 66 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| å•äººæ¸¸æˆ | 94K | ZH | 66 |'
- en: '| Domain |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| é¢†åŸŸ |'
- en: '| Financial Reports | 117.5K | EN, ZH | 700 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| è´¢åŠ¡æŠ¥å‘Š | 117.5K | EN, ZH | 700 |'
- en: '| Legal Cases | 107.2K | ZH | 500 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| æ³•å¾‹æ¡ˆä»¶ | 107.2K | ZH | 500 |'
- en: '| Academic Papers | 100.9K | EN | 400 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| å­¦æœ¯è®ºæ–‡ | 100.9K | EN | 400 |'
- en: '| Length Set |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| é•¿åº¦é›†åˆ |'
- en: '| $\mathtt{Set1}$ (10-50K) | 37.8K | EN, ZH | 323 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Set1}$ (10-50K) | 37.8K | EN, ZH | 323 |'
- en: '| $\mathtt{Set2}$ (50-100K) | 75.6K | EN, ZH | 564 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Set2}$ (50-100K) | 75.6K | EN, ZH | 564 |'
- en: '| $\mathtt{Set3}$ (100-200K) | 138.9K | EN, ZH | 481 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Set3}$ (100-200K) | 138.9K | EN, ZH | 481 |'
- en: '| $\mathtt{Set4}$ (200-250K) | 233.9K | EN, ZH | 232 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Set4}$ (200-250K) | 233.9K | EN, ZH | 232 |'
- en: 'Table 2: Data statistics of Loong benchmark.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¡¨ 2: Loong åŸºå‡†æµ‹è¯•çš„æ•°æ®ç»Ÿè®¡ã€‚'
- en: 'Based on various multi-document semantic relationships and LLMsâ€™ handling of
    multi-document input, we propose new task categories for multi-document long-context
    modeling and closer alignment with real-world scenarios. [FigureÂ 2](#S1.F2 "In
    1 Introduction â€£ Leave No Document Behind: Benchmarking Long-Context LLMs with
    Extended Multi-Doc QA") illustrates the evaluation tasks of the Loong benchmark.
    [AppendixÂ B](#A2 "Appendix B Test Case â€£ Leave No Document Behind: Benchmarking
    Long-Context LLMs with Extended Multi-Doc QA") shows the detailed test case and
    prompt of each task. [AppendixÂ E](#A5 "Appendix E Comparison of Evidence Distribution
    â€£ Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc
    QA") presents a comparison of evidence distribution between Loong and LongBench.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºå„ç§å¤šæ–‡æ¡£è¯­ä¹‰å…³ç³»å’ŒLLMså¤„ç†å¤šæ–‡æ¡£è¾“å…¥çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ–‡æ¡£é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡çš„æ–°ä»»åŠ¡ç±»åˆ«ï¼Œä»¥æ›´è´´è¿‘çœŸå®ä¸–ç•Œåœºæ™¯ã€‚ [å›¾2](#S1.F2 "åœ¨1å¼•è¨€
    â€£ ä¸é—æ¼æ–‡æ¡£ï¼šåŸºäºæ‰©å±•å¤šæ–‡æ¡£QAçš„é•¿ä¸Šä¸‹æ–‡LLMsåŸºå‡†æµ‹è¯•")å±•ç¤ºäº†LoongåŸºå‡†çš„è¯„ä¼°ä»»åŠ¡ã€‚ [é™„å½•B](#A2 "é™„å½•B æµ‹è¯•æ¡ˆä¾‹ â€£ ä¸é—æ¼æ–‡æ¡£ï¼šåŸºäºæ‰©å±•å¤šæ–‡æ¡£QAçš„é•¿ä¸Šä¸‹æ–‡LLMsåŸºå‡†æµ‹è¯•")å±•ç¤ºäº†æ¯ä¸ªä»»åŠ¡çš„è¯¦ç»†æµ‹è¯•æ¡ˆä¾‹å’Œæç¤ºã€‚
    [é™„å½•E](#A5 "é™„å½•E è¯æ®åˆ†å¸ƒæ¯”è¾ƒ â€£ ä¸é—æ¼æ–‡æ¡£ï¼šåŸºäºæ‰©å±•å¤šæ–‡æ¡£QAçš„é•¿ä¸Šä¸‹æ–‡LLMsåŸºå‡†æµ‹è¯•")å±•ç¤ºäº†Loongå’ŒLongBenchä¹‹é—´çš„è¯æ®åˆ†å¸ƒæ¯”è¾ƒã€‚
- en: 3.2.1 Spotlight Locating
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 èšå…‰ç¯å®šä½
- en: 'The spotlight locating task is designed to assess the modelâ€™s capability for
    knowledge localization, which constitutes the foundation ability of long-context
    processing. In this task, the evidences are contained in only one of multiple
    documents, which is the atomic setting of the key information locating. Spotlight
    locating task is aimed at examining the LLMsâ€™ ability to search the evidence within
    one document from multiple ones. Other documents, which are in the same domain
    and have similar semantics as the document but are unrelated to the question,
    will serve as noise texts. The upper left of [FigureÂ 2](#S1.F2 "In 1 Introduction
    â€£ Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc
    QA") provides an example of the spotlight locating task.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: èšå…‰ç¯å®šä½ä»»åŠ¡æ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨çŸ¥è¯†å®šä½æ–¹é¢çš„èƒ½åŠ›ï¼Œè¿™æ„æˆäº†é•¿ä¸Šä¸‹æ–‡å¤„ç†çš„åŸºç¡€èƒ½åŠ›ã€‚åœ¨æ­¤ä»»åŠ¡ä¸­ï¼Œè¯æ®ä»…åŒ…å«åœ¨å¤šä¸ªæ–‡æ¡£ä¸­çš„ä¸€ä¸ªï¼Œè¿™å°±æ˜¯å…³é”®æ€§ä¿¡æ¯å®šä½çš„åŸå­è®¾ç½®ã€‚èšå…‰ç¯å®šä½ä»»åŠ¡æ—¨åœ¨è€ƒå¯ŸLLMsä»å¤šä¸ªæ–‡æ¡£ä¸­æœç´¢ä¸€ä¸ªæ–‡æ¡£ä¸­çš„è¯æ®çš„èƒ½åŠ›ã€‚å…¶ä»–æ–‡æ¡£ï¼Œè™½ç„¶åœ¨ç›¸åŒé¢†åŸŸå¹¶ä¸”ä¸æ–‡æ¡£æœ‰ç›¸ä¼¼è¯­ä¹‰ï¼Œä½†ä¸é—®é¢˜æ— å…³ï¼Œå°†ä½œä¸ºå™ªå£°æ–‡æœ¬ã€‚
    [å›¾2](#S1.F2 "åœ¨1å¼•è¨€ â€£ ä¸é—æ¼æ–‡æ¡£ï¼šåŸºäºæ‰©å±•å¤šæ–‡æ¡£QAçš„é•¿ä¸Šä¸‹æ–‡LLMsåŸºå‡†æµ‹è¯•")çš„å·¦ä¸Šè§’æä¾›äº†ä¸€ä¸ªèšå…‰ç¯å®šä½ä»»åŠ¡çš„ç¤ºä¾‹ã€‚
- en: 3.2.2 Comparison
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 æ¯”è¾ƒ
- en: The comparison task is primarily aimed at evaluating the modelâ€™s ability to
    compare multi-source information with long contexts. In this event, the evidence
    supporting the answer are distributed across multiple documents, testing the LLMsâ€™
    ability to locate dispersed evidence and to correlate and compare them.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯”è¾ƒä»»åŠ¡ä¸»è¦æ—¨åœ¨è¯„ä¼°æ¨¡å‹æ¯”è¾ƒå¤šæºä¿¡æ¯å’Œé•¿ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ã€‚åœ¨æ­¤äº‹ä»¶ä¸­ï¼Œæ”¯æŒç­”æ¡ˆçš„è¯æ®åˆ†å¸ƒåœ¨å¤šä¸ªæ–‡æ¡£ä¸­ï¼Œæµ‹è¯•LLMså®šä½åˆ†æ•£è¯æ®ã€å…³è”å’Œæ¯”è¾ƒå®ƒä»¬çš„èƒ½åŠ›ã€‚
- en: 'Comparison task includes three sub-tasks: 1) Sequential Enumeration: Based
    on the concrete numerical value of a specific attribute, it requires the model
    to list all specific values corresponding to that attribute across multiple documents
    in a given order. 2) Extremum Acquisition: It requires the model to deduce the
    extremum of all values corresponding to certain attributes in multiple documents.
    3) Range Awareness: Given a specific numerical or conceptual range, the model
    should output all objects within multiple documents that meet the condition. The
    upper right of [FigureÂ 2](#S1.F2 "In 1 Introduction â€£ Leave No Document Behind:
    Benchmarking Long-Context LLMs with Extended Multi-Doc QA") gives an example of
    comparison task.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯”è¾ƒä»»åŠ¡åŒ…æ‹¬ä¸‰ä¸ªå­ä»»åŠ¡ï¼š1ï¼‰é¡ºåºæšä¸¾ï¼šæ ¹æ®ç‰¹å®šå±æ€§çš„å…·ä½“æ•°å€¼ï¼Œè¦æ±‚æ¨¡å‹åˆ—å‡ºå¤šä¸ªæ–‡æ¡£ä¸­ä¸è¯¥å±æ€§å¯¹åº”çš„æ‰€æœ‰å…·ä½“å€¼ï¼Œå¹¶æŒ‰ç»™å®šé¡ºåºæ’åˆ—ã€‚2ï¼‰æå€¼è·å–ï¼šè¦æ±‚æ¨¡å‹æ¨æ–­å¤šä¸ªæ–‡æ¡£ä¸­æŸäº›å±æ€§çš„æ‰€æœ‰å€¼çš„æå€¼ã€‚3ï¼‰èŒƒå›´æ„ŸçŸ¥ï¼šç»™å®šç‰¹å®šçš„æ•°å€¼æˆ–æ¦‚å¿µèŒƒå›´ï¼Œæ¨¡å‹åº”è¾“å‡ºå¤šä¸ªæ–‡æ¡£ä¸­ç¬¦åˆæ¡ä»¶çš„æ‰€æœ‰å¯¹è±¡ã€‚
    [å›¾2](#S1.F2 "åœ¨1å¼•è¨€ â€£ ä¸é—æ¼æ–‡æ¡£ï¼šåŸºäºæ‰©å±•å¤šæ–‡æ¡£QAçš„é•¿ä¸Šä¸‹æ–‡LLMsåŸºå‡†æµ‹è¯•")çš„å³ä¸Šè§’ç»™å‡ºäº†æ¯”è¾ƒä»»åŠ¡çš„ç¤ºä¾‹ã€‚
- en: 3.2.3 Clustering
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 èšç±»
- en: The clustering task entails an assessment of the modelâ€™s ability to cluster
    key information based on specific conditions across multi-document long contexts.
    This task claims that LLMs cluster relevant evidence scattered in multiple documents
    based on the specified criteria. Furthermore, it necessitates the extraction of
    pertinent information from documents and the integration of this information by
    grouping according to conditions.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: èšç±»ä»»åŠ¡æ¶‰åŠè¯„ä¼°æ¨¡å‹æ ¹æ®ç‰¹å®šæ¡ä»¶åœ¨å¤šæ–‡æ¡£é•¿ä¸Šä¸‹æ–‡ä¸­èšç±»å…³é”®ä¿¡æ¯çš„èƒ½åŠ›ã€‚æ­¤ä»»åŠ¡å£°æ˜ LLMs æ ¹æ®æŒ‡å®šæ ‡å‡†èšç±»åˆ†æ•£åœ¨å¤šä¸ªæ–‡æ¡£ä¸­çš„ç›¸å…³è¯æ®ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜è¦æ±‚ä»æ–‡æ¡£ä¸­æå–ç›¸å…³ä¿¡æ¯ï¼Œå¹¶æ ¹æ®æ¡ä»¶å¯¹è¿™äº›ä¿¡æ¯è¿›è¡Œæ•´åˆã€‚
- en: 'The clustering task encompasses three sub-tasks: 1) Report Integration: This
    sub-task requires the model to group the evidence existing in the provided financial
    reports into corresponding sets based on textual or numerical criteria. 2) Citation&Reference:
    For a given paper, the model is tasked with identifying its citations and references
    from the candidate papers. 3) Case Classification: Given the causes of several
    legal cases, the model is required to accurately categorize judgment documents.
    The bottom left of [FigureÂ 2](#S1.F2 "In 1 Introduction â€£ Leave No Document Behind:
    Benchmarking Long-Context LLMs with Extended Multi-Doc QA") depicts an example
    of the clustering task.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: èšç±»ä»»åŠ¡åŒ…æ‹¬ä¸‰ä¸ªå­ä»»åŠ¡ï¼š1) æŠ¥å‘Šæ•´åˆï¼šæ­¤å­ä»»åŠ¡è¦æ±‚æ¨¡å‹æ ¹æ®æ–‡æœ¬æˆ–æ•°å€¼æ ‡å‡†å°†æä¾›çš„è´¢åŠ¡æŠ¥å‘Šä¸­çš„è¯æ®åˆ†ç»„åˆ°ç›¸åº”çš„é›†åˆä¸­ã€‚2) å¼•ç”¨&å‚è€ƒï¼šå¯¹äºç»™å®šçš„è®ºæ–‡ï¼Œæ¨¡å‹çš„ä»»åŠ¡æ˜¯ä»å€™é€‰è®ºæ–‡ä¸­è¯†åˆ«å…¶å¼•ç”¨å’Œå‚è€ƒæ–‡çŒ®ã€‚3)
    æ¡ˆä¾‹åˆ†ç±»ï¼šæ ¹æ®å‡ ä¸ªæ³•å¾‹æ¡ˆä»¶çš„è¯‰å› ï¼Œæ¨¡å‹éœ€è¦å‡†ç¡®åˆ†ç±»åˆ¤æ–­æ–‡ä»¶ã€‚[å›¾ 2](#S1.F2 "åœ¨ 1 å¼•è¨€ â€£ ä¸ç•™æ–‡æ¡£ï¼šåŸºå‡†æµ‹è¯•é•¿ä¸Šä¸‹æ–‡ LLM çš„æ‰©å±•å¤šæ–‡æ¡£
    QA")çš„å·¦ä¸‹è§’æç»˜äº†èšç±»ä»»åŠ¡çš„ä¸€ä¸ªç¤ºä¾‹ã€‚
- en: 3.2.4 Chain of Reasoning
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4 æ¨ç†é“¾
- en: The chain of reasoning task requires the model to engage in multi-document reasoning
    along a logical pathway. This task evaluates the modelâ€™s proficiency in logical
    reasoning, which requires LLMs to locate the corresponding evidence within multiple
    documents and model the logical relationships among them for deducing the answer.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨ç†é“¾ä»»åŠ¡è¦æ±‚æ¨¡å‹æ²¿é€»è¾‘è·¯å¾„è¿›è¡Œå¤šæ–‡æ¡£æ¨ç†ã€‚æ­¤ä»»åŠ¡è¯„ä¼°æ¨¡å‹çš„é€»è¾‘æ¨ç†èƒ½åŠ›ï¼Œè¦æ±‚ LLMs åœ¨å¤šä¸ªæ–‡æ¡£ä¸­å®šä½ç›¸åº”çš„è¯æ®ï¼Œå¹¶å»ºæ¨¡å®ƒä»¬ä¹‹é—´çš„é€»è¾‘å…³ç³»ä»¥æ¨å¯¼ç­”æ¡ˆã€‚
- en: 'The chain of reasoning task contains four sub-tasks: 1) Temporal Analysis:
    This task requires the model to analyze the changes or trends of a particular
    attribute based on the temporal relationship, such as taking into account the
    financial reports of a certain company over consecutive years or multiple quarters.
    2) Citation Chain: This task requires the model to accurately understand each
    paperâ€™s content and its interconnections, ultimately inferring the linear citation
    relationships among them. 3) Link the Links: This task involves presenting fact
    descriptions and trial results from different judgment documents separately. The
    model is tasked with accurately pairing each fact description with its corresponding
    trial result. 4) Solitaire: This task first requires the model to match causes
    of action with judgment documents correctly, and then to sequentially infer multiple
    judgment documents based on the given sequence of causes of action. The bottom
    right of [FigureÂ 2](#S1.F2 "In 1 Introduction â€£ Leave No Document Behind: Benchmarking
    Long-Context LLMs with Extended Multi-Doc QA") gives an example of the chain of
    reasoning task.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨ç†é“¾ä»»åŠ¡åŒ…å«å››ä¸ªå­ä»»åŠ¡ï¼š1) æ—¶é—´åˆ†æï¼šæ­¤ä»»åŠ¡è¦æ±‚æ¨¡å‹åŸºäºæ—¶é—´å…³ç³»åˆ†æç‰¹å®šå±æ€§çš„å˜åŒ–æˆ–è¶‹åŠ¿ï¼Œä¾‹å¦‚è€ƒè™‘æŸå…¬å¸è¿ç»­å‡ å¹´æˆ–å¤šä¸ªå­£åº¦çš„è´¢åŠ¡æŠ¥å‘Šã€‚2) å¼•ç”¨é“¾ï¼šæ­¤ä»»åŠ¡è¦æ±‚æ¨¡å‹å‡†ç¡®ç†è§£æ¯ç¯‡è®ºæ–‡çš„å†…å®¹åŠå…¶ç›¸äº’è”ç³»ï¼Œæœ€ç»ˆæ¨æ–­å®ƒä»¬ä¹‹é—´çš„çº¿æ€§å¼•ç”¨å…³ç³»ã€‚3)
    é“¾æ¥é“¾æ¥ï¼šæ­¤ä»»åŠ¡æ¶‰åŠåˆ†åˆ«å‘ˆç°æ¥è‡ªä¸åŒåˆ¤æ–­æ–‡ä»¶çš„äº‹å®æè¿°å’Œå®¡åˆ¤ç»“æœã€‚æ¨¡å‹çš„ä»»åŠ¡æ˜¯å‡†ç¡®åœ°å°†æ¯ä¸ªäº‹å®æè¿°ä¸å…¶å¯¹åº”çš„å®¡åˆ¤ç»“æœé…å¯¹ã€‚4) æ’åæ¸¸æˆï¼šæ­¤ä»»åŠ¡é¦–å…ˆè¦æ±‚æ¨¡å‹æ­£ç¡®åŒ¹é…è¯‰å› ä¸åˆ¤æ–­æ–‡ä»¶ï¼Œç„¶åæ ¹æ®ç»™å®šçš„è¯‰å› é¡ºåºä¾æ¬¡æ¨æ–­å¤šä¸ªåˆ¤æ–­æ–‡ä»¶ã€‚[å›¾
    2](#S1.F2 "åœ¨ 1 å¼•è¨€ â€£ ä¸ç•™æ–‡æ¡£ï¼šåŸºå‡†æµ‹è¯•é•¿ä¸Šä¸‹æ–‡ LLM çš„æ‰©å±•å¤šæ–‡æ¡£ QA")çš„å³ä¸‹è§’ç»™å‡ºäº†æ¨ç†é“¾ä»»åŠ¡çš„ä¸€ä¸ªç¤ºä¾‹ã€‚
- en: '| Model | Spotlight Locating | Comparison | Clustering | Chain of Reasoning
    | Overall |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| æ¨¡å‹ | èšç„¦å®šä½ | æ¯”è¾ƒ | èšç±» | æ¨ç†é“¾ | æ€»ä½“ |'
- en: '| GPT-4o (128K) | 73.95 | 0.62 | 50.50 | 0.28 | 44.29 | 0.09 | 57.95 | 0.28
    | 53.47 | 0.26 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o (128K) | 73.95 | 0.62 | 50.50 | 0.28 | 44.29 | 0.09 | 57.95 | 0.28
    | 53.47 | 0.26 |'
- en: '| Gemini-Pro1.5 (1000K) | 75.02 | 0.56 | 49.94 | 0.27 | 44.10 | 0.09 | 64.97
    | 0.37 | 55.37 | 0.27 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro1.5 (1000K) | 75.02 | 0.56 | 49.94 | 0.27 | 44.10 | 0.09 | 64.97
    | 0.37 | 55.37 | 0.27 |'
- en: '| Claude3.5-Sonnet (200K) | 58.45 | 0.49 | 54.21 | 0.35 | 45.77 | 0.07 | 43.92
    | 0.25 | 48.85 | 0.23 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| Claude3.5-Sonnet (200K) | 58.45 | 0.49 | 54.21 | 0.35 | 45.77 | 0.07 | 43.92
    | 0.25 | 48.85 | 0.23 |'
- en: '| Qwen2-72B-Instruct (128K) | 54.17 | 0.36 | 42.38 | 0.20 | 36.71 | 0.04 |
    47.76 | 0.18 | 43.29 | 0.15 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Qwen2-72B-Instruct (128K) | 54.17 | 0.36 | 42.38 | 0.20 | 36.71 | 0.04 |
    47.76 | 0.18 | 43.29 | 0.15 |'
- en: '| Claude3-Haiku (200K) | 68.68 | 0.59 | 42.10 | 0.21 | 35.04 | 0.02 | 47.59
    | 0.17 | 44.88 | 0.19 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| Claude3-Haiku (200K) | 68.68 | 0.59 | 42.10 | 0.21 | 35.04 | 0.02 | 47.59
    | 0.17 | 44.88 | 0.19 |'
- en: '| Kimi-Chat (200k) | 60.98 | 0.50 | 34.74 | 0.13 | 28.76 | 0.04 | 38.52 | 0.15
    | 37.49 | 0.16 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| Kimi-Chat (200k) | 60.98 | 0.50 | 34.74 | 0.13 | 28.76 | 0.04 | 38.52 | 0.15
    | 37.49 | 0.16 |'
- en: '| GLM4-9B-Chat (1000K) | 57.35 | 0.47 | 40.38 | 0.20 | 28.52 | 0.02 | 39.94
    | 0.16 | 38.31 | 0.16 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| GLM4-9B-Chat (1000K) | 57.35 | 0.47 | 40.38 | 0.20 | 28.52 | 0.02 | 39.94
    | 0.16 | 38.31 | 0.16 |'
- en: 'Table 3: Overall results on four evaluation tasks. For each task, the indicator
    on the left represents the Avg Scores (0~100), while the right one represents
    the Perfect Rate (0~1).'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 3: å››é¡¹è¯„ä¼°ä»»åŠ¡çš„æ€»ä½“ç»“æœã€‚å¯¹äºæ¯é¡¹ä»»åŠ¡ï¼Œå·¦ä¾§æŒ‡æ ‡è¡¨ç¤ºå¹³å‡åˆ†æ•°ï¼ˆ0~100ï¼‰ï¼Œå³ä¾§æŒ‡æ ‡è¡¨ç¤ºå®Œç¾ç‡ï¼ˆ0~1ï¼‰ã€‚'
- en: '| Model | Spotlight Locating | Comparison | Clustering | Chain of Reasoning
    | Overall |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| æ¨¡å‹ | èšç„¦å®šä½ | æ¯”è¾ƒ | èšç±» | æ¨ç†é“¾ | æ€»ä½“ |'
- en: '| $\mathtt{Set1}$ (10K-50K) |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Set1}$ (10K-50K) |'
- en: '| GPT-4o (128K) | 85.67 | 0.81 | 64.27 | 0.33 | 57.01 | 0.24 | 81.58 | 0.55
    | 70.40 | 0.44 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o (128K) | 85.67 | 0.81 | 64.27 | 0.33 | 57.01 | 0.24 | 81.58 | 0.55
    | 70.40 | 0.44 |'
- en: '| Gemini-Pro1.5 (1000K) | 75.00 | 0.60 | 54.88 | 0.28 | 56.15 | 0.23 | 70.64
    | 0.37 | 63.36 | 0.34 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro1.5 (1000K) | 75.00 | 0.60 | 54.88 | 0.28 | 56.15 | 0.23 | 70.64
    | 0.37 | 63.36 | 0.34 |'
- en: '| Claude3.5-Sonnet (200K) | 60.85 | 0.55 | 69.07 | 0.47 | 58.63 | 0.13 | 68.57
    | 0.50 | 63.69 | 0.37 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| Claude3.5-Sonnet (200K) | 60.85 | 0.55 | 69.07 | 0.47 | 58.63 | 0.13 | 68.57
    | 0.50 | 63.69 | 0.37 |'
- en: '| Qwen2-72B-Instruct (128K) | 68.49 | 0.55 | 60.60 | 0.37 | 47.08 | 0.08 |
    70.39 | 0.36 | 60.11 | 0.29 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Qwen2-72B-Instruct (128K) | 68.49 | 0.55 | 60.60 | 0.37 | 47.08 | 0.08 |
    70.39 | 0.36 | 60.11 | 0.29 |'
- en: '| Claude3-Haiku (200K) | 60.94 | 0.55 | 59.97 | 0.40 | 45.53 | 0.04 | 66.85
    | 0.34 | 57.14 | 0.28 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Claude3-Haiku (200K) | 60.94 | 0.55 | 59.97 | 0.40 | 45.53 | 0.04 | 66.85
    | 0.34 | 57.14 | 0.28 |'
- en: '| Kimi-Chat (200k) | 81.11 | 0.74 | 46.70 | 0.20 | 47,84 | 0.07 | 53.77 | 0.17
    | 55.02 | 0.24 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Kimi-Chat (200k) | 81.11 | 0.74 | 46.70 | 0.20 | 47,84 | 0.07 | 53.77 | 0.17
    | 55.02 | 0.24 |'
- en: '| GLM4-9B-Chat (1000K) | 63.11 | 0.53 | 54.10 | 0.27 | 39.50 | 0.08 | 56.32
    | 0.28 | 51.43 | 0.25 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| GLM4-9B-Chat (1000K) | 63.11 | 0.53 | 54.10 | 0.27 | 39.50 | 0.08 | 56.32
    | 0.28 | 51.43 | 0.25 |'
- en: '| $\mathtt{Set2}$ (50K-100K) |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Set2}$ (50K-100K) |'
- en: '| GPT-4o (128K) | 86.76 | 0.72 | 59.81 | 0.40 | 47.83 | 0.11 | 62.09 | 0.34
    | 58.38 | 0.29 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o (128K) | 86.76 | 0.72 | 59.81 | 0.40 | 47.83 | 0.11 | 62.09 | 0.34
    | 58.38 | 0.29 |'
- en: '| Gemini-Pro1.5 (1000K) | 76.50 | 0.57 | 54.51 | 0.34 | 44.58 | 0.09 | 64.87
    | 0.34 | 55.56 | 0.26 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro1.5 (1000K) | 76.50 | 0.57 | 54.51 | 0.34 | 44.58 | 0.09 | 64.87
    | 0.34 | 55.56 | 0.26 |'
- en: '| Claude3.5-Sonnet (200K) | 63.83 | 0.53 | 58.90 | 0.39 | 50.96 | 0.10 | 46.09
    | 0.26 | 52.73 | 0.24 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Claude3.5-Sonnet (200K) | 63.83 | 0.53 | 58.90 | 0.39 | 50.96 | 0.10 | 46.09
    | 0.26 | 52.73 | 0.24 |'
- en: '| Qwen2-72B-Instruct (128K) | 64.53 | 0.43 | 42.60 | 0.21 | 38.52 | 0.05 |
    51.18 | 0.20 | 45.71 | 0.17 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Qwen2-72B-Instruct (128K) | 64.53 | 0.43 | 42.60 | 0.21 | 38.52 | 0.05 |
    51.18 | 0.20 | 45.71 | 0.17 |'
- en: '| Claude3-Haiku (200K) | 73.71 | 0.66 | 41.90 | 0.22 | 36.18 | 0.02 | 50.20
    | 0.15 | 45.45 | 0.17 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Claude3-Haiku (200K) | 73.71 | 0.66 | 41.90 | 0.22 | 36.18 | 0.02 | 50.20
    | 0.15 | 45.45 | 0.17 |'
- en: '| Kimi-Chat (200k) | 72.82 | 0.52 | 46.77 | 0.21 | 33.46 | 0.06 | 40.51 | 0.15
    | 42.40 | 0.16 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Kimi-Chat (200k) | 72.82 | 0.52 | 46.77 | 0.21 | 33.46 | 0.06 | 40.51 | 0.15
    | 42.40 | 0.16 |'
- en: '| GLM4-9B-Chat (1000K) | 65.04 | 0.54 | 41.80 | 0.23 | 30.72 | 0.02 | 42.34
    | 0.17 | 40.19 | 0.17 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| GLM4-9B-Chat (1000K) | 65.04 | 0.54 | 41.80 | 0.23 | 30.72 | 0.02 | 42.34
    | 0.17 | 40.19 | 0.17 |'
- en: '| $\mathtt{Set3}$ (100K-200K) |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Set3}$ (100K-200K) |'
- en: '| GPT-4o (128K) | 74.84 | 0.65 | 42.40 | 0.21 | 38.70 | 0.04 | 45.06 | 0.09
    | 46.95 | 0.19 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o (128K) | 74.84 | 0.65 | 42.40 | 0.21 | 38.70 | 0.04 | 45.06 | 0.09
    | 46.95 | 0.19 |'
- en: '| Gemini-Pro1.5 (1000K) | 81.25 | 0.56 | 44.66 | 0.20 | 39.90 | 0.05 | 58.38
    | 0.36 | 52.05 | 0.24 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro1.5 (1000K) | 81.25 | 0.56 | 44.66 | 0.20 | 39.90 | 0.05 | 58.38
    | 0.36 | 52.05 | 0.24 |'
- en: '| Claude3.5-Sonnet (200K) | 65.36 | 0.56 | 50.32 | 0.34 | 37.79 | 0.03 | 25.95
    | 0.11 | 42.06 | 0.19 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| Claude3.5-Sonnet (200K) | 65.36 | 0.56 | 50.32 | 0.34 | 37.79 | 0.03 | 25.95
    | 0.11 | 42.06 | 0.19 |'
- en: '| Qwen2-72B-Instruct (128K) | 46.99 | 0.27 | 37.06 | 0.13 | 31.50 | 0.02 |
    35.01 | 0.07 | 35.94 | 0.09 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| Qwen2-72B-Instruct (128K) | 46.99 | 0.27 | 37.06 | 0.13 | 31.50 | 0.02 |
    35.01 | 0.07 | 35.94 | 0.09 |'
- en: '| Claude3-Haiku (200K) | 77.81 | 0.67 | 37.07 | 0.17 | 30.94 | 0.01 | 36.87
    | 0.12 | 41.41 | 0.18 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| Claude3-Haiku (200K) | 77.81 | 0.67 | 37.07 | 0.17 | 30.94 | 0.01 | 36.87
    | 0.12 | 41.41 | 0.18 |'
- en: '| Kimi-Chat (200k) | 62.13 | 0.54 | 24.20 | 0.05 | 21.98 | 0.01 | 31.02 | 0.14
    | 31.37 | 0.14 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| Kimi-Chat (200k) | 62.13 | 0.54 | 24.20 | 0.05 | 21.98 | 0.01 | 31.02 | 0.14
    | 31.37 | 0.14 |'
- en: '| GLM4-9B-Chat (1000K) | 69.19 | 0.56 | 37.99 | 0.18 | 26.63 | 0.01 | 32.30
    | 0.09 | 37.36 | 0.16 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| GLM4-9B-Chat (1000K) | 69.19 | 0.56 | 37.99 | 0.18 | 26.63 | 0.01 | 32.30
    | 0.09 | 37.36 | 0.16 |'
- en: '| $\mathtt{Set4}$ (200K-250K) |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Set4}$ (200K-250K) |'
- en: '| GPT-4o (128K) | 36.79 | 0.19 | 23.97 | 0.08 | 30.40 | 0.00 | 32.89 | 0.07
    | 31.11 | 0.07 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o (128K) | 36.79 | 0.19 | 23.97 | 0.08 | 30.40 | 0.00 | 32.89 | 0.07
    | 31.11 | 0.07 |'
- en: '| Gemini-Pro1.5 (1000K) | 62.23 | 0.49 | 43.08 | 0.20 | 36.48 | 0.00 | 68.51
    | 0.49 | 50.70 | 0.25 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro1.5 (1000K) | 62.23 | 0.49 | 43.08 | 0.20 | 36.48 | 0.00 | 68.51
    | 0.49 | 50.70 | 0.25 |'
- en: '| Claude3.5-Sonnet (200K) | 36.91 | 0.24 | 28.82 | 0.05 | 28.68 | 0.00 | 28.77
    | 0.08 | 30.51 | 0.08 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| Claude3.5-Sonnet (200K) | 36.91 | 0.24 | 28.82 | 0.05 | 28.68 | 0.00 | 28.77
    | 0.08 | 30.51 | 0.08 |'
- en: '| Qwen2-72B-Instruct (128K) | 33.18 | 0.16 | 26.59 | 0.08 | 29.84 | 0.01 |
    25.81 | 0.04 | 28.92 | 0.06 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| Qwen2-72B-Instruct (128K) | 33.18 | 0.16 | 26.59 | 0.08 | 29.84 | 0.01 |
    25.81 | 0.04 | 28.92 | 0.06 |'
- en: '| Claude3-Haiku (200K) | 53.26 | 0.40 | 27.00 | 0.03 | 25.36 | 0.00 | 28.11
    | 0.05 | 32.15 | 0.10 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| Claude3-Haiku (200K) | 53.26 | 0.40 | 27.00 | 0.03 | 25.36 | 0.00 | 28.11
    | 0.05 | 32.15 | 0.10 |'
- en: '| Kimi-Chat (200k) | 20.17 | 0.12 | 9.17 | 0.00 | 5.65 | 0.00 | 22.61 | 0.11
    | 13.50 | 0.05 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| Kimi-Chat (200k) | 20.17 | 0.12 | 9.17 | 0.00 | 5.65 | 0.00 | 22.61 | 0.11
    | 13.50 | 0.05 |'
- en: '| GLM4-9B-Chat (1000K) | 15.67 | 0.12 | 21.33 | 0.05 | 12.35 | 0.00 | 21.04
    | 0.05 | 16.84 | 0.05 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| GLM4-9B-Chat (1000K) | 15.67 | 0.12 | 21.33 | 0.05 | 12.35 | 0.00 | 21.04
    | 0.05 | 16.84 | 0.05 |'
- en: 'Table 4: The performance of LLMs on four evaluation tasks with different length
    sets. For each task, the indicator on the left represents the Avg Scores (0~100),
    while the right one represents the Perfect Rate (0~1).'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 4ï¼šä¸åŒé•¿åº¦é›†ä¸ŠLLMçš„å››ä¸ªè¯„ä¼°ä»»åŠ¡çš„è¡¨ç°ã€‚æ¯ä¸ªä»»åŠ¡çš„å·¦ä¾§æŒ‡æ ‡è¡¨ç¤ºå¹³å‡åˆ†æ•°ï¼ˆ0~100ï¼‰ï¼Œå³ä¾§æŒ‡æ ‡è¡¨ç¤ºå®Œç¾ç‡ï¼ˆ0~1ï¼‰ã€‚
- en: 3.3 Benchmark Construction
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 åŸºå‡†æµ‹è¯•æ„å»º
- en: 3.3.1 Data Collection
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 æ•°æ®æ”¶é›†
- en: 'We established six criteria for the manual collection of the required English
    and Chinese documents: (1) Timeliness: The majority of the documents are the latest
    ones from the year 2024; (2) Accessibility: The data is publicly available and
    permitted for download and collection; (3) Appropriate Length: Collecting longer
    documents as much as possible and ensure they fit within the four designated length
    sets; (4) Parseability: Chosen documents are easy to process and parse, facilitating
    conversion into natural language text; (5) Categorizability: Documents can be
    manually sorted based on certain attributes, such as case type, research theme,
    or company category, allowing for organized archival; (6) Authoritativeness: All
    documents are collected from scratch from official websites (e.g. China Judge
    OnlineÂ³Â³3[https://wenshu.court.gov.cn/](https://wenshu.court.gov.cn/), U.S. SECâ´â´4[https://www.sec.gov/](https://www.sec.gov/),
    cninfâµâµ5[http://www.cninfo.com.cn/](http://www.cninfo.com.cn/), Arxivâ¶â¶6[https://arxiv.org/](https://arxiv.org/),
    Semantic Scholarâ·â·7[https://www.semanticscholar.org/](https://www.semanticscholar.org/)),
    ensuring the quality and authority of the documents.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸ºæ‰‹åŠ¨æ”¶é›†æ‰€éœ€çš„è‹±æ–‡å’Œä¸­æ–‡æ–‡æ¡£å»ºç«‹äº†å…­ä¸ªæ ‡å‡†ï¼šï¼ˆ1ï¼‰æ—¶æ•ˆæ€§ï¼šå¤§å¤šæ•°æ–‡æ¡£ä¸º2024å¹´çš„æœ€æ–°æ–‡æ¡£ï¼›ï¼ˆ2ï¼‰å¯è·å–æ€§ï¼šæ•°æ®å…¬å¼€å¯ç”¨ï¼Œå…è®¸ä¸‹è½½å’Œæ”¶é›†ï¼›ï¼ˆ3ï¼‰é€‚å½“é•¿åº¦ï¼šå°½å¯èƒ½æ”¶é›†è¾ƒé•¿çš„æ–‡æ¡£ï¼Œå¹¶ç¡®ä¿å®ƒä»¬ç¬¦åˆå››ä¸ªæŒ‡å®šçš„é•¿åº¦é›†ï¼›ï¼ˆ4ï¼‰å¯è§£ææ€§ï¼šé€‰æ‹©çš„æ–‡æ¡£æ˜“äºå¤„ç†å’Œè§£æï¼Œä¾¿äºè½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æ–‡æœ¬ï¼›ï¼ˆ5ï¼‰å¯åˆ†ç±»æ€§ï¼šæ–‡æ¡£å¯ä»¥æ ¹æ®æŸäº›å±æ€§ï¼ˆå¦‚æ¡ˆä»¶ç±»å‹ã€ç ”ç©¶ä¸»é¢˜æˆ–å…¬å¸ç±»åˆ«ï¼‰æ‰‹åŠ¨æ’åºï¼Œæ–¹ä¾¿æœ‰åºå½’æ¡£ï¼›ï¼ˆ6ï¼‰æƒå¨æ€§ï¼šæ‰€æœ‰æ–‡æ¡£å‡ä»å®˜æ–¹ç½‘ç«™ï¼ˆä¾‹å¦‚ï¼Œä¸­å›½è£åˆ¤æ–‡ä¹¦ç½‘Â³Â³Â³[https://wenshu.court.gov.cn/](https://wenshu.court.gov.cn/)ï¼Œç¾å›½è¯åˆ¸äº¤æ˜“å§”å‘˜ä¼šâ´â´â´[https://www.sec.gov/](https://www.sec.gov/)ï¼Œcninfâµâµâµ[http://www.cninfo.com.cn/](http://www.cninfo.com.cn/)ï¼ŒArxivâ¶â¶â¶[https://arxiv.org/](https://arxiv.org/)ï¼ŒSemantic
    Scholarâ·â·â·[https://www.semanticscholar.org/](https://www.semanticscholar.org/))ä»å¤´æ”¶é›†ï¼Œç¡®ä¿æ–‡æ¡£çš„è´¨é‡å’Œæƒå¨æ€§ã€‚
- en: Specifically, regarding financial reports, we primarily collect the latest quarterly
    and annual reports for the year 2024, totaling 574 documents. For legal documents,
    our collection consists exclusively of cases adjudicated by the higher and intermediate
    courts in 2024, amounting to 629 documents. As for academic papers, our focus
    is on procuring the latest articles from arXiv in 2024, with a total of 764 papers.
    Additionally, to meet the requirements of the chain of reasoning task, we gather
    a small portion of financial reports and academic papers from before 2024. Upon
    the collection of documents, we first parse these documents, converting them uniformly
    into TXT format. Subsequently, we carry out further data cleansing, removing any
    portions that contain personal information.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: å…·ä½“è€Œè¨€ï¼Œå…³äºè´¢åŠ¡æŠ¥å‘Šï¼Œæˆ‘ä»¬ä¸»è¦æ”¶é›†2024å¹´çš„æœ€æ–°å­£åº¦å’Œå¹´åº¦æŠ¥å‘Šï¼Œæ€»è®¡574ä»½ã€‚å¯¹äºæ³•å¾‹æ–‡æ¡£ï¼Œæˆ‘ä»¬çš„æ”¶é›†ä»…åŒ…æ‹¬2024å¹´ç”±é«˜çº§å’Œä¸­çº§æ³•é™¢è£å†³çš„æ¡ˆä»¶ï¼Œæ€»è®¡629ä»½ã€‚è‡³äºå­¦æœ¯è®ºæ–‡ï¼Œæˆ‘ä»¬ä¸“æ³¨äºé‡‡è´­2024å¹´æ¥è‡ªarXivçš„æœ€æ–°æ–‡ç« ï¼Œæ€»è®¡764ç¯‡ã€‚æ­¤å¤–ï¼Œä¸ºæ»¡è¶³æ¨ç†ä»»åŠ¡çš„è¦æ±‚ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸€å°éƒ¨åˆ†2024å¹´ä¹‹å‰çš„è´¢åŠ¡æŠ¥å‘Šå’Œå­¦æœ¯è®ºæ–‡ã€‚æ–‡æ¡£æ”¶é›†åï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹è¿™äº›æ–‡æ¡£è¿›è¡Œè§£æï¼Œç»Ÿä¸€è½¬æ¢ä¸ºTXTæ ¼å¼ã€‚éšåï¼Œæˆ‘ä»¬è¿›è¡Œè¿›ä¸€æ­¥çš„æ•°æ®æ¸…ç†ï¼Œå»é™¤åŒ…å«ä¸ªäººä¿¡æ¯çš„éƒ¨åˆ†ã€‚
- en: 3.3.2 Annotation Process
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 æ³¨é‡Šè¿‡ç¨‹
- en: Compared to annotating short texts, annotating long texts is more challenging.
    To address this issue, we designed innovative annotation workflows to reduce the
    cost of annotation while ensuring quality.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ ‡æ³¨çŸ­æ–‡æœ¬ç›¸æ¯”ï¼Œæ ‡æ³¨é•¿æ–‡æœ¬æ›´åŠ å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†åˆ›æ–°çš„æ ‡æ³¨å·¥ä½œæµç¨‹ï¼Œä»¥é™ä½æ ‡æ³¨æˆæœ¬ï¼ŒåŒæ—¶ç¡®ä¿è´¨é‡ã€‚
- en: For financial reports, we compress the information contained within the long
    context, breaking down the annotation process into numerous simple tasks. We initially
    manually identify hundreds of key attributes that cover the important information
    in the long context. Subsequently, we employ GPT-4o to execute the relatively
    simple task of information extraction, pulling the values corresponding to these
    key attributes. After obtaining the key attributes and their corresponding values,
    we can proceed to annotate only the compressed information, eliminating the need
    to refer back to the original lengthy texts. For legal cases, we follow the classification
    provided by China Judge Online, manually downloading judgment documents sorted
    by different causes of action and case types. Additionally, we use a rule-based
    method to segment each judgment document into its factual statement and verdict
    sections. For academic papers, we leverage the Semantic Scholar websiteâ€™s API
    to access the target paperâ€™s citations and references. Moreover, by utilizing
    the bbl files of each arXiv paper, we write scripts to recursively collect articles
    that meet the requirements of the linear citation chain task.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè´¢åŠ¡æŠ¥å‘Šï¼Œæˆ‘ä»¬å‹ç¼©é•¿æ–‡æœ¬ä¸­çš„ä¿¡æ¯ï¼Œå°†æ ‡æ³¨è¿‡ç¨‹åˆ†è§£ä¸ºå¤šä¸ªç®€å•ä»»åŠ¡ã€‚æˆ‘ä»¬é¦–å…ˆæ‰‹åŠ¨è¯†åˆ«æ•°ç™¾ä¸ªå…³é”®å±æ€§ï¼Œè¿™äº›å±æ€§æ¶µç›–äº†é•¿æ–‡æœ¬ä¸­çš„é‡è¦ä¿¡æ¯ã€‚éšåï¼Œæˆ‘ä»¬åˆ©ç”¨GPT-4oæ‰§è¡Œç›¸å¯¹ç®€å•çš„ä¿¡æ¯æå–ä»»åŠ¡ï¼Œæå–ä¸è¿™äº›å…³é”®å±æ€§å¯¹åº”çš„å€¼ã€‚åœ¨è·å¾—å…³é”®å±æ€§åŠå…¶å¯¹åº”å€¼åï¼Œæˆ‘ä»¬å¯ä»¥ä»…å¯¹å‹ç¼©ä¿¡æ¯è¿›è¡Œæ ‡æ³¨ï¼Œæ— éœ€å‚è€ƒåŸå§‹å†—é•¿çš„æ–‡æœ¬ã€‚å¯¹äºæ³•å¾‹æ¡ˆä»¶ï¼Œæˆ‘ä»¬éµå¾ªä¸­å›½æ³•å®˜ç½‘æä¾›çš„åˆ†ç±»ï¼Œæ‰‹åŠ¨ä¸‹è½½æŒ‰ä¸åŒè¯‰è®¼åŸå› å’Œæ¡ˆä»¶ç±»å‹æ’åºçš„åˆ¤å†³æ–‡ä¹¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºäºè§„åˆ™çš„æ–¹æ³•å°†æ¯ä»½åˆ¤å†³æ–‡ä¹¦åˆ†å‰²ä¸ºäº‹å®é™ˆè¿°å’Œè£å†³éƒ¨åˆ†ã€‚å¯¹äºå­¦æœ¯è®ºæ–‡ï¼Œæˆ‘ä»¬åˆ©ç”¨Semantic
    Scholarç½‘ç«™çš„APIè®¿é—®ç›®æ ‡è®ºæ–‡çš„å¼•æ–‡å’Œå‚è€ƒæ–‡çŒ®ã€‚æ­¤å¤–ï¼Œé€šè¿‡åˆ©ç”¨æ¯ç¯‡arXivè®ºæ–‡çš„bblæ–‡ä»¶ï¼Œæˆ‘ä»¬ç¼–å†™è„šæœ¬é€’å½’æ”¶é›†ç¬¦åˆçº¿æ€§å¼•æ–‡é“¾ä»»åŠ¡è¦æ±‚çš„æ–‡ç« ã€‚
- en: 'During the question-and-answer annotation phase, we adopt two approaches: (1)
    Template-based: We design question types and templates, and based on pre-classified
    documents, we construct Q&A pairs using rules. (2) Free annotation: Referring
    to the compressed information of multiple documents, we design prompts with four
    different task descriptions. We employ GPT-4o to generate Q&A pairs for each task.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é—®ç­”æ ‡æ³¨é˜¶æ®µï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸¤ç§æ–¹æ³•ï¼šï¼ˆ1ï¼‰åŸºäºæ¨¡æ¿çš„æ–¹æ³•ï¼šæˆ‘ä»¬è®¾è®¡é—®é¢˜ç±»å‹å’Œæ¨¡æ¿ï¼Œå¹¶æ ¹æ®é¢„åˆ†ç±»çš„æ–‡æ¡£ï¼Œä½¿ç”¨è§„åˆ™æ„å»ºé—®ç­”å¯¹ã€‚ï¼ˆ2ï¼‰è‡ªç”±æ ‡æ³¨ï¼šå‚è€ƒå¤šä»½æ–‡æ¡£çš„å‹ç¼©ä¿¡æ¯ï¼Œæˆ‘ä»¬è®¾è®¡äº†å››ç§ä¸åŒä»»åŠ¡æè¿°çš„æç¤ºã€‚æˆ‘ä»¬åˆ©ç”¨GPT-4oç”Ÿæˆæ¯ä¸ªä»»åŠ¡çš„é—®ç­”å¯¹ã€‚
- en: 3.3.3 Quality Control
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 è´¨é‡æ§åˆ¶
- en: 'Throughout the annotation process, we employ several methods to ensure accuracy:
    (1) Evidence Recall: By designing prompts that not only prompt GPT-4o to generate
    labels but also to recall evidence supporting the labels from the text, significantly
    enhancing the accuracy in practical applications. (2) Self-Check: GPT-4o reviews
    the original text to re-evaluate and correct any mistakes in the generated labels.
    (3) Manual Check: We manually review and confirm the quality of annotations, eliminating
    any unreasonable or low-quality questions. Additionally, we also take into account
    the distribution and number of different length sets, sub-tasks, and language.
    From a pool of 2,814 entries, we conduct a secondary selection process, ultimately
    choosing 1,600 entries for our final benchmark.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ•´ä¸ªæ ‡æ³¨è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å‡ ç§æ–¹æ³•ä»¥ç¡®ä¿å‡†ç¡®æ€§ï¼šï¼ˆ1ï¼‰è¯æ®å›é¡¾ï¼šé€šè¿‡è®¾è®¡æç¤ºï¼Œä¸ä»…ä¿ƒä½¿GPT-4oç”Ÿæˆæ ‡ç­¾ï¼Œè¿˜ä»æ–‡æœ¬ä¸­å›é¡¾æ”¯æŒæ ‡ç­¾çš„è¯æ®ï¼Œæ˜¾è‘—æé«˜äº†å®é™…åº”ç”¨ä¸­çš„å‡†ç¡®æ€§ã€‚ï¼ˆ2ï¼‰è‡ªæˆ‘æ£€æŸ¥ï¼šGPT-4oå®¡æŸ¥åŸå§‹æ–‡æœ¬ï¼Œä»¥é‡æ–°è¯„ä¼°å¹¶çº æ­£ç”Ÿæˆæ ‡ç­¾ä¸­çš„ä»»ä½•é”™è¯¯ã€‚ï¼ˆ3ï¼‰äººå·¥æ£€æŸ¥ï¼šæˆ‘ä»¬æ‰‹åŠ¨å®¡æŸ¥å’Œç¡®è®¤æ ‡æ³¨è´¨é‡ï¼Œæ¶ˆé™¤ä»»ä½•ä¸åˆç†æˆ–ä½è´¨é‡çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è€ƒè™‘äº†ä¸åŒé•¿åº¦é›†ã€å­ä»»åŠ¡å’Œè¯­è¨€çš„åˆ†å¸ƒå’Œæ•°é‡ã€‚ä»2,814æ¡æ¡ç›®ä¸­ï¼Œæˆ‘ä»¬è¿›è¡ŒäºŒæ¬¡ç­›é€‰ï¼Œæœ€ç»ˆé€‰æ‹©äº†1,600æ¡ä½œä¸ºæˆ‘ä»¬çš„æœ€ç»ˆåŸºå‡†ã€‚
- en: 4 Experiments
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 å®éªŒ
- en: 4.1 Experimental Setup
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 å®éªŒè®¾ç½®
- en: 'Models We evaluate six advanced long-context LLMs, with their context window
    sizes ranging from 128K to 1000K, including API-based LLMs: GPT-4o-128KÂ (OpenAI,
    [2023](#bib.bib19)), Gemini-Pro1.5-1000KÂ (Reid etÂ al., [2024](#bib.bib25)), Claude3.5-Sonnet-200KÂ (Anthropic,
    [2024b](#bib.bib3)), Claude3-Haiku-200KÂ (Anthropic, [2024a](#bib.bib2)), Kimi-Chat-200Kâ¸â¸8[https://kimi.moonshot.cn/](https://kimi.moonshot.cn/)
    and Open-sourced LLMs: Qwen2-72B-Instruct-128KÂ (Bai etÂ al., [2023a](#bib.bib4)),
    GLM4-9B-Chat-1000KÂ (Du etÂ al., [2022](#bib.bib12)).'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹ æˆ‘ä»¬è¯„ä¼°äº†å…­ä¸ªå…ˆè¿›çš„é•¿æ–‡æœ¬ä¸Šä¸‹æ–‡ LLMï¼Œå…¶ä¸Šä¸‹æ–‡çª—å£å¤§å°ä»128Kåˆ°1000Kï¼ŒåŒ…æ‹¬åŸºäº API çš„ LLMï¼šGPT-4o-128K (OpenAI,
    [2023](#bib.bib19))ï¼ŒGemini-Pro1.5-1000K (Reid et al., [2024](#bib.bib25))ï¼ŒClaude3.5-Sonnet-200K
    (Anthropic, [2024b](#bib.bib3))ï¼ŒClaude3-Haiku-200K (Anthropic, [2024a](#bib.bib2))ï¼ŒKimi-Chat-200Kâ¸â¸8
    [https://kimi.moonshot.cn/](https://kimi.moonshot.cn/) å’Œå¼€æº LLMï¼šQwen2-72B-Instruct-128K
    (Bai et al., [2023a](#bib.bib4))ï¼ŒGLM4-9B-Chat-1000K (Du et al., [2022](#bib.bib12))ã€‚
- en: 'Evaluation Metric In the long-context question-answering scenarios, traditional
    evaluation metrics F1 and Rouge-L may lead to inaccurate responses. Recent researchÂ (Liu
    etÂ al., [2024](#bib.bib18); Wang etÂ al., [2024](#bib.bib32)) indicates that the
    GPT-4Â (OpenAI, [2023](#bib.bib19)) evaluator demonstrates high consistency with
    human evaluations, making it a reasonably reliable annotator. Building on these
    considerations, we prompt GPT-4 as a judge to evaluate the modelâ€™s output based
    on the golden answer and the questionâ€™s requirements from three aspects: Accuracy,
    Hallucinations, and Completeness, scoring from 0 to 100\. For a detailed prompt,
    please refer to the [AppendixÂ A](#A1 "Appendix A GPT4-as-the-Judge Prompt â€£ Leave
    No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA").
    We also design two indicators: (1) Avg Scores: the average value of scores given
    by GPT-4 for all questions; (2) Perfect Rate: the proportion of cases scoring
    100 out of the total cases. The latter is a more stringent evaluation metric compared
    to the former.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¯„ä¼°æŒ‡æ ‡ åœ¨é•¿æ–‡æœ¬é—®ç­”åœºæ™¯ä¸­ï¼Œä¼ ç»Ÿè¯„ä¼°æŒ‡æ ‡ F1 å’Œ Rouge-L å¯èƒ½å¯¼è‡´ä¸å‡†ç¡®çš„å›ç­”ã€‚æœ€è¿‘çš„ç ”ç©¶ (Liu et al., [2024](#bib.bib18)ï¼›Wang
    et al., [2024](#bib.bib32)) è¡¨æ˜ï¼ŒGPT-4 (OpenAI, [2023](#bib.bib19)) è¯„ä¼°å™¨ä¸äººå·¥è¯„ä¼°å…·æœ‰é«˜åº¦ä¸€è‡´æ€§ï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªç›¸å¯¹å¯é çš„æ³¨é‡Šå·¥å…·ã€‚åŸºäºè¿™äº›è€ƒè™‘ï¼Œæˆ‘ä»¬ä½¿ç”¨
    GPT-4 ä½œä¸ºè¯„åˆ¤è€…ï¼Œä»å‡†ç¡®æ€§ã€å¹»è§‰å’Œå®Œæ•´æ€§ä¸‰ä¸ªæ–¹é¢å¯¹æ¨¡å‹è¾“å‡ºè¿›è¡Œè¯„ä¼°ï¼Œè¯„åˆ†èŒƒå›´ä» 0 åˆ° 100ã€‚æœ‰å…³è¯¦ç»†æç¤ºï¼Œè¯·å‚è§[é™„å½• A](#A1 "Appendix
    A GPT4-as-the-Judge Prompt â€£ Leave No Document Behind: Benchmarking Long-Context
    LLMs with Extended Multi-Doc QA")ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸¤ä¸ªæŒ‡æ ‡ï¼š(1) å¹³å‡å¾—åˆ†ï¼šGPT-4 å¯¹æ‰€æœ‰é—®é¢˜ç»™å‡ºçš„å¾—åˆ†çš„å¹³å‡å€¼ï¼›(2)
    å®Œç¾ç‡ï¼šåœ¨æ€»æ¡ˆä¾‹ä¸­å¾—åˆ†ä¸º100çš„æ¡ˆä¾‹æ¯”ä¾‹ã€‚åè€…æ˜¯æ¯”å‰è€…æ›´ä¸¥æ ¼çš„è¯„ä¼°æŒ‡æ ‡ã€‚'
- en: 'Prompt Templates For different sub-tasks, we require the model to follow the
    given instructions and output the answer according to the specific prompts shown
    in [AppendixÂ B](#A2 "Appendix B Test Case â€£ Leave No Document Behind: Benchmarking
    Long-Context LLMs with Extended Multi-Doc QA").'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 'é’ˆå¯¹ä¸åŒçš„å­ä»»åŠ¡ï¼Œæˆ‘ä»¬è¦æ±‚æ¨¡å‹éµå¾ªç»™å®šçš„æŒ‡ä»¤ï¼Œå¹¶æ ¹æ®[é™„å½• B](#A2 "Appendix B Test Case â€£ Leave No Document
    Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA")ä¸­æ‰€ç¤ºçš„å…·ä½“æç¤ºè¾“å‡ºç­”æ¡ˆã€‚'
- en: Input Truncation Due to input length limits, we assess whether adding a document
    would exceed the modelâ€™s processing length when concatenating multiple documents.
    If appending the document would surpass the modelâ€™s capacity, we discard it from
    the concatenation process. The evaluation and selection process continues until
    we have reviewed all documents that need concatenation.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥æˆªæ–­ ç”±äºè¾“å…¥é•¿åº¦é™åˆ¶ï¼Œæˆ‘ä»¬è¯„ä¼°åœ¨è¿æ¥å¤šä¸ªæ–‡æ¡£æ—¶ï¼Œæ·»åŠ æ–‡æ¡£æ˜¯å¦ä¼šè¶…å‡ºæ¨¡å‹çš„å¤„ç†é•¿åº¦ã€‚å¦‚æœé™„åŠ æ–‡æ¡£ä¼šè¶…è¿‡æ¨¡å‹çš„å®¹é‡ï¼Œæˆ‘ä»¬å°†å…¶ä»è¿æ¥è¿‡ç¨‹ä¸­åˆ é™¤ã€‚è¯„ä¼°å’Œé€‰æ‹©è¿‡ç¨‹å°†ç»§ç»­ï¼Œç›´åˆ°æˆ‘ä»¬å®¡æŸ¥å®Œæ‰€æœ‰éœ€è¦è¿æ¥çš„æ–‡æ¡£ã€‚
- en: Implement Details We set â€˜temperature = 0â€™ to eliminate randomness and keep
    other hyper-parameters default. For API-Based LLMs, we directly utilize the official
    API for testing. Since the Kimi-Chat-200k currently does not provide an interface,
    we manually input content on the web. As for open-source models, we conduct experiments
    on a server with 8$\times$A100 80GB.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: å®æ–½ç»†èŠ‚ æˆ‘ä»¬è®¾ç½®äº†â€˜temperature = 0â€™ä»¥æ¶ˆé™¤éšæœºæ€§ï¼Œå…¶ä»–è¶…å‚æ•°ä¿æŒé»˜è®¤ã€‚å¯¹äºåŸºäº API çš„ LLMï¼Œæˆ‘ä»¬ç›´æ¥åˆ©ç”¨å®˜æ–¹ API è¿›è¡Œæµ‹è¯•ã€‚ç”±äº
    Kimi-Chat-200k å½“å‰æœªæä¾›æ¥å£ï¼Œæˆ‘ä»¬åœ¨ç½‘é¡µä¸Šæ‰‹åŠ¨è¾“å…¥å†…å®¹ã€‚è‡³äºå¼€æºæ¨¡å‹ï¼Œæˆ‘ä»¬åœ¨é…ç½®ä¸º8$\times$A100 80GBçš„æœåŠ¡å™¨ä¸Šè¿›è¡Œå®éªŒã€‚
- en: 4.2 Main Results
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 ä¸»è¦ç»“æœ
- en: 'We assess six advanced LLMs on the Loong benchmark. The main results are shown
    in [TableÂ 3](#S3.T3 "In 3.2.4 Chain of Reasoning â€£ 3.2 Evaluation Task â€£ 3 Loong:
    A Long-Context Benchmark â€£ Leave No Document Behind: Benchmarking Long-Context
    LLMs with Extended Multi-Doc QA") and [TableÂ 4](#S3.T4 "In 3.2.4 Chain of Reasoning
    â€£ 3.2 Evaluation Task â€£ 3 Loong: A Long-Context Benchmark â€£ Leave No Document
    Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA"). We can see
    that Gemini-Pro-1.5 shows the best overall performance, especially excelling in
    the processing of ultra-long context within $\mathtt{Set3}$. Its comprehensive
    score reached 55.37 with the perfect rate of 27%, followed by GPT-4o. Besides,
    the long-context modeling capacity of open-source models still falls short when
    compared to that of the most powerful closed-source models in the Loong. Additionally,
    larger-parameter models outperform their smaller counterparts within the same
    window size, indicating the advantages of scaling up model sizes for improved
    long-context modeling. The overall assessment results highlight that even the
    most advanced long-context LLMs currently fail to achieve passing marks, particularly
    in terms of the perfect rate. This suggests that there exists significant room
    for improvement in the long-context modeling capabilities of LLMs.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 'æˆ‘ä»¬åœ¨LoongåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†å…­ä¸ªå…ˆè¿›çš„LLMã€‚ä¸»è¦ç»“æœå±•ç¤ºåœ¨[è¡¨3](#S3.T3 "åœ¨3.2.4 æ¨ç†é“¾ â€£ 3.2 è¯„ä¼°ä»»åŠ¡ â€£ 3 Loong:
    ä¸€ä¸ªé•¿æ–‡æ¡£åŸºå‡† â€£ ä¸é—æ¼ä»»ä½•æ–‡æ¡£ï¼šç”¨æ‰©å±•çš„å¤šæ–‡æ¡£é—®ç­”æ¥åŸºå‡†æµ‹è¯•é•¿æ–‡æ¡£LLM")å’Œ[è¡¨4](#S3.T4 "åœ¨3.2.4 æ¨ç†é“¾ â€£ 3.2 è¯„ä¼°ä»»åŠ¡ â€£
    3 Loong: ä¸€ä¸ªé•¿æ–‡æ¡£åŸºå‡† â€£ ä¸é—æ¼ä»»ä½•æ–‡æ¡£ï¼šç”¨æ‰©å±•çš„å¤šæ–‡æ¡£é—®ç­”æ¥åŸºå‡†æµ‹è¯•é•¿æ–‡æ¡£LLM")ä¸­ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒGemini-Pro-1.5è¡¨ç°æœ€ä½³ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†$\mathtt{Set3}$ä¸­çš„è¶…é•¿æ–‡æ¡£æ—¶è¡¨ç°çªå‡ºã€‚å…¶ç»¼åˆå¾—åˆ†è¾¾åˆ°55.37ï¼Œå®Œç¾ç‡ä¸º27%ï¼Œå…¶æ¬¡æ˜¯GPT-4oã€‚æ­¤å¤–ï¼Œä¸æœ€å¼ºçš„é—­æºæ¨¡å‹ç›¸æ¯”ï¼Œå¼€æºæ¨¡å‹çš„é•¿æ–‡æ¡£å»ºæ¨¡èƒ½åŠ›ä»æ˜¾ä¸è¶³ã€‚æ­¤å¤–ï¼Œåœ¨ç›¸åŒçª—å£å¤§å°ä¸‹ï¼Œå‚æ•°è¾ƒå¤§çš„æ¨¡å‹ä¼˜äºå‚æ•°è¾ƒå°çš„æ¨¡å‹ï¼Œè¿™è¡¨æ˜æ¨¡å‹è§„æ¨¡æ‰©å¤§æœ‰åŠ©äºæé«˜é•¿æ–‡æ¡£å»ºæ¨¡èƒ½åŠ›ã€‚æ€»ä½“è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„é•¿æ–‡æ¡£LLMï¼Œç›®å‰ä¹Ÿæœªèƒ½è¾¾åˆ°åŠæ ¼åˆ†æ•°ï¼Œç‰¹åˆ«æ˜¯åœ¨å®Œç¾ç‡æ–¹é¢ã€‚è¿™è¡¨æ˜LLMçš„é•¿æ–‡æ¡£å»ºæ¨¡èƒ½åŠ›ä»æœ‰æ˜¾è‘—æå‡ç©ºé—´ã€‚'
- en: 4.3 Task Analysis
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 ä»»åŠ¡åˆ†æ
- en: Analyzing performance across different tasks, models exhibit their best performance
    in the spotlight locating task. This can be attributed to the taskâ€™s relative
    simplicity, which tests the foundational capabilities of long-context modeling.
    Moreover, the evidence is only distributed within a single document, making it
    easier to locate and less prone to confusion. In contrast, due to the requirements
    of multi-source information inference, the comparison and cluster tasks present
    greater challenges, leading to model underperformance. These tasks necessitate
    not only the collection of evidence across documents but also involve complex
    reasoning processes such as matching, contrasting, and classification. Thus, they
    more rigorously test the higher-order capabilities of long-context modeling, revealing
    significant gaps in the current modelsâ€™ abilities. Regarding the chain of reasoning
    task, models perform well within Set1\. However, as the context length increases,
    their performance drastically declines. This suggests that within the scope of
    long-context modeling capabilities, LLMs possess adequate skills in temporal analysis,
    logical sequencing, and linking multiple concepts. Nevertheless, an overflow in
    context length leads to the loss of key evidence, severely impacting the accuracy
    of chain reasoning tasks.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†æä¸åŒä»»åŠ¡çš„è¡¨ç°ï¼Œæ¨¡å‹åœ¨èšå…‰ç¯å®šä½ä»»åŠ¡ä¸­çš„è¡¨ç°æœ€ä½³ã€‚è¿™å¯ä»¥å½’å› äºä»»åŠ¡çš„ç›¸å¯¹ç®€å•æ€§ï¼Œå®ƒæµ‹è¯•äº†é•¿æ–‡æ¡£å»ºæ¨¡çš„åŸºç¡€èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯æ®ä»…åˆ†å¸ƒåœ¨å•ä¸ªæ–‡æ¡£å†…ï¼Œä½¿å…¶æ›´æ˜“äºå®šä½ä¸”ä¸æ˜“æ··æ·†ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç”±äºéœ€è¦å¤šæºä¿¡æ¯æ¨ç†ï¼Œæ¯”è¾ƒå’Œèšç±»ä»»åŠ¡é¢ä¸´æ›´å¤§æŒ‘æˆ˜ï¼Œå¯¼è‡´æ¨¡å‹è¡¨ç°ä¸ä½³ã€‚è¿™äº›ä»»åŠ¡ä¸ä»…éœ€è¦è·¨æ–‡æ¡£æ”¶é›†è¯æ®ï¼Œè¿˜æ¶‰åŠåŒ¹é…ã€å¯¹æ¯”å’Œåˆ†ç±»ç­‰å¤æ‚æ¨ç†è¿‡ç¨‹ã€‚å› æ­¤ï¼Œå®ƒä»¬æ›´ä¸¥æ ¼åœ°æµ‹è¯•äº†é•¿æ–‡æ¡£å»ºæ¨¡çš„é«˜é˜¶èƒ½åŠ›ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹èƒ½åŠ›çš„æ˜¾è‘—å·®è·ã€‚å…³äºæ¨ç†é“¾ä»»åŠ¡ï¼Œæ¨¡å‹åœ¨Set1ä¸­è¡¨ç°è‰¯å¥½ã€‚ç„¶è€Œï¼Œéšç€ä¸Šä¸‹æ–‡é•¿åº¦çš„å¢åŠ ï¼Œå®ƒä»¬çš„è¡¨ç°æ€¥å‰§ä¸‹é™ã€‚è¿™è¡¨æ˜ï¼Œåœ¨é•¿æ–‡æ¡£å»ºæ¨¡èƒ½åŠ›çš„èŒƒå›´å†…ï¼ŒLLMåœ¨æ—¶é—´åˆ†æã€é€»è¾‘æ’åºå’Œé“¾æ¥å¤šä¸ªæ¦‚å¿µæ–¹é¢å…·æœ‰è¶³å¤Ÿçš„æŠ€èƒ½ã€‚ç„¶è€Œï¼Œä¸Šä¸‹æ–‡é•¿åº¦çš„æº¢å‡ºå¯¼è‡´å…³é”®è¯æ®ä¸¢å¤±ï¼Œä¸¥é‡å½±å“æ¨ç†é“¾ä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚
- en: '![Refer to caption](img/e260e607c02eb69963bf19736f0570f0.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/e260e607c02eb69963bf19736f0570f0.png)'
- en: (a) The overall results on all length sets.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: (a) æ‰€æœ‰é•¿åº¦é›†çš„æ•´ä½“ç»“æœã€‚
- en: '![Refer to caption](img/8f0c9ffeba6740bb162ef5d0ca17bf00.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/8f0c9ffeba6740bb162ef5d0ca17bf00.png)'
- en: (b) The detailed results on different length sets. The baseline means the setting
    without RAG.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: (b) ä¸åŒé•¿åº¦é›†çš„è¯¦ç»†ç»“æœã€‚åŸºçº¿æŒ‡çš„æ˜¯æ²¡æœ‰RAGçš„è®¾ç½®ã€‚
- en: 'Figure 3: The results on all tasks after adding RAG module. We only represents
    the Avg Scores (0~100).'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3ï¼šæ·»åŠ  RAG æ¨¡å—åçš„æ‰€æœ‰ä»»åŠ¡ç»“æœã€‚æˆ‘ä»¬ä»…è¡¨ç¤ºå¹³å‡åˆ†æ•°ï¼ˆ0~100ï¼‰ã€‚
- en: 4.4 Scaling Law of Context Window
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 ä¸Šä¸‹æ–‡çª—å£çš„è§„æ¨¡æ³•åˆ™
- en: 'Itâ€™s observed that the general performance of all models deteriorates with
    the increase in context size. As observed from [TableÂ 4](#S3.T4 "In 3.2.4 Chain
    of Reasoning â€£ 3.2 Evaluation Task â€£ 3 Loong: A Long-Context Benchmark â€£ Leave
    No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA"),
    it is apparent that for the same task, models perform well within small length
    sets but exhibit a notable performance decline as the length increases. This indicates
    that the models possess a certain capability to process the task, yet their performance
    is constrained by the context window. Moreover, despite being trained on 128K
    data, the GPT-4o and Qwen2-72B-Instruct begin to show performance degradation
    within the 50-100K interval, revealing that their actual capability boundary is
    significantly lower than the claimed window size. This suggests the presence of
    an ineffective zone within the claimed window. There exists a Scaling Law for
    model window sizes: to truly equip an LLM with the ability to handle 128K long
    texts, it should be trained on data exceeding 128K, meaning the training length
    should be greater than the actual processable length. Among numerous models, only
    the Gemini is less affected by changes in context length, which was training on
    the ultra-long context of 1000K. To ensure your model genuinely possesses the
    desired context window size, train it on longer data!'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 'è§‚å¯Ÿåˆ°æ‰€æœ‰æ¨¡å‹çš„æ€»ä½“æ€§èƒ½éšç€ä¸Šä¸‹æ–‡å¤§å°çš„å¢åŠ è€Œæ¶åŒ–ã€‚ä»[è¡¨ 4](#S3.T4 "åœ¨ 3.2.4 æ¨ç†é“¾ â€£ 3.2 è¯„ä¼°ä»»åŠ¡ â€£ 3 Loong:
    é•¿ä¸Šä¸‹æ–‡åŸºå‡† â€£ ä¸é—æ¼ä»»ä½•æ–‡æ¡£ï¼šåŸºäºæ‰©å±•å¤šæ–‡æ¡£ QA çš„é•¿ä¸Šä¸‹æ–‡ LLM åŸºå‡†æµ‹è¯•")ä¸­å¯ä»¥çœ‹å‡ºï¼Œå¯¹äºç›¸åŒçš„ä»»åŠ¡ï¼Œæ¨¡å‹åœ¨è¾ƒå°é•¿åº¦çš„é›†åˆä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†éšç€é•¿åº¦çš„å¢åŠ è¡¨ç°æ˜¾è‘—ä¸‹é™ã€‚è¿™è¡¨æ˜æ¨¡å‹å…·å¤‡ä¸€å®šçš„å¤„ç†ä»»åŠ¡çš„èƒ½åŠ›ï¼Œä½†å…¶æ€§èƒ½å—åˆ°ä¸Šä¸‹æ–‡çª—å£çš„é™åˆ¶ã€‚æ­¤å¤–ï¼Œå°½ç®¡åœ¨
    128K æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒGPT-4o å’Œ Qwen2-72B-Instruct åœ¨ 50-100K åŒºé—´å†…å¼€å§‹å‡ºç°æ€§èƒ½ä¸‹é™ï¼Œè¡¨æ˜å®ƒä»¬çš„å®é™…èƒ½åŠ›è¾¹ç•Œè¿œä½äºå£°æ˜çš„çª—å£å¤§å°ã€‚è¿™è¡¨æ˜åœ¨å£°æ˜çš„çª—å£å†…å­˜åœ¨æ— æ•ˆåŒºåŸŸã€‚æ¨¡å‹çª—å£å¤§å°å­˜åœ¨è§„æ¨¡æ³•åˆ™ï¼šè¦çœŸæ­£ä½¿
    LLM èƒ½å¤Ÿå¤„ç† 128K é•¿æ–‡æœ¬ï¼Œåº”è¯¥åœ¨è¶…è¿‡ 128K çš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå³è®­ç»ƒé•¿åº¦åº”å¤§äºå®é™…å¯å¤„ç†é•¿åº¦ã€‚åœ¨ä¼—å¤šæ¨¡å‹ä¸­ï¼Œåªæœ‰ Gemini å¯¹ä¸Šä¸‹æ–‡é•¿åº¦å˜åŒ–çš„å½±å“è¾ƒå°ï¼Œå› ä¸ºå®ƒåœ¨
    1000K çš„è¶…é•¿ä¸Šä¸‹æ–‡ä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚ä¸ºäº†ç¡®ä¿ä½ çš„æ¨¡å‹çœŸæ­£å…·å¤‡æ‰€éœ€çš„ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼Œåº”è¯¥åœ¨æ›´é•¿çš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼'
- en: 4.5 RAG or Not
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 æ˜¯å¦ä½¿ç”¨ RAG
- en: 'We have also incorporated the Embedding RAG module into the GPT-4o and Qwen2-72B-Instruct
    to explore whether RAG can enhance the modelâ€™s performance on Loong. For the Embedding
    choice, we employ two distinct models: the OpenAI Embedding modelâ¹â¹9[https://huggingface.co/Xenova/text-embedding-ada-002](https://huggingface.co/Xenova/text-embedding-ada-002)
    and the BGE Embedding model^(10)^(10)10[https://huggingface.co/BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3).
    Besides, we set the top-k value of 5, 10, 30, and 50 for each model respectively,
    and the chunk size is 1024\. The result is shown in [FigureÂ 3](#S4.F3 "In 4.3
    Task Analysis â€£ 4 Experiments â€£ Leave No Document Behind: Benchmarking Long-Context
    LLMs with Extended Multi-Doc QA") and the details can be seen in [AppendixÂ D](#A4
    "Appendix D RAG Detailed Results â€£ Leave No Document Behind: Benchmarking Long-Context
    LLMs with Extended Multi-Doc QA").'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å°†åµŒå…¥å¼ RAG æ¨¡å—æ•´åˆåˆ°äº† GPT-4o å’Œ Qwen2-72B-Instruct ä¸­ï¼Œä»¥æ¢ç´¢ RAG æ˜¯å¦èƒ½æå‡æ¨¡å‹åœ¨ Loong ä¸Šçš„è¡¨ç°ã€‚å¯¹äºåµŒå…¥å¼é€‰æ‹©ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸¤ç§ä¸åŒçš„æ¨¡å‹ï¼šOpenAI
    åµŒå…¥æ¨¡å‹â¹â¹9[https://huggingface.co/Xenova/text-embedding-ada-002](https://huggingface.co/Xenova/text-embedding-ada-002)
    å’Œ BGE åµŒå…¥æ¨¡å‹^(10)^(10)10[https://huggingface.co/BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ†åˆ«ä¸ºæ¯ä¸ªæ¨¡å‹è®¾ç½®äº†
    top-k å€¼ä¸º 5ã€10ã€30 å’Œ 50ï¼Œå—å¤§å°ä¸º 1024ã€‚ç»“æœè§äº[å›¾ 3](#S4.F3 "åœ¨ 4.3 ä»»åŠ¡åˆ†æ â€£ 4 å®éªŒ â€£ ä¸é—æ¼ä»»ä½•æ–‡æ¡£ï¼šåŸºäºæ‰©å±•å¤šæ–‡æ¡£
    QA çš„é•¿ä¸Šä¸‹æ–‡ LLM åŸºå‡†æµ‹è¯•")ï¼Œè¯¦ç»†ä¿¡æ¯å¯è§äº[é™„å½• D](#A4 "é™„å½• D RAG è¯¦ç»†ç»“æœ â€£ ä¸é—æ¼ä»»ä½•æ–‡æ¡£ï¼šåŸºäºæ‰©å±•å¤šæ–‡æ¡£ QA çš„é•¿ä¸Šä¸‹æ–‡
    LLM åŸºå‡†æµ‹è¯•")ã€‚
- en: Benchmark Analysis It is evident that the inclusion of RAG does not enhance
    the modelâ€™s overall performance on the Loong, and there is a noticeable decline
    in assessment. This is because the evidence in the Loong is distributed relatively
    evenly across multiple documents, requiring a comprehensive understanding of long
    texts by the model. RAG, being more limited, only shows some effectiveness in
    the task with sparse evidence, such as spotlight locating. However, RAGâ€™s negative
    impact is significant for tasks requiring a high level of comprehensiveness. Integrating
    RAG does not enhance the performance, indicating that Loong focuses on evaluating
    the modelâ€™s complex reasoning and comprehensive analysis capabilities for long
    contexts, thereby effectively assessing the LLMâ€™s long-context modeling ability.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºå‡†åˆ†æ æ˜¾è€Œæ˜“è§ï¼ŒRAG çš„å¼•å…¥å¹¶æœªæå‡æ¨¡å‹åœ¨ Loong ä¸Šçš„æ•´ä½“æ€§èƒ½ï¼Œåè€Œåœ¨è¯„ä¼°ä¸­æ˜æ˜¾ä¸‹é™ã€‚è¿™æ˜¯å› ä¸º Loong ä¸­çš„è¯æ®åˆ†å¸ƒç›¸å¯¹å‡åŒ€åœ¨å¤šä¸ªæ–‡æ¡£ä¸­ï¼Œæ¨¡å‹éœ€è¦å¯¹é•¿æ–‡æœ¬æœ‰å…¨é¢çš„ç†è§£ã€‚RAG
    çš„æ•ˆæœç›¸å¯¹æœ‰é™ï¼Œä»…åœ¨è¯æ®ç¨€ç–çš„ä»»åŠ¡ä¸­ï¼Œå¦‚èšå…‰ç¯å®šä½ï¼Œè¡¨ç°å‡ºä¸€å®šçš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼ŒRAG å¯¹éœ€è¦é«˜åº¦å…¨é¢æ€§çš„ä»»åŠ¡çš„è´Ÿé¢å½±å“æ˜¾è‘—ã€‚é›†æˆ RAG å¹¶æœªæå‡æ€§èƒ½ï¼Œè¡¨æ˜
    Loong ä¾§é‡äºè¯„ä¼°æ¨¡å‹å¯¹é•¿ä¸Šä¸‹æ–‡çš„å¤æ‚æ¨ç†å’Œå…¨é¢åˆ†æèƒ½åŠ›ï¼Œä»è€Œæœ‰æ•ˆè¯„ä¼° LLM çš„é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡èƒ½åŠ›ã€‚
- en: Model Analysis Comparing the performance between GPT-4o and Qwen2-72B-Instruct,
    it is evident that the powerful long-context LLM significantly outperforms RAG.
    On the other hand, the performance of RAG is closer to the original performance
    when used with a weak context model. This is because a strong long-context LLM
    can fully exploit the complete information flow of long contexts, capturing complex
    dependencies and semantic information. However, RAG causes context fragmentation
    and information loss, impairing the modelâ€™s understanding and reasoning capabilities,
    thereby preventing the full utilization of its inherent modeling advantages. Consequently,
    a strong long-text modeling capability is not suitable for enhancement through
    RAG. Conversely, a weak model with poor long-context modeling capability cannot
    effectively capture information, and RAG cannot compensate for this deficiency.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹åˆ†æ å¯¹æ¯” GPT-4o å’Œ Qwen2-72B-Instruct çš„æ€§èƒ½ï¼Œå¯ä»¥æ˜æ˜¾çœ‹å‡ºï¼Œå¼ºå¤§çš„é•¿ä¸Šä¸‹æ–‡ LLM æ˜æ˜¾ä¼˜äº RAGã€‚å¦ä¸€æ–¹é¢ï¼Œå½“ä¸è¾ƒå¼±çš„ä¸Šä¸‹æ–‡æ¨¡å‹ç»“åˆä½¿ç”¨æ—¶ï¼ŒRAG
    çš„è¡¨ç°æ›´æ¥è¿‘åŸå§‹æ€§èƒ½ã€‚è¿™æ˜¯å› ä¸ºå¼ºå¤§çš„é•¿ä¸Šä¸‹æ–‡ LLM å¯ä»¥å……åˆ†åˆ©ç”¨é•¿ä¸Šä¸‹æ–‡çš„å®Œæ•´ä¿¡æ¯æµï¼Œæ•æ‰å¤æ‚çš„ä¾èµ–å…³ç³»å’Œè¯­ä¹‰ä¿¡æ¯ã€‚ç„¶è€Œï¼ŒRAG ä¼šå¯¼è‡´ä¸Šä¸‹æ–‡ç¢ç‰‡åŒ–å’Œä¿¡æ¯ä¸¢å¤±ï¼ŒæŸå®³æ¨¡å‹çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œä»è€Œé˜»ç¢å…¶å›ºæœ‰å»ºæ¨¡ä¼˜åŠ¿çš„å……åˆ†å‘æŒ¥ã€‚å› æ­¤ï¼Œå¼ºå¤§çš„é•¿æ–‡æœ¬å»ºæ¨¡èƒ½åŠ›ä¸é€‚åˆé€šè¿‡
    RAG æ¥å¢å¼ºã€‚ç›¸åï¼Œä¸€ä¸ªé•¿ä¸Šä¸‹æ–‡å»ºæ¨¡èƒ½åŠ›è¾ƒå·®çš„å¼±æ¨¡å‹æ— æ³•æœ‰æ•ˆæ•æ‰ä¿¡æ¯ï¼Œè€Œ RAG æ— æ³•å¼¥è¡¥è¿™ä¸€ä¸è¶³ã€‚
- en: Length Analysis Within the context window size that the model can handle, RAG
    does not offer an advantage. However, for ultra-long context sets, a high top-k
    setting of RAG can produce certain effects. This is because, in short context
    sets, the modelâ€™s inherent modeling capability can effectively handle the entire
    text length without losing information. The introduction of RAG, conversely, may
    result in the loss of certain evidence, leading to information gaps. In ultra-long
    context collections, RAG can effectively compress information, recalling evidence
    that the LLM could not access due to length truncation, thereby enhancing the
    modelâ€™s performance on Loong.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: é•¿åº¦åˆ†æ åœ¨æ¨¡å‹èƒ½å¤Ÿå¤„ç†çš„ä¸Šä¸‹æ–‡çª—å£å¤§å°èŒƒå›´å†…ï¼ŒRAG å¹¶æœªæä¾›ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œå¯¹äºè¶…é•¿ä¸Šä¸‹æ–‡é›†ï¼ŒRAG çš„é«˜ top-k è®¾ç½®å¯ä»¥äº§ç”Ÿä¸€å®šæ•ˆæœã€‚è¿™æ˜¯å› ä¸ºåœ¨çŸ­ä¸Šä¸‹æ–‡é›†é‡Œï¼Œæ¨¡å‹çš„å›ºæœ‰å»ºæ¨¡èƒ½åŠ›å¯ä»¥æœ‰æ•ˆå¤„ç†æ•´ä¸ªæ–‡æœ¬é•¿åº¦è€Œä¸ä¼šä¸¢å¤±ä¿¡æ¯ã€‚å¼•å…¥
    RAG å¯èƒ½å¯¼è‡´æŸäº›è¯æ®çš„ä¸¢å¤±ï¼Œä»è€Œäº§ç”Ÿä¿¡æ¯ç©ºç™½ã€‚åœ¨è¶…é•¿ä¸Šä¸‹æ–‡é›†åˆä¸­ï¼ŒRAG å¯ä»¥æœ‰æ•ˆå‹ç¼©ä¿¡æ¯ï¼Œå›å¿†å‡º LLM å› é•¿åº¦æˆªæ–­è€Œæ— æ³•è®¿é—®çš„è¯æ®ï¼Œä»è€Œæå‡æ¨¡å‹åœ¨
    Loong ä¸Šçš„æ€§èƒ½ã€‚
- en: Relying on RAG cannot resolve all the problems associated with long-text modeling.
    To genuinely improve the long-context modeling capability, stronger training methods
    and effective training on longer texts are required, rather than merely integrating
    the RAG module.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾èµ– RAG å¹¶ä¸èƒ½è§£å†³æ‰€æœ‰ä¸é•¿æ–‡æœ¬å»ºæ¨¡ç›¸å…³çš„é—®é¢˜ã€‚è¦çœŸæ­£æé«˜é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡èƒ½åŠ›ï¼Œéœ€è¦æ›´å¼ºçš„è®­ç»ƒæ–¹æ³•å’Œå¯¹æ›´é•¿æ–‡æœ¬çš„æœ‰æ•ˆè®­ç»ƒï¼Œè€Œä¸ä»…ä»…æ˜¯é›†æˆ RAG æ¨¡å—ã€‚
- en: 5 Conclusion
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 ç»“è®º
- en: In this study, we propose Loong, a question-answering format benchmark designed
    to evaluate long-context comprehension in real-world multi-document scenarios.
    We analyze six advanced language models (LLMs), considering variations in their
    parameter sizes and context windows, including GPT-4o and Gemini-Pro1.5\. Notably,
    even the most powerful long-context LLMs fail to achieve satisfactory performance.
    Furthermore, we conduct in-depth analyses to enhance long-context modeling capabilities
    by comparing the RAG approach and the scaling laws related to context size.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Loongï¼Œä¸€ä¸ªè®¾è®¡ç”¨äºè¯„ä¼°ç°å®ä¸–ç•Œå¤šæ–‡æ¡£åœºæ™¯ä¸­é•¿ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›çš„é—®é¢˜å›ç­”æ ¼å¼åŸºå‡†ã€‚æˆ‘ä»¬åˆ†æäº†å…­ç§å…ˆè¿›çš„è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œè€ƒè™‘äº†å®ƒä»¬çš„å‚æ•°å¤§å°å’Œä¸Šä¸‹æ–‡çª—å£çš„å˜åŒ–ï¼ŒåŒ…æ‹¬GPT-4oå’ŒGemini-Pro1.5ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿æ˜¯æœ€å¼ºå¤§çš„é•¿ä¸Šä¸‹æ–‡LLMsä¹Ÿæœªèƒ½è¾¾åˆ°ä»¤äººæ»¡æ„çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡æ¯”è¾ƒRAGæ–¹æ³•å’Œä¸ä¸Šä¸‹æ–‡å¤§å°ç›¸å…³çš„æ‰©å±•è§„å¾‹ï¼Œè¿›è¡Œæ·±å…¥åˆ†æï¼Œä»¥æå‡é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡èƒ½åŠ›ã€‚
- en: Limitations
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å±€é™æ€§
- en: 'Here we list some of the limitations that are not considered when designing
    Loong: (1) Limited Domains. The purpose of Loong is to evaluate the long-context
    understanding capabilities in real-world multi-document scenarios. However, a
    sea of multi-document domains exists in the real world. Considering annotation
    costs and model evaluation efficiency, we only cover the most representative parts
    of them: financial, legal, and academic. (2) High Annotation Cost. To enhance
    the reliability of Loong in assessing the LLMâ€™s long-context understanding capabilities,
    we recruited a group of experts for each of the three domains to proofread the
    data, and they are proficient in both English and Chinese. They need to understand
    the question and search for relevant evidence in multiple documents with an average
    length of up to 110k to judge the consistency between the question and the answer,
    which requires a significant amount of time and effort.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œåˆ—å‡ºäº†ä¸€äº›åœ¨è®¾è®¡Loongæ—¶æœªè€ƒè™‘çš„å±€é™æ€§ï¼šï¼ˆ1ï¼‰æœ‰é™çš„é¢†åŸŸã€‚Loongçš„ç›®çš„æ˜¯è¯„ä¼°ç°å®ä¸–ç•Œå¤šæ–‡æ¡£åœºæ™¯ä¸­çš„é•¿ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œä¸­å­˜åœ¨å¤§é‡çš„å¤šæ–‡æ¡£é¢†åŸŸã€‚è€ƒè™‘åˆ°æ³¨é‡Šæˆæœ¬å’Œæ¨¡å‹è¯„ä¼°æ•ˆç‡ï¼Œæˆ‘ä»¬ä»…æ¶µç›–äº†å…¶ä¸­æœ€å…·ä»£è¡¨æ€§çš„éƒ¨åˆ†ï¼šé‡‘èã€æ³•å¾‹å’Œå­¦æœ¯ã€‚ï¼ˆ2ï¼‰é«˜æ˜‚çš„æ³¨é‡Šæˆæœ¬ã€‚ä¸ºäº†æé«˜Loongåœ¨è¯„ä¼°LLMé•¿ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›æ–¹é¢çš„å¯é æ€§ï¼Œæˆ‘ä»¬ä¸ºè¿™ä¸‰ä¸ªé¢†åŸŸæ‹›å‹Ÿäº†ä¸€ç»„ä¸“å®¶è¿›è¡Œæ•°æ®æ ¡å¯¹ï¼Œä»–ä»¬ç²¾é€šè‹±è¯­å’Œä¸­æ–‡ã€‚ä»–ä»¬éœ€è¦ç†è§£é—®é¢˜ï¼Œå¹¶åœ¨å¹³å‡é•¿åº¦è¾¾åˆ°110kçš„å¤šä¸ªæ–‡æ¡£ä¸­å¯»æ‰¾ç›¸å…³è¯æ®ï¼Œä»¥åˆ¤æ–­é—®é¢˜ä¸ç­”æ¡ˆä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œè¿™éœ€è¦å¤§é‡çš„æ—¶é—´å’Œç²¾åŠ›ã€‚
- en: References
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: 'An etÂ al. (2023) Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang,
    Lingpeng Kong, and Xipeng Qiu. 2023. L-eval: Instituting standardized evaluation
    for long context language models. *arXiv preprint arXiv:2307.11088*.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'An et al. (2023) é™ˆæ¬£å®‰, é¾šå–„å–„, é’Ÿé“­, æç©†å‡¯, å¼ ä¿Š, å­”çµé¹, å’Œé‚±è¥¿é¹. 2023. L-eval: ä¸ºé•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹å»ºç«‹æ ‡å‡†åŒ–è¯„ä¼°ã€‚*arXiv
    é¢„å°æœ¬ arXiv:2307.11088*ã€‚'
- en: 'Anthropic (2024a) AIÂ Anthropic. 2024a. The claude 3 model family: Opus, sonnet,
    haiku. *Claude-3 Model Card*.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic (2024a) AI Anthropic. 2024a. Claude 3æ¨¡å‹å®¶æ—ï¼šOpus, sonnet, haikuã€‚*Claude-3æ¨¡å‹å¡*ã€‚
- en: Anthropic (2024b) AIÂ Anthropic. 2024b. Claude 3.5 sonnet model card addendum.
    *Claude-3.5 Model Card*.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic (2024b) AI Anthropic. 2024b. Claude 3.5 sonnetæ¨¡å‹å¡é™„å½•ã€‚*Claude-3.5æ¨¡å‹å¡*ã€‚
- en: Bai etÂ al. (2023a) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong
    Deng, Yang Fan, Wenbin Ge, YuÂ Han, Fei Huang, etÂ al. 2023a. Qwen technical report.
    *arXiv preprint arXiv:2309.16609*.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai et al. (2023a) ç™½é‡‘æ³½, ç™½å¸…, æœ±äº‘é£, å´”æ³½å®‡, é»¨å‡¯, é‚“æ™“ä¸œ, èŒƒæ¨, è‘›æ–‡å½¬, éŸ©å®‡, é»„é£, ç­‰. 2023a. QwenæŠ€æœ¯æŠ¥å‘Šã€‚*arXiv
    é¢„å°æœ¬ arXiv:2309.16609*ã€‚
- en: 'Bai etÂ al. (2023b) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai
    Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, etÂ al. 2023b.
    Longbench: A bilingual, multitask benchmark for long context understanding. *arXiv
    preprint arXiv:2308.14508*.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bai et al. (2023b) ç™½ç‰çŸ³, å•é‘«, å¼ å®¶æ°, å•æ´ªæ˜Œ, å”å»ºå‡¯, é»„å¿—ç‚¹, æœæ­£å­, åˆ˜æ™“, æ›¾æ•–æ±‰, ä¾¯ç£Š, ç­‰. 2023b.
    Longbench: ä¸€ä¸ªç”¨äºé•¿ä¸Šä¸‹æ–‡ç†è§£çš„åŒè¯­å¤šä»»åŠ¡åŸºå‡†ã€‚*arXiv é¢„å°æœ¬ arXiv:2308.14508*ã€‚'
- en: bloc97 (2023) bloc97\. 2023. Ntk-aware scaled rope. [https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/](https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/).
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: bloc97 (2023) bloc97. 2023. Ntk-aware scaled rope. [https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/](https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/)ã€‚
- en: Borgeaud etÂ al. (2022) Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor
    Cai, Eliza Rutherford, Katie Millican, GeorgeÂ Bm Van DenÂ Driessche, Jean-Baptiste
    Lespiau, Bogdan Damoc, Aidan Clark, etÂ al. 2022. Improving language models by
    retrieving from trillions of tokens. In *Proceedings of ICML*, pages 2206â€“2240.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Borgeaud et al. (2022) Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor
    Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste
    Lespiau, Bogdan Damoc, Aidan Clark, ç­‰ã€‚2022ã€‚é€šè¿‡ä»ä¸‡äº¿æ ‡è®°ä¸­æ£€ç´¢æ¥æ”¹è¿›è¯­è¨€æ¨¡å‹ã€‚åœ¨ *ICML ä¼šè®®è®ºæ–‡é›†* ä¸­ï¼Œç¬¬2206â€“2240é¡µã€‚
- en: Chen etÂ al. (2024a) Jiawei Chen, Hongyu Lin, Xianpei Han, and LeÂ Sun. 2024a.
    Benchmarking large language models in retrieval-augmented generation. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, pages 17754â€“17762.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2024a) Jiawei Chen, Hongyu Lin, Xianpei Han, å’Œ Le Sunã€‚2024aã€‚åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆä¸­åŸºå‡†æµ‹è¯•å¤§å‹è¯­è¨€æ¨¡å‹ã€‚åœ¨
    *AAAI äººå·¥æ™ºèƒ½ä¼šè®®è®ºæ–‡é›†* ä¸­ï¼Œç¬¬17754â€“17762é¡µã€‚
- en: 'Chen etÂ al. (2024b) Longze Chen, Ziqiang Liu, Wanwei He, Yunshui Li, Run Luo,
    and Min Yang. 2024b. Long context is not long at all: A prospector of long-dependency
    data for large language models. *arXiv preprint arXiv:2405.17915*.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. (2024b) Longze Chen, Ziqiang Liu, Wanwei He, Yunshui Li, Run Luo,
    å’Œ Min Yangã€‚2024bã€‚é•¿ä¸Šä¸‹æ–‡æ ¹æœ¬ä¸é•¿: å¤§å‹è¯­è¨€æ¨¡å‹é•¿ä¾èµ–æ•°æ®çš„æ¢ç´¢è€…ã€‚*arXiv é¢„å°æœ¬ arXiv:2405.17915*ã€‚'
- en: Chen etÂ al. (2023) Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong
    Tian. 2023. Extending context window of large language models via positional interpolation.
    *arXiv preprint arXiv:2306.15595*.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2023) Shouyuan Chen, Sherman Wong, Liangjian Chen, å’Œ Yuandong Tianã€‚2023ã€‚é€šè¿‡ä½ç½®æ’å€¼æ‰©å±•å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£ã€‚*arXiv
    é¢„å°æœ¬ arXiv:2306.15595*ã€‚
- en: 'Dong etÂ al. (2023) Zican Dong, Tianyi Tang, Junyi Li, WayneÂ Xin Zhao, and Ji-Rong
    Wen. 2023. Bamboo: A comprehensive benchmark for evaluating long text modeling
    capacities of large language models. *arXiv preprint arXiv:2309.13345*.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dong et al. (2023) Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, å’Œ Ji-Rong
    Wenã€‚2023ã€‚Bamboo: è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹é•¿æ–‡æœ¬å»ºæ¨¡èƒ½åŠ›çš„å…¨é¢åŸºå‡†ã€‚*arXiv é¢„å°æœ¬ arXiv:2309.13345*ã€‚'
- en: 'Du etÂ al. (2022) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu,
    Zhilin Yang, and Jie Tang. 2022. Glm: General language model pretraining with
    autoregressive blank infilling. In *Proceedings of ACL*, pages 320â€“335.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Du et al. (2022) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu,
    Zhilin Yang, å’Œ Jie Tangã€‚2022ã€‚GLM: è‡ªå›å½’ç©ºç™½å¡«å……çš„é€šç”¨è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒã€‚åœ¨ *ACL ä¼šè®®è®ºæ–‡é›†* ä¸­ï¼Œç¬¬320â€“335é¡µã€‚'
- en: 'Golchin and Surdeanu (2023) Shahriar Golchin and Mihai Surdeanu. 2023. Time
    travel in llms: Tracing data contamination in large language models. *arXiv preprint
    arXiv:2308.08493*.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Golchin and Surdeanu (2023) Shahriar Golchin å’Œ Mihai Surdeanuã€‚2023ã€‚LLMs ä¸­çš„æ—¶é—´æ—…è¡Œ:
    è¿½è¸ªå¤§å‹è¯­è¨€æ¨¡å‹çš„æ•°æ®æ±¡æŸ“ã€‚*arXiv é¢„å°æœ¬ arXiv:2308.08493*ã€‚'
- en: 'Han etÂ al. (2023) Chi Han, Qifan Wang, Wenhan Xiong, YuÂ Chen, Heng Ji, and
    Sinong Wang. 2023. Lm-infinite: Simple on-the-fly length generalization for large
    language models. *arXiv preprint arXiv:2308.16137*.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Han et al. (2023) Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, å’Œ Sinong
    Wangã€‚2023ã€‚LM-Infinite: å¤§å‹è¯­è¨€æ¨¡å‹çš„å³æ—¶é•¿åº¦æ³›åŒ–ã€‚*arXiv é¢„å°æœ¬ arXiv:2308.16137*ã€‚'
- en: 'Hsieh etÂ al. (2024) Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya,
    Dima Rekesh, Fei Jia, and Boris Ginsburg. 2024. Ruler: Whatâ€™s the real context
    size of your long-context language models? *arXiv preprint arXiv:2404.06654*.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hsieh et al. (2024) Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya,
    Dima Rekesh, Fei Jia, å’Œ Boris Ginsburgã€‚2024ã€‚Ruler: ä½ çš„é•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹çš„çœŸå®ä¸Šä¸‹æ–‡å¤§å°æ˜¯å¤šå°‘ï¼Ÿ*arXiv
    é¢„å°æœ¬ arXiv:2404.06654*ã€‚'
- en: Kamradt (2023) Greg Kamradt. 2023. Needle in a haystack - pressure testing llms.
    [https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack).
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kamradt (2023) Greg Kamradtã€‚2023ã€‚é’ˆåœ¨ç¨»è‰å †ä¸­ - å‹åŠ›æµ‹è¯• LLMsã€‚[https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)ã€‚
- en: 'Li etÂ al. (2023) Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. 2023.
    Loogle: Can long-context language models understand long contexts? *arXiv preprint
    arXiv:2311.04939*.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2023) Jiaqi Li, Mengmeng Wang, Zilong Zheng, å’Œ Muhan Zhangã€‚2023ã€‚Loogle:
    é•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹èƒ½ç†è§£é•¿ä¸Šä¸‹æ–‡å—ï¼Ÿ*arXiv é¢„å°æœ¬ arXiv:2311.04939*ã€‚'
- en: Liu etÂ al. (2024) Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen
    Huang, Furu Wei, Weiwei Deng, Feng Sun, and QiÂ Zhang. 2024. Calibrating LLM-based
    evaluator. In *Proceedings of LREC-COLING*, pages 2638â€“2656.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2024) Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen
    Huang, Furu Wei, Weiwei Deng, Feng Sun, å’Œ Qi Zhangã€‚2024ã€‚æ ¡å‡†åŸºäº LLM çš„è¯„ä¼°å™¨ã€‚åœ¨ *LREC-COLING
    ä¼šè®®è®ºæ–‡é›†* ä¸­ï¼Œç¬¬2638â€“2656é¡µã€‚
- en: OpenAI (2023) OpenAI. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAIã€‚2023ã€‚GPT-4 æŠ€æœ¯æŠ¥å‘Šã€‚*arXiv é¢„å°æœ¬ arXiv:2303.08774*ã€‚
- en: 'Pal etÂ al. (2023) Arka Pal, Deep Karkhanis, Manley Roberts, Samuel Dooley,
    Arvind Sundararajan, and Siddartha Naidu. 2023. Giraffe: Adventures in expanding
    context lengths in llms. *arXiv preprint arXiv:2308.10882*.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pal et al. (2023) Arka Pal, Deep Karkhanis, Manley Roberts, Samuel Dooley,
    Arvind Sundararajan, å’Œ Siddartha Naiduã€‚2023ã€‚Giraffe: æ‰©å±•ä¸Šä¸‹æ–‡é•¿åº¦çš„å†’é™©ã€‚*arXiv é¢„å°æœ¬ arXiv:2308.10882*ã€‚'
- en: 'Peng etÂ al. (2023) Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.
    2023. Yarn: Efficient context window extension of large language models. *arXiv
    preprint arXiv:2309.00071*.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Peng et al. (2023) **å½­åšæ–‡**, **æ°å¼—é‡ŒÂ·å…‹æ–¯å†…å°”**, **èŒƒæ´ªç’**, å’Œ **æ©é‡Œç§‘Â·å¸Œæ³¢å°”**ã€‚2023ã€‚ã€ŠYarn:
    é«˜æ•ˆæ‰©å±•å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£ã€‹ã€‚*arXiv é¢„å°æœ¬ arXiv:2309.00071*ã€‚'
- en: 'Press etÂ al. (2021) Ofir Press, NoahÂ A Smith, and Mike Lewis. 2021. Train short,
    test long: Attention with linear biases enables input length extrapolation. *arXiv
    preprint arXiv:2108.12409*.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Press et al. (2021) **å¥¥è²å°”Â·æ™®é›·æ–¯**, **è¯ºäºšÂ·AÂ·å²å¯†æ–¯**, å’Œ **è¿ˆå…‹Â·åˆ˜æ˜“æ–¯**ã€‚2021ã€‚ã€ŠçŸ­æœŸè®­ç»ƒï¼Œé•¿æœŸæµ‹è¯•:
    çº¿æ€§åå·®çš„æ³¨æ„åŠ›æœºåˆ¶å®ç°è¾“å…¥é•¿åº¦å¤–æ¨ã€‹ã€‚*arXiv é¢„å°æœ¬ arXiv:2108.12409*ã€‚'
- en: 'Qiu etÂ al. (2024) Zexuan Qiu, Jingjing Li, Shijue Huang, Wanjun Zhong, and
    Irwin King. 2024. Clongeval: A chinese benchmark for evaluating long-context large
    language models. *arXiv preprint arXiv:2403.03514*.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qiu et al. (2024) **é‚±æ³½è½©**, **ææ™¶æ™¶**, **é»„ä¸–è§‰**, **é’Ÿä¸‡å†›**, å’Œ **ç‹å°”æ–‡**ã€‚2024ã€‚ã€ŠClongeval:
    ä¸€ä¸ªç”¨äºè¯„ä¼°é•¿ä¸Šä¸‹æ–‡å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸­æ–‡åŸºå‡†ã€‹ã€‚*arXiv é¢„å°æœ¬ arXiv:2403.03514*ã€‚'
- en: Ratner etÂ al. (2022) Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal
    Magar, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.
    2022. Parallel context windows for large language models. *arXiv preprint arXiv:2212.10947*.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ratner et al. (2022) **æ‹‰ç‰¹çº³**, **è±æ–‡**, **è´æ—ç§‘å¤«**, **æ‹‰å§†**, **é©¬åŠ å°”**, **é˜¿æœ¬å¾·**, **å¡å°”å¸•æ–¯**,
    **æ²™ä¼‘é˜¿**, **é›·é¡¿-å¸ƒæœ—**, å’Œ **è‚–æ±‰**ã€‚2022ã€‚ã€Šå¤§å‹è¯­è¨€æ¨¡å‹çš„å¹¶è¡Œä¸Šä¸‹æ–‡çª—å£ã€‹ã€‚*arXiv é¢„å°æœ¬ arXiv:2212.10947*ã€‚
- en: 'Reid etÂ al. (2024) Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin,
    Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan
    Firat, Julian Schrittwieser, etÂ al. 2024. Gemini 1.5: Unlocking multimodal understanding
    across millions of tokens of context. *arXiv preprint arXiv:2403.05530*.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Reid et al. (2024) **é©¬æ­‡å°”Â·é‡Œå¾·**, **å°¼å¤æ‹‰Â·è¨ç»´è¯ºå¤«**, **å¾·å°¼æ–¯Â·ç‰¹æ™®åˆ©äºšæ¬£**, **å¾·ç±³ç‰¹é‡ŒÂ·åˆ—çš®æ¬£**, **è’‚è«è¥¿Â·åˆ©åˆ©å…‹æ‹‰æ™®**,
    **è®©-å·´æ™®è’‚æ–¯ç‰¹Â·é˜¿æ‹‰äºšå…‹**, **æ‹‰æœÂ·ç´¢é‡Œåº“ç‰¹**, **å®‰æ°ä¸½åŸºÂ·æ‹‰æ‰é‡Œæœ**, **å¥¥å°”æ±‰Â·è´¹æ‹‰ç‰¹**, **æœ±åˆ©å®‰Â·æ–½ç‘ç‰¹ç»´ç‘Ÿ**, ç­‰äººã€‚2024ã€‚ã€ŠGemini
    1.5: è§£é”è·¨è¶Šç™¾ä¸‡ä¸ªä¸Šä¸‹æ–‡æ ‡è®°çš„å¤šæ¨¡æ€ç†è§£ã€‹ã€‚*arXiv é¢„å°æœ¬ arXiv:2403.05530*ã€‚'
- en: 'Roziere etÂ al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, XiaoqingÂ Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, JÃ©rÃ©my
    Rapin, etÂ al. 2023. Code llama: Open foundation models for code. *arXiv preprint
    arXiv:2308.12950*.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Roziere et al. (2023) **å·´æ™®è’‚æ–¯ç‰¹Â·ç½—å…¹è€¶å°”**, **ä¹”çº³æ–¯Â·ç›–æ—**, **æ³•æ¯”å®‰Â·æ ¼æ´›å…‹å°”**, **æ–¯æ»•Â·ç´¢ç‰¹æ‹‰**,
    **ä¼Šæ³°Â·åŠ ç‰¹**, **è‚–é’Â·è‰¾ä¼¦Â·è°­**, **çº¦è¥¿Â·é˜¿è¿ª**, **åˆ˜æ™¶å®‡**, **å¡”å°”Â·é›·æ¢…å…¹**, **çƒ­å°”ç±³Â·æ‹‰æ½˜**, ç­‰äººã€‚2023ã€‚ã€ŠCode
    llama: å¼€æ”¾ä»£ç åŸºç¡€æ¨¡å‹ã€‹ã€‚*arXiv é¢„å°æœ¬ arXiv:2308.12950*ã€‚'
- en: 'Shi etÂ al. (2023) Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich
    James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. Replug: Retrieval-augmented
    black-box language models. *arXiv preprint arXiv:2301.12652*.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shi et al. (2023) **çŸ³ä¼Ÿä½³**, **é—µä¸–æ–‡**, **å®‰çºªå®**, **å¾æ•ä¿Š**, **è©¹å§†æ–¯Â·é‡Œå¥‡**, **è¿ˆå…‹Â·åˆ˜æ˜“æ–¯**,
    **å¢å…‹Â·æ³½ç‰¹å°”æ‘©è€¶**, å’Œ **æ˜“æ–‡æ¶›**ã€‚2023ã€‚ã€ŠReplug: æ£€ç´¢å¢å¼ºå‹é»‘ç®±è¯­è¨€æ¨¡å‹ã€‹ã€‚*arXiv é¢„å°æœ¬ arXiv:2301.12652*ã€‚'
- en: 'Song etÂ al. (2024) Mingyang Song, Mao Zheng, and Xuan Luo. 2024. Counting-stars:
    A simple, efficient, and reasonable strategy for evaluating long-context large
    language models. *arXiv preprint arXiv:2403.11802*.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Song et al. (2024) **å®‹æ˜æ‰¬**, **éƒ‘èŒ‚**, å’Œ **ç½—è½©**ã€‚2024ã€‚ã€ŠCounting-stars: ä¸€ç§ç®€å•ã€é«˜æ•ˆã€åˆç†çš„è¯„ä¼°é•¿ä¸Šä¸‹æ–‡å¤§å‹è¯­è¨€æ¨¡å‹çš„ç­–ç•¥ã€‹ã€‚*arXiv
    é¢„å°æœ¬ arXiv:2403.11802*ã€‚'
- en: 'Su etÂ al. (2024) Jianlin Su, Murtadha Ahmed, YuÂ Lu, Shengfeng Pan, Wen Bo,
    and Yunfeng Liu. 2024. Roformer: Enhanced transformer with rotary position embedding.
    *Neurocomputing*, 568:127063.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Su et al. (2024) **è‹å‰‘æ—**, **è‰¾å“ˆè¿ˆå¾·**, **å¢é›¨**, **æ½˜èƒœå³°**, **åšæ–‡**, å’Œ **åˆ˜äº‘é”‹**ã€‚2024ã€‚ã€ŠRoformer:
    å¸¦æœ‰æ—‹è½¬ä½ç½®åµŒå…¥çš„å¢å¼ºå‹å˜æ¢å™¨ã€‹ã€‚*Neurocomputing*ï¼Œ568:127063ã€‚'
- en: Sun etÂ al. (2022) Yutao Sun, LiÂ Dong, Barun Patra, Shuming Ma, Shaohan Huang,
    Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. 2022. A length-extrapolatable
    transformer. *arXiv preprint arXiv:2212.10554*.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2022) **å­™ç‰æ¶›**, **è‘£åŠ›**, **å·´ä¼¦Â·å¸•ç‰¹æ‹‰**, **é©¬æ ‘æ˜**, **é»„ç»æ±‰**, **é˜¿é¾™Â·æœ¬æµ·å§†**,
    **ç»´æ–¯æ‹‰å¤«Â·ä¹”æœé‡Œ**, **å®‹éœ**, å’Œ **é­å¯Œå¦‚**ã€‚2022ã€‚ã€Šå¯é•¿åº¦å¤–æ¨çš„å˜æ¢å™¨ã€‹ã€‚*arXiv é¢„å°æœ¬ arXiv:2212.10554*ã€‚
- en: Vaswani etÂ al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, AidanÂ N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. In *Proceedings of NeurIPs*, pageÂ 30.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. (2017) **é˜¿å¸Œä»€Â·ç“¦æ–¯ç“¦å°¼**, **è¯ºå§†Â·æ²™æ³½å°”**, **å°¼åŸºÂ·å¸•å°”é©¬å°”**, **é›…å„å¸ƒÂ·ä¹Œæ–¯ç§‘é›·ç‰¹**,
    **åˆ©æ˜‚Â·ç¼æ–¯**, **è‰¾ä¸¹Â·NÂ·æˆˆéº¦æ–¯**, **å¢å¡æ–¯Â·å‡¯ç‘Ÿå°”**, å’Œ **ä¼Šåˆ©äºšÂ·æ³¢æ´›è‹é‡‘**ã€‚2017ã€‚ã€Šæ³¨æ„åŠ›æœºåˆ¶å°±æ˜¯ä½ æ‰€éœ€è¦çš„ä¸€åˆ‡ã€‹ã€‚å‘è¡¨äº
    *NeurIPs ä¼šè®®è®ºæ–‡é›†*ï¼Œç¬¬ 30 é¡µã€‚
- en: Wang etÂ al. (2024) Cunxiang Wang, Sirui Cheng, Qipeng Guo, Yuanhao Yue, Bowen
    Ding, Zhikun Xu, Yidong Wang, Xiangkun Hu, Zheng Zhang, and Yue Zhang. 2024. Evaluating
    open-qa evaluation. In *Proceedings of NeurIPs*, pageÂ 36.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2024) **æ±ªå­˜ç¥¥**, **ç¨‹æ€é”**, **éƒ­å¯é¹**, **å²³å…ƒæµ©**, **ä¸åšæ–‡**, **å¾æ™ºå¤**, **ç‹ä¸€ä¸œ**,
    **èƒ¡å‘å¤**, **å¼ æ­£**, å’Œ **å¼ è·ƒ**ã€‚2024ã€‚ã€Šå¼€æ”¾é—®ç­”è¯„ä¼°ã€‹ã€‚å‘è¡¨äº *NeurIPs ä¼šè®®è®ºæ–‡é›†*ï¼Œç¬¬ 36 é¡µã€‚
- en: Wu etÂ al. (2024) Kevin Wu, Eric Wu, and James Zou. 2024. How faithful are rag
    models? quantifying the tug-of-war between rag and llmsâ€™ internal prior. *arXiv
    preprint arXiv:2404.10198*.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2024) **å´å‡¯æ–‡**, **å´è‰¾ç‘å…‹**, å’Œ **è©¹å§†æ–¯Â·é‚¹**ã€‚2024ã€‚ã€ŠRAG æ¨¡å‹æœ‰å¤šå¯é ï¼Ÿé‡åŒ– RAG ä¸ LLM
    å†…éƒ¨å…ˆéªŒçš„æ‹‰é”¯æˆ˜ã€‹ã€‚*arXiv é¢„å°æœ¬ arXiv:2404.10198*ã€‚
- en: Xiao etÂ al. (2023) Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and
    Mike Lewis. 2023. Efficient streaming language models with attention sinks. *arXiv
    preprint arXiv:2309.17453*.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao ç­‰äººï¼ˆ2023ï¼‰å…‰è½©Â·è‚–ï¼Œè¿œä¸œÂ·ç”°ï¼Œè´è¿ªÂ·é™ˆï¼Œå®‹Â·éŸ©ï¼Œè¿ˆå…‹Â·åˆ˜æ˜“æ–¯ã€‚2023ã€‚å…·æœ‰æ³¨æ„åŠ›æ±‡èšçš„é«˜æ•ˆæµå¼è¯­è¨€æ¨¡å‹ã€‚*arXiv é¢„å°æœ¬ arXiv:2309.17453*ã€‚
- en: Xiong etÂ al. (2023) Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal
    Bhargava, Rui Hou, Louis Martin, Rashi Rungta, KarthikÂ Abinav Sankararaman, Barlas
    Oguz, etÂ al. 2023. Effective long-context scaling of foundation models. *arXiv
    preprint arXiv:2309.16039*.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiong ç­‰äººï¼ˆ2023ï¼‰æ–‡æ±‰Â·ç†Šï¼Œæ™¶å®‡Â·åˆ˜ï¼Œä¼Šæˆˆå°”Â·è«åˆ©åšæ ¼ï¼Œè´ºä½³Â·å¼ ï¼Œæ™®æ‹‰å‰ç“¦å°”Â·å·´å°”åŠ ç“¦ï¼Œç‘Â·ä¾¯ï¼Œè·¯æ˜“æ–¯Â·é©¬ä¸ï¼Œæ‹‰å¸ŒÂ·éš†å¡”ï¼Œå¡å°”æå…‹Â·é˜¿æ¯”çº³å¤«Â·æ¡‘å¡æ‹‰æ‹‰æ›¼ï¼Œå·´å°”æ‹‰æ–¯Â·å¥¥å¤å…¹
    ç­‰äººã€‚2023ã€‚æœ‰æ•ˆçš„åŸºç¡€æ¨¡å‹é•¿ä¸Šä¸‹æ–‡æ‰©å±•ã€‚*arXiv é¢„å°æœ¬ arXiv:2309.16039*ã€‚
- en: Xu etÂ al. (2023) Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu,
    Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan
    Catanzaro. 2023. Retrieval meets long context large language models. *arXiv preprint
    arXiv:2310.03025*.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu ç­‰äººï¼ˆ2023ï¼‰é¹Â·å¾ï¼Œé­Â·å¹³ï¼Œè´¤è¶…Â·å´ï¼ŒåŠ³ä¼¦æ–¯Â·éº¦å…‹é˜¿è²ï¼Œé™ˆÂ·æœ±ï¼Œå­æ¶µÂ·åˆ˜ï¼Œæ¡‘è¿ªæ™®Â·è‹å¸ƒæ‹‰é©¬å°¼å®‰ï¼ŒåŸƒç»´è‰å¨œÂ·å·´èµ«å›¾é‡Œçº³ï¼Œç©†ç½•é»˜å¾·Â·è‚–è€¶æ¯”ï¼Œå¸ƒèµ–æ©Â·å¡å¦æ‰ç½—ã€‚2023ã€‚æ£€ç´¢é‡åˆ°é•¿ä¸Šä¸‹æ–‡å¤§è¯­è¨€æ¨¡å‹ã€‚*arXiv
    é¢„å°æœ¬ arXiv:2310.03025*ã€‚
- en: 'Zhang etÂ al. (2023) Lei Zhang, Yunshui Li, Ziqiang Liu, Junhao Liu, Min Yang,
    etÂ al. 2023. Marathon: A race through the realm of long context with large language
    models. *arXiv preprint arXiv:2312.09542*.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang ç­‰äººï¼ˆ2023ï¼‰é›·Â·å¼ ï¼Œäº‘æ°´Â·æï¼Œå­å¼ºÂ·åˆ˜ï¼Œä¿Šè±ªÂ·åˆ˜ï¼Œæ•Â·æ¨ ç­‰äººã€‚2023ã€‚é©¬æ‹‰æ¾ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„é•¿ä¸Šä¸‹æ–‡é¢†åŸŸç«èµ›ã€‚*arXiv é¢„å°æœ¬
    arXiv:2312.09542*ã€‚
- en: 'Zhang etÂ al. (2024) Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao
    Chen, MooÂ Khai Hao, XuÂ Han, ZhenÂ Leng Thai, Shuo Wang, Zhiyuan Liu, etÂ al. 2024.
    $\infty$bench: Extending long context evaluation beyond 100k tokens. *arXiv preprint
    arXiv:2402.13718*.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang ç­‰äººï¼ˆ2024ï¼‰æ¬£è£Â·å¼ ï¼Œè‹±å‘Â·é™ˆï¼Œåœ£ä¸Â·èƒ¡ï¼Œç´«æ­Â·å¾ï¼Œä¿Šè±ªÂ·é™ˆï¼Œç©†Â·å‡¯Â·éƒï¼Œå¾Â·éŸ©ï¼Œé•‡Â·å†·Â·æ³°ï¼Œç¡•Â·ç‹ï¼Œæ™ºè¿œÂ·åˆ˜ ç­‰äººã€‚2024ã€‚$\infty$benchï¼šå°†é•¿ä¸Šä¸‹æ–‡è¯„ä¼°æ‰©å±•è‡³
    100k æ ‡è®°ä»¥ä¸Šã€‚*arXiv é¢„å°æœ¬ arXiv:2402.13718*ã€‚
- en: 'Zhu etÂ al. (2023) Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu
    Wei, and Sujian Li. 2023. Pose: Efficient context window extension of llms via
    positional skip-wise training. *arXiv preprint arXiv:2309.10400*.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu ç­‰äººï¼ˆ2023ï¼‰å¤§ä¼ŸÂ·æœ±ï¼Œå—Â·æ¨ï¼Œæ¢Â·ç‹ï¼Œé€¸å‡¡Â·å®‹ï¼Œæ–‡æµ©Â·å´ï¼Œç¦å¦‚Â·é­ï¼Œè‹å»ºÂ·æã€‚2023ã€‚Poseï¼šé€šè¿‡ä½ç½®è·³è·ƒè®­ç»ƒæ‰©å±• llms çš„é«˜æ•ˆä¸Šä¸‹æ–‡çª—å£ã€‚*arXiv
    é¢„å°æœ¬ arXiv:2309.10400*ã€‚
- en: Appendix A GPT4-as-the-Judge Prompt
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é™„å½• A GPT4 ä½œä¸ºè¯„åˆ¤è€…çš„æç¤º
- en: In Loong, GPT4 is used as a Judger to evaluate the correctness of the model-generated
    content, with the prompt used shown in the following. With this evaluation method,
    we expect the Judger model to output a percentage score along with its corresponding
    explanation.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ Loong ä¸­ï¼ŒGPT4 è¢«ç”¨ä½œåˆ¤æ–­å™¨æ¥è¯„ä¼°æ¨¡å‹ç”Ÿæˆå†…å®¹çš„æ­£ç¡®æ€§ï¼Œä»¥ä¸‹æ˜¯æ‰€ç”¨çš„æç¤ºã€‚é€šè¿‡è¿™ç§è¯„ä¼°æ–¹æ³•ï¼Œæˆ‘ä»¬æœŸæœ›åˆ¤æ–­å™¨æ¨¡å‹èƒ½å¤Ÿè¾“å‡ºä¸€ä¸ªç™¾åˆ†æ¯”è¯„åˆ†åŠå…¶å¯¹åº”çš„è§£é‡Šã€‚
- en: '[Gold
    Answer]  [The Start of Assistantâ€™s Predicted Answer] 
    [The End of Assistantâ€™s Predicted Answer] [System] We would like to request your
    feedback on the performance of the AI assistant in response to the user question
    displayed above according to the gold answer. Please use the following listed
    aspects and their descriptions as evaluation criteria: - Accuracy and Hallucinations:
    The assistantâ€™s answer is semantically consistent with the gold answer; The numerical
    value and order need to be accurate, and there should be no hallucinations. -
    Completeness: Referring to the reference answers, the assistantâ€™s answer should
    contain all the key points needed to answer the userâ€™s question; further elaboration
    on these key points can be omitted. Please rate whether this answer is suitable
    for the question. Please note that the gold answer can be considered as a correct
    answer to the question. The assistant receives an overall score on a scale of
    1 to 100, where a higher score indicates better overall performance.Please note
    that if the assistantâ€™s answer and the gold answer fully meet the above criteria,
    its overall rating should be the full marks (100). Please first provide a comprehensive
    explanation of your evaluation, avoiding any potential bias.Then, output a line
    indicating the score of the Assistant. PLEASE OUTPUT WITH THE FOLLOWING FORMAT,
    WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: "[[score]]",
    FOR EXAMPLE "Rating: [[100]]":  Evaluation evidence: your evluation
    explanation here, no more than 100 words Rating: [[score]]  Now, start
    your evaluation:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '[Gold
    Answer]  [The Start of Assistantâ€™s Predicted Answer] 
    [The End of Assistantâ€™s Predicted Answer] [System] We would like to request your
    feedback on the performance of the AI assistant in response to the user question
    displayed above according to the gold answer. Please use the following listed
    aspects and their descriptions as evaluation criteria: - Accuracy and Hallucinations:
    The assistantâ€™s answer is semantically consistent with the gold answer; The numerical
    value and order need to be accurate, and there should be no hallucinations. -
    Completeness: Referring to the reference answers, the assistantâ€™s answer should
    contain all the key points needed to answer the userâ€™s question; further elaboration
    on these key points can be omitted. Please rate whether this answer is suitable
    for the question. Please note that the gold answer can be considered as a correct
    answer to the question. The assistant receives an overall score on a scale of
    1 to 100, where a higher score indicates better overall performance.Please note
    that if the assistantâ€™s answer and the gold answer fully meet the above criteria,
    its overall rating should be the full marks (100). Please first provide a comprehensive
    explanation of your evaluation, avoiding any potential bias.Then, output a line
    indicating the score of the Assistant. PLEASE OUTPUT WITH THE FOLLOWING FORMAT,
    WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: "[[score]]",
    FOR EXAMPLE "Rating: [[100]]":  Evaluation evidence: your evluation
    explanation here, no more than 100 words Rating: [[score]]  Now, start
    your evaluation:'
- en: Appendix B Test Case
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é™„å½• B æµ‹è¯•æ¡ˆä¾‹
- en: To facilitate understanding of Loongâ€™s data examples, we present examples of
    11 sub-tasks in the following, showing the format we input to the model as well
    as the prompts we used.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä¾¿äºç†è§£ Loong çš„æ•°æ®ç¤ºä¾‹ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä»¥ä¸‹ 11 ä¸ªå­ä»»åŠ¡çš„ç¤ºä¾‹ï¼Œå±•ç¤ºäº†æˆ‘ä»¬è¾“å…¥æ¨¡å‹çš„æ ¼å¼ä»¥åŠä½¿ç”¨çš„æç¤ºã€‚
- en: B.1 Spotlight Locating
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 èšå…‰ç¯å®šä½
- en: 
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 
- en: 'Prompt: Please answer the following questions based only on the judgment documents
    you have seen above. You only need to give the titles of the judgment documents
    that meet the requirements.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: æç¤ºï¼šè¯·ä»…æ ¹æ®æ‚¨ä¸Šé¢çœ‹åˆ°çš„åˆ¤æ–­æ–‡æ¡£å›ç­”ä»¥ä¸‹é—®é¢˜ã€‚æ‚¨åªéœ€ç»™å‡ºç¬¦åˆè¦æ±‚çš„åˆ¤æ–­æ–‡æ¡£æ ‡é¢˜ã€‚
- en: 'Question: Among the above judgment documents, which one is the case of â€™crime
    of endangering public securityâ€™?'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜ï¼šåœ¨ä¸Šè¿°åˆ¤æ–­æ–‡æ¡£ä¸­ï¼Œå“ªä¸ªæ˜¯â€œå±å®³å…¬å…±å®‰å…¨ç½ªâ€æ¡ˆä»¶ï¼Ÿ
- en: 'Answer: Judgment Document 5'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ç­”æ¡ˆï¼šåˆ¤æ–­æ–‡æ¡£ 5
- en: B.2 Sequential Enumeration
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 é¡ºåºæšä¸¾
- en: 
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 
- en: 'Prompt: We kindly ask you to review the financial statements of the companies
    provided above and answer the following questions based solely on the information
    you have seen. If the question involves content not found in the financial statements,
    you may ignore this part and only answer the other parts.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: æç¤ºï¼šæˆ‘ä»¬è¯·æ‚¨å®¡é˜…ä¸Šè¿°å…¬å¸æä¾›çš„è´¢åŠ¡æŠ¥è¡¨ï¼Œå¹¶ä»…æ ¹æ®æ‚¨æ‰€çœ‹åˆ°çš„ä¿¡æ¯å›ç­”ä»¥ä¸‹é—®é¢˜ã€‚å¦‚æœé—®é¢˜æ¶‰åŠè´¢åŠ¡æŠ¥è¡¨ä¸­æœªæ‰¾åˆ°çš„å†…å®¹ï¼Œæ‚¨å¯ä»¥å¿½ç•¥è¿™ä¸€éƒ¨åˆ†ï¼Œä»…å›ç­”å…¶ä»–éƒ¨åˆ†ã€‚
- en: 'Question: Please list the Cash and Cash Equivalents of each of the aforementioned
    companies in ascending order?'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜ï¼šè¯·æŒ‰å‡åºåˆ—å‡ºä¸Šè¿°æ¯å®¶å…¬å¸ç°é‡‘åŠç°é‡‘ç­‰ä»·ç‰©çš„é‡‘é¢ï¼Ÿ
- en: 'Answer: $ 1,273 thousand, $ 1,360 thousand, $ 9,364 thousand'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ç­”æ¡ˆï¼š$1,273 åƒï¼Œ$1,360 åƒï¼Œ$9,364 åƒ
- en: B.3 Extremum Acquisition
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3 æå€¼è·å–
- en: 
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 
- en: 'Prompt: We kindly ask you to review the financial statements of the companies
    provided above and answer the following questions based solely on the information
    you have seen. If the question involves content not found in the financial statements,
    you may ignore this part and only answer the other parts.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: æç¤ºï¼šæˆ‘ä»¬æ³è¯·ä½ æŸ¥çœ‹ä¸Šè¿°æä¾›çš„å…¬å¸è´¢åŠ¡æŠ¥è¡¨ï¼Œå¹¶ä»…æ ¹æ®ä½ æ‰€çœ‹åˆ°çš„ä¿¡æ¯å›ç­”ä»¥ä¸‹é—®é¢˜ã€‚å¦‚æœé—®é¢˜æ¶‰åŠè´¢åŠ¡æŠ¥è¡¨ä¸­æœªåŒ…å«çš„å†…å®¹ï¼Œä½ å¯ä»¥å¿½ç•¥è¿™éƒ¨åˆ†ï¼Œåªå›ç­”å…¶ä»–éƒ¨åˆ†ã€‚
- en: 'Question: Which company has the highest â€™Total Non-current Assetsâ€™?'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜ï¼šå“ªå®¶å…¬å¸æ‹¥æœ‰æœ€é«˜çš„â€œæ€»éæµåŠ¨èµ„äº§â€ï¼Ÿ
- en: 'Answer: BLUE DOLPHIN ENERGY CO with $56,787,000'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ç­”æ¡ˆï¼šè“è‰²æµ·è±šèƒ½æºå…¬å¸ï¼Œæ€»é¢ä¸º56,787,000ç¾å…ƒ
- en: B.4 Range Awareness
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4 èŒƒå›´æ„è¯†
- en: 
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 
- en: 'Prompt: We kindly ask you to review the financial statements of the companies
    provided above and answer the following questions based solely on the information
    you have seen. If the question involves content not found in the financial statements,
    you may ignore this part and only answer the other parts.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: æç¤ºï¼šæˆ‘ä»¬æ³è¯·ä½ æŸ¥çœ‹ä¸Šè¿°æä¾›çš„å…¬å¸è´¢åŠ¡æŠ¥è¡¨ï¼Œå¹¶ä»…æ ¹æ®ä½ æ‰€çœ‹åˆ°çš„ä¿¡æ¯å›ç­”ä»¥ä¸‹é—®é¢˜ã€‚å¦‚æœé—®é¢˜æ¶‰åŠè´¢åŠ¡æŠ¥è¡¨ä¸­æœªåŒ…å«çš„å†…å®¹ï¼Œä½ å¯ä»¥å¿½ç•¥è¿™éƒ¨åˆ†ï¼Œåªå›ç­”å…¶ä»–éƒ¨åˆ†ã€‚
- en: 'Question: Which company has the highest â€™Total Current Liabilitiesâ€™?'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜ï¼šå“ªå®¶å…¬å¸æ‹¥æœ‰æœ€é«˜çš„â€œæ€»æµåŠ¨è´Ÿå€ºâ€ï¼Ÿ
- en: 'Answer: 4 companies'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ç­”æ¡ˆï¼š4å®¶å…¬å¸
- en: B.5 Report Integration
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.5 æŠ¥å‘Šæ•´åˆ
- en: 
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 
- en: 'Prompt: We kindly ask you to review the financial statements of the companies
    provided above and answer the following questions based solely on the information
    you have seen. If the question involves content not found in the financial statements,
    you may ignore this part and only answer the other parts.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: æç¤ºï¼šæˆ‘ä»¬æ³è¯·ä½ æŸ¥çœ‹ä¸Šè¿°æä¾›çš„å…¬å¸è´¢åŠ¡æŠ¥è¡¨ï¼Œå¹¶ä»…æ ¹æ®ä½ æ‰€çœ‹åˆ°çš„ä¿¡æ¯å›ç­”ä»¥ä¸‹é—®é¢˜ã€‚å¦‚æœé—®é¢˜æ¶‰åŠè´¢åŠ¡æŠ¥è¡¨ä¸­æœªåŒ…å«çš„å†…å®¹ï¼Œä½ å¯ä»¥å¿½ç•¥è¿™éƒ¨åˆ†ï¼Œåªå›ç­”å…¶ä»–éƒ¨åˆ†ã€‚
- en: 'Question: Please categorize the companies listed above by â€™Total Shares Outstandingâ€™
    into the following groups: below 10,000,000 shares and 10,000,000 shares or more.
    Place companies into the same collection for the same category and into different
    collections for different categories.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜ï¼šè¯·å°†ä¸Šè¿°åˆ—å‡ºçš„å…¬å¸æŒ‰â€œæ€»æµé€šè‚¡æ•°â€åˆ†ä¸ºä»¥ä¸‹å‡ ç»„ï¼šä½äº10,000,000è‚¡å’Œ10,000,000è‚¡æˆ–æ›´å¤šã€‚å°†åŒä¸€ç±»åˆ«çš„å…¬å¸æ”¾å…¥åŒä¸€é›†åˆï¼Œå°†ä¸åŒç±»åˆ«çš„å…¬å¸æ”¾å…¥ä¸åŒé›†åˆã€‚
- en: 'Answer: {"below 10,000,000 shares": ["GSE SYSTEMS INC", "CROSS TIMBERS ROYALTY
    TRUST"], "10,000,000 shares or more": ["HUGOTON ROYALTY TRUST"]}'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç­”æ¡ˆï¼š{"ä½äº10,000,000è‚¡": ["GSE SYSTEMS INC", "CROSS TIMBERS ROYALTY TRUST"], "10,000,000è‚¡æˆ–æ›´å¤š":
    ["HUGOTON ROYALTY TRUST"]}'
- en: B.6 Citation&Reference
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.6 å¼•ç”¨ä¸å‚è€ƒ
- en: 
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 
- en: 'Prompt: We hope you will carefully study the provided papers and determine
    the citation relationships between them. Please follow the instructions below
    strictly to complete the task:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: æç¤ºï¼šæˆ‘ä»¬å¸Œæœ›ä½ ä»”ç»†ç ”ç©¶æä¾›çš„è®ºæ–‡ï¼Œå¹¶ç¡®å®šå®ƒä»¬ä¹‹é—´çš„å¼•ç”¨å…³ç³»ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è¯´æ˜å®Œæˆä»»åŠ¡ï¼š
- en: '#Specific Requirements:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '#å…·ä½“è¦æ±‚ï¼š'
- en: '1\. Reference: When a given paper mentions other provided papers, those other
    papers are considered as "references" for the given paper. To summarize in this
    specific context, references are about what the given paper is using.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 1. å‚è€ƒæ–‡çŒ®ï¼šå½“ç»™å®šçš„è®ºæ–‡æåˆ°å…¶ä»–æä¾›çš„è®ºæ–‡æ—¶ï¼Œè¿™äº›å…¶ä»–è®ºæ–‡è¢«è§†ä¸ºç»™å®šè®ºæ–‡çš„â€œå‚è€ƒæ–‡çŒ®â€ã€‚åœ¨è¿™ä¸ªå…·ä½“çš„èƒŒæ™¯ä¸‹ï¼Œå‚è€ƒæ–‡çŒ®æ˜¯å…³äºç»™å®šè®ºæ–‡æ‰€ä½¿ç”¨çš„å†…å®¹ã€‚
- en: '2\. Citation: Conversely, when other provided papers mention the given paper
    in their works, the given paper is being "cited" by those other papers. To summarize
    in this specific context, citations are about who is using the given paper.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 2. å¼•ç”¨ï¼šç›¸åï¼Œå½“å…¶ä»–æä¾›çš„è®ºæ–‡åœ¨å…¶å·¥ä½œä¸­æåˆ°ç»™å®šçš„è®ºæ–‡æ—¶ï¼Œç»™å®šçš„è®ºæ–‡è¢«è¿™äº›å…¶ä»–è®ºæ–‡â€œå¼•ç”¨â€ã€‚åœ¨è¿™ä¸ªå…·ä½“çš„èƒŒæ™¯ä¸‹ï¼Œå¼•ç”¨æ˜¯å…³äºè°åœ¨ä½¿ç”¨ç»™å®šçš„è®ºæ–‡ã€‚
- en: 3\. Given a paper, you need to determine the citation or reference relationship
    between this paper and the other papers. Do not consider papers that are not provided.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 3. ç»™å®šä¸€ç¯‡è®ºæ–‡ï¼Œä½ éœ€è¦ç¡®å®šè¯¥è®ºæ–‡ä¸å…¶ä»–è®ºæ–‡ä¹‹é—´çš„å¼•ç”¨æˆ–å‚è€ƒå…³ç³»ã€‚è¯·ä¸è¦è€ƒè™‘æœªæä¾›çš„è®ºæ–‡ã€‚
- en: '3\. Please present the paper titles in a json format as follows: {"Reference":["Reference
    Title 1", "Reference Title 2", â€¦, "Reference Title n"], "Citation":["Citation
    Title 1", "Citation Title 2", â€¦, "Citation Title n"]}.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 3. è¯·ä»¥jsonæ ¼å¼å‘ˆç°è®ºæ–‡æ ‡é¢˜ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š{"Reference":["å‚è€ƒæ ‡é¢˜1", "å‚è€ƒæ ‡é¢˜2", â€¦, "å‚è€ƒæ ‡é¢˜n"], "Citation":["å¼•ç”¨æ ‡é¢˜1",
    "å¼•ç”¨æ ‡é¢˜2", â€¦, "å¼•ç”¨æ ‡é¢˜n"]}.
- en: 4\. If a paper does not have any references or citations, please leave the corresponding
    list empty, e.g.{"Refernce":[]}, {"Citation":[]}.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 4. å¦‚æœä¸€ç¯‡è®ºæ–‡æ²¡æœ‰ä»»ä½•å‚è€ƒæ–‡çŒ®æˆ–å¼•ç”¨ï¼Œè¯·å°†ç›¸åº”åˆ—è¡¨ç•™ç©ºï¼Œä¾‹å¦‚{"Refernce":[]}, {"Citation":[]}.
- en: 'Question: The paper you need to analyze:Self-Discover: Large Language Models
    Self-Compose Reasoning Structures'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜ï¼šä½ éœ€è¦åˆ†æçš„è®ºæ–‡æ˜¯ï¼šã€Šè‡ªæˆ‘å‘ç°ï¼šå¤§è¯­è¨€æ¨¡å‹è‡ªæˆ‘æ„å»ºæ¨ç†ç»“æ„ã€‹
- en: 'Answer: {â€™Referenceâ€™: [â€™# Plan, Verify and Switch: Integrated Reasoning with
    Diverse X-of-Thoughts â€™, â€™# StrategyLLM: Large Language Models as Strategy Generators,
    Executors, Optimizers, and Evaluators for Problem Solving â€™], â€™Citationâ€™: [â€™#
    Can LLMs Solve Longer Math Word Problems Better? â€™]}'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç­”æ¡ˆï¼š{â€™å‚è€ƒæ–‡çŒ®â€™: [â€™# è®¡åˆ’ã€éªŒè¯å’Œåˆ‡æ¢ï¼šå¤šæ ·åŒ–æ€ç»´æ•´åˆæ¨ç†â€™, â€™# StrategyLLMï¼šå¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºç­–ç•¥ç”Ÿæˆè€…ã€æ‰§è¡Œè€…ã€ä¼˜åŒ–è€…å’Œè¯„ä¼°è€…ç”¨äºé—®é¢˜è§£å†³â€™],
    â€™å¼•ç”¨â€™: [â€™# LLM èƒ½å¦æ›´å¥½åœ°è§£å†³æ›´é•¿çš„æ•°å­¦æ–‡å­—é—®é¢˜ï¼Ÿâ€™]}'
- en: B.7 Case Classification
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.7 æ¡ˆä¾‹åˆ†ç±»
- en: 
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 
- en: 'Prompt: Please answer the following questions based only on the judgment documents
    you have seen above. You only need to give the titles of the judgment documents
    that meet the requirements.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: æç¤ºï¼šè¯·ä»…æ ¹æ®æ‚¨ä¸Šé¢æ‰€è§çš„åˆ¤å†³æ–‡æ¡£å›ç­”ä»¥ä¸‹é—®é¢˜ã€‚æ‚¨åªéœ€è¦æä¾›ç¬¦åˆè¦æ±‚çš„åˆ¤å†³æ–‡æ¡£æ ‡é¢˜ã€‚
- en: 'Question: After reading the above judgments, please classify all the judgments
    according to the following three types of cases: â€™Civil Casesâ€™, â€™Enforcement Casesâ€™,
    and â€™Administrative Casesâ€™.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜ï¼šé˜…è¯»ä»¥ä¸Šåˆ¤å†³åï¼Œè¯·æ ¹æ®ä»¥ä¸‹ä¸‰ç§æ¡ˆä»¶ç±»å‹å¯¹æ‰€æœ‰åˆ¤å†³è¿›è¡Œåˆ†ç±»ï¼šâ€™æ°‘äº‹æ¡ˆä»¶â€™ï¼Œâ€™æ‰§è¡Œæ¡ˆä»¶â€™ï¼Œå’Œâ€™è¡Œæ”¿æ¡ˆä»¶â€™ã€‚
- en: 'Answer: {"Civil Cases": ["Judgment Document 2"], "Enforcement Cases": ["Judgment
    Document 4"], "Administrative Cases": ["Judgment Document 1", "Judgment Document
    3"]}'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç­”æ¡ˆï¼š{"æ°‘äº‹æ¡ˆä»¶": ["åˆ¤å†³æ–‡æ¡£ 2"], "æ‰§è¡Œæ¡ˆä»¶": ["åˆ¤å†³æ–‡æ¡£ 4"], "è¡Œæ”¿æ¡ˆä»¶": ["åˆ¤å†³æ–‡æ¡£ 1", "åˆ¤å†³æ–‡æ¡£ 3"]}'
- en: B.8 Temporal Analysis
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.8 æ—¶é—´åˆ†æ
- en: 
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 
- en: 'Prompt: We kindly ask you to review the financial statements of the companies
    provided above and answer the following questions based solely on the information
    you have seen. If the question involves content not found in the financial statements,
    you may ignore this part and only answer the other parts.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: æç¤ºï¼šæˆ‘ä»¬æ³è¯·æ‚¨å®¡æŸ¥ä¸Šè¿°å…¬å¸æä¾›çš„è´¢åŠ¡æŠ¥è¡¨ï¼Œå¹¶ä»…æ ¹æ®æ‚¨æ‰€è§çš„ä¿¡æ¯å›ç­”ä»¥ä¸‹é—®é¢˜ã€‚å¦‚æœé—®é¢˜æ¶‰åŠè´¢åŠ¡æŠ¥è¡¨ä¸­æœªå‡ºç°çš„å†…å®¹ï¼Œæ‚¨å¯ä»¥å¿½ç•¥è¿™ä¸€éƒ¨åˆ†ï¼Œåªå›ç­”å…¶ä»–éƒ¨åˆ†ã€‚
- en: 'Question: What is the trend in ARVANA INCâ€™s share capital from 2021 to 2024?'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜ï¼šARVANA INC ä» 2021 å¹´åˆ° 2024 å¹´çš„è‚¡æœ¬è¶‹åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ
- en: 'Answer: ARVANA INCâ€™s share capital has consistently increased from $4,611 in
    2021 to $34,149 in 2022, $35,949 in 2023, and $107,847 in 2024.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: ç­”æ¡ˆï¼šARVANA INC çš„è‚¡æœ¬ä» 2021 å¹´çš„ $4,611 ç¨³æ­¥å¢é•¿åˆ° 2022 å¹´çš„ $34,149ã€2023 å¹´çš„ $35,949ï¼Œä»¥åŠ 2024
    å¹´çš„ $107,847ã€‚
- en: B.9 Citation Chain
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.9 å¼•ç”¨é“¾
- en: 
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 
- en: 'Prompt: We kindly ask you to thoroughly review the provided papers and construct
    a citation chain from them. Please adhere to the following instructions strictly
    while completing the task:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: æç¤ºï¼šæˆ‘ä»¬æ³è¯·æ‚¨å½»åº•å®¡æŸ¥æä¾›çš„è®ºæ–‡ï¼Œå¹¶ä»ä¸­æ„å»ºä¸€ä¸ªå¼•ç”¨é“¾ã€‚è¯·åœ¨å®Œæˆä»»åŠ¡æ—¶ä¸¥æ ¼éµå¾ªä»¥ä¸‹è¯´æ˜ï¼š
- en: '#Task Instructions:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '#ä»»åŠ¡è¯´æ˜ï¼š'
- en: Given several papers, you are required to identify and list the longest citation
    chain, which demonstrates the citation relationship among the provided papers.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šå‡ ç¯‡è®ºæ–‡ï¼Œæ‚¨éœ€è¦è¯†åˆ«å¹¶åˆ—å‡ºæœ€é•¿çš„å¼•ç”¨é“¾ï¼Œå±•ç¤ºæ‰€æä¾›è®ºæ–‡ä¹‹é—´çš„å¼•ç”¨å…³ç³»ã€‚
- en: '#Specific Requirements:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '#å…·ä½“è¦æ±‚ï¼š'
- en: '1.Please present the titles of the papers in the form of a list, as follows:
    ["Title of Paper 1", "Title of Paper 2", â€¦, "Title of Paper n"].'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 1. è¯·ä»¥åˆ—è¡¨å½¢å¼å‘ˆç°è®ºæ–‡æ ‡é¢˜ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š["è®ºæ–‡ 1 çš„æ ‡é¢˜", "è®ºæ–‡ 2 çš„æ ‡é¢˜", â€¦, "è®ºæ–‡ n çš„æ ‡é¢˜"]ã€‚
- en: 2.Ensure that the citation chain in the list is linear and continuous, meaning
    that the first paper title in the list (Paper 1) should not cite any other works.
    Instead, it should be cited by the next paper in the list (Paper 2); subsequently,
    each paper should then be cited by the next one in the list, continuing up to
    the last paper (Paper n).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 2. ç¡®ä¿åˆ—è¡¨ä¸­çš„å¼•ç”¨é“¾æ˜¯çº¿æ€§çš„å’Œè¿ç»­çš„ï¼Œå³åˆ—è¡¨ä¸­çš„ç¬¬ä¸€ç¯‡è®ºæ–‡æ ‡é¢˜ï¼ˆè®ºæ–‡ 1ï¼‰ä¸åº”å¼•ç”¨ä»»ä½•å…¶ä»–ä½œå“ã€‚ç›¸åï¼Œå®ƒåº”è¯¥è¢«åˆ—è¡¨ä¸­çš„ä¸‹ä¸€ç¯‡è®ºæ–‡ï¼ˆè®ºæ–‡ 2ï¼‰å¼•ç”¨ï¼›éšåï¼Œæ¯ç¯‡è®ºæ–‡åº”è¢«åˆ—è¡¨ä¸­çš„ä¸‹ä¸€ç¯‡è®ºæ–‡å¼•ç”¨ï¼Œä¸€ç›´æŒç»­åˆ°æœ€åä¸€ç¯‡è®ºæ–‡ï¼ˆè®ºæ–‡
    nï¼‰ã€‚
- en: 3.Consider only the citation relationships within the supplied collection of
    papers, and ensure that the citation chain accurately reflects the sequential
    citation order among these documents.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 3. ä»…è€ƒè™‘æä¾›çš„è®ºæ–‡é›†åˆä¸­çš„å¼•ç”¨å…³ç³»ï¼Œå¹¶ç¡®ä¿å¼•ç”¨é“¾å‡†ç¡®åæ˜ è¿™äº›æ–‡æ¡£ä¹‹é—´çš„é¡ºåºå¼•ç”¨ã€‚
- en: 4.Do not take into account any articles not provided, and disregard other non-linear
    citation relationships.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 4. ä¸è€ƒè™‘æœªæä¾›çš„ä»»ä½•æ–‡ç« ï¼Œå¹¶å¿½ç•¥å…¶ä»–éçº¿æ€§å¼•ç”¨å…³ç³»ã€‚
- en: 'Answer: ["# Very Deep Transformers for Neural Machine Translation ", "# Understanding
    the Difficulty of Training Transformers ", "# MonaCoBERT: Monotonic attention
    based ConvBERT for Knowledge Tracing"]'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ç­”æ¡ˆï¼š["# éå¸¸æ·±åº¦çš„å˜æ¢å™¨ç”¨äºç¥ç»æœºå™¨ç¿»è¯‘", "# ç†è§£è®­ç»ƒå˜æ¢å™¨çš„å›°éš¾", "# MonaCoBERTï¼šåŸºäºå•è°ƒæ³¨æ„åŠ›çš„ConvBERTç”¨äºçŸ¥è¯†è¿½è¸ª"]
- en: B.10 Link the Links
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.10 é“¾æ¥é“¾æ¥
- en: 
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 
- en: 'Prompt: Answer the following questions based solely on the judgment document
    you have seen above.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 'æç¤º: æ ¹æ®ä½ ä¸Šé¢çœ‹åˆ°çš„åˆ¤å†³æ–‡ä¹¦ï¼Œä»…æ ¹æ®è¿™äº›æ–‡ä¹¦å›ç­”ä»¥ä¸‹é—®é¢˜ã€‚'
- en: 'Question:After reading the above judgment document, I will give you several
    judgment results:  You need to
    determine the most likely judgment result for each of the above judgment documents.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 'é—®é¢˜: åœ¨é˜…è¯»ä¸Šè¿°åˆ¤å†³æ–‡ä¹¦åï¼Œæˆ‘å°†æä¾›å‡ ä¸ªåˆ¤å†³ç»“æœ:  ä½ éœ€è¦ç¡®å®šæ¯ä¸ªä¸Šè¿°åˆ¤å†³æ–‡ä¹¦æœ€å¯èƒ½çš„åˆ¤å†³ç»“æœã€‚'
- en: 'Answer: {"Judgment Document 1": "Judgment Result 1", "Judgment Document 2":
    "Judgment Result 6", "Judgment Document 3": "Judgment Result 2", "Judgment Document
    4": "Judgment Result 5"}'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç­”æ¡ˆ: {"åˆ¤å†³æ–‡ä¹¦ 1": "åˆ¤å†³ç»“æœ 1", "åˆ¤å†³æ–‡ä¹¦ 2": "åˆ¤å†³ç»“æœ 6", "åˆ¤å†³æ–‡ä¹¦ 3": "åˆ¤å†³ç»“æœ 2", "åˆ¤å†³æ–‡ä¹¦ 4":
    "åˆ¤å†³ç»“æœ 5"}'
- en: B.11 Solitaire
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.11 æ‰‘å…‹æ¸¸æˆ
- en: 
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 
- en: 'Prompt: Answer the following questions based solely on the judgment document
    you have seen above.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 'æç¤º: æ ¹æ®ä½ ä¸Šé¢çœ‹åˆ°çš„åˆ¤å†³æ–‡ä¹¦ï¼Œä»…æ ¹æ®è¿™äº›æ–‡ä¹¦å›ç­”ä»¥ä¸‹é—®é¢˜ã€‚'
- en: 'Question: Reading the above judgments, I will provide several case types arranged
    in a left-to-right sequence: [â€™CaseType1â€™, â€™CaseType2â€™, â€™CaseType3â€™, â€™CaseType4â€™].
    You need to sort all the judgment documents according to the above sequence of
    case types. The judgment documents only need to include the titles.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 'é—®é¢˜: é˜…è¯»ä¸Šè¿°åˆ¤å†³åï¼Œæˆ‘å°†æä¾›å‡ ä¸ªæŒ‰ä»å·¦åˆ°å³é¡ºåºæ’åˆ—çš„æ¡ˆä»¶ç±»å‹: [â€™CaseType1â€™, â€™CaseType2â€™, â€™CaseType3â€™, â€™CaseType4â€™]ã€‚ä½ éœ€è¦æ ¹æ®ä¸Šè¿°æ¡ˆä»¶ç±»å‹çš„é¡ºåºå¯¹æ‰€æœ‰åˆ¤å†³æ–‡ä¹¦è¿›è¡Œæ’åºã€‚åˆ¤å†³æ–‡ä¹¦åªéœ€åŒ…æ‹¬æ ‡é¢˜ã€‚'
- en: 'Please provide the answer:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¯·æä¾›ç­”æ¡ˆ:'
- en: 'Answer: {"CaseType1": "Judgment Document 3", "CaseType2": "Judgment Document
    1", "CaseType3": "Judgment Document 4", "CaseType4": "Judgment Document 6"}'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç­”æ¡ˆ: {"CaseType1": "åˆ¤å†³æ–‡ä¹¦ 3", "CaseType2": "åˆ¤å†³æ–‡ä¹¦ 1", "CaseType3": "åˆ¤å†³æ–‡ä¹¦ 4", "CaseType4":
    "åˆ¤å†³æ–‡ä¹¦ 6"}'
- en: Appendix C Length Distribution
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é™„å½• C é•¿åº¦åˆ†å¸ƒ
- en: 'As shown in [FigureÂ 4](#A3.F4 "In Appendix C Length Distribution â€£ Leave No
    Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA") and
    [TableÂ 5](#A5.T5 "In Appendix E Comparison of Evidence Distribution â€£ Leave No
    Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA"),
    we present the distribution of data lengths in Loong. It can be observed that
    the data is primarily distributed around 30-150k. Moreover, we have sufficient
    data in both shorter and longer ranges, allowing us to assess the modelâ€™s capabilities
    across each length interval.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¦‚[å›¾ 4](#A3.F4 "åœ¨é™„å½• C é•¿åº¦åˆ†å¸ƒ â€£ ä¸é—æ¼ä»»ä½•æ–‡æ¡£: åŸºå‡†æµ‹è¯•é•¿æœŸä¸Šä¸‹æ–‡ LLM çš„æ‰©å±•å¤šæ–‡æ¡£é—®ç­”")å’Œ[è¡¨ 5](#A5.T5
    "åœ¨é™„å½• E è¯æ®åˆ†å¸ƒæ¯”è¾ƒ â€£ ä¸é—æ¼ä»»ä½•æ–‡æ¡£: åŸºå‡†æµ‹è¯•é•¿æœŸä¸Šä¸‹æ–‡ LLM çš„æ‰©å±•å¤šæ–‡æ¡£é—®ç­”")æ‰€ç¤ºï¼Œæˆ‘ä»¬å±•ç¤ºäº† Loong ä¸­æ•°æ®é•¿åº¦çš„åˆ†å¸ƒæƒ…å†µã€‚å¯ä»¥è§‚å¯Ÿåˆ°æ•°æ®ä¸»è¦é›†ä¸­åœ¨
    30-150k ä¹‹é—´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨è¾ƒçŸ­å’Œè¾ƒé•¿èŒƒå›´å†…éƒ½æœ‰è¶³å¤Ÿçš„æ•°æ®ï¼Œå…è®¸æˆ‘ä»¬è¯„ä¼°æ¨¡å‹åœ¨æ¯ä¸ªé•¿åº¦åŒºé—´çš„èƒ½åŠ›ã€‚'
- en: '![Refer to caption](img/692808769763ab264d1aa427b7533b4f.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/692808769763ab264d1aa427b7533b4f.png)'
- en: 'Figure 4: Test Case Length Distribution in Loong.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 4: Loong æµ‹è¯•ç”¨ä¾‹é•¿åº¦åˆ†å¸ƒ'
- en: Appendix D RAG Detailed Results
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é™„å½• D RAG è¯¦ç»†ç»“æœ
- en: 'We conducted experiments on GPT-4o and Qwen2-72B-Instruct with the addition
    of a RAG module. As shown in [TableÂ 6](#A5.T6 "In Appendix E Comparison of Evidence
    Distribution â€£ Leave No Document Behind: Benchmarking Long-Context LLMs with Extended
    Multi-Doc QA"), [TableÂ 7](#A5.T7 "In Appendix E Comparison of Evidence Distribution
    â€£ Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc
    QA"), and [TableÂ 8](#A5.T8 "In Appendix E Comparison of Evidence Distribution
    â€£ Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc
    QA"), we have published detailed experimental results. It can be seen that RAG
    achieved subpar results on our Loong, indicating that Loong requires the model
    to have genuine long-context understanding capabilities.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 'æˆ‘ä»¬å¯¹ GPT-4o å’Œ Qwen2-72B-Instruct è¿›è¡Œäº†å®éªŒï¼Œå¹¶å¢åŠ äº† RAG æ¨¡å—ã€‚å¦‚[è¡¨ 6](#A5.T6 "åœ¨é™„å½• E è¯æ®åˆ†å¸ƒæ¯”è¾ƒ
    â€£ ä¸é—æ¼ä»»ä½•æ–‡æ¡£: åŸºå‡†æµ‹è¯•é•¿æœŸä¸Šä¸‹æ–‡ LLM çš„æ‰©å±•å¤šæ–‡æ¡£é—®ç­”")ã€[è¡¨ 7](#A5.T7 "åœ¨é™„å½• E è¯æ®åˆ†å¸ƒæ¯”è¾ƒ â€£ ä¸é—æ¼ä»»ä½•æ–‡æ¡£: åŸºå‡†æµ‹è¯•é•¿æœŸä¸Šä¸‹æ–‡
    LLM çš„æ‰©å±•å¤šæ–‡æ¡£é—®ç­”")å’Œ[è¡¨ 8](#A5.T8 "åœ¨é™„å½• E è¯æ®åˆ†å¸ƒæ¯”è¾ƒ â€£ ä¸é—æ¼ä»»ä½•æ–‡æ¡£: åŸºå‡†æµ‹è¯•é•¿æœŸä¸Šä¸‹æ–‡ LLM çš„æ‰©å±•å¤šæ–‡æ¡£é—®ç­”")ä¸­æ‰€ç¤ºï¼Œæˆ‘ä»¬å…¬å¸ƒäº†è¯¦ç»†çš„å®éªŒç»“æœã€‚å¯ä»¥çœ‹å‡ºï¼ŒRAG
    åœ¨æˆ‘ä»¬çš„ Loong ä¸Šè¡¨ç°ä¸ä½³ï¼Œè¿™è¡¨æ˜ Loong éœ€è¦æ¨¡å‹å…·å¤‡çœŸæ­£çš„é•¿æœŸä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ã€‚'
- en: Appendix E Comparison of Evidence Distribution
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é™„å½• E è¯æ®åˆ†å¸ƒæ¯”è¾ƒ
- en: 'In the same multi-document question-answering task, we compared the distribution
    of evidence related to the answers in the context for Loong (Ours) and LongbenchÂ Bai
    etÂ al. ([2023b](#bib.bib5)). As shown in [TableÂ 9](#A5.T9 "In Appendix E Comparison
    of Evidence Distribution â€£ Leave No Document Behind: Benchmarking Long-Context
    LLMs with Extended Multi-Doc QA"), we present data examples from Loong and LongBench.
    It can be observed that although Longbench contains a large number of passages,
    the evidence is only distributed within Passage 1\. In contrast, in our Loong,
    the evidence is distributed across every document, requiring the model to understand
    each document in order to provide the correct answer.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç›¸åŒçš„å¤šæ–‡æ¡£é—®ç­”ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº† Loongï¼ˆæˆ‘ä»¬çš„ç³»ç»Ÿï¼‰å’Œ Longbench Bai ç­‰äººï¼ˆ[2023b](#bib.bib5)ï¼‰åœ¨ä¸Šä¸‹æ–‡ä¸­ä¸ç­”æ¡ˆç›¸å…³çš„è¯æ®åˆ†å¸ƒã€‚å¦‚
    [è¡¨ 9](#A5.T9 "é™„å½• E è¯æ®åˆ†å¸ƒæ¯”è¾ƒ â€£ ä¸é—æ¼ä»»ä½•æ–‡æ¡£ï¼šåŸºå‡†æµ‹è¯•é•¿ä¸Šä¸‹æ–‡ LLM çš„æ‰©å±•å¤šæ–‡æ¡£ QA") æ‰€ç¤ºï¼Œæˆ‘ä»¬å±•ç¤ºäº†æ¥è‡ª Loong
    å’Œ LongBench çš„æ•°æ®ç¤ºä¾‹ã€‚å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œå°½ç®¡ Longbench åŒ…å«å¤§é‡æ–‡æ®µï¼Œä½†è¯æ®ä»…åˆ†å¸ƒåœ¨ Passage 1 ä¸­ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåœ¨æˆ‘ä»¬çš„ Loong
    ä¸­ï¼Œè¯æ®åˆ†å¸ƒåœ¨æ¯ä¸ªæ–‡æ¡£ä¸­ï¼Œéœ€è¦æ¨¡å‹ç†è§£æ¯ä¸ªæ–‡æ¡£æ‰èƒ½æä¾›æ­£ç¡®çš„ç­”æ¡ˆã€‚
- en: '| Dataset | #data in 10-50k | #data in 50-100K | #data in 100K-200K | #data
    in 200-250K |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| æ•°æ®é›† | 10-50K æ•°æ®é‡ | 50-100K æ•°æ®é‡ | 100K-200K æ•°æ®é‡ | 200-250K æ•°æ®é‡ |'
- en: '| *Spotlight Locating* | *53* | *70* | *80* | *47* |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| *èšç„¦å®šä½* | *53* | *70* | *80* | *47* |'
- en: '| *Comparison* | *60* | *105* | *95* | *40* |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| *æ¯”è¾ƒ* | *60* | *105* | *95* | *40* |'
- en: '| Sequential Enumeration | 24 | 29 | 20 | 14 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| é¡ºåºæšä¸¾ | 24 | 29 | 20 | 14 |'
- en: '| Extremum Acquisition | 16 | 55 | 59 | 13 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| æå€¼è·å– | 16 | 55 | 59 | 13 |'
- en: '| Range Awareness | 20 | 21 | 16 | 13 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| èŒƒå›´æ„è¯† | 20 | 21 | 16 | 13 |'
- en: '| *Clustering* | *113* | *246* | *194* | *88* |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| *èšç±»* | *113* | *246* | *194* | *88* |'
- en: '| Report Integration | 40 | 90 | 90 | 30 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| æŠ¥å‘Šæ•´åˆ | 40 | 90 | 90 | 30 |'
- en: '| Citation&Reference | 37 | 120 | 79 | 34 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| å¼•ç”¨&å‚è€ƒ | 37 | 120 | 79 | 34 |'
- en: '| Case Classification | 36 | 36 | 25 | 24 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| æ¡ˆä¾‹åˆ†ç±» | 36 | 36 | 25 | 24 |'
- en: '| *Chain of Reasoning* | *97* | *143* | *112* | *57* |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| *æ¨ç†é“¾* | *97* | *143* | *112* | *57* |'
- en: '| Temporal Analysis | 10 | 40 | 35 | 15 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| æ—¶åºåˆ†æ | 10 | 40 | 35 | 15 |'
- en: '| Citation Chain | 33 | 50 | 41 | 6 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| å¼•ç”¨é“¾ | 33 | 50 | 41 | 6 |'
- en: '| Link the Links | 35 | 25 | 28 | 25 |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| é“¾æ¥é“¾æ¥ | 35 | 25 | 28 | 25 |'
- en: '| Solitaire | 28 | 19 | 8 | 11 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| æ¥é¾™ | 28 | 19 | 8 | 11 |'
- en: '| *Overall* | *323* | *564* | *281* | *232* |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| *æ€»ä½“* | *323* | *564* | *281* | *232* |'
- en: '| Chinese | 240 | 284 | 251 | 130 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| ä¸­æ–‡ | 240 | 284 | 251 | 130 |'
- en: '| English | 83 | 280 | 230 | 102 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| è‹±æ–‡ | 83 | 280 | 230 | 102 |'
- en: 'Table 5: Data length distributions in Loong benchmark.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 5ï¼šLoong åŸºå‡†ä¸­çš„æ•°æ®é•¿åº¦åˆ†å¸ƒã€‚
- en: '| Model | Spotlight Locating | Comparison | Clustering | Chain of Reasoning
    | Overall |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| æ¨¡å‹ | èšç„¦å®šä½ | æ¯”è¾ƒ | èšç±» | æ¨ç†é“¾ | æ€»ä½“ |'
- en: '| GPT4o (128K) | 73.95 | 0.62 | 50.50 | 0.28 | 44.29 | 0.09 | 57.95 | 0.28
    | 53.47 | 0.26 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| GPT4o (128K) | 73.95 | 0.62 | 50.50 | 0.28 | 44.29 | 0.09 | 57.95 | 0.28
    | 53.47 | 0.26 |'
- en: '| w/ Openai Embedding, Top k=5 | 56.97 | 0.36 | 31.28 | 0.14 | 27.71 | 0.03
    | 26.65 | 0.04 | 32.85 | 0.11 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=5 | 56.97 | 0.36 | 31.28 | 0.14 | 27.71 | 0.03 | 26.65
    | 0.04 | 32.85 | 0.11 |'
- en: '| w/ BGE Embedding, Top k=5 | 63.32 | 0.44 | 34.63 | 0.17 | 26.74 | 0.03 |
    26.21 | 0.04 | 34.01 | 0.13 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=5 | 63.32 | 0.44 | 34.63 | 0.17 | 26.74 | 0.03 | 26.21 |
    0.04 | 34.01 | 0.13 |'
- en: '| w/ Openai Embedding, Top k=10 | 65.20 | 0.46 | 36.80 | 0.19 | 33.06 | 0.04
    | 33.26 | 0.08 | 38.80 | 0.14 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=10 | 65.20 | 0.46 | 36.80 | 0.19 | 33.06 | 0.04 | 33.26
    | 0.08 | 38.80 | 0.14 |'
- en: '| w/ BGE Embedding, Top k=10 | 68.27 | 0.50 | 39.51 | 0.22 | 31.91 | 0.04 |
    30.71 | 0.07 | 38.71 | 0.15 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=10 | 68.27 | 0.50 | 39.51 | 0.22 | 31.91 | 0.04 | 30.71 |
    0.07 | 38.71 | 0.15 |'
- en: '| w/ Openai Embedding, Top k=30 | 64.32 | 0.43 | 42.15 | 0.26 | 41.02 | 0.08
    | 40.14 | 0.14 | 44.62 | 0.18 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=30 | 64.32 | 0.43 | 42.15 | 0.26 | 41.02 | 0.08 | 40.14
    | 0.14 | 44.62 | 0.18 |'
- en: '| w/ BGE Embedding, Top k=30 | 64.76 | 0.45 | 47.56 | 0.32 | 40.43 | 0.08 |
    40.82 | 0.17 | 45.67 | 0.21 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=30 | 64.76 | 0.45 | 47.56 | 0.32 | 40.43 | 0.08 | 40.82 |
    0.17 | 45.67 | 0.21 |'
- en: '| w/ Openai Embedding, Top k=50 | 65.59 | 0.45 | 41.69 | 0.28 | 34.49 | 0.04
    | 39.74 | 0.14 | 42.70 | 0.18 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=50 | 65.59 | 0.45 | 41.69 | 0.28 | 34.49 | 0.04 | 39.74
    | 0.14 | 42.70 | 0.18 |'
- en: '| w/ BGE Embedding, Top k=50 | 63.28 | 0.42 | 47.05 | 0.32 | 42.64 | 0.10 |
    41.97 | 0.18 | 46.52 | 0.21 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=50 | 63.28 | 0.42 | 47.05 | 0.32 | 42.64 | 0.10 | 41.97 |
    0.18 | 46.52 | 0.21 |'
- en: '| Qwen2-72B-Instruct (128K) | 54.17 | 0.36 | 42.38 | 0.20 | 36.71 | 0.04 |
    47.76 | 0.18 | 43.29 | 0.15 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| Qwen2-72B-Instruct (128K) | 54.17 | 0.36 | 42.38 | 0.20 | 36.71 | 0.04 |
    47.76 | 0.18 | 43.29 | 0.15 |'
- en: '| w/ Openai Embedding, Top k=5 | 57.57 | 0.40 | 30.98 | 0.14 | 27.91 | 0.02
    | 28.30 | 0.04 | 33.22 | 0.10 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=5 | 57.57 | 0.40 | 30.98 | 0.14 | 27.91 | 0.02 | 28.30
    | 0.04 | 33.22 | 0.10 |'
- en: '| w/ BGE Embedding, Top k=5 | 62.02 | 0.44 | 32.90 | 0.16 | 27.05 | 0.02 |
    29.26 | 0.06 | 34.18 | 0.12 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=5 | 62.02 | 0.44 | 32.90 | 0.16 | 27.05 | 0.02 | 29.26 |
    0.06 | 34.18 | 0.12 |'
- en: '| w/ Openai Embedding, Top k=10 | 62.52 | 0.44 | 35.79 | 0.18 | 30.16 | 0.03
    | 32.67 | 0.08 | 36.92 | 0.13 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=10 | 62.52 | 0.44 | 35.79 | 0.18 | 30.16 | 0.03 | 32.67
    | 0.08 | 36.92 | 0.13 |'
- en: '| w/ BGE Embedding, Top k=10 | 69.24 | 0.51 | 36.78 | 0.18 | 29.07 | 0.02 |
    31.90 | 0.07 | 37.50 | 0.14 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=10 | 69.24 | 0.51 | 36.78 | 0.18 | 29.07 | 0.02 | 31.90 |
    0.07 | 37.50 | 0.14 |'
- en: '| w/ Openai Embedding, Top k=30 | 64.11 | 0.43 | 41.91 | 0.26 | 35.61 | 0.04
    | 42.61 | 0.19 | 42.98 | 0.18 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=30 | 64.11 | 0.43 | 41.91 | 0.26 | 35.61 | 0.04 | 42.61
    | 0.19 | 42.98 | 0.18 |'
- en: '| w/ BGE Embedding, Top k=30 | 58.59 | 0.38 | 42.66 | 0.23 | 33.77 | 0.05 |
    40.39 | 0.17 | 40.94 | 0.16 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=30 | 58.59 | 0.38 | 42.66 | 0.23 | 33.77 | 0.05 | 40.39 |
    0.17 | 40.94 | 0.16 |'
- en: '| w/ Openai Embedding, Top k=50 | 57.87 | 0.37 | 42.10 | 0.25 | 32.78 | 0.03
    | 42.39 | 0.19 | 40.86 | 0.17 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=50 | 57.87 | 0.37 | 42.10 | 0.25 | 32.78 | 0.03 | 42.39
    | 0.19 | 40.86 | 0.17 |'
- en: '| w/ BGE Embedding, Top k=50 | 56.93 | 0.37 | 39.51 | 0.20 | 32.67 | 0.04 |
    43.44 | 0.21 | 40.46 | 0.16 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=50 | 56.93 | 0.37 | 39.51 | 0.20 | 32.67 | 0.04 | 43.44 |
    0.21 | 40.46 | 0.16 |'
- en: 'Table 6: Overall results (%) of adding RAG module on GPT4o and Qwen2-72B-Instruct.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 6ï¼šæ·»åŠ  RAG æ¨¡å—åœ¨ GPT4o å’Œ Qwen2-72B-Instruct ä¸Šçš„æ€»ä½“ç»“æœï¼ˆ%ï¼‰ã€‚
- en: '| Model | Spotlight Locating | Comparison | Clustering | Chain of Reasoning
    | Overall |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| æ¨¡å‹ | èšç„¦å®šä½ | æ¯”è¾ƒ | èšç±» | æ¨ç†é“¾ | æ€»ä½“ |'
- en: '| $\mathtt{Set1}$ (10K-50K) |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Set1}$ (10K-50K) |'
- en: '| GPT-4o (128K) | 85.67 | 0.81 | 64.27 | 0.33 | 57.01 | 0.24 | 81.58 | 0.55
    | 70.40 | 0.44 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o (128K) | 85.67 | 0.81 | 64.27 | 0.33 | 57.01 | 0.24 | 81.58 | 0.55
    | 70.40 | 0.44 |'
- en: '| w/ Openai Embedding, Top k=5 | 47.60 | 0.31 | 29.75 | 0.10 | 29.10 | 0.06
    | 31.46 | 0.08 | 32.98 | 0.11 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=5 | 47.60 | 0.31 | 29.75 | 0.10 | 29.10 | 0.06 | 31.46
    | 0.08 | 32.98 | 0.11 |'
- en: '| w/ BGE Embedding, Top k=5 | 57.17 | 0.43 | 34.15 | 0.12 | 30.71 | 0.07 |
    28.77 | 0.08 | 35.23 | 0.14 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=5 | 57.17 | 0.43 | 34.15 | 0.12 | 30.71 | 0.07 | 28.77 |
    0.08 | 35.23 | 0.14 |'
- en: '| w/ Openai Embedding, Top k=10 | 61.25 | 0.44 | 38.33 | 0.17 | 37.00 | 0.08
    | 41.67 | 0.16 | 42.63 | 0.18 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=10 | 61.25 | 0.44 | 38.33 | 0.17 | 37.00 | 0.08 | 41.67
    | 0.16 | 42.63 | 0.18 |'
- en: '| w/ BGE Embedding, Top k=10 | 61.00 | 0.44 | 39.74 | 0.19 | 36.14 | 0.08 |
    34.90 | 0.11 | 40.44 | 0.17 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=10 | 61.00 | 0.44 | 39.74 | 0.19 | 36.14 | 0.08 | 34.90 |
    0.11 | 40.44 | 0.17 |'
- en: '| w/ Openai Embedding, Top k=30 | 55.15 | 0.37 | 46.60 | 0.28 | 45.54 | 0.13
    | 51.98 | 0.27 | 49.23 | 0.24 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=30 | 55.15 | 0.37 | 46.60 | 0.28 | 45.54 | 0.13 | 51.98
    | 0.27 | 49.23 | 0.24 |'
- en: '| w/ BGE Embedding, Top k=30 | 57.40 | 0.38 | 52.25 | 0.32 | 46.54 | 0.18 |
    50.02 | 0.25 | 50.41 | 0.26 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=30 | 57.40 | 0.38 | 52.25 | 0.32 | 46.54 | 0.18 | 50.02 |
    0.25 | 50.41 | 0.26 |'
- en: '| w/ Openai Embedding, Top k=50 | 55.47 | 0.40 | 49.62 | 0.33 | 39.61 | 0.10
    | 46.08 | 0.20 | 46.82 | 0.24 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=50 | 55.47 | 0.40 | 49.62 | 0.33 | 39.61 | 0.10 | 46.08
    | 0.20 | 46.82 | 0.24 |'
- en: '| w/ BGE Embedding, Top k=50 | 52.08 | 0.38 | 53.42 | 0.37 | 49.83 | 0.21 |
    48.88 | 0.24 | 50.55 | 0.27 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=50 | 52.08 | 0.38 | 53.42 | 0.37 | 49.83 | 0.21 | 48.88 |
    0.24 | 50.55 | 0.27 |'
- en: '| $\mathtt{Set2}$ (50K-100K) |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Set2}$ (50K-100K) |'
- en: '| GPT-4o (128K) | 86.76 | 0.72 | 59.81 | 0.40 | 47.83 | 0.11 | 62.09 | 0.34
    | 58.38 | 0.29 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o (128K) | 86.76 | 0.72 | 59.81 | 0.40 | 47.83 | 0.11 | 62.09 | 0.34
    | 58.38 | 0.29 |'
- en: '| w/ Openai Embedding, Top k=5 | 56.01 | 0.35 | 39.56 | 0.22 | 31.84 | 0.04
    | 27.01 | 0.03 | 35.31 | 0.11 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=5 | 56.01 | 0.35 | 39.56 | 0.22 | 31.84 | 0.04 | 27.01
    | 0.03 | 35.31 | 0.11 |'
- en: '| w/ BGE Embedding, Top k=5 | 67.33 | 0.43 | 43.90 | 0.28 | 29.37 | 0.04 |
    27.84 | 0.04 | 36.72 | 0.14 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=5 | 67.33 | 0.43 | 43.90 | 0.28 | 29.37 | 0.04 | 27.84 |
    0.04 | 36.72 | 0.14 |'
- en: '| w/ Openai Embedding, Top k=10 | 64.77 | 0.45 | 45.44 | 0.31 | 36.07 | 0.05
    | 32.29 | 0.05 | 40.54 | 0.15 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=10 | 64.77 | 0.45 | 45.44 | 0.31 | 36.07 | 0.05 | 32.29
    | 0.05 | 40.54 | 0.15 |'
- en: '| w/ BGE Embedding, Top k=10 | 72.07 | 0.52 | 50.15 | 0.32 | 34.35 | 0.05 |
    33.49 | 0.07 | 41.90 | 0.17 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=10 | 72.07 | 0.52 | 50.15 | 0.32 | 34.35 | 0.05 | 33.49 |
    0.07 | 41.90 | 0.17 |'
- en: '| w/ Openai Embedding, Top k=30 | 65.87 | 0.42 | 50.05 | 0.34 | 44.08 | 0.11
    | 42.60 | 0.15 | 47.48 | 0.20 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=30 | 65.87 | 0.42 | 50.05 | 0.34 | 44.08 | 0.11 | 42.60
    | 0.15 | 47.48 | 0.20 |'
- en: '| w/ BGE Embedding, Top k=30 | 65.26 | 0.49 | 55.21 | 0.43 | 43.82 | 0.10 |
    42.80 | 0.18 | 48.31 | 0.23 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=30 | 65.26 | 0.49 | 55.21 | 0.43 | 43.82 | 0.10 | 42.80 |
    0.18 | 48.31 | 0.23 |'
- en: '| w/ Openai Embedding, Top k=50 | 67.21 | 0.46 | 51.38 | 0.38 | 31.65 | 0.03
    | 40.69 | 0.12 | 43.52 | 0.19 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=50 | 67.21 | 0.46 | 51.38 | 0.38 | 31.65 | 0.03 | 40.69
    | 0.12 | 43.52 | 0.19 |'
- en: '| w/ BGE Embedding, Top k=50 | 67.43 | 0.46 | 53.98 | 0.39 | 45.04 | 0.12 |
    46.94 | 0.21 | 49.96 | 0.24 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=50 | 67.43 | 0.46 | 53.98 | 0.39 | 45.04 | 0.12 | 46.94 |
    0.21 | 49.96 | 0.24 |'
- en: '| $\mathtt{Set3}$ (100K-200K) |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Set3}$ (100K-200K) |'
- en: '| GPT-4o (128K) | 74.84 | 0.65 | 42.40 | 0.21 | 38.70 | 0.04 | 45.06 | 0.09
    | 46.95 | 0.19 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o (128K) | 74.84 | 0.65 | 42.40 | 0.21 | 38.70 | 0.04 | 45.06 | 0.09
    | 46.95 | 0.19 |'
- en: '| w/ Openai Embedding, Top k=5 | 67.45 | 0.49 | 29.00 | 0.13 | 25.09 | 0.01
    | 27.22 | 0.02 | 33.69 | 0.12 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=5 | 67.45 | 0.49 | 29.00 | 0.13 | 25.09 | 0.01 | 27.22
    | 0.02 | 33.69 | 0.12 |'
- en: '| w/ BGE Embedding, Top k=5 | 71.12 | 0.56 | 31.36 | 0.14 | 25.32 | 0.00 |
    25.78 | 0.04 | 34.43 | 0.13 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=5 | 71.12 | 0.56 | 31.36 | 0.14 | 25.32 | 0.00 | 25.78 |
    0.04 | 34.43 | 0.13 |'
- en: '| w/ Openai Embedding, Top k=10 | 72.37 | 0.55 | 31.41 | 0.13 | 30.59 | 0.01
    | 33.14 | 0.08 | 38.38 | 0.14 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=10 | 72.37 | 0.55 | 31.41 | 0.13 | 30.59 | 0.01 | 33.14
    | 0.08 | 38.38 | 0.14 |'
- en: '| w/ BGE Embedding, Top k=10 | 79.04 | 0.67 | 34.29 | 0.18 | 30.59 | 0.02 |
    29.69 | 0.06 | 39.22 | 0.17 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=10 | 79.04 | 0.67 | 34.29 | 0.18 | 30.59 | 0.02 | 29.69 |
    0.06 | 39.22 | 0.17 |'
- en: '| w/ Openai Embedding, Top k=30 | 74.03 | 0.57 | 37.00 | 0.22 | 39.53 | 0.04
    | 36.07 | 0.09 | 43.91 | 0.18 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=30 | 74.03 | 0.57 | 37.00 | 0.22 | 39.53 | 0.04 | 36.07
    | 0.09 | 43.91 | 0.18 |'
- en: '| w/ BGE Embedding, Top k=30 | 75.45 | 0.59 | 45.96 | 0.31 | 36.91 | 0.04 |
    39.16 | 0.14 | 45.68 | 0.21 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=30 | 75.45 | 0.59 | 45.96 | 0.31 | 36.91 | 0.04 | 39.16 |
    0.14 | 45.68 | 0.21 |'
- en: '| w/ Openai Embedding, Top k=50 | 77.24 | 0.59 | 25.02 | 0.12 | 37.55 | 0.05
    | 40.93 | 0.15 | 44.52 | 0.19 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=50 | 77.24 | 0.59 | 25.02 | 0.12 | 37.55 | 0.05 | 40.93
    | 0.15 | 44.52 | 0.19 |'
- en: '| w/ BGE Embedding, Top k=50 | 74.19 | 0.55 | 46.15 | 0.31 | 39.71 | 0.04 |
    36.56 | 0.15 | 45.99 | 0.20 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=50 | 74.19 | 0.55 | 46.15 | 0.31 | 39.71 | 0.04 | 36.56 |
    0.15 | 45.99 | 0.20 |'
- en: '| $\mathtt{Set4}$ (200K-250K) |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Set4}$ (200K-250K) |'
- en: '| GPT-4o (128K) | 36.79 | 0.19 | 23.97 | 0.08 | 30.40 | 0.00 | 32.89 | 0.07
    | 31.11 | 0.07 |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o (128K) | 36.79 | 0.19 | 23.97 | 0.08 | 30.40 | 0.00 | 32.89 | 0.07
    | 31.11 | 0.07 |'
- en: '| w/ Openai Embedding, Top k=5 | 50.76 | 0.22 | 17.25 | 0.00 | 19.53 | 0.00
    | 16.61 | 0.00 | 24.91 | 0.05 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=5 | 50.76 | 0.22 | 17.25 | 0.00 | 19.53 | 0.00 | 16.61
    | 0.00 | 24.91 | 0.05 |'
- en: '| w/ BGE Embedding, Top k=5 | 51.02 | 0.26 | 18.75 | 0.03 | 17.83 | 0.00 |
    18.77 | 0.02 | 25.07 | 0.06 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=5 | 51.02 | 0.26 | 18.75 | 0.03 | 17.83 | 0.00 | 18.77 |
    0.02 | 25.07 | 0.06 |'
- en: '| w/ Openai Embedding, Top k=10 | 57.98 | 0.31 | 23.00 | 0.03 | 25.08 | 0.00
    | 21.29 | 0.02 | 30.00 | 0.07 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=10 | 57.98 | 0.31 | 23.00 | 0.03 | 25.08 | 0.00 | 21.29
    | 0.02 | 30.00 | 0.07 |'
- en: '| w/ BGE Embedding, Top k=10 | 51.48 | 0.25 | 23.36 | 0.05 | 22.55 | 0.00 |
    18.95 | 0.02 | 27.48 | 0.06 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=10 | 51.48 | 0.25 | 23.36 | 0.05 | 22.55 | 0.00 | 18.95 |
    0.02 | 27.48 | 0.06 |'
- en: '| w/ Openai Embedding, Top k=30 | 55.85 | 0.26 | 26.38 | 0.08 | 29.94 | 0.00
    | 21.81 | 0.02 | 32.66 | 0.07 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=30 | 55.85 | 0.26 | 26.38 | 0.08 | 29.94 | 0.00 | 21.81
    | 0.02 | 32.66 | 0.07 |'
- en: '| w/ BGE Embedding, Top k=30 | 53.94 | 0.21 | 24.82 | 0.07 | 30.77 | 0.00 |
    23.61 | 0.05 | 32.68 | 0.07 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=30 | 53.94 | 0.21 | 24.82 | 0.07 | 30.77 | 0.00 | 23.61 |
    0.05 | 32.68 | 0.07 |'
- en: '| w/ Openai Embedding, Top k=50 | 55.00 | 0.28 | 12.79 | 0.04 | 29.64 | 0.00
    | 24.67 | 0.07 | 31.88 | 0.08 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=50 | 55.00 | 0.28 | 12.79 | 0.04 | 29.64 | 0.00 | 24.67
    | 0.07 | 31.88 | 0.08 |'
- en: '| w/ BGE Embedding, Top k=50 | 51.17 | 0.21 | 23.36 | 0.10 | 33.08 | 0.02 |
    28.39 | 0.09 | 33.82 | 0.09 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=50 | 51.17 | 0.21 | 23.36 | 0.10 | 33.08 | 0.02 | 28.39 |
    0.09 | 33.82 | 0.09 |'
- en: 'Table 7: The result of adding RAG module on GPT-4o with different length sets.'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 7ï¼šåœ¨ä¸åŒé•¿åº¦æ•°æ®é›†ä¸Šæ·»åŠ  RAG æ¨¡å—åçš„ GPT-4o ç»“æœã€‚
- en: '| Model | Spotlight Locating | Comparison | Clustering | Chain of Reasoning
    | Overall |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| æ¨¡å‹ | èšå…‰å®šä½ | æ¯”è¾ƒ | èšç±» | æ¨ç†é“¾ | æ€»ä½“ |'
- en: '| $\mathtt{Set1}$ (10K-50K) |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Set1}$ (10K-50K) |'
- en: '| Qwen2-72B-Instruct (128K) | 68.49 | 0.55 | 60.60 | 0.37 | 47.08 | 0.08 |
    70.39 | 0.36 | 60.11 | 0.29 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| Qwen2-72B-Instruct (128K) | 68.49 | 0.55 | 60.60 | 0.37 | 47.08 | 0.08 |
    70.39 | 0.36 | 60.11 | 0.29 |'
- en: '| w/ Openai Embedding, Top k=5 | 54.62 | 0.45 | 26.17 | 0.08 | 29.60 | 0.03
    | 34.41 | 0.08 | 34.51 | 0.12 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=5 | 54.62 | 0.45 | 26.17 | 0.08 | 29.60 | 0.03 | 34.41
    | 0.08 | 34.51 | 0.12 |'
- en: '| w/ BGE Embedding, Top k=5 | 62.92 | 0.53 | 30.92 | 0.08 | 31.28 | 0.03 |
    32.95 | 0.11 | 36.91 | 0.15 |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=5 | 62.92 | 0.53 | 30.92 | 0.08 | 31.28 | 0.03 | 32.95 |
    0.11 | 36.91 | 0.15 |'
- en: '| w/ Openai Embedding, Top k=10 | 59.81 | 0.43 | 34.93 | 0.15 | 29.33 | 0.02
    | 41.27 | 0.15 | 38.96 | 0.15 |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=10 | 59.81 | 0.43 | 34.93 | 0.15 | 29.33 | 0.02 | 41.27
    | 0.15 | 38.96 | 0.15 |'
- en: '| w/ BGE Embedding, Top k=10 | 72.13 | 0.62 | 32.42 | 0.12 | 31.90 | 0.05 |
    44.12 | 0.20 | 42.27 | 0.20 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=10 | 72.13 | 0.62 | 32.42 | 0.12 | 31.90 | 0.05 | 44.12 |
    0.20 | 42.27 | 0.20 |'
- en: '| w/ Openai Embedding, Top k=30 | 57.26 | 0.40 | 45.43 | 0.28 | 40.04 | 0.06
    | 57.32 | 0.35 | 49.06 | 0.24 |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=30 | 57.26 | 0.40 | 45.43 | 0.28 | 40.04 | 0.06 | 57.32
    | 0.35 | 49.06 | 0.24 |'
- en: '| w/ BGE Embedding, Top k=30 | 56.37 | 0.33 | 46.27 | 0.30 | 38.35 | 0.10 |
    51.49 | 0.29 | 46.69 | 0.23 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=30 | 56.37 | 0.33 | 46.27 | 0.30 | 38.35 | 0.10 | 51.49 |
    0.29 | 46.69 | 0.23 |'
- en: '| w/ Openai Embedding, Top k=50 | 51.08 | 0.35 | 44.53 | 0.27 | 37.96 | 0.05
    | 53.95 | 0.35 | 46.11 | 0.23 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=50 | 51.08 | 0.35 | 44.53 | 0.27 | 37.96 | 0.05 | 53.95
    | 0.35 | 46.11 | 0.23 |'
- en: '| w/ BGE Embedding, Top k=50 | 53.47 | 0.37 | 47.31 | 0.29 | 36.42 | 0.06 |
    54.65 | 0.35 | 46.62 | 0.24 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=50 | 53.47 | 0.37 | 47.31 | 0.29 | 36.42 | 0.06 | 54.65 |
    0.35 | 46.62 | 0.24 |'
- en: '| $\mathtt{Set2}$ (50K-100K) |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Set2}$ (50K-100K) |'
- en: '| Qwen2-72B-Instruct (128K) | 64.53 | 0.43 | 42.60 | 0.21 | 38.52 | 0.05 |
    51.18 | 0.20 | 45.71 | 0.17 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| Qwen2-72B-Instruct (128K) | 64.53 | 0.43 | 42.60 | 0.21 | 38.52 | 0.05 |
    51.18 | 0.20 | 45.71 | 0.17 |'
- en: '| w/ Openai Embedding, Top k=5 | 56.64 | 0.40 | 36.68 | 0.19 | 30.91 | 0.03
    | 28.38 | 0.01 | 34.54 | 0.10 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=5 | 56.64 | 0.40 | 36.68 | 0.19 | 30.91 | 0.03 | 28.38
    | 0.01 | 34.54 | 0.10 |'
- en: '| w/ BGE Embedding, Top k=5 | 67.29 | 0.47 | 43.39 | 0.28 | 28.31 | 0.03 |
    32.22 | 0.07 | 36.95 | 0.14 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=5 | 67.29 | 0.47 | 43.39 | 0.28 | 28.31 | 0.03 | 32.22 |
    0.07 | 36.95 | 0.14 |'
- en: '| w/ Openai Embedding, Top k=10 | 67.07 | 0.53 | 44.30 | 0.27 | 34.31 | 0.05
    | 34.03 | 0.06 | 40.17 | 0.15 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=10 | 67.07 | 0.53 | 44.30 | 0.27 | 34.31 | 0.05 | 34.03
    | 0.06 | 40.17 | 0.15 |'
- en: '| w/ BGE Embedding, Top k=10 | 71.74 | 0.54 | 47.68 | 0.30 | 30.55 | 0.03 |
    30.57 | 0.03 | 38.80 | 0.14 |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=10 | 71.74 | 0.54 | 47.68 | 0.30 | 30.55 | 0.03 | 30.57 |
    0.03 | 38.80 | 0.14 |'
- en: '| w/ Openai Embedding, Top k=30 | 66.27 | 0.46 | 46.28 | 0.31 | 38.95 | 0.05
    | 46.15 | 0.22 | 45.42 | 0.19 |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=30 | 66.27 | 0.46 | 46.28 | 0.31 | 38.95 | 0.05 | 46.15
    | 0.22 | 45.42 | 0.19 |'
- en: '| w/ BGE Embedding, Top k=30 | 57.35 | 0.41 | 46.92 | 0.29 | 35.30 | 0.05 |
    42.82 | 0.20 | 42.04 | 0.18 |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=30 | 57.35 | 0.41 | 46.92 | 0.29 | 35.30 | 0.05 | 42.82 |
    0.20 | 42.04 | 0.18 |'
- en: '| w/ Openai Embedding, Top k=50 | 55.94 | 0.32 | 47.94 | 0.31 | 34.32 | 0.03
    | 46.64 | 0.21 | 42.60 | 0.16 |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=50 | 55.94 | 0.32 | 47.94 | 0.31 | 34.32 | 0.03 | 46.64
    | 0.21 | 42.60 | 0.16 |'
- en: '| w/ BGE Embedding, Top k=50 | 59.41 | 0.39 | 38.52 | 0.21 | 35.40 | 0.06 |
    45.47 | 0.24 | 41.51 | 0.17 |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=50 | 59.41 | 0.39 | 38.52 | 0.21 | 35.40 | 0.06 | 45.47 |
    0.24 | 41.51 | 0.17 |'
- en: '| $\mathtt{Set3}$ (100K-200K) |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Set3}$ (100K-200K) |'
- en: '| Qwen2-72B-Instruct (128K) | 46.99 | 0.27 | 37.06 | 0.13 | 31.50 | 0.02 |
    35.01 | 0.07 | 35.94 | 0.09 |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| Qwen2-72B-Instruct (128K) | 46.99 | 0.27 | 37.06 | 0.13 | 31.50 | 0.02 |
    35.01 | 0.07 | 35.94 | 0.09 |'
- en: '| w/ Openai Embedding, Top k=5 | 63.91 | 0.44 | 33.56 | 0.17 | 25.98 | 0.01
    | 28.98 | 0.04 | 34.48 | 0.12 |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=5 | 63.91 | 0.44 | 33.56 | 0.17 | 25.98 | 0.01 | 28.98
    | 0.04 | 34.48 | 0.12 |'
- en: '| w/ BGE Embedding, Top k=5 | 64.81 | 0.47 | 30.27 | 0.14 | 25.88 | 0.01 |
    27.86 | 0.05 | 33.70 | 0.12 |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=5 | 64.81 | 0.47 | 30.27 | 0.14 | 25.88 | 0.01 | 27.86 |
    0.05 | 33.70 | 0.12 |'
- en: '| w/ Openai Embedding, Top k=10 | 67.50 | 0.46 | 33.44 | 0.16 | 27.94 | 0.02
    | 31.62 | 0.06 | 36.47 | 0.13 |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=10 | 67.50 | 0.46 | 33.44 | 0.16 | 27.94 | 0.02 | 31.62
    | 0.06 | 36.47 | 0.13 |'
- en: '| w/ BGE Embedding, Top k=10 | 75.88 | 0.56 | 33.76 | 0.15 | 27.20 | 0.01 |
    30.17 | 0.04 | 37.28 | 0.14 |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=10 | 75.88 | 0.56 | 33.76 | 0.15 | 27.20 | 0.01 | 30.17 |
    0.04 | 37.28 | 0.14 |'
- en: '| w/ Openai Embedding, Top k=30 | 73.69 | 0.55 | 42.20 | 0.27 | 32.78 | 0.02
    | 37.65 | 0.13 | 42.60 | 0.18 |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=30 | 73.69 | 0.55 | 42.20 | 0.27 | 32.78 | 0.02 | 37.65
    | 0.13 | 42.60 | 0.18 |'
- en: '| w/ BGE Embedding, Top k=30 | 67.50 | 0.47 | 42.42 | 0.18 | 32.34 | 0.03 |
    37.85 | 0.12 | 41.35 | 0.15 |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=30 | 67.50 | 0.47 | 42.42 | 0.18 | 32.34 | 0.03 | 37.85 |
    0.12 | 41.35 | 0.15 |'
- en: '| w/ Openai Embedding, Top k=50 | 67.44 | 0.50 | 41.82 | 0.24 | 31.59 | 0.04
    | 37.29 | 0.12 | 40.90 | 0.18 |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=50 | 67.44 | 0.50 | 41.82 | 0.24 | 31.59 | 0.04 | 37.29
    | 0.12 | 40.90 | 0.18 |'
- en: '| w/ BGE Embedding, Top k=50 | 62.56 | 0.42 | 40.41 | 0.18 | 29.82 | 0.02 |
    40.31 | 0.14 | 39.84 | 0.15 |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=50 | 62.56 | 0.42 | 40.41 | 0.18 | 29.82 | 0.02 | 40.31 |
    0.14 | 39.84 | 0.15 |'
- en: '| $\mathtt{Set4}$ (200K-250K) |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Set4}$ (200K-250K) |'
- en: '| Qwen2-72B-Instruct (128K) | 33.18 | 0.16 | 26.59 | 0.08 | 29.84 | 0.01 |
    25.81 | 0.04 | 28.92 | 0.06 |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| Qwen2-72B-Instruct (128K) | 33.18 | 0.16 | 26.59 | 0.08 | 29.84 | 0.01 |
    25.81 | 0.04 | 28.92 | 0.06 |'
- en: '| w/ Openai Embedding, Top k=5 | 51.49 | 0.26 | 17.12 | 0.03 | 21.59 | 0.00
    | 16.37 | 0.00 | 25.59 | 0.06 |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=5 | 51.49 | 0.26 | 17.12 | 0.03 | 21.59 | 0.00 | 16.37
    | 0.00 | 25.59 | 0.06 |'
- en: '| w/ BGE Embedding, Top k=5 | 48.40 | 0.26 | 14.55 | 0.00 | 20.69 | 0.00 |
    18.07 | 0.00 | 24.63 | 0.05 |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=5 | 48.40 | 0.26 | 14.55 | 0.00 | 20.69 | 0.00 | 18.07 |
    0.00 | 24.63 | 0.05 |'
- en: '| w/ Openai Embedding, Top k=10 | 50.32 | 0.28 | 20.30 | 0.03 | 24.56 | 0.00
    | 16.38 | 0.00 | 27.08 | 0.06 |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=10 | 50.32 | 0.28 | 20.30 | 0.03 | 24.56 | 0.00 | 16.38
    | 0.00 | 27.08 | 0.06 |'
- en: '| w/ BGE Embedding, Top k=10 | 51.02 | 0.28 | 21.88 | 0.03 | 25.45 | 0.00 |
    17.29 | 0.00 | 28.10 | 0.06 |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=10 | 51.02 | 0.28 | 21.88 | 0.03 | 25.45 | 0.00 | 17.29 |
    0.00 | 28.10 | 0.06 |'
- en: '| w/ Openai Embedding, Top k=30 | 52.17 | 0.24 | 24.60 | 0.10 | 26.78 | 0.00
    | 17.79 | 0.00 | 29.29 | 0.07 |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=30 | 52.17 | 0.24 | 24.60 | 0.10 | 26.78 | 0.00 | 17.79
    | 0.00 | 29.29 | 0.07 |'
- en: '| w/ BGE Embedding, Top k=30 | 47.98 | 0.21 | 26.82 | 0.10 | 26.70 | 0.00 |
    20.02 | 0.00 | 29.44 | 0.06 |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=30 | 47.98 | 0.21 | 26.82 | 0.10 | 26.70 | 0.00 | 20.02 |
    0.00 | 29.44 | 0.06 |'
- en: '| w/ Openai Embedding, Top k=50 | 51.63 | 0.26 | 23.62 | 0.08 | 24.49 | 0.00
    | 21.84 | 0.02 | 29.14 | 0.07 |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ Openai åµŒå…¥ï¼ŒTop k=50 | 51.63 | 0.26 | 23.62 | 0.08 | 24.49 | 0.00 | 21.84
    | 0.02 | 29.14 | 0.07 |'
- en: '| w/ BGE Embedding, Top k=50 | 47.23 | 0.28 | 27.78 | 0.08 | 26.48 | 0.00 |
    24.44 | 0.02 | 30.52 | 0.08 |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| ä½¿ç”¨ BGE åµŒå…¥ï¼ŒTop k=50 | 47.23 | 0.28 | 27.78 | 0.08 | 26.48 | 0.00 | 24.44 |
    0.02 | 30.52 | 0.08 |'
- en: 'Table 8: The result of adding RAG module on Qwen2-72B-Instruct with different
    length sets.'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨æ ¼ 8ï¼šåœ¨ä¸åŒé•¿åº¦é›†åˆä¸Šå¯¹Qwen2-72B-Instructæ·»åŠ RAGæ¨¡å—çš„ç»“æœã€‚
- en: '| LongBench |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| LongBench |'
- en: '| Passage 1: The Real Glory. The Real Glory is a 1939 Samuel Goldwyn Productions
    adventure film starring Gary Cooper, David Niven, Andrea Leeds and Broderick Crawford
    released by United Artists in the weeks immediately following Nazi Germanyâ€™s invasion
    of Poland. Based on a 1937 novel of the same name by Charles L. Clifford and directed
    by Henry Hathaway, the film is set against the backdrop of the Moro Rebellion
    during the American occupation of the Philippines at the beginning of the 20th
    century $\ldots$ |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| ç‰‡æ®µ 1ï¼šçœŸæ­£çš„è£è€€ã€‚ã€ŠçœŸæ­£çš„è£è€€ã€‹æ˜¯ä¸€éƒ¨1939å¹´ç”±å¡ç¼ªå°”Â·æˆˆå¾·æ¸©åˆ¶ç‰‡å…¬å¸å‡ºå“çš„å†’é™©ç‰‡ï¼Œç”±åŠ é‡ŒÂ·åº“ç€ã€å¤§å«Â·å°¼æ–‡ã€å®‰å¾·çƒˆäºšÂ·åˆ©å…¹å’Œå¸ƒç½—å¾·é‡Œå…‹Â·å…‹åŠ³ç¦å¾·ä¸»æ¼”ï¼Œç”±è”åˆè‰ºæœ¯å®¶åœ¨çº³ç²¹å¾·å›½å…¥ä¾µæ³¢å…°åçš„å‡ å‘¨å†…å‘è¡Œã€‚æ ¹æ®æŸ¥å°”æ–¯Â·LÂ·å…‹åˆ©ç¦å¾·1937å¹´çš„åŒåå°è¯´æ”¹ç¼–ï¼Œç”±äº¨åˆ©Â·å“ˆæ’’éŸ¦æ‰§å¯¼ï¼Œè¯¥ç‰‡èƒŒæ™¯è®¾å®šåœ¨20ä¸–çºªåˆç¾å›½å é¢†è²å¾‹å®¾æœŸé—´çš„æ‘©æ´›å›ä¹±ä¸­
    $\ldots$'
- en: '| Passage 2: Jay Sheffield. Jay Howard Sheffield (September 25, 1934 â€“ June
    25, 1998) was an American actor, who appeared on the stage, in films, and on television.
    He married Barbara Babcock on June 9, 1962, in San Mateo, California. They later
    divorced $\ldots$ |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| ç‰‡æ®µ 2ï¼šæ°ä¼ŠÂ·è°¢è²å°”å¾·ã€‚æ°ä¼ŠÂ·éœåå¾·Â·è°¢è²å°”å¾·ï¼ˆ1934å¹´9æœˆ25æ—¥ â€“ 1998å¹´6æœˆ25æ—¥ï¼‰æ˜¯ç¾å›½æ¼”å‘˜ï¼Œæ›¾åœ¨èˆå°ã€ç”µå½±å’Œç”µè§†ä¸Šå‡ºæ¼”ã€‚ä»–äº1962å¹´6æœˆ9æ—¥åœ¨åŠ åˆ©ç¦å°¼äºšå·åœ£é©¬ç‰¹å¥¥ä¸èŠ­èŠ­æ‹‰Â·å·´å¸ƒç§‘å…‹ç»“å©šï¼Œåæ¥ä»–ä»¬ç¦»å©šäº†
    $\ldots$ |'
- en: '| Passage 3: David Niven. James David Graham Niven (; 1 March 1910 â€“ 29 July
    1983) was a British actor, soldier, memoirist, and novelist. Niven was known as
    a handsome and debonair leading man in Classic Hollywood films. He received an
    Academy Award and a Golden Globe Award $\ldots$ |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| ç‰‡æ®µ 3ï¼šå¤§å«Â·å°¼æ–‡ã€‚è©¹å§†æ–¯Â·å¤§å«Â·æ ¼é›·å„å§†Â·å°¼æ–‡ï¼ˆ1910å¹´3æœˆ1æ—¥ â€“ 1983å¹´7æœˆ29æ—¥ï¼‰æ˜¯è‹±å›½æ¼”å‘˜ã€å†›äººã€å›å¿†å½•ä½œè€…å’Œå°è¯´å®¶ã€‚å°¼æ–‡ä»¥å…¶è‹±ä¿Šæ½‡æ´’çš„å½¢è±¡åœ¨ç»å…¸å¥½è±åå½±ç‰‡ä¸­è‘—ç§°ã€‚ä»–è·å¾—äº†å¥¥æ–¯å¡å¥–å’Œé‡‘çƒå¥–
    $\ldots$ |'
- en: '| Passage 4: Phileas Fogg snacks. Phileas Fogg Ltd is a company that produces
    snack products in the United Kingdom that was created in 1982 by Derwent Valley
    Foods. The brand is named for Phileas Fogg, the protagonist of Jules Verneâ€™s Around
    the World in Eighty Days $\ldots$ |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| ç‰‡æ®µ 4ï¼šè²å‹’æ–¯Â·ç¦å…‹é›¶é£Ÿã€‚è²å‹’æ–¯Â·ç¦å…‹æœ‰é™å…¬å¸æ˜¯ä¸€å®¶ç”Ÿäº§é›¶é£Ÿçš„è‹±å›½å…¬å¸ï¼Œç”±Derwent Valley Foodsäº1982å¹´åˆ›å»ºã€‚è¯¥å“ç‰Œä»¥å„’å‹’Â·å‡¡å°”çº³çš„ã€Šç¯æ¸¸ä¸–ç•Œå…«åå¤©ã€‹ä¸­çš„ä¸»è§’è²å‹’æ–¯Â·ç¦å…‹å‘½å
    $\ldots$ |'
- en: '| Passage 5: Jules Verne Trophy. The Jules Verne Trophy is a prize for the
    fastest circumnavigation of the world by any type of yacht with no restrictions
    on the size of the crew provided the vessel has registered with the organization
    and paid an entry fee. A vessel holding the Jules Verne trophy will not necessarily
    hold the absolute round the world record $\ldots$ |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| ç‰‡æ®µ 5ï¼šå„’å‹’Â·å‡¡å°”çº³å¥–ã€‚å„’å‹’Â·å‡¡å°”çº³å¥–æ˜¯ä¸€é¡¹æˆäºˆä»»ä½•ç±»å‹çš„æ¸¸è‰‡è¿›è¡Œä¸–ç•Œæœ€å¿«ç¯çƒèˆªè¡Œçš„å¥–é¡¹ï¼Œèˆ¹å‘˜äººæ•°æ²¡æœ‰é™åˆ¶ï¼Œåªè¦èˆ¹åªå·²åœ¨ç»„ç»‡æ³¨å†Œå¹¶æ”¯ä»˜äº†å‚èµ›è´¹ã€‚è·å¾—å„’å‹’Â·å‡¡å°”çº³å¥–çš„èˆ¹åªä¸ä¸€å®šä¼šä¿æŒç»å¯¹çš„ç¯çƒè®°å½•
    $\ldots$ |'
- en: '| Question: The actor that plays Phileas Fogg in "Around the World in 80 Days",
    co-starred with Gary Cooper in a 1939 Goldwyn Productions film based on a novel
    by what author? |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| é—®é¢˜ï¼šåœ¨ã€Š80å¤©ç¯æ¸¸åœ°çƒã€‹ä¸­æ‰®æ¼”è²å‹’æ–¯Â·ç¦å…‹çš„æ¼”å‘˜ï¼Œä¸åŠ é‡ŒÂ·åº“ç€åœ¨1939å¹´é‡‘æ€€å¾·åˆ¶ç‰‡å…¬å¸å‡ºå“çš„ç”µå½±ä¸­å…±åŒå‡ºæ¼”ï¼Œè¯¥å°è¯´çš„ä½œè€…æ˜¯è°ï¼Ÿ |'
- en: '| Answer: Charles L. Clifford |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| ç­”æ¡ˆï¼šæŸ¥å°”æ–¯Â·LÂ·å…‹åˆ©ç¦å¾· |'
- en: '| Loong (Ours) |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| é¾™ï¼ˆæˆ‘ä»¬çš„ï¼‰ |'
- en: '| Document 1: THE ARENA GROUP HOLDINGS, INC. Proceeds from Simplify loan: $7,748\.
    Unearned revenue: $(11,665). Amortization of debt discounts: $536\. Cash and cash
    equivalents: $4,003. Noncash and accrued interest: $2,839\. Loss on impairment
    of assets: $40,589\. Accounts receivable, net: $12,029\. Subscription refund liability:
    $18\. Accounts payable: $(102). Subscription acquisition costs: $6,131\. Change
    in fair value of contingent consideration: $313\. ($ in thousands, except share
    data) $\ldots$ |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| æ–‡æ¡£ 1ï¼šTHE ARENA GROUP HOLDINGS, INC. ä»ç®€åŒ–è´·æ¬¾ä¸­è·å¾—çš„æ”¶ç›Šï¼š$7,748ã€‚æœªèµšå–æ”¶å…¥ï¼š$(11,665)ã€‚å€ºåŠ¡æŠ˜æ‰£çš„æ‘Šé”€ï¼š$536ã€‚ç°é‡‘åŠç°é‡‘ç­‰ä»·ç‰©ï¼š$4,003ã€‚éç°é‡‘åŠåº”è®¡åˆ©æ¯ï¼š$2,839ã€‚èµ„äº§å‡å€¼æŸå¤±ï¼š$40,589ã€‚åº”æ”¶è´¦æ¬¾å‡€é¢ï¼š$12,029ã€‚è®¢é˜…é€€æ¬¾è´Ÿå€ºï¼š$18ã€‚åº”ä»˜è´¦æ¬¾ï¼š$(102)ã€‚è®¢é˜…è·å–æˆæœ¬ï¼š$6,131ã€‚æˆ–æœ‰å¯¹ä»·å…¬å…ä»·å€¼å˜åŠ¨ï¼š$313ã€‚($
    in thousands, except share data) $\ldots$ |'
- en: '| Document 2: General Enterprise Ventures, Inc. For purposes of balance sheet
    presentation and reporting of cash flows, the Company considers all unrestricted
    demand deposits, money market funds and highly liquid debt instruments with an
    original maturity of less than 90 days to be cash and cash equivalents. The Company
    did not have any cash equivalents at March 31, 2024\. The Company had cash of
    $549,755 at March 31, 2024 $\ldots$ |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| æ–‡æ¡£ 2ï¼šGeneral Enterprise Ventures, Inc. ä¸ºäº†å¹³è¡¡è¡¨çš„å±•ç¤ºå’Œç°é‡‘æµçš„æŠ¥å‘Šï¼Œå…¬å¸å°†æ‰€æœ‰ä¸å—é™åˆ¶çš„æ´»æœŸå­˜æ¬¾ã€è´§å¸å¸‚åœºåŸºé‡‘å’ŒåŸå§‹åˆ°æœŸæ—¥å°‘äº90å¤©çš„é«˜æµåŠ¨æ€§å€ºåŠ¡å·¥å…·è§†ä¸ºç°é‡‘åŠç°é‡‘ç­‰ä»·ç‰©ã€‚å…¬å¸åœ¨2024å¹´3æœˆ31æ—¥æ²¡æœ‰ä»»ä½•ç°é‡‘ç­‰ä»·ç‰©ã€‚å…¬å¸åœ¨2024å¹´3æœˆ31æ—¥æ‹¥æœ‰ç°é‡‘$549,755
    $\ldots$ |'
- en: '| Document 3: BROAD STREET REALTY, INC. The carrying amounts of cash and cash
    equivalents, restricted cash, receivables and payables are reasonable estimates
    of their fair value as of March 31, 2024 due to the short-term nature of these
    instruments. Reconciliation of cash and cash equivalents and restricted cash:
    Cash and cash equivalents: $14,631 (in thousands) $\ldots$ |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| æ–‡æ¡£ 3: BROAD STREET REALTY, INC. ç”±äºè¿™äº›å·¥å…·çš„çŸ­æœŸæ€§è´¨ï¼Œç°é‡‘å’Œç°é‡‘ç­‰ä»·ç‰©ã€å—é™ç°é‡‘ã€åº”æ”¶è´¦æ¬¾å’Œåº”ä»˜è´¦æ¬¾çš„è´¦é¢é‡‘é¢æ˜¯å¯¹å…¶å…¬å…ä»·å€¼çš„åˆç†ä¼°è®¡ï¼Œæˆªæ­¢æ—¥æœŸä¸º
    2024 å¹´ 3 æœˆ 31 æ—¥ã€‚ç°é‡‘å’Œç°é‡‘ç­‰ä»·ç‰©ä¸å—é™ç°é‡‘çš„å¯¹è´¦ï¼šç°é‡‘å’Œç°é‡‘ç­‰ä»·ç‰©ï¼š$14,631ï¼ˆåƒç¾å…ƒï¼‰$\ldots$ |'
- en: '| Question: Please list the â€˜Cash and Cash Equivalentsâ€™ of the aforementioned
    companies in ascending order. |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| é—®é¢˜ï¼šè¯·æŒ‰å‡åºåˆ—å‡ºä¸Šè¿°å…¬å¸çš„â€˜ç°é‡‘å’Œç°é‡‘ç­‰ä»·ç‰©â€™ã€‚ |'
- en: '| Answer: 1\. General Enterprise Ventures, Inc.: $549,755\. 2\. Arena Group
    Holdings, Inc.: $4,003 in thousands. 3\. Broad Street Realty, Inc.: $14,631 in
    thousands. |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| ç­”æ¡ˆï¼š1\. General Enterprise Ventures, Inc.: $549,755ã€‚2\. Arena Group Holdings,
    Inc.: $4,003ï¼ˆåƒç¾å…ƒï¼‰ã€‚3\. Broad Street Realty, Inc.: $14,631ï¼ˆåƒç¾å…ƒï¼‰ã€‚ |'
- en: 'Table 9: Comparison of Evidence Distribution in Examples from LongBench and
    Loong (Ours). Evidence related to the answers is highlighted with Orange Background.'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¡¨ 9: LongBench å’Œ Loongï¼ˆæˆ‘ä»¬ï¼‰çš„ç¤ºä¾‹ä¸­è¯æ®åˆ†å¸ƒçš„æ¯”è¾ƒã€‚ä¸ç­”æ¡ˆç›¸å…³çš„è¯æ®ç”¨æ©™è‰²èƒŒæ™¯çªå‡ºæ˜¾ç¤ºã€‚'
