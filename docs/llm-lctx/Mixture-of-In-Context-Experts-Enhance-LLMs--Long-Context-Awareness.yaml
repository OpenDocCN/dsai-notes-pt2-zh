- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: Êú™ÂàÜÁ±ª'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:01:16'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Mixture of In-Context Experts Enhance LLMs‚Äô Long Context Awareness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Êù•Ê∫êÔºö[https://ar5iv.labs.arxiv.org/html/2406.19598](https://ar5iv.labs.arxiv.org/html/2406.19598)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Hongzhan Lin¬π ‚ÄÉAng Lv¬π¬π¬πfootnotemark: 1 ‚ÄÉYuhan Chen¬≤¬π¬πfootnotemark: 1'
  prefs: []
  type: TYPE_NORMAL
- en: Chen Zhu¬≥‚ÄÉYang Song‚Å¥‚ÄÉHengshu Zhu¬≥ ‚ÄÉRui Yan¬π
  prefs: []
  type: TYPE_NORMAL
- en: ¬π Gaoling School of Artificial Intelligence, Renmin University of China
  prefs: []
  type: TYPE_NORMAL
- en: ¬≤ XiaoMi AI Lab ¬≥ Career Science Lab, BOSS Zhipin ‚Å¥ NLP Center, BOSS Zhipin
  prefs: []
  type: TYPE_NORMAL
- en: '{linhongzhan, anglv, ruiyan}@ruc.edu.cn'
  prefs: []
  type: TYPE_NORMAL
- en: '{chenyuhan5}@xiaomi.com Equal contribution. Hongzhan Lin and Ang Lv proposed
    the idea of MoICE. Hongzhan Lin and Yuhan Chen designed the MoICE router architecture
    and implemented efficient code. Experiments were conducted by Hongzhan Lin, while
    Ang Lv led the writing. Code is available at [https://github.com/p1nksnow/MoICE](https://github.com/p1nksnow/MoICE).
    Corresponding author: Rui Yan ([ruiyan@ruc.edu.cn](ruiyan@ruc.edu.cn))'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Many studies have revealed that large language models (LLMs) exhibit uneven
    awareness of different contextual positions. Their limited context awareness can
    lead to overlooking critical information and subsequent task failures. While several
    approaches have been proposed to enhance LLMs‚Äô context awareness, achieving both
    effectiveness and efficiency remains challenging. In this paper, for LLMs utilizing
    RoPE as position embeddings, we introduce a novel method called ‚ÄúMixture of In-Context
    Experts‚Äù (MoICE) to address this challenge. MoICE comprises two key components:
    a router integrated into each attention head within LLMs and a lightweight router-only
    training optimization strategy: (1) MoICE views each RoPE angle as an ‚Äòin-context‚Äô
    expert, demonstrated to be capable of directing the attention of a head to specific
    contextual positions. Consequently, each attention head flexibly processes tokens
    using multiple RoPE angles dynamically selected by the router to attend to the
    needed positions. This approach mitigates the risk of overlooking essential contextual
    information. (2) The router-only training strategy entails freezing LLM parameters
    and exclusively updating routers for only a few steps. When applied to open-source
    LLMs including Llama and Mistral, MoICE surpasses prior methods across multiple
    tasks on long context understanding and generation, all while maintaining commendable
    inference efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although large language models (LLMs) have demonstrated impressive capabilities
    across diverse NLP tasks, several studies¬†[[22](#bib.bib22), [6](#bib.bib6)] have
    pointed out that the contextual awareness of LLMs is not as powerful as widely
    believed, constraining their application in tasks demanding extensive contextual
    awareness, such as coherent long text generation¬†[[38](#bib.bib38)] and Retrieval-Augmented
    Generation (RAG, ¬†[[15](#bib.bib15), [4](#bib.bib4), [8](#bib.bib8)]) tasks necessitating
    in-context retrieval¬†[[6](#bib.bib6)]. Liu et al.¬†[[22](#bib.bib22)] identified
    a common issue termed the ‚Äúlost-in-middle‚Äù phenomenon, indicating that LLMs often
    exhibit a weaker awareness of information situated in the middle of the long context
    compared to the beginning or end. Chen et al.¬†[[6](#bib.bib6)] highlighted challenges
    arising from a mathematical property of RoPE¬†[[29](#bib.bib29)], a wide-used positional
    embedding in LLMs, which impedes attention to specific positions within the long
    context. Consequently, if critical information coincides with such positions,
    task performance suffers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many works¬†[[19](#bib.bib19), [38](#bib.bib38), [6](#bib.bib6), [37](#bib.bib37)]
    have attempted to enhance the long-context awareness of LLMs. Central to these
    efforts is the enhancement of attention heads which serve as the linchpin for
    contextual awareness, given that FFNs in language models do not introduce token
    interaction. Chen et al.¬†[[6](#bib.bib6)] proposed an inference algorithm named
    Attention Buckets (AB), which enhanced the context awareness of LLMs by executing
    $N$ inference instances, each with a distinct RoPE angle, and aggregated the outputs
    at the final layer. Zhang et al.¬†[[38](#bib.bib38)] observed the varying awareness
    of attention heads to contextual positions. They proposed an inference algorithm
    named Ms-PoE. Ms-PoE enhances the utility of position-aware heads by re-scaling
    the positional embedding indices, equivalent to assigning each head a unique RoPE
    angle. Figure¬†[1](#S1.F1 "Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Mixture of In-Context Experts
    Enhance LLMs‚Äô Long Context Awareness") illustrates these approaches. However,
    these approaches each come with their own drawbacks: AB conducts excessive redundant
    FFNs calculations, leading to high memory consumption. In Ms-PoE, determining
    a distinct re-scale factor for every attention head needs an additional forward
    pass. Meanwhile, each attention head still depends on a single re-scaled static
    RoPE. As highlighted by AB¬†[[6](#bib.bib6)], this leads to limited awareness of
    certain contextual positions, thereby constraining its potential. Moreover, a
    significant drawback of both AB and Ms-PoE lies in their static assignment of
    the RoPE angle for each attention head throughout the generation. However, as
    the generation progresses, the positions of crucial tokens shift, necessitating
    corresponding adjustments in the required RoPE angles for each head.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ebb584353387501dddfc702bcf3243e3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Some methods developed to enhance LLMs‚Äô context awareness. (a) Attention
    Buckets¬†[[6](#bib.bib6)] selects $N$ parallel inferences for each input. The outputs
    are then aggregated in the final layer. (b) Ms-PoE¬†[[38](#bib.bib38)] employs
    a unique RoPE angle for each attention head. However, it needs an additional forward
    pass for RoPE angle assignment. (c) MoICE integrates a router within each attention
    head. This novel plug-in selects several of the most suitable RoPE angles for
    each token. The selected RoPE angles collectively contribute to computing the
    attention scores. MoICE demonstrates superior memory efficiency and performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this study, we present Mixture of In-Context Experts (MoICE), a novel plug-in
    of LLMs for enhancing context awareness. Specifically, We conceptualize a unique
    RoPE angle as an ‚Äúin-context expert,‚Äù as it can allocate a head‚Äôs more attention
    to certain contextual positions¬†[[6](#bib.bib6)]. We integrate a router within
    each attention head, which discerns the potentially important tokens for the head
    and dynamically selects $K$ RoPE angles that provide comprehensive awareness of
    these tokens for attention computation. Through the re-computation of only a few
    query-key dot products, attention patterns computed with selected RoPE angles
    are aggregated to produce the final attention pattern. This approach yields two
    primary advantages: (1) It eliminates unnecessary computational overhead in AB,
    enhancing efficiency. (2) The dynamic expert selection of each head for arbitrary
    tokens introduces flexibility not attained in previous studies. This minimizes
    the risk of the initial RoPE angle assigned to a head failing to work due to crucial
    token positions shifting during generation.'
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, MoICE not only surpasses AB‚Äôs effectiveness but also achieves
    commendable efficiency. We name our approach as ‚ÄúMixture of In-Context Experts‚Äù
    (MoICE) due to the aggregation of attention patterns calculated with different
    RoPE angles resembling the concept of ‚ÄúMixture of Experts‚Äù (MoE,¬†[[28](#bib.bib28)]).
    When applying MoICE to open-source LLMs, we freeze LLMs‚Äô parameters and conduct
    lightweight training only on the MoICE routers. With only a few quick updates,
    MoICE surpasses many competitive baselines in tasks involving long-context generation
    and understanding.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, our main contribution is the introduction of MoICE, a novel plug-in
    for enhancing LLMs‚Äô context awareness. It achieves head-and token-specific dynamic
    multiple RoPE angles assignment, outperforms previous methods across various tasks,
    and maintains commendable inference efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We introduce some background of Mixture of In-Context Experts, including (1)
    the rotary position embeddings commonly used by mainstream LLMs, (2) the primary
    problem addressed in this paper: the limited context awareness of LLMs, (3) an
    explanation of the underlying reasons for this limitation, and (4) the Mixture
    of Expert techniques employed to mitigate the limitation.'
  prefs: []
  type: TYPE_NORMAL
- en: Position embedding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Positional embedding is crucial for Transformer¬†[[33](#bib.bib33)] to perceive
    sequence order and compensate for the position-agnostic nature of the attention
    mechanism. In this paper, we mainly focus on LLMs using Rotary Position Embedding
    (RoPE,¬†[[29](#bib.bib29)]) which is the prevalent position embedding in current
    LLMs. We discuss other position embeddings in Appendix¬†[B](#A2 "Appendix B Discussions
    on more position embeddings ‚Ä£ Mixture of In-Context Experts Enhance LLMs‚Äô Long
    Context Awareness").
  prefs: []
  type: TYPE_NORMAL
- en: 'In a Transformer layer with $H$-th head. To encode position information, RoPE
    initially applies a rotary matrix to the query and key vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  $$\mathbf{R}_{\Theta_{j},n}=\left[\begin{array}[]{cccc}\mathbf{r}_{\theta_{j,0},n}&amp;O&amp;\cdots&amp;O\\
    O&amp;\mathbf{r}_{\theta_{j,1},n}&amp;\cdots&amp;O\\'
  prefs: []
  type: TYPE_NORMAL
- en: \vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
  prefs: []
  type: TYPE_NORMAL
- en: O&amp;O&amp;\cdots&amp;\mathbf{r}_{\theta_{j,d/2-1},n}\\
  prefs: []
  type: TYPE_NORMAL
- en: \end{array}\right],\text{\ where\ \ }\mathbf{r}_{\theta_{j,i},n}=\left[\begin{array}[]{cc}\cos
    n\theta_{j,i}&amp;-\sin n\theta_{j,i}\\
  prefs: []
  type: TYPE_NORMAL
- en: \sin n\theta_{j,i}&amp;\cos n\theta_{j,i}\end{array}\right],O=\left[\begin{array}[]{cc}0&amp;0\\
  prefs: []
  type: TYPE_NORMAL
- en: 0&amp;0\end{array}\right].$$  |  | (2) |
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, $\theta_{j,i}=B^{-2i/d}_{j},i\in[0,\cdots,d/2-1]$ in the query-key product
    during attention computation:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: Here, $\mathbf{Attn}^{h}_{nm}$.
  prefs: []
  type: TYPE_NORMAL
- en: Context awareness of LLMs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/94d0fe05375548aec5c04278e6720488.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Different RoPE angles $\Theta_{j}$.'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs struggle with limited context awareness, significantly impacting their
    performance in tasks like long-text generation¬†[[38](#bib.bib38)], Retrieval-Augmented
    Generation (RAG,¬†[[15](#bib.bib15), [4](#bib.bib4), [8](#bib.bib8)]), and multi-turn
    human-agent interactions¬†[[6](#bib.bib6)] involving complex contexts. Liu et al.¬†[[22](#bib.bib22)]
    identified a problem known as ‚ÄúLost-in-the-Middle,‚Äù where LLMs process the beginning
    and end of the context well but have reduced awareness of the middle. Chen et
    al.¬†[[6](#bib.bib6)] observed that LLMs using RoPE exhibit uneven context awareness,
    favoring certain positions. Peysakhovich et al.¬†[[26](#bib.bib26)] further highlighted
    that LLMs exhibit variable attention to document-level token segments based on
    their contextual positions.
  prefs: []
  type: TYPE_NORMAL
- en: Attention waveforms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: According to Chen et al.¬†[[6](#bib.bib6)], LLM‚Äôs uneven awareness of different
    contextual positions is due to RoPE‚Äôs mathematical characteristics. Within RoPE,
    the attention score exhibits ‚Äúwaveforms‚Äù when retrieving the same token from the
    context, based on their relative positions. The troughs in these waveforms can
    impair task performance, especially when critical tokens are situated at these
    positions during generation. Different RoPE angles produce waveforms with troughs
    occurring at different positions. These phenomena are depicted in Figure¬†[2](#S2.F2
    "Figure 2 ‚Ä£ Context awareness of LLMs ‚Ä£ 2 Background ‚Ä£ Mixture of In-Context Experts
    Enhance LLMs‚Äô Long Context Awareness"). A detailed derivation of the depicted
    curves in Figure¬†[2](#S2.F2 "Figure 2 ‚Ä£ Context awareness of LLMs ‚Ä£ 2 Background
    ‚Ä£ Mixture of In-Context Experts Enhance LLMs‚Äô Long Context Awareness") is provided
    in Appendix¬†[A](#A1 "Appendix A Attention waveforms ‚Ä£ Mixture of In-Context Experts
    Enhance LLMs‚Äô Long Context Awareness").
  prefs: []
  type: TYPE_NORMAL
- en: 3 Mixture of In-Context Experts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we first introduce the core component of MoICE, the MoICE router,
    detailed in Section¬†[3.1](#S3.SS1 "3.1 Architecture ‚Ä£ 3 Mixture of In-Context
    Experts ‚Ä£ Mixture of In-Context Experts Enhance LLMs‚Äô Long Context Awareness").
    Subsequently, we delve into the optimization of MoICE in Section¬†[3.2](#S3.SS2
    "3.2 Router-only training ‚Ä£ 3 Mixture of In-Context Experts ‚Ä£ Mixture of In-Context
    Experts Enhance LLMs‚Äô Long Context Awareness"). Figure¬†[3](#S3.F3 "Figure 3 ‚Ä£
    3.1 Architecture ‚Ä£ 3 Mixture of In-Context Experts ‚Ä£ Mixture of In-Context Experts
    Enhance LLMs‚Äô Long Context Awareness") provides an overview of MoICE. The discussion
    in this section focuses solely on a single layer of transformer for clarity, with
    the same principles applying to any other layers.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a6e74e85b6a440dbd00b26a2864b0ce8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The structure of MoICE. Only the router‚Äôs parameters are trainable
    when plugged into an LLM. For clarity, the figure illustrates a single head, with
    $N$=2 as toy demonstration examples.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We aim to design an enhanced attention mechanism in LLMs that dynamically attends
    to crucial information across various contextual positions required for completing
    the head‚Äôs function. As a result, we can mitigate the performance drop caused
    by inadequate context awareness. Motivated by insights of Chen et al.¬†[[6](#bib.bib6)],
    who demonstrated that a distinct RoPE angle $\Theta_{j}$ could direct the attention
    heads more focus on specific contextual positions, we propose the integration
    of a contextual-aware routing mechanism. This routing mechanism is designed to
    select the appropriate RoPE angles for processing a token. We implement the router
    as a Multi-Layer Perceptron (MLP) with the SiLU activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\texttt{Router}\left(\mathbf{q}\right):=\mathbf{W}_{3}\left(\texttt{SiLU}\left(\mathbf{W}_{1}\mathbf{q}\right)\odot\left(\mathbf{W}_{2}\mathbf{q}\right)\right).$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: Here, q is the query vector that encapsulates the contextual information for
    the task. This router input indicates the specific information for which the current
    token is ‚Äúquerying.‚Äù $\mathbf{W}_{1},\mathbf{W}_{2}\in\mathbb{R}^{N\times d}$
    denotes the number of the number of RoPE angle candidates. Considering each head‚Äôs
    distinct function¬†[[24](#bib.bib24), [35](#bib.bib35), [23](#bib.bib23)], we integrate
    a router into every attention head in the LLM. Notably, a router‚Äôs decision is
    independent of other heads and dynamic to the context.
  prefs: []
  type: TYPE_NORMAL
- en: 'As defined in Eq.¬†[5](#S3.E5 "In 3.1 Architecture ‚Ä£ 3 Mixture of In-Context
    Experts ‚Ä£ Mixture of In-Context Experts Enhance LLMs‚Äô Long Context Awareness"),
    the router outputs an $N$-th head:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\text{TopK-Indices}_{n}^{h}=\texttt{argsort}(\texttt{Router}\left(\mathbf{q}_{n}^{h}\right))[:K],$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\textbf{p}_{n}^{h}=\texttt{Softmax}\left(\texttt{Router}\left(\mathbf{q}_{n}^{h}\right)[\text{TopK-Indices}_{n}^{h}]\right),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{q}_{n}^{h}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: 'Considering RoPE angles impact how attention heads allocate attention and focus
    on specific contextual positions, we view each distinct RoPE angle as an in-context
    expert, in contrast to traditional in-weight experts¬†[[14](#bib.bib14), [17](#bib.bib17),
    [10](#bib.bib10)], where the experts are learnable parameter weights. Given that
    these in-context experts together augment LLMs‚Äô context awareness, we term this
    method Mixture of In-Context Experts (MoICE, Section¬†[3](#S3 "3 Mixture of In-Context
    Experts ‚Ä£ Mixture of In-Context Experts Enhance LLMs‚Äô Long Context Awareness")).
    Figure¬†[3](#S3.F3 "Figure 3 ‚Ä£ 3.1 Architecture ‚Ä£ 3 Mixture of In-Context Experts
    ‚Ä£ Mixture of In-Context Experts Enhance LLMs‚Äô Long Context Awareness") illustrates
    the overview of MoICE. Our proposed MoICE has three major advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: (1) We only add additional computational overhead to the query-key dot products,
    resulting in a minimal increase in memory usage and a negligible impact on inference
    speed (Section¬†[4.2](#S4.SS2 "4.2 Long context understanding and generation ‚Ä£
    4 Experiment ‚Ä£ Mixture of In-Context Experts Enhance LLMs‚Äô Long Context Awareness")).
  prefs: []
  type: TYPE_NORMAL
- en: (2) MoICE dynamically selects suitable RoPE angles token-wise and head-wise,
    offering unprecedented flexibility and unlocking the full potential of each attention
    head.
  prefs: []
  type: TYPE_NORMAL
- en: '(3) Concerning LLMs‚Äô context awareness enhancement, MoICE addresses a longstanding
    issue: the relative position of the relevant information will shift during generation,
    leading to previous static modification of the attention heads¬†[[6](#bib.bib6),
    [36](#bib.bib36)] will be sub-optimal during practical generation. The contextual-aware
    dynamic routing in MoICE is not bothered by this issue.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Router-only training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To train the newly incorporated MoICE routers in LLMs, the most straightforward
    way is to simultaneously update the LLMs‚Äô parameters alongside the routers. However,
    updating the original LLMs‚Äô parameters can result in catastrophic forgetting.
    Therefore, we propose a more effective and efficient strategy, the router-only
    training strategy, which freezes the LLMs‚Äô parameters and solely optimizing the
    routers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given an input sequence, we calculate the negative log-likelihood loss ($\mathcal{L}_{nll}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{aux}=\alpha\cdot N\cdot\sum_{j=1}^{N}\mathbf{F}_{j}\cdot\mathbf{P}_{j}.$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: 'Eq.¬†[8](#S3.E8 "In 3.2 Router-only training ‚Ä£ 3 Mixture of In-Context Experts
    ‚Ä£ Mixture of In-Context Experts Enhance LLMs‚Äô Long Context Awareness") avoids
    the router falling into a sub-optimal solution favoring specific experts overwhelmingly,
    as its minimal is achieved when the routing probability is uniform. Here, $\alpha$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\mathbf{F}_{j}=\frac{1}{T\times H}\sum_{t=1}^{T}\sum_{h=1}^{H}\mathbbm{1}\{j\in\text{TopK-Indices}_{t}^{h}\},$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'Our overall training objective is to minimize the following loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}=\mathcal{L}_{nll}+\mathcal{L}_{aux}.$ |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: 4 Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To evaluate the efficacy of MoICE, we implement it with open-source LLMs, which
    we will introduce later, and conduct lightweight training of MoICE routers on
    a small and general dataset. Subsequently, we evaluate the enhanced LLM‚Äôs capability
    to zero-shot undertake multiple tasks in long context understanding and generation,
    as detailed in Section¬†[4.2](#S4.SS2 "4.2 Long context understanding and generation
    ‚Ä£ 4 Experiment ‚Ä£ Mixture of In-Context Experts Enhance LLMs‚Äô Long Context Awareness")
    and Section¬†[4.3](#S4.SS3 "4.3 Retrieval-augmented generation (RAG) ‚Ä£ 4 Experiment
    ‚Ä£ Mixture of In-Context Experts Enhance LLMs‚Äô Long Context Awareness").
  prefs: []
  type: TYPE_NORMAL
- en: Training data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We use a training dataset¬π¬π1[https://huggingface.co/datasets/HuggingFaceH4/OpenHermes-2.5-1k-longest](https://huggingface.co/datasets/HuggingFaceH4/OpenHermes-2.5-1k-longest)
    which extracts the one thousand longest entries from OpenHermes¬†[[31](#bib.bib31)].
    OpenHermes is a multi-source integrated dataset containing high-quality synthetically
    generated instruction and chat samples. A detailed analysis of other training
    data is in Section¬†[5.3](#S5.SS3 "5.3 MoICE is robust to training data ‚Ä£ 5 Method
    analysis ‚Ä£ Mixture of In-Context Experts Enhance LLMs‚Äô Long Context Awareness").
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters for MoICE-router-only training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We froze all the original parameters of the open-source LLMs we used and only
    trained the MoICE router. Following Attention Buckets¬†[[6](#bib.bib6)], we employed
    the RoPE angle set of $N=7$=7 bases to ensure a fair comparison with¬†[[6](#bib.bib6)].
    Section¬†[4.2](#S4.SS2 "4.2 Long context understanding and generation ‚Ä£ 4 Experiment
    ‚Ä£ Mixture of In-Context Experts Enhance LLMs‚Äô Long Context Awareness") introduces
    our baselines in detail. Section¬†[5](#S5 "5 Method analysis ‚Ä£ Mixture of In-Context
    Experts Enhance LLMs‚Äô Long Context Awareness") delves into the impact of set size
    and the number of selected items.
  prefs: []
  type: TYPE_NORMAL
- en: We implement a warm-up strategy comprising 20% of the total steps, with a maximum
    learning rate of 0.0001. The batch size is 128. $\alpha$ is set as 0.3. We train
    the MoICE routers for 1 epoch (about 8 minutes) on four A800-80G GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Long context understanding and generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Following the L-Eval benchmark¬†[[1](#bib.bib1)], we evaluated the LLM with
    tasks categorized into two main groups: closed-ended and open-ended tasks. Closed-ended
    tasks primarily focus on the capacity for understanding and reasoning within long
    contexts, including tasks like multiple-choice questions from QuALITY¬†[[3](#bib.bib3)],
    Coursera,¬†¬≤¬≤2[https://coursera.org/](https://coursera.org/) TOEFL¬†[[9](#bib.bib9)],
    and True/False question answering from SFiction.¬†¬≥¬≥3[https://github.com/nschaetti/SFGram-dataset](https://github.com/nschaetti/SFGram-dataset)
    On the other hand, open-ended tasks include summarization generation and open-format
    question-answering tasks, requiring extracting information from lengthy in-context
    documents. The open-ended tasks comprise a subset of 181 questions drawn from
    29 diverse long documents.'
  prefs: []
  type: TYPE_NORMAL
- en: Baselines and open-source LLMs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In evaluating the efficacy of our proposed MoICE, we compare it against several
    state-of-the-art methods known for enhancing the capacity of LLMs to understand
    and generate long contexts. These baselines include two context extrapolation
    techniques: Positional Interpolation (PI,¬†[[5](#bib.bib5)]) and Dynamic NTK¬†[[13](#bib.bib13)].
    Additionally, we consider two inference algorithms for context-awareness enhancement:
    Ms-PoE¬†[[38](#bib.bib38)] and Attention Buckets¬†[[6](#bib.bib6)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'We evaluate all these methods alongside our MoICE on two representative open-source
    LLMs that utilize RoPE for positional embeddings: Llama2-7B-chat¬†[[32](#bib.bib32)]
    with a pre-trained context length of 4,096, and Mistral-7B-Instruct-v0.1¬†[[16](#bib.bib16)].
    Mistral-7B employs a sliding window attention (SWA) mechanism with a window size
    of 4,096 tokens, enabling it to accommodate longer contexts than the default.
    Therefore, we conduct experiments with a context length of 8,192 on Mistral-7B,
    using SWA as the exclusive baseline for comparison. For PI and Dynamic NTK, we
    apply a scaling ratio of 1.5, while for the remaining baselines, we adhere to
    the hyperparameters specified in their original papers. All methods are tested
    on a single A800-80G GPU, except for applying AB to Mistral-7B-8k, which needs
    2 GPUs due to substantial memory requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Experimental results on the L-Eval Benchmark¬†[[1](#bib.bib1)]. Applying
    to various models, MoICE demonstrate superior performance compared to previous
    competitive approaches. We emphasize the highest score in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Closed - Ended Task | Open - Ended Task |'
  prefs: []
  type: TYPE_TB
- en: '| Coursera | QuALITY | TOEFL | SFiction | Average | wins | ties | win-rate%^*
    |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-7B-chat¬†[[32](#bib.bib32)] | 36.77 | 38.12 | 55.02 | 60.16 | 47.52
    | 68 | 117 | 34.94 |'
  prefs: []
  type: TYPE_TB
- en: '| + Fine-tuning | 32.85 | 30.20 | 51.30 | 59.38 | 43.43 | 65 | 91 | 30.52 |'
  prefs: []
  type: TYPE_TB
- en: '| + PI¬†[[5](#bib.bib5)] | 38.23 | 38.61 | 56.51 | 61.72 | 48.77 | 76 | 112
    | 36.46 |'
  prefs: []
  type: TYPE_TB
- en: '| + Dynamic NTK¬†[[13](#bib.bib13)] | 40.26 | 39.11 | 55.76 | 62.50 | 49.41
    | 82 | 112 | 38.12 |'
  prefs: []
  type: TYPE_TB
- en: '| + Ms-PoE¬†[[38](#bib.bib38)] | 39.24 | 40.10 | 55.76 | 63.28 | 49.60 | 86
    | 110 | 38.95 |'
  prefs: []
  type: TYPE_TB
- en: '| + AB¬†[[6](#bib.bib6)] | 40.41 | 41.09 | 56.88 | 61.72 | 50.02 | 85 | 114
    | 39.23 |'
  prefs: []
  type: TYPE_TB
- en: '| + MoICE (Ours) | 39.83 | 42.08 | 56.13 | 64.84 | 50.72 | 89 | 118 | 40.88
    |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B-Instruct-8k¬†[[16](#bib.bib16)] | 45.20 | 44.06 | 62.08 | 61.72
    | 53.27 | 71 | 105 | 34.11 |'
  prefs: []
  type: TYPE_TB
- en: '| + Fine-tuning | 25.29 | 26.73 | 25.65 | 50.00 | 31.92 | 53 | 85 | 26.38 |'
  prefs: []
  type: TYPE_TB
- en: '| + SWA | 44.77 | 42.57 | 62.08 | 60.94 | 52.59 | 73 | 89 | 32.45 |'
  prefs: []
  type: TYPE_TB
- en: '| + PI¬†[[5](#bib.bib5)] | 44.19 | 44.06 | 64.68 | 62.50 | 53.86 | 73 | 96 |
    33.43 |'
  prefs: []
  type: TYPE_TB
- en: '| + Dynamic NTK¬†[[13](#bib.bib13)] | 45.35 | 42.08 | 62.08 | 63.28 | 53.20
    | 78 | 103 | 35.77 |'
  prefs: []
  type: TYPE_TB
- en: '| + Ms-PoE¬†[[38](#bib.bib38)] | 46.37 | 45.05 | 61.34 | 57.03 | 52.45 | 84
    | 106 | 37.84 |'
  prefs: []
  type: TYPE_TB
- en: '| + AB¬†[[6](#bib.bib6)] | 46.08 | 42.57 | 62.08 | 62.50 | 53.31 | 87 | 110
    | 39.22 |'
  prefs: []
  type: TYPE_TB
- en: '| + MoICE (Ours) | 47.82 | 46.53 | 64.68 | 62.50 | 55.38 | 85 | 117 | 39.36
    |'
  prefs: []
  type: TYPE_TB
- en: '*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Following¬†[[1](#bib.bib1)], win-rate = (win counts + 0.5 * tie counts)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Table 2: Practical inference time (in minutes) / GPU memory costs (GB) on a
    single A800-80G GPU for each method applied to Llama2-7B-chat (top) and Mistral-7B-Instruct-8k
    (bottom), respectively. Due to out-of-memory issues, AB can not accomplish many
    tasks, denoted as OOM in the table.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Coursera $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| AB¬†[[6](#bib.bib6)] | 10.9 / 78.7 | 18.1 / 62.5 | 19.9 / 56.5 | 5.0 / 33.2
    | 45.9 / 78.2 | 20.0 / 61.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Ms-PoE¬†[[38](#bib.bib38)] | 4.1 / 27.2 | 6.0 / 27.8 | 6.7 / 28.6 | 6.0 /
    27.8 | 20.2 / 28.9 | 8.6 / 28.1 |'
  prefs: []
  type: TYPE_TB
- en: '| MoICE (Ours) | 5.0 / 19.6 | 11.0 / 19.7 | 10.2 / 19.5 | 1.6 / 15.2 | 34.2
    / 23.2 | 12.4 / 19.4 |'
  prefs: []
  type: TYPE_TB
- en: '| AB¬†[[6](#bib.bib6)] | OOM | OOM | 37.2 / 71.4 | OOM | OOM | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Ms-PoE¬†[[38](#bib.bib38)] | 14.1 / 50.3 | 11.2 / 48.4 | 9.8 / 25.4 | 4.5
    / 50.3 | 72.8 / 62.4 | 22.5 / 47.4 |'
  prefs: []
  type: TYPE_TB
- en: '| MoICE (Ours) | 13.4 / 25.7 | 7.7 / 22.9 | 11.3 / 20.4 | 2.3 / 22.8 | 77.8
    / 29.3 | 22.5 / 24.2 |'
  prefs: []
  type: TYPE_TB
- en: Evaluation metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We adopt the exact match for closed-ended tasks. For open-ended tasks, we employ
    GPT-4-Turbo¬†[[25](#bib.bib25)] as the judge to evaluate the effectiveness of various
    enhancement methods on open-source LLMs. This evaluation compares their performance
    against GPT3.5-Turbo-16k-0613 across 181 questions.
  prefs: []
  type: TYPE_NORMAL
- en: Results and analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We report our experimental results in Table¬†[1](#S4.T1 "Table 1 ‚Ä£ Baselines
    and open-source LLMs ‚Ä£ 4.2 Long context understanding and generation ‚Ä£ 4 Experiment
    ‚Ä£ Mixture of In-Context Experts Enhance LLMs‚Äô Long Context Awareness"). MoICE
    significantly enhances the overall performance of Llama-2-7B-chat (with p-value
    < 0.02 in the t-test) in both closed-ended and open-ended tasks. On Mistral, MoICE
    outperforms all baseline models significantly (p-value < 0.02). Standard fine-tuning
    degrades the performance of original LLMs, demonstrating catastrophic forgetting
    and proving that the improvement of MoICE does not stem from more training. These
    results underscore MoICE‚Äôs efficacy in enhancing LLMs‚Äô ability to understand and
    generate long contexts, both of which require high context awareness. Furthermore,
    these results underscore the broad applicability of MoICE across different LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding efficiency, we provide practical inference time and memory costs associated
    with AB, Ms-PoE, and MoICE in Table¬†[2](#S4.T2 "Table 2 ‚Ä£ Baselines and open-source
    LLMs ‚Ä£ 4.2 Long context understanding and generation ‚Ä£ 4 Experiment ‚Ä£ Mixture
    of In-Context Experts Enhance LLMs‚Äô Long Context Awareness"). For a fair comparison,
    we utilize Flash Attention 2¬†[[11](#bib.bib11)] across all approaches. While achieving
    superior overall performance, MoICE remains at an inference speed similar to Ms-PoE
    and notably excels in memory efficiency compared to these two baselines.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Retrieval-augmented generation (RAG)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Retrieval-augmented generation (RAG) tasks involve retrieving numerous documents
    related to the current generation. The retrieved documents are arranged in the
    context. RAG necessitates that LLMs have robust context awareness to pinpoint
    crucial documents, process the retrieved information effectively, and integrate
    it to generate responses.
  prefs: []
  type: TYPE_NORMAL
- en: Following¬†[[6](#bib.bib6), [38](#bib.bib38)], we employ the MDQA task to evaluate
    the efficacy of MoICE in enhancing LLMs‚Äô performance in RAG tasks. Meanwhile,
    MDQA offers the bonus of allowing flexible control over the location of documents,
    enabling a more precise evaluation of LLMs‚Äô context awareness across various contextual
    positions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: The experiment results on the MDQA task. MoICE achieve superior average
    performance compared to previous competitive approaches. We emphasize the highest
    score in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | 1 | 3 | 5 | 7 | 10 | Gap | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-7B-chat | 64.14 | 65.95 | 64.97 | 62.67 | 67.53 | 4.86 | 65.05 |'
  prefs: []
  type: TYPE_TB
- en: '| + Ms-PoE¬†[[38](#bib.bib38)] | 66.06 | 64.29 | 63.99 | 62.22 | 64.75 | 3.84
    | 64.34 |'
  prefs: []
  type: TYPE_TB
- en: '| + AB¬†[[5](#bib.bib5)] | 66.36 | 66.14 | 65.25 | 63.20 | 64.93 | 3.16 | 65.18
    |'
  prefs: []
  type: TYPE_TB
- en: '| + MoICE (Ours) | 65.50 | 66.33 | 65.61 | 64.11 | 65.84 | 2.22 | 65.48 |'
  prefs: []
  type: TYPE_TB
- en: '| Method | 1 | 8 | 15 | 23 | 30 | Gap | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B-Instruct-8k | 58.38 | 47.42 | 46.97 | 49.68 | 50.81 | 11.41 |
    50.65 |'
  prefs: []
  type: TYPE_TB
- en: '| + Ms-PoE¬†[[38](#bib.bib38)] | 52.76 | 41.24 | 42.80 | 42.90 | 43.58 | 11.52
    | 44.66 |'
  prefs: []
  type: TYPE_TB
- en: '| + AB¬†[[5](#bib.bib5)] | 58.57 | 47.57 | 47.12 | 49.83 | 50.96 | 11.45 | 50.81
    |'
  prefs: []
  type: TYPE_TB
- en: '| + MoICE (Ours) | 61.81 | 52.54 | 52.43 | 50.36 | 49.34 | 12.47 | 53.30 |'
  prefs: []
  type: TYPE_TB
- en: Our MDQA experiments leverage a subset of NaturalQuestions-Open¬†[[21](#bib.bib21),
    [20](#bib.bib20)], consisting of 2,655 queries, following¬†[[38](#bib.bib38), [22](#bib.bib22)].
    Each query is paired with a context consisting of 10 or 30 documents (with an
    average of 1,722 or 5,046 tokens), depending on the model (Llama-2-7B-chat or
    Mistral-7B-Instruct-8k), tasked with answering based on this contextual information.
    Only one document among these comprises useful information for the given query.
    We compare Ms-PoE, AB, and MoICE, testing each method through 5 iterations. For
    Llama, the relevant document is positioned 1st, 3rd, 5th, 7th, and 10th within
    the context, while for Mistral, it is positioned 1st, 8th, 15th, 23rd, and 30th,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: In Table¬†[3](#S4.T3 "Table 3 ‚Ä£ 4.3 Retrieval-augmented generation (RAG) ‚Ä£ 4
    Experiment ‚Ä£ Mixture of In-Context Experts Enhance LLMs‚Äô Long Context Awareness"),
    MoICE on Llama demonstrates the highest average performance across most positions,
    showcasing its remarkable stability. Its accuracy scores show minimal variation,
    with only a marginal difference of 2.22 points between its highest and lowest
    values. On Mistral, MoICE exhibits significant average improvement (p-value <
    0.02). Notably, when the relevant document is positioned at the end of the context,
    all methods on Llama exhibit a decrease compared to the original model, although
    MoICE shows a minimal decline. This phenomenon also happens in the Mistral model.
    We posit that this decline may stem from the original model predominantly directing
    attention towards nearest documents¬†[[22](#bib.bib22), [26](#bib.bib26)]. However,
    as approaches enhance awareness of various contextual positions, the model‚Äôs attention
    to the nearest documents is diffused by other positions, as its overall capacity
    for context awareness is constant and limited. Nevertheless, MoICE consistently
    emerges as the superior-performing method overall across language models.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Method analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we delve into a comprehensive analysis of the properties of
    MoICE. We illustrate how $N$, the specific number of selected in-context experts
    (Section¬†[5.2](#S5.SS2 "5.2 The effect of selected experts number ùêæ ‚Ä£ 5 Method
    analysis ‚Ä£ Mixture of In-Context Experts Enhance LLMs‚Äô Long Context Awareness")),
    influence MoICE. We further demonstrate that MoICE is robust to training data
    (Section¬†[5.3](#S5.SS3 "5.3 MoICE is robust to training data ‚Ä£ 5 Method analysis
    ‚Ä£ Mixture of In-Context Experts Enhance LLMs‚Äô Long Context Awareness")) Additionally,
    we present a case study demonstrating the dynamic selection of in-context experts
    for tokens during generation (Section¬†[5.4](#S5.SS4 "5.4 The visualization of
    dynamic routing states ‚Ä£ 5 Method analysis ‚Ä£ Mixture of In-Context Experts Enhance
    LLMs‚Äô Long Context Awareness")).
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 The effect of expert total numbers $N$
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We investigate the impact of the total number of experts. Employing the search
    algorithm proposed by Chen et al.¬†[[6](#bib.bib6)], we obtain various sets of
    different sizes, each comprising complementary base values. The searched expert
    sets are detailed in Appendix¬†[C](#A3 "Appendix C Details on expert sets ‚Ä£ Mixture
    of In-Context Experts Enhance LLMs‚Äô Long Context Awareness"). We apply MoICE to
    Llama-2-7B-chat and test the model on L-Eval tasks. The results are presented
    in Table¬†[4](#S5.T4 "Table 4 ‚Ä£ 5.2 The effect of selected experts number ùêæ ‚Ä£ 5
    Method analysis ‚Ä£ Mixture of In-Context Experts Enhance LLMs‚Äô Long Context Awareness").
    The results of the original Llama are denoted as ($N$=7 experts is sufficient
    for general usage.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 The effect of selected experts number $K$
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With a fixed number of experts ($N$ greater than 3, performance improvements
    become evident. This shows that the MoICE router in our method can select the
    appropriate combination of experts to better aware the context. Randomly selecting
    experts ruins the model‚Äôs language modeling ability, leading to aberrant outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: The performance of Llama-2-7B-chat enhanced by MoICE with $N$ in-context
    experts. We show results marked with color to emphasize the improvements over
    the original model.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Coursera | QuALITY | TOEFL | SFiction | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| Original ($N$=1) | 36.77 | 38.12 | 55.02 | 60.16 | 47.52 |'
  prefs: []
  type: TYPE_TB
- en: '| $N$=3 | 37.65 | 40.10 | 55.76 | 62.50 | 49.00 |'
  prefs: []
  type: TYPE_TB
- en: '| $N$=5 | 38.23 | 39.60 | 56.13 | 63.28 | 49.32 |'
  prefs: []
  type: TYPE_TB
- en: '| $N$=7 | 39.83 | 42.08 | 56.13 | 64.84 | 50.72 |'
  prefs: []
  type: TYPE_TB
- en: '| $N$=9 | 40.26 | 41.58 | 56.13 | 64.84 | 50.70 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: The improvement of context awareness of Llama-2-7B-chat by MoICE,
    wherein each head dynamically selects diverse $K$=7). We show results marked with
    color to emphasize the improvements over the original model.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Coursera | QuALITY | TOEFL | SFiction | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| Original ($N$=1) | 36.77 | 38.12 | 55.02 | 60.16 | 47.52 |'
  prefs: []
  type: TYPE_TB
- en: '| $K$=1 | 35.03 | 35.64 | 56.51 | 61.72 | 47.22 |'
  prefs: []
  type: TYPE_TB
- en: '| $K$=3 | 39.83 | 41.58 | 56.13 | 64.84 | 50.60 |'
  prefs: []
  type: TYPE_TB
- en: '| $K$=5 | 38.52 | 39.60 | 56.13 | 64.84 | 49.77 |'
  prefs: []
  type: TYPE_TB
- en: '| $K$=7 | 39.83 | 42.08 | 56.13 | 64.84 | 50.72 |'
  prefs: []
  type: TYPE_TB
- en: '| Equal Weights | 36.48 | 38.12 | 53.90 | 61.72 | 47.56 |'
  prefs: []
  type: TYPE_TB
- en: '| Random Weights | 15.55 | 28.71 | 21.75 | 8.59 | 18.65 |'
  prefs: []
  type: TYPE_TB
- en: 5.3 MoICE is robust to training data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We further analyze the impact of the data for training routers. We additionally
    use three instruction fine-tuning datasets from different sources: a self-instruct
    dataset, Airoboros¬†[[18](#bib.bib18)]; and two datasets for LLM alignment with
    long context, Long-Alpaca¬†[[7](#bib.bib7)], and LongAlign¬†[[2](#bib.bib2)]. The
    hyperparameters remain consistent as mentioned in Section¬†[4](#S4 "4 Experiment
    ‚Ä£ Mixture of In-Context Experts Enhance LLMs‚Äô Long Context Awareness"). As presented
    in Table¬†[6](#S5.T6 "Table 6 ‚Ä£ 5.3 MoICE is robust to training data ‚Ä£ 5 Method
    analysis ‚Ä£ Mixture of In-Context Experts Enhance LLMs‚Äô Long Context Awareness"),
    MoICE exhibits almost identical scores when trained on different data, showcasing
    the robustness of our method.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: The improvement of context awareness of Llama-2-7B-chat by MoICE trained
    on various data.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Training Data | Coursera | QuALITY | TOEFL | SFiction | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| OpenHermes¬†[[31](#bib.bib31)] | 39.83 | 42.08 | 56.13 | 64.84 | 50.72 |'
  prefs: []
  type: TYPE_TB
- en: '| Airoboros¬†[[18](#bib.bib18)] | 39.68 | 41.58 | 56.13 | 64.84 | 50.56 |'
  prefs: []
  type: TYPE_TB
- en: '| Long-Alpaca¬†[[7](#bib.bib7)] | 39.68 | 42.08 | 56.13 | 64.84 | 50.68 |'
  prefs: []
  type: TYPE_TB
- en: '| LongAlign¬†[[2](#bib.bib2)] | 39.68 | 41.58 | 56.13 | 64.84 | 50.56 |'
  prefs: []
  type: TYPE_TB
- en: 5.4 The visualization of dynamic routing states
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We provide a case study exemplifying the dynamic routing mechanism within MoICE
    during text generation. Depicted in Figure¬†[4](#A5.F4 "Figure 4 ‚Ä£ Appendix E Broader
    impacts and safety issues ‚Ä£ Mixture of In-Context Experts Enhance LLMs‚Äô Long Context
    Awareness") in the Appendix, the MoICE router of each head independently selects
    distinct experts. At each step of the generation process, these heads dynamically
    choose experts for each new token. This dynamic utilization of diverse RoPE angles
    within each attention head maximizes the potential of attention heads across various
    inputs, a capability not attained in prior research, including both Attention
    Buckets and Ms-PoE.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we introduce a novel approach to enhancing the context awareness
    of LLMs termed Mixture of In-Context Experts (MoICE). Through lightweight training,
    open-source LLMs such as Llama and Mistral, enhanced by MoICE, demonstrate improved
    context awareness. Across numerous tasks demanding substantial context awareness,
    MoICE-enhanced LLMs consistently outperform competitive baselines, all the while
    maintaining commendable efficiency. A distinctive feature of MoICE is that it
    first implements head- and token-specific RoPE angles assignment for attention
    heads, a pivotal factor contributing to its success. This paper underscores the
    need to address the inherent limitations in current LLMs and advocates for a thorough
    exploration of their existing capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments and Disclosure of Funding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work was supported by Boss Zhipin for providing computational resources.
    We appreciate Ting-En Lin for his discussions on training hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong,
    and Xipeng Qiu. L-eval: Instituting standardized evaluation for long context language
    models. arXiv preprint arXiv:2307.11088, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji¬†Qi, Lei Hou, Jie Tang, Yuxiao
    Dong, and Juanzi Li. Longalign: A recipe for long context alignment of large language
    models. arXiv preprint arXiv:2401.18058, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Samuel¬†R Bowman, Angelica Chen, He¬†He, Nitish Joshi, Johnny Ma, Nikita
    Nangia, Vishakh Padmakumar, Richard¬†Yuanzhe Pang, Alicia Parrish, Jason Phang,
    et¬†al. Quality: Question answering with long input texts, yes! NAACL 2022, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le¬†Sun. Benchmarking large language
    models in retrieval-augmented generation. In Proceedings of the AAAI Conference
    on Artificial Intelligence, volume¬†38, pages 17754‚Äì17762, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending
    context window of large language models via positional interpolation. arXiv preprint
    arXiv:2306.15595, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Yuhan Chen, Ang Lv, Ting-En Lin, Changyu Chen, Yuchuan Wu, Fei Huang, Yongbin
    Li, and Rui Yan. Fortify the shortest stave in attention: Enhancing context awareness
    of large language models for effective tool use. arXiv preprint arXiv:2312.04455,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Yukang Chen, Shaozuo Yu, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu,
    Song Han, and Jiaya Jia. Long alpaca: Long-context instruction-following models.
    [https://github.com/dvlab-research/LongLoRA](https://github.com/dvlab-research/LongLoRA),
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Xin Cheng, Di¬†Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan.
    Lift yourself up: Retrieval-augmented text generation with self-memory. Advances
    in Neural Information Processing Systems, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Yu-An Chung, Hung-Yi Lee, and James Glass. Supervised and unsupervised
    transfer learning for question answering. In Marilyn Walker, Heng Ji, and Amanda
    Stent, editors, Proceedings of the 2018 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers), pages 1585‚Äì1594, New Orleans, Louisiana, June 2018\. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Damai Dai, Chengqi Deng, Chenggang Zhao, RX¬†Xu, Huazuo Gao, Deli Chen,
    Jiashi Li, Wangding Zeng, Xingkai Yu, Y¬†Wu, et¬†al. Deepseekmoe: Towards ultimate
    expert specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Tri Dao. Flashattention-2: Faster attention with better parallelism and
    work partitioning. In The Twelfth International Conference on Learning Representations,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT:
    Pre-training of deep bidirectional transformers for language understanding. In
    Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the
    2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages
    4171‚Äì4186, Minneapolis, Minnesota, June 2019\. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] emozilla. Dynamically scaled rope further increases performance of long
    context llama with zero fine-tuning. [https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases](https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling
    to trillion parameter models with simple and efficient sparsity. Journal of Machine
    Learning Research, 23(120):1‚Äì39, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Yizheng Huang and Jimmy Huang. A survey on retrieval-augmented text generation
    for large language models. arXiv preprint arXiv:2404.10981, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Albert¬†Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
    Devendra¬†Singh Chaplot, Diego de¬†las Casas, Florian Bressand, Gianna Lengyel,
    Guillaume Lample, Lucile Saulnier, et¬†al. Mistral 7b. arXiv preprint arXiv:2310.06825,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Albert¬†Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche
    Savary, Chris Bamford, Devendra¬†Singh Chaplot, Diego de¬†las Casas, Emma¬†Bou Hanna,
    Florian Bressand, et¬†al. Mixtral of experts. arXiv preprint arXiv:2401.04088,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] jondurbin. airoboros: using large language models to fine-tune large language
    models. [https://github.com/jondurbin/airoboros](https://github.com/jondurbin/airoboros).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] He¬†Junqing, Pan Kunhao, Dong Xiaoqun, Song Zhuoyang, Liu Yibo, Liang Yuxin,
    Wang Hao, Sun Qianguo, Zhang Songxin, Xie Zejian, et¬†al. Never lost in the middle:
    Improving large language models via attention strengthening question answering.
    arXiv preprint arXiv:2311.09198, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins,
    Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin,
    Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew¬†M.
    Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark
    for question answering research. Transactions of the Association for Computational
    Linguistics, 7:452‚Äì466, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for
    weakly supervised open domain question answering. In Anna Korhonen, David Traum,
    and Llu√≠s M√†rquez, editors, Proceedings of the 57th Annual Meeting of the Association
    for Computational Linguistics, pages 6086‚Äì6096, Florence, Italy, July 2019\. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Nelson¬†F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua,
    Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long
    contexts, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Ang Lv, Kaiyi Zhang, Yuhan Chen, Yulong Wang, Lifeng Liu, Ji-Rong Wen,
    Jian Xie, and Rui Yan. Interpreting key mechanisms of factual recall in transformer-based
    language models, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma,
    Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn
    Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy
    Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown,
    Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning
    and induction heads. Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] OpenAI. Gpt-4 technical report, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Alexander Peysakhovich and Adam Lerer. Attention sorting combats recency
    bias in long context language models, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Ofir Press, Noah¬†A. Smith, and Mike Lewis. Train short, test long: Attention
    with linear biases enables input length extrapolation, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Noam Shazeer, *Azalia Mirhoseini, *Krzysztof Maziarz, Andy Davis, Quoc
    Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated
    mixture-of-experts layer. In International Conference on Learning Representations,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Jianlin Su, Murtadha Ahmed, Yu¬†Lu, Shengfeng Pan, Wen Bo, and Yunfeng
    Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing,
    568:127063, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Yutao Sun, Li¬†Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim,
    Vishrav Chaudhary, Xia Song, and Furu Wei. A length-extrapolatable transformer.
    In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of
    the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers), pages 14590‚Äì14604, Toronto, Canada, July 2023\. Association for
    Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Teknium. Openhermes 2.5: An open dataset of synthetic data for generalist
    llm assistants, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et¬†al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan¬†N Gomez, ≈Å¬†ukasz Kaiser, and Illia Polosukhin. Attention is all you need.
    In I.¬†Guyon, U.¬†Von Luxburg, S.¬†Bengio, H.¬†Wallach, R.¬†Fergus, S.¬†Vishwanathan,
    and R.¬†Garnett, editors, Advances in Neural Information Processing Systems, volume¬†30\.
    Curran Associates, Inc., 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Haotao Wang, Ziyu Jiang, Yuning You, Yan Han, Gaowen Liu, Jayanth Srinivasa,
    Ramana Kompella, Zhangyang Wang, et¬†al. Graph mixture of experts: Learning on
    large-scale graphs with explicit diversity modeling. Advances in Neural Information
    Processing Systems, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Kevin¬†Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and
    Jacob Steinhardt. Interpretability in the wild: a circuit for indirect object
    identification in GPT-2 small. In The Eleventh International Conference on Learning
    Representations, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Liang Zhang, Katherine Jijo, Spurthi Setty, Eden Chung, Fatima Javid,
    Natan Vidra, and Tommy Clifford. Enhancing large language model performance to
    answer questions and extract information more accurately. arXiv preprint arXiv:2402.01722,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Qingru Zhang, Chandan Singh, Liyuan Liu, Xiaodong Liu, Bin Yu, Jianfeng
    Gao, and Tuo Zhao. Tell your model where to attend: Post-hoc attention steering
    for llms. arXiv preprint arXiv:2311.02262, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Zhenyu Zhang, Runjin Chen, Shiwei Liu, Zhewei Yao, Olatunji Ruwase, Beidi
    Chen, Xiaoxia Wu, and Zhangyang Wang. Found in the middle: How language models
    use long contexts better via plug-and-play positional encoding. arXiv preprint
    arXiv:2403.04797, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Attention waveforms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will elaborate on attention waveforms and the concept of
    complementarity. Assuming $\mathbf{\hat{q}}_{n}^{h}\cdot\mathbf{\hat{k}}_{m}^{h}$-th
    attention head. The attention score can be formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbf{\hat{q}}_{n}^{h}\cdot\mathbf{\hat{k}}_{m}^{h}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\operatorname{Re}\left[\sum_{j=0}^{d/2-1}\mathbf{q_{n}^{h}}[2j:2j+1]\mathbf{k_{m}^{h*}}[2j:2j+1]e^{i(n-m)\theta_{j}}\right]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where $\theta_{j}=B^{-2j/d}$ by¬†[[6](#bib.bib6)].
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{W}\leq\sum_{j=0}^{d/2-1}2\cos\left((n-m)\theta_{j}\right).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'As illustrated in Figure ¬†[2](#S2.F2 "Figure 2 ‚Ä£ Context awareness of LLMs
    ‚Ä£ 2 Background ‚Ä£ Mixture of In-Context Experts Enhance LLMs‚Äô Long Context Awareness"),
    the waveform exhibits two notable mathematical properties concerning attention
    scores: it demonstrates fluctuations and undergoes a gradual decay with the increasing
    relative position (i.e., long-term decay).'
  prefs: []
  type: TYPE_NORMAL
- en: Chen et al.¬†[[6](#bib.bib6)] observed that crucial information falling within
    the troughs of a waveform might diminish the performance of models employing RoPE.
    Meanwhile, they pointed out the waveform, characterized by peaks and troughs,
    vary across RoPE bases. When leveraging the peaks of one attention wave to compensate
    for the overlook of the troughs in another, the model‚Äôs capability to perceive
    and process information from diverse contextual positions can be enhanced. When
    a set of bases possesses this waveform characteristic, they are termed ‚Äúcomplementary.‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Discussions on more position embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we discuss other position embeddings and demonstrate why they
    are not studied, e.g., discarded in LLMs, do not exhibit attention waveform pattern,
    or are in the same family of RoPE: Firstly, the waveform pattern only exists in
    position embeddings constructed by cosine functions. Regarding the cosine embedding
    used in the original Transformer, it does exhibit long-term decay and periodic
    waveforms. However, this embedding is disregarded in modern LLMs. Moreover, these
    embeddings are incorporated before the initial model layer rather than during
    the attention computation, making it hard to assess their impact on attention
    patterns. Secondly, the learned positional embeddings utilized in BERT¬†[[12](#bib.bib12)]
    lack mathematical constraints to display periodic patterns. They are similarly
    added before the first model layer. Thirdly, Alibi¬†[[27](#bib.bib27)] introduces
    a linear bias to attention scores. The linear bias is devoid of wave patterns.
    The remaining popular positional embeddings used in LLMs such as xPos¬†[[30](#bib.bib30)]
    are RoPE-based variants. These variants are predominantly modified for long-context
    extrapolation rather than better context awareness. Therefore, they share the
    same shortcoming: tokens in attention trough are less focused on, thereby limiting
    context awareness, which is the study focus in our paper.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Details on expert sets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Utilizing the RoPE-base searching algorithm as proposed by Chen et al.¬†[[6](#bib.bib6)],
    Table¬†[7](#A3.T7 "Table 7 ‚Ä£ Appendix C Details on expert sets ‚Ä£ Mixture of In-Context
    Experts Enhance LLMs‚Äô Long Context Awareness") illustrates the resulting sets
    for different values of $N$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Searched Sets for Different $N$'
  prefs: []
  type: TYPE_NORMAL
- en: '| $N$ | Searched Set |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | {10,000, 18,000, 19,000} |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | {10,000, 17,500, 18,000, 19,000, 20,000} |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | {10,000, 17,500, 18,000, 19,000, 20,000, 22,500, 25,000} |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | {10,000, 13,500, 17,500, 18,000, 19,000, 20,000, 22,500, 24,000, 25,000}
    |'
  prefs: []
  type: TYPE_TB
- en: Appendix D Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we introduce a plug-in module called MoICE, which is integrated
    into the attention heads of open-source LLMs to enhance their context awareness.
    One limitation is that, due to limited computational resources, we did not investigate
    the effectiveness of pretraining a language model using the MoICE architecture.
    Furthermore, our proposed method exploits the potential for context awareness
    within LLMs, but it does not imbue the models with additional inherent context
    awareness abilities. Achieving this may necessitate more extensive data to train
    all model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Broader impacts and safety issues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our novel lightweight plug-in approach efficiently enhances the context awareness
    of open-source LLMs. This advancement holds great promise for enhancing the effectiveness
    of LLMs across diverse scenarios characterized by extensive and complex contexts,
    such as RAG, tool utilization, and role-playing. The safety issue of our method
    mainly comes from the large language models we used, as they might output toxic
    and biased texts, which is a common safety issue regarding LLM research.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5f118a2dd0ecc0d8c46fb6115f0cc7ef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The routing weights across two distinct attention heads at the 27th
    layer in Llama-2-7B-chat. The input tokens are randomly sampled from the training
    data, and the attention heads under observation are also randomly selected. The
    horizontal axis depicts the input tokens, while the vertical axis represents experts
    with varying RoPE angles. Due to their distinct functions, each head dynamically
    chooses different experts to process individual tokens. Input text can be found
    in Figure ¬†[5](#A5.F5 "Figure 5 ‚Ä£ Appendix E Broader impacts and safety issues
    ‚Ä£ Mixture of In-Context Experts Enhance LLMs‚Äô Long Context Awareness").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5e818ab172df62252a2ba0d1238e2020.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The input text in Figure ¬†[4](#A5.F4 "Figure 4 ‚Ä£ Appendix E Broader
    impacts and safety issues ‚Ä£ Mixture of In-Context Experts Enhance LLMs‚Äô Long Context
    Awareness"). To clearly display, we only show part of the input text, where the
    text with a yellow background corresponds to the decoded tokens.'
  prefs: []
  type: TYPE_NORMAL
