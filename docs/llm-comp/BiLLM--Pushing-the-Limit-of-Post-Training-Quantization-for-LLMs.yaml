- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†ç±»ï¼šæœªåˆ†ç±»
- en: 'date: 2024-09-08 18:50:17'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¥æœŸï¼š2024-09-08 18:50:17
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'BiLLM: Pushing the Limit of Post-Training Quantization for LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'BiLLM: æ¨åŠ¨ LLM åè®­ç»ƒé‡åŒ–çš„æé™'
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2402.04291](https://ar5iv.labs.arxiv.org/html/2402.04291)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2402.04291](https://ar5iv.labs.arxiv.org/html/2402.04291)
- en: Wei Huang â€ƒâ€ƒ Yangdong Liu â€ƒâ€ƒ Haotong Qin^(ğŸ–‚) â€ƒâ€ƒ Ying Li â€ƒâ€ƒ Shiming Zhang â€ƒâ€ƒ
    Xianglong Liu â€ƒâ€ƒ Michele Magno â€ƒâ€ƒ Xiaojuan Qi
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Wei Huang â€ƒâ€ƒ Yangdong Liu â€ƒâ€ƒ Haotong Qin^(ğŸ–‚) â€ƒâ€ƒ Ying Li â€ƒâ€ƒ Shiming Zhang â€ƒâ€ƒ
    Xianglong Liu â€ƒâ€ƒ Michele Magno â€ƒâ€ƒ Xiaojuan Qi
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: Pretrained large language models (LLMs) exhibit exceptional general language
    processing capabilities but come with significant demands on memory and computational
    resources. As a powerful compression technology, binarization can extremely reduce
    model weights to a mere 1 bit, lowering the expensive computation and memory requirements.
    However, existing quantization techniques fall short of maintaining LLM performance
    under ultra-low bit-widths. In response to this challenge, we present BiLLM, a
    groundbreaking 1-bit post-training quantization scheme tailored for pretrained
    LLMs. Based on the weight distribution of LLMs, BiLLM first identifies and structurally
    selects salient weights, and minimizes the compression loss through an effective
    binary residual approximation strategy. Moreover, considering the bell-shaped
    distribution of the non-salient weights, we propose an optimal splitting search
    to group and binarize them accurately. BiLLM achieving for the first time high-accuracy
    inference (e.g. 8.41 perplexity on LLaMA2-70B) with only 1.08-bit weights across
    various LLMs families and evaluation metrics, outperforms SOTA quantization methods
    of LLM by significant margins. Moreover, BiLLM enables the binarization process
    of the LLM with 7 billion weights within 0.5 hours on a single GPU, demonstrating
    satisfactory time efficiency. The code is available at [https://github.com/Aaronhuang-778/BiLLM](https://github.com/Aaronhuang-778/BiLLM).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å±•ç°äº†å“è¶Šçš„è¯­è¨€å¤„ç†èƒ½åŠ›ï¼Œä½†å¯¹å†…å­˜å’Œè®¡ç®—èµ„æºæœ‰ç€æå¤§çš„éœ€æ±‚ã€‚ä½œä¸ºä¸€ç§å¼ºå¤§çš„å‹ç¼©æŠ€æœ¯ï¼ŒäºŒå€¼åŒ–å¯ä»¥å°†æ¨¡å‹æƒé‡å¤§å¹…å‡å°‘åˆ°ä»…1ä½ï¼Œä»è€Œé™ä½æ˜‚è´µçš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é‡åŒ–æŠ€æœ¯åœ¨è¶…ä½æ¯”ç‰¹å®½åº¦ä¸‹æ— æ³•ä¿æŒ
    LLM çš„æ€§èƒ½ã€‚é’ˆå¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº† BiLLMï¼Œè¿™æ˜¯ä¸€ç§çªç ´æ€§çš„ 1 ä½åè®­ç»ƒé‡åŒ–æ–¹æ¡ˆï¼Œä¸“ä¸ºé¢„è®­ç»ƒçš„ LLM è®¾è®¡ã€‚åŸºäº LLM çš„æƒé‡åˆ†å¸ƒï¼ŒBiLLM
    é¦–å…ˆè¯†åˆ«å¹¶ç»“æ„æ€§åœ°é€‰æ‹©æ˜¾è‘—æƒé‡ï¼Œå¹¶é€šè¿‡æœ‰æ•ˆçš„äºŒè¿›åˆ¶æ®‹å·®è¿‘ä¼¼ç­–ç•¥æœ€å°åŒ–å‹ç¼©æŸå¤±ã€‚æ­¤å¤–ï¼Œè€ƒè™‘åˆ°éæ˜¾è‘—æƒé‡çš„é’Ÿå½¢åˆ†å¸ƒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ€ä½³åˆ†å‰²æœç´¢æ–¹æ³•ï¼Œä»¥å‡†ç¡®åœ°å¯¹å…¶è¿›è¡Œåˆ†ç»„å’ŒäºŒå€¼åŒ–ã€‚BiLLM
    é¦–æ¬¡å®ç°äº†åœ¨ä¸åŒ LLM å®¶æ—å’Œè¯„ä¼°æŒ‡æ ‡ä¸‹ï¼Œä»…ä½¿ç”¨ 1.08 ä½æƒé‡çš„æƒ…å†µä¸‹è¿›è¡Œé«˜ç²¾åº¦æ¨ç†ï¼ˆä¾‹å¦‚ LLaMA2-70B ä¸Šçš„ 8.41 å›°æƒ‘åº¦ï¼‰ï¼Œæ˜¾è‘—è¶…è¶Šäº†
    SOTA é‡åŒ–æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒBiLLM åœ¨å•ä¸ª GPU ä¸Šåœ¨ 0.5 å°æ—¶å†…å®Œæˆäº† 70 äº¿æƒé‡çš„ LLM çš„äºŒå€¼åŒ–ï¼Œå±•ç°äº†ä»¤äººæ»¡æ„çš„æ—¶é—´æ•ˆç‡ã€‚ä»£ç å¯åœ¨ [https://github.com/Aaronhuang-778/BiLLM](https://github.com/Aaronhuang-778/BiLLM)
    è·å–ã€‚
- en: Model Binarization, Large Language Model, Model Compression, Deep Learning
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹äºŒå€¼åŒ–ã€å¤§å‹è¯­è¨€æ¨¡å‹ã€æ¨¡å‹å‹ç¼©ã€æ·±åº¦å­¦ä¹ 
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 å¼•è¨€
- en: Recently, large language models (LLMs) based on transformersÂ (Vaswani etÂ al.,
    [2017](#bib.bib42)) have garnered significant attention in natural language processing.
    Pretrained LLMs like OPTÂ (Zhang etÂ al., [2022](#bib.bib51)) and LLaMAÂ (Touvron
    etÂ al., [2023a](#bib.bib40)), have demonstrated excellent performance across a
    range of evaluation benchmarks. However, LLMs pose substantial challenges in deployment
    on memory-constrained devices due to their immense parameter size and computation
    requirements. For instance, the widely-used LLaMA2-70BÂ (Touvron etÂ al., [2023b](#bib.bib41))
    model, with its 70 billion parameters, requires 150 GB of storage in half-precision
    (FP16) format. This necessitates at least two A100 GPUs, each with 80 GB of storage
    space, for inference.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘ï¼ŒåŸºäºå˜æ¢å™¨çš„è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼ˆVaswani et al., [2017](#bib.bib42)ï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå¼•èµ·äº†æå¤§çš„å…³æ³¨ã€‚é¢„è®­ç»ƒçš„
    LLMï¼Œå¦‚ OPTï¼ˆZhang et al., [2022](#bib.bib51)ï¼‰å’Œ LLaMAï¼ˆTouvron et al., [2023a](#bib.bib40)ï¼‰ï¼Œåœ¨å¤šä¸ªè¯„ä¼°åŸºå‡†ä¸Šå±•ç¤ºäº†å‡ºè‰²çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºå…¶åºå¤§çš„å‚æ•°è§„æ¨¡å’Œè®¡ç®—éœ€æ±‚ï¼ŒLLMs
    åœ¨å†…å­˜å—é™çš„è®¾å¤‡ä¸Šéƒ¨ç½²æ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ä¾‹å¦‚ï¼Œå¹¿æ³›ä½¿ç”¨çš„ LLaMA2-70Bï¼ˆTouvron et al., [2023b](#bib.bib41)ï¼‰æ¨¡å‹ï¼Œå…¶
    700 äº¿ä¸ªå‚æ•°åœ¨åŠç²¾åº¦ï¼ˆFP16ï¼‰æ ¼å¼ä¸‹éœ€è¦ 150 GB çš„å­˜å‚¨ã€‚è¿™éœ€è¦è‡³å°‘ä¸¤ä¸ª A100 GPUï¼Œæ¯ä¸ª GPU æ‹¥æœ‰ 80 GB çš„å­˜å‚¨ç©ºé—´ï¼Œä»¥è¿›è¡Œæ¨ç†ã€‚
- en: '![Refer to caption](img/43e017d9c8f673628e426639c9d858e0.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/43e017d9c8f673628e426639c9d858e0.png)'
- en: 'Figure 1: The perplexity of LLaMA-13B on WikiText2 under different bit-widths.
    Round-to-nearest (RTN), GPTQ, and PB-LLM (10% weight of INT8) suffer accuracy
    loss at ultra-low bits, facing the sharply increasing perplexity ($\downarrow$).
    BiLLM demonstrates exceptional performance under binarization.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1ï¼šLLaMA-13Båœ¨WikiText2ä¸‹ä¸åŒä½å®½çš„å›°æƒ‘åº¦ã€‚Round-to-nearest (RTN)ã€GPTQå’ŒPB-LLMï¼ˆ10% INT8æƒé‡ï¼‰åœ¨è¶…ä½ä½ä¸‹å‡†ç¡®ç‡ä¸‹é™ï¼Œé¢ä¸´å›°æƒ‘åº¦æ€¥å‰§ä¸Šå‡ï¼ˆ$\downarrow$ï¼‰ã€‚BiLLMåœ¨äºŒå€¼åŒ–ä¸‹è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚
- en: Model quantization has emerged as a highly effective technology for compressing
    neural networks, thereby reducing the model size of LLMs and substantially saving
    GPU memory consumptionÂ (Dettmers etÂ al., [2022](#bib.bib9)). Current quantization
    techniques primarily fall into Quantization-Aware Training (QAT) and Post-Training
    Quantization (PTQ). QAT involves fine-tuning and retraining during the quantization
    process, while PTQ significantly streamlines the computation by eliminating back-propagation,
    enabling a faster quantization process and promoting the practicality of quantizationÂ (Frantar
    etÂ al., [2022](#bib.bib17); Shang etÂ al., [2023](#bib.bib39); Lin etÂ al., [2023](#bib.bib25)).
    Given the deep structures and numerous parameters of LLMs, PTQ stands out for
    its ability to rapidly perform the quantization process, especially on time and
    resource-constrained scenariosÂ (Zhu etÂ al., [2023](#bib.bib53)).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹é‡åŒ–å·²ç»æˆä¸ºä¸€ç§é«˜åº¦æœ‰æ•ˆçš„ç¥ç»ç½‘ç»œå‹ç¼©æŠ€æœ¯ï¼Œä»è€Œå‡å°‘äº†LLMçš„æ¨¡å‹å¤§å°ï¼Œå¹¶å¤§å¹…èŠ‚çœäº†GPUå†…å­˜æ¶ˆè€—ï¼ˆDettmers et al., [2022](#bib.bib9)ï¼‰ã€‚å½“å‰çš„é‡åŒ–æŠ€æœ¯ä¸»è¦åˆ†ä¸ºé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰å’Œåè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰ã€‚QATæ¶‰åŠåœ¨é‡åŒ–è¿‡ç¨‹ä¸­è¿›è¡Œå¾®è°ƒå’Œé‡æ–°è®­ç»ƒï¼Œè€ŒPTQé€šè¿‡æ¶ˆé™¤åå‘ä¼ æ’­æ˜¾è‘—ç®€åŒ–äº†è®¡ç®—ï¼Œä»è€ŒåŠ å¿«äº†é‡åŒ–è¿‡ç¨‹ï¼Œå¹¶ä¿ƒè¿›äº†é‡åŒ–çš„å®ç”¨æ€§ï¼ˆFrantar
    et al., [2022](#bib.bib17); Shang et al., [2023](#bib.bib39); Lin et al., [2023](#bib.bib25)ï¼‰ã€‚é‰´äºLLMçš„æ·±å±‚ç»“æ„å’Œä¼—å¤šå‚æ•°ï¼ŒPTQå› å…¶åœ¨æ—¶é—´å’Œèµ„æºæœ‰é™çš„åœºæ™¯ä¸‹èƒ½å¤Ÿå¿«é€Ÿè¿›è¡Œé‡åŒ–å¤„ç†è€Œè„±é¢–è€Œå‡ºï¼ˆZhu
    et al., [2023](#bib.bib53)ï¼‰ã€‚
- en: 'Despite the success of previous PTQ methods in 8-bit and 4-bit quantizationÂ (Dettmers
    etÂ al., [2022](#bib.bib9), [2023b](#bib.bib11); Frantar etÂ al., [2022](#bib.bib17);
    Xiao etÂ al., [2023](#bib.bib46); Frantar & Alistarh, [2022](#bib.bib15)), the
    expanding size of LLMs demands more aggressive quantization approachesÂ (Shang
    etÂ al., [2023](#bib.bib39)). Neural network binarization, which reduces the weight
    bit-width to only 1 bit, is a promising approachÂ (Helwegen etÂ al., [2019](#bib.bib18);
    Qin etÂ al., [2020](#bib.bib32), [2023](#bib.bib34)). However, as depicted in FigureÂ [1](#S1.F1
    "Figure 1 â€£ 1 Introduction â€£ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs"), current advanced PTQ methods for LLMs exhibit a performance collapse
    under ultra-low bit ($\leqslant$3 bits) quantization. This phenomenon can be attributed
    to the significant difference between quantized and original weights. Even the
    recent binary PTQ method for LLMs, PB-LLMÂ (Shang etÂ al., [2023](#bib.bib39)),
    only maintains a perplexity metric of around 800 with an average weight of 1.7
    bits. This observation underscores the challenges existing PTQ methods face in
    promoting the weight binarization of LLMs.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 'å°½ç®¡ä¹‹å‰çš„PTQæ–¹æ³•åœ¨8-bitå’Œ4-bité‡åŒ–ä¸­å–å¾—äº†æˆåŠŸï¼ˆDettmers et al., [2022](#bib.bib9), [2023b](#bib.bib11);
    Frantar et al., [2022](#bib.bib17); Xiao et al., [2023](#bib.bib46); Frantar &
    Alistarh, [2022](#bib.bib15)ï¼‰ï¼ŒLLMçš„ä¸æ–­æ‰©å±•çš„è§„æ¨¡è¦æ±‚æ›´åŠ æ¿€è¿›çš„é‡åŒ–æ–¹æ³•ï¼ˆShang et al., [2023](#bib.bib39)ï¼‰ã€‚ç¥ç»ç½‘ç»œäºŒå€¼åŒ–å°†æƒé‡ä½å®½å‡å°‘åˆ°ä»…1ä½ï¼Œæ˜¯ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ï¼ˆHelwegen
    et al., [2019](#bib.bib18); Qin et al., [2020](#bib.bib32), [2023](#bib.bib34)ï¼‰ã€‚ç„¶è€Œï¼Œå¦‚å›¾[1](#S1.F1
    "Figure 1 â€£ 1 Introduction â€£ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs")æ‰€ç¤ºï¼Œå½“å‰é’ˆå¯¹LLMçš„å…ˆè¿›PTQæ–¹æ³•åœ¨è¶…ä½ä½ï¼ˆ$\leqslant$3ä½ï¼‰é‡åŒ–ä¸‹è¡¨ç°å‡ºæ€§èƒ½å´©æºƒã€‚è¿™ä¸€ç°è±¡å¯ä»¥å½’å› äºé‡åŒ–æƒé‡ä¸åŸå§‹æƒé‡ä¹‹é—´çš„æ˜¾è‘—å·®å¼‚ã€‚å³ä½¿æ˜¯æœ€è¿‘çš„LLMäºŒå€¼åŒ–PTQæ–¹æ³•PB-LLMï¼ˆShang
    et al., [2023](#bib.bib39)ï¼‰ï¼Œä¹Ÿä»…ç»´æŒäº†å¤§çº¦800çš„å›°æƒ‘åº¦æŒ‡æ ‡ï¼Œå¹³å‡æƒé‡ä¸º1.7ä½ã€‚è¿™ä¸€è§‚å¯Ÿç»“æœçªæ˜¾äº†ç°æœ‰PTQæ–¹æ³•åœ¨æ¨åŠ¨LLMæƒé‡äºŒå€¼åŒ–æ–¹é¢é¢ä¸´çš„æŒ‘æˆ˜ã€‚'
- en: 'In pursuit of this goal, we conducted an empirical study to analyze the distribution
    of pre-trained weights in LLMs. The findings derived from this study are presented
    in Appendix [G](#A7 "Appendix G Magnitude and Hessian Distribution of LLMs â€£ BiLLM:
    Pushing the Limit of Post-Training Quantization for LLMs"), revealing two key
    observations:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å®è¯ç ”ç©¶ä»¥åˆ†æLLMä¸­é¢„è®­ç»ƒæƒé‡çš„åˆ†å¸ƒã€‚ç ”ç©¶ç»“æœåœ¨é™„å½•[G](#A7 "Appendix G Magnitude and
    Hessian Distribution of LLMs â€£ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs")ä¸­å±•ç¤ºï¼Œæ­ç¤ºäº†ä¸¤ä¸ªå…³é”®è§‚å¯Ÿç»“æœï¼š'
- en: â€¢
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'The second-order Hessian matrix of weights demonstrates an exceptionally long-tail
    distribution and is often used to measure the importance of weight elements in
    neural networksÂ (LeCun etÂ al., [1989](#bib.bib21); Dong etÂ al., [2019](#bib.bib12)).
    As depicted in FigureÂ [2](#S1.F2 "Figure 2 â€£ 1 Introduction â€£ BiLLM: Pushing the
    Limit of Post-Training Quantization for LLMs"), a small fraction of weights elements
    possesses significantly high Hessian values, substantially influencing the layer
    output. In contrast, most Hessian values cluster around 0.'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æƒé‡çš„äºŒé˜¶ Hessian çŸ©é˜µå±•ç¤ºäº†ä¸€ä¸ªæé•¿å°¾åˆ†å¸ƒï¼Œé€šå¸¸ç”¨äºè¡¡é‡ç¥ç»ç½‘ç»œä¸­æƒé‡å…ƒç´ çš„é‡è¦æ€§ï¼ˆLeCun ç­‰ï¼Œ[1989](#bib.bib21)ï¼›Dong
    ç­‰ï¼Œ[2019](#bib.bib12)ï¼‰ã€‚å¦‚å›¾[2](#S1.F2 "å›¾ 2 â€£ 1 ä»‹ç» â€£ BiLLMï¼šæ¨åŠ¨ LLMs åè®­ç»ƒé‡åŒ–çš„æé™") æ‰€ç¤ºï¼Œå°‘é‡çš„æƒé‡å…ƒç´ å…·æœ‰æ˜¾è‘—é«˜çš„
    Hessian å€¼ï¼Œå¯¹å±‚è¾“å‡ºæœ‰æ˜¾è‘—å½±å“ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤§å¤šæ•° Hessian å€¼èšé›†åœ¨ 0 é™„è¿‘ã€‚
- en: â€¢
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'The density distribution of weight magnitudes in LLMs follows a bell-shaped
    pattern. This bell-shaped distribution exhibits a significant resemblance to both
    the Gaussian or Laplace distribution in terms of its characteristicsÂ (Blundell
    etÂ al., [2015](#bib.bib3)). FigureÂ [2](#S1.F2 "Figure 2 â€£ 1 Introduction â€£ BiLLM:
    Pushing the Limit of Post-Training Quantization for LLMs") illustrates that most
    weight values cluster around zero with a non-uniform bell-shaped distribution.'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLMs ä¸­æƒé‡å¤§å°çš„å¯†åº¦åˆ†å¸ƒéµå¾ªé’Ÿå½¢æ¨¡å¼ã€‚è¿™ç§é’Ÿå½¢åˆ†å¸ƒåœ¨ç‰¹å¾ä¸Šä¸é«˜æ–¯åˆ†å¸ƒæˆ–æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒéå¸¸ç›¸ä¼¼ï¼ˆBlundell ç­‰ï¼Œ[2015](#bib.bib3)ï¼‰ã€‚å›¾[2](#S1.F2
    "å›¾ 2 â€£ 1 ä»‹ç» â€£ BiLLMï¼šæ¨åŠ¨ LLMs åè®­ç»ƒé‡åŒ–çš„æé™") è¯´æ˜å¤§å¤šæ•°æƒé‡å€¼é›†ä¸­åœ¨é›¶é™„è¿‘ï¼Œå‘ˆç°éå‡åŒ€çš„é’Ÿå½¢åˆ†å¸ƒã€‚
- en: '![Refer to caption](img/a0c5162c44aa45e79d77edf21a72ebd1.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/a0c5162c44aa45e79d77edf21a72ebd1.png)'
- en: 'Figure 2: The Hessian metrics (sensitivity) and magnitude (value) of weights
    in LLMs. The weights of different layers in LLMs are characterized by bell-shaped
    distribution, accompanied by a few salient values.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2ï¼šLLMs ä¸­æƒé‡çš„ Hessian åº¦é‡ï¼ˆæ•æ„Ÿæ€§ï¼‰å’Œå¹…åº¦ï¼ˆå€¼ï¼‰ã€‚LLMs ä¸­ä¸åŒå±‚çš„æƒé‡ç‰¹å¾ä¸ºé’Ÿå½¢åˆ†å¸ƒï¼Œå¹¶ä¼´æœ‰å°‘æ•°æ˜¾è‘—å€¼ã€‚
- en: 'The above implies: a) A minority of weights play an important role in LLMs,
    whereas the majority of weights exhibit characteristics of redundancyÂ (Shang etÂ al.,
    [2023](#bib.bib39); Dettmers etÂ al., [2023b](#bib.bib11)); b) With the most aggressive
    bit-width, binarization incurs most severe error among quantization under bell-shaped
    distributions in LLMsÂ (Jacob etÂ al., [2018](#bib.bib19)).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°å†…å®¹æ„å‘³ç€ï¼ša) åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ï¼Œå°‘æ•°æƒé‡åœ¨æ¨¡å‹ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œè€Œå¤§å¤šæ•°æƒé‡åˆ™è¡¨ç°å‡ºå†—ä½™ç‰¹å¾ï¼ˆShang ç­‰ï¼Œ[2023](#bib.bib39)ï¼›Dettmers
    ç­‰ï¼Œ[2023b](#bib.bib11)ï¼‰ï¼›b) åœ¨ LLMs çš„é’Ÿå½¢åˆ†å¸ƒä¸‹ï¼Œæœ€å…·æ”»å‡»æ€§çš„ä½å®½ä¸‹é‡åŒ–ä¼šå¯¼è‡´æœ€ä¸¥é‡çš„è¯¯å·®ï¼ˆJacob ç­‰ï¼Œ[2018](#bib.bib19)ï¼‰ã€‚
- en: 'Motivated by the above observation, we propose a novel 1-bit PTQ framework
    for LLMs, namely BiLLM, incorporating two core designs to achieve highly accurate
    weight binarization. First, guided by the Hessian-based metric, we select the
    salient weights structurally (FigureÂ [3](#S2.F3 "Figure 3 â€£ 2 Related Works â€£
    BiLLM: Pushing the Limit of Post-Training Quantization for LLMs") upper-right)
    to achieve a trade-off between accuracy and storage savings and develop a residual
    approximation to maximize the restoration of salient weights with highly dynamic
    range. Second, for the remaining non-salient weights (FigureÂ [3](#S2.F3 "Figure
    3 â€£ 2 Related Works â€£ BiLLM: Pushing the Limit of Post-Training Quantization for
    LLMs") lower-right), we design an optimal splitting binarization strategy, where
    a meticulous search process is applied to determine an optimal break-point for
    weight distribution and binarization of the segments is then processed separately
    to minimize binarization errors. Moreover, BiLLM incorporates error compensation
    on a block-wise basis by default following existing common practicesÂ (Frantar
    etÂ al., [2022](#bib.bib17); Shang etÂ al., [2023](#bib.bib39)), which further reduces
    quantization error.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºä¸Šè¿°è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ 1-bit PTQ æ¡†æ¶ï¼Œç”¨äº LLMsï¼Œå³ BiLLMï¼Œç»“åˆäº†ä¸¤ä¸ªæ ¸å¿ƒè®¾è®¡ä»¥å®ç°é«˜åº¦å‡†ç¡®çš„æƒé‡äºŒå€¼åŒ–ã€‚é¦–å…ˆï¼Œåœ¨ Hessian
    åŸºäºåº¦é‡çš„æŒ‡å¯¼ä¸‹ï¼Œæˆ‘ä»¬ç»“æ„æ€§åœ°é€‰æ‹©æ˜¾è‘—æƒé‡ï¼ˆå›¾[3](#S2.F3 "å›¾ 3 â€£ 2 ç›¸å…³å·¥ä½œ â€£ BiLLMï¼šæ¨åŠ¨ LLMs åè®­ç»ƒé‡åŒ–çš„æé™") å³ä¸Šï¼‰ä»¥å®ç°å‡†ç¡®æ€§ä¸å­˜å‚¨èŠ‚çœä¹‹é—´çš„æƒè¡¡ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªæ®‹å·®è¿‘ä¼¼æ–¹æ³•ä»¥æœ€å¤§åŒ–æ˜¾è‘—æƒé‡çš„æ¢å¤ï¼ŒåŒæ—¶å…·æœ‰é«˜åº¦åŠ¨æ€èŒƒå›´ã€‚å…¶æ¬¡ï¼Œå¯¹äºå‰©ä½™çš„éæ˜¾è‘—æƒé‡ï¼ˆå›¾[3](#S2.F3
    "å›¾ 3 â€£ 2 ç›¸å…³å·¥ä½œ â€£ BiLLMï¼šæ¨åŠ¨ LLMs åè®­ç»ƒé‡åŒ–çš„æé™") å³ä¸‹ï¼‰ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æœ€ä¼˜çš„æ‹†åˆ†äºŒå€¼åŒ–ç­–ç•¥ï¼Œå…¶ä¸­åº”ç”¨äº†ç²¾ç»†çš„æœç´¢è¿‡ç¨‹æ¥ç¡®å®šæƒé‡åˆ†å¸ƒçš„æœ€ä½³æ–­ç‚¹ï¼Œç„¶ååˆ†åˆ«å¤„ç†å„æ®µçš„äºŒå€¼åŒ–ï¼Œä»¥æœ€å°åŒ–äºŒå€¼åŒ–è¯¯å·®ã€‚æ­¤å¤–ï¼ŒBiLLM
    é»˜è®¤éµå¾ªç°æœ‰çš„å¸¸è§åšæ³•ï¼ˆFrantar ç­‰ï¼Œ[2022](#bib.bib17)ï¼›Shang ç­‰ï¼Œ[2023](#bib.bib39)ï¼‰è¿›è¡Œå—çº§è¯¯å·®è¡¥å¿ï¼Œè¿™è¿›ä¸€æ­¥å‡å°‘äº†é‡åŒ–è¯¯å·®ã€‚
- en: Extensive experiments demonstrate that BiLLM achieve the state-of-the-art (SOTA)
    performance for LLMs across multiple LLM families on various evaluation metrics,
    and first achieves extremely compact 1.07$\sim$1.11 bit-width in average for the
    PTQ binarization. For example, on the Wikitext2(Merity etÂ al., [2016](#bib.bib28))
    metric, BiLLM achieved perplexities of 8.49 and 8.41 with only 1.08-bit weights
    on LLaMA-65BÂ (Touvron etÂ al., [2023a](#bib.bib40))and LLaMA2-70BÂ (Touvron etÂ al.,
    [2023b](#bib.bib41)), respectively, even surpassing the 9.34 performance of the
    FP16 OPT-66BÂ (Zhang etÂ al., [2022](#bib.bib51)).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§é‡å®éªŒè¡¨æ˜ï¼ŒBiLLM åœ¨å„ç§è¯„ä¼°æŒ‡æ ‡ä¸Šå¯¹å¤šä¸ª LLM å®¶æ—å®ç°äº†æœ€æ–°çš„ (SOTA) æ€§èƒ½ï¼Œå¹¶é¦–æ¬¡åœ¨ PTQ äºŒå€¼åŒ–ä¸­å®ç°äº†å¹³å‡ 1.07$\sim$1.11
    ä½å®½çš„æè‡´ç´§å‡‘æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨ Wikitext2 (Merity et al., [2016](#bib.bib28)) æŒ‡æ ‡ä¸Šï¼ŒBiLLM åœ¨ LLaMA-65B
    (Touvron et al., [2023a](#bib.bib40)) å’Œ LLaMA2-70B (Touvron et al., [2023b](#bib.bib41))
    ä¸Šä»…ä½¿ç”¨ 1.08 ä½æƒé‡åˆ†åˆ«è¾¾åˆ°äº† 8.49 å’Œ 8.41 çš„å›°æƒ‘åº¦ï¼Œç”šè‡³è¶…è¶Šäº† FP16 OPT-66B (Zhang et al., [2022](#bib.bib51))
    çš„ 9.34 æ€§èƒ½ã€‚
- en: 2 Related Works
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 ç›¸å…³å·¥ä½œ
- en: '![Refer to caption](img/0d3e934ddf53468c1d4f7d992dc6f337.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/0d3e934ddf53468c1d4f7d992dc6f337.png)'
- en: 'Figure 3: Schematic of the PTQ binarization framework for LLMs. The left side
    shows the structure of the Transformer block after binarization. The right side
    shows the binarization process of BiLLM, which consists of two parts, Residual
    Approximation for salient weights and Bell-shaped Splitting for non-salient weights.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3ï¼šLLMs çš„ PTQ äºŒå€¼åŒ–æ¡†æ¶ç¤ºæ„å›¾ã€‚å·¦ä¾§å±•ç¤ºäº†äºŒå€¼åŒ–åçš„ Transformer å—çš„ç»“æ„ã€‚å³ä¾§å±•ç¤ºäº† BiLLM çš„äºŒå€¼åŒ–è¿‡ç¨‹ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªéƒ¨åˆ†ï¼šç”¨äºæ˜¾è‘—æƒé‡çš„æ®‹å·®è¿‘ä¼¼å’Œç”¨äºéæ˜¾è‘—æƒé‡çš„é’Ÿå½¢åˆ†å‰²ã€‚
- en: 2.1 Large Language Model Quantization
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 å¤§å‹è¯­è¨€æ¨¡å‹é‡åŒ–
- en: Quantization maps high-precision parameters to a discrete range. This method,
    which compresses parameters without altering the model structure, effectively
    reduces the storage and computational overhead of deep neural networks. Recent
    work has successfully applied QAT and PTQ to LLMs. QAT, through a quantization-aware
    retraining strategy, better preserves the performance of quantized models. LLM-QATÂ (Liu
    etÂ al., [2023](#bib.bib26)) addressed data barrier issues in QAT training through
    data-free distillation. However, for LLMs with extremely large parameter sizes,
    the cost of retraining is prohibitively high and inefficient. Therefore, techniques
    such as QLoRAÂ (Dettmers etÂ al., [2023a](#bib.bib10)) focus on parameter-efficient
    fine-tuning (PEFT) methods for quantizing LLMs, enhancing the efficiency of QAT.
    Nevertheless, even these efficient fine-tuning quantization strategies require
    over 24 hours of GPU time.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: é‡åŒ–å°†é«˜ç²¾åº¦å‚æ•°æ˜ å°„åˆ°ç¦»æ•£èŒƒå›´ã€‚è¿™ç§æ–¹æ³•åœ¨ä¸æ”¹å˜æ¨¡å‹ç»“æ„çš„æƒ…å†µä¸‹å‹ç¼©å‚æ•°ï¼Œæœ‰æ•ˆå‡å°‘äº†æ·±åº¦ç¥ç»ç½‘ç»œçš„å­˜å‚¨å’Œè®¡ç®—å¼€é”€ã€‚è¿‘æœŸå·¥ä½œæˆåŠŸå°† QAT å’Œ PTQ åº”ç”¨äº
    LLMsã€‚QAT é€šè¿‡é‡åŒ–æ„ŸçŸ¥å†è®­ç»ƒç­–ç•¥ï¼Œæ›´å¥½åœ°ä¿æŒäº†é‡åŒ–æ¨¡å‹çš„æ€§èƒ½ã€‚LLM-QAT (Liu et al., [2023](#bib.bib26)) é€šè¿‡æ— æ•°æ®è’¸é¦è§£å†³äº†
    QAT è®­ç»ƒä¸­çš„æ•°æ®éšœç¢é—®é¢˜ã€‚ç„¶è€Œï¼Œå¯¹äºå‚æ•°è§„æ¨¡æå¤§çš„ LLMsï¼Œé‡æ–°è®­ç»ƒçš„æˆæœ¬éå¸¸é«˜ä¸”æ•ˆç‡ä½ã€‚å› æ­¤ï¼Œåƒ QLoRA (Dettmers et al., [2023a](#bib.bib10))
    è¿™æ ·çš„æŠ€æœ¯ä¸“æ³¨äºé‡åŒ– LLMs çš„å‚æ•°é«˜æ•ˆå¾®è°ƒ (PEFT) æ–¹æ³•ï¼Œä»è€Œæé«˜ QAT çš„æ•ˆç‡ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå³ä¾¿æ˜¯è¿™äº›é«˜æ•ˆçš„å¾®è°ƒé‡åŒ–ç­–ç•¥ä¹Ÿéœ€è¦è¶…è¿‡ 24 å°æ—¶çš„
    GPU æ—¶é—´ã€‚
- en: Therefore, the PTQ strategy has become a significant option for quantizing LLMs
    efficiently. Works like BRECQÂ (Li etÂ al., [2021](#bib.bib23)), ZerqQuantÂ ([Yao
    etÂ al.,](#bib.bib47) ) and LLM.int8()Â (Dettmers etÂ al., [2022](#bib.bib9)) enhance
    quantization accuracy by adding additional grouping labels for custom quantization
    blocks. Other studies adopt a feature segmentation strategy, such as PB-LLMÂ (Shang
    etÂ al., [2023](#bib.bib39)) and SpQRÂ (Dettmers etÂ al., [2023b](#bib.bib11)). They
    preserve the bit-width of outlier features or those with higher quantization errors
    to FP16 or INT8, mitigating the precision loss due to quantization. GPTQÂ (Frantar
    etÂ al., [2022](#bib.bib17)) employs a more precise quantization framework, reducing
    the block quantization errors of LLMs through Hessian-based second-order error
    compensationÂ (Frantar & Alistarh, [2022](#bib.bib15)), achieving commendable performance
    in low-bits (4 bits) quantization. SmoothquantÂ (Xiao etÂ al., [2023](#bib.bib46))
    introduces a strategy of scaling weight and activation outliers to simplify quantization.
    Subsequently, AWQÂ (Lin etÂ al., [2023](#bib.bib25)) and OWQÂ (Lee etÂ al., [2023](#bib.bib22))
    also proposed scale transformations of more crucial weight channels for activation
    features, preserving their information representation capacity.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼ŒPTQ ç­–ç•¥å·²ç»æˆä¸ºæœ‰æ•ˆé‡åŒ– LLM çš„é‡è¦é€‰é¡¹ã€‚åƒ BRECQï¼ˆLi ç­‰ï¼Œ[2021](#bib.bib23)ï¼‰ã€ZerqQuantï¼ˆ[Yao
    ç­‰](#bib.bib47)ï¼‰å’Œ LLM.int8()ï¼ˆDettmers ç­‰ï¼Œ[2022](#bib.bib9)ï¼‰ç­‰å·¥ä½œé€šè¿‡ä¸ºè‡ªå®šä¹‰é‡åŒ–å—æ·»åŠ é¢å¤–çš„åˆ†ç»„æ ‡ç­¾æ¥æé«˜é‡åŒ–ç²¾åº¦ã€‚å…¶ä»–ç ”ç©¶åˆ™é‡‡ç”¨ç‰¹å¾åˆ†å‰²ç­–ç•¥ï¼Œå¦‚
    PB-LLMï¼ˆShang ç­‰ï¼Œ[2023](#bib.bib39)ï¼‰å’Œ SpQRï¼ˆDettmers ç­‰ï¼Œ[2023b](#bib.bib11)ï¼‰ã€‚å®ƒä»¬å°†å…·æœ‰æ›´é«˜é‡åŒ–è¯¯å·®æˆ–å¼‚å¸¸ç‰¹å¾çš„ä½å®½ä¿æŒåœ¨
    FP16 æˆ– INT8ï¼Œä»¥å‡è½»ç”±äºé‡åŒ–é€ æˆçš„ç²¾åº¦æŸå¤±ã€‚GPTQï¼ˆFrantar ç­‰ï¼Œ[2022](#bib.bib17)ï¼‰é‡‡ç”¨äº†æ›´ç²¾ç¡®çš„é‡åŒ–æ¡†æ¶ï¼Œé€šè¿‡åŸºäº
    Hessian çš„äºŒé˜¶è¯¯å·®è¡¥å¿ï¼ˆFrantar & Alistarhï¼Œ[2022](#bib.bib15)ï¼‰å‡å°‘ LLM çš„å—é‡åŒ–è¯¯å·®ï¼Œåœ¨ä½æ¯”ç‰¹ï¼ˆ4 ä½ï¼‰é‡åŒ–ä¸­è¡¨ç°å‡ºè‰²ã€‚Smoothquantï¼ˆXiao
    ç­‰ï¼Œ[2023](#bib.bib46)ï¼‰å¼•å…¥äº†ç¼©æ”¾æƒé‡å’Œæ¿€æ´»å¼‚å¸¸å€¼ä»¥ç®€åŒ–é‡åŒ–çš„ç­–ç•¥ã€‚éšåï¼ŒAWQï¼ˆLin ç­‰ï¼Œ[2023](#bib.bib25)ï¼‰å’Œ OWQï¼ˆLee
    ç­‰ï¼Œ[2023](#bib.bib22)ï¼‰ä¹Ÿæå‡ºäº†å¯¹æ›´é‡è¦çš„æƒé‡é€šé“è¿›è¡Œç¼©æ”¾å˜æ¢ä»¥ä¿æŒå…¶ä¿¡æ¯è¡¨ç¤ºèƒ½åŠ›ã€‚
- en: 2.2 Network Binarization
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 ç½‘ç»œäºŒå€¼åŒ–
- en: 'Binarized compression can quantize parameters to only 1 bit, expressed as $\pm$1\.
    In forward propagation, the sign function is used to binarize the original parameter
    tensor:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: äºŒå€¼åŒ–å‹ç¼©å¯ä»¥å°†å‚æ•°é‡åŒ–ä¸ºä»… 1 ä½ï¼Œè¡¨ç¤ºä¸º $\pm$1ã€‚åœ¨å‰å‘ä¼ æ’­ä¸­ï¼Œä½¿ç”¨ç¬¦å·å‡½æ•°å°†åŸå§‹å‚æ•°å¼ é‡äºŒå€¼åŒ–ï¼š
- en: '|  | $\vspace{-0.1in}\mathbf{W}_{b}=\alpha\cdot\operatorname{sign}(\mathbf{W}_{f}),$
    |  | (1) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | $\vspace{-0.1in}\mathbf{W}_{b}=\alpha\cdot\operatorname{sign}(\mathbf{W}_{f}),$
    |  | (1) |'
- en: '|  | $\operatorname{sign}(x)=\begin{cases}1&amp;\text{if $x\geq 0$},\\ -1&amp;\text{others}.\end{cases}$
    |  | (2) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\operatorname{sign}(x)=\begin{cases}1&\text{å¦‚æœ $x\geq 0$},\\ -1&\text{å…¶ä»–}.\end{cases}$
    |  | (2) |'
- en: where $\mathbf{W}_{f}\in\mathbb{R}^{n\times m}$.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\mathbf{W}_{f}\in\mathbb{R}^{n\times m}$ã€‚
- en: Most previous binarization works adopt a framework based on QAT for quantizationÂ (Qin
    etÂ al., [2023](#bib.bib34)). Straight through estimator (STE)Â (Bengio etÂ al.,
    [2013](#bib.bib1)) is deployed to address the issue of gradient vanishing caused
    by the $\operatorname{sign}(\cdot)$. DoReFa-NetÂ (Zhou etÂ al., [2016](#bib.bib52))
    further expands upon XNOR-Net, employing quantized gradients to accelerate network
    training. Group segmentation is also applied in binarization tasks, with SyqÂ (Faraone
    etÂ al., [2018](#bib.bib14)) utilizing network weight to the small size of groups
    for minimizing binarization errors.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹‹å‰çš„å¤§å¤šæ•°äºŒå€¼åŒ–å·¥ä½œé‡‡ç”¨äº†åŸºäº QAT çš„é‡åŒ–æ¡†æ¶ï¼ˆQin ç­‰ï¼Œ[2023](#bib.bib34)ï¼‰ã€‚é‡‡ç”¨ç›´é€šä¼°è®¡å™¨ï¼ˆSTEï¼‰ï¼ˆBengio ç­‰ï¼Œ[2013](#bib.bib1)ï¼‰æ¥è§£å†³ç”±
    $\operatorname{sign}(\cdot)$ å¼•èµ·çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚DoReFa-Netï¼ˆZhou ç­‰ï¼Œ[2016](#bib.bib52)ï¼‰åœ¨
    XNOR-Net çš„åŸºç¡€ä¸Šè¿›ä¸€æ­¥æ‰©å±•ï¼Œä½¿ç”¨é‡åŒ–æ¢¯åº¦åŠ é€Ÿç½‘ç»œè®­ç»ƒã€‚äºŒå€¼åŒ–ä»»åŠ¡ä¸­ä¹Ÿåº”ç”¨äº†ç»„åˆ†å‰²ç­–ç•¥ï¼Œå…¶ä¸­ Syqï¼ˆFaraone ç­‰ï¼Œ[2018](#bib.bib14)ï¼‰åˆ©ç”¨ç½‘ç»œæƒé‡å°†å°ç»„çš„å¤§å°æœ€å°åŒ–ï¼Œä»¥å‡å°‘äºŒå€¼åŒ–è¯¯å·®ã€‚
- en: Based on the successful application of binarization in TransformersÂ (Wang etÂ al.,
    [2023](#bib.bib43)) and BertÂ (Qin etÂ al., [2022](#bib.bib33)), we believe that
    the binarization of LLMs is filled with potential. PB-LLMÂ (Shang etÂ al., [2023](#bib.bib39))
    investigates the impact of binarized QAT and PTQ strategies on LLMs, but it is
    necessary to retain a significant proportion (over 30%) of the weights at 8 bits
    to enable LLMs to produce reasonable answers. Due to the presence of a large amount
    of INT8, LLMs still have a relatively high average bit-width. To address this
    issue, we proposed BiLLM, which aims to push the limit of PTQ binarization for
    LLMs.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºäºŒå€¼åŒ–åœ¨å˜å‹å™¨ï¼ˆWang et al., [2023](#bib.bib43)ï¼‰å’ŒBertï¼ˆQin et al., [2022](#bib.bib33)ï¼‰ä¸­çš„æˆåŠŸåº”ç”¨ï¼Œæˆ‘ä»¬è®¤ä¸ºLLMsçš„äºŒå€¼åŒ–å……æ»¡æ½œåŠ›ã€‚PB-LLMï¼ˆShang
    et al., [2023](#bib.bib39)ï¼‰ç ”ç©¶äº†äºŒå€¼åŒ–QATå’ŒPTQç­–ç•¥å¯¹LLMsçš„å½±å“ï¼Œä½†éœ€è¦ä¿ç•™è¶…è¿‡30%çš„8ä½æƒé‡ï¼Œä»¥ä½¿LLMsèƒ½å¤Ÿäº§ç”Ÿåˆç†çš„ç­”æ¡ˆã€‚ç”±äºå­˜åœ¨å¤§é‡INT8ï¼ŒLLMsä»å…·æœ‰ç›¸å¯¹è¾ƒé«˜çš„å¹³å‡æ¯”ç‰¹å®½åº¦ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†BiLLMï¼Œæ—¨åœ¨æ¨åŠ¨LLMs
    PTQäºŒå€¼åŒ–çš„æé™ã€‚
- en: 3 Method
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 æ–¹æ³•
- en: 'To achieve accurate binarization of LLMs, our approach is designing distinct
    binarization strategies for salient and non-salient weights. We first introduce
    the selection rules for salient weights and their binarization strategies in Section
    [3.1](#S3.SS1 "3.1 Salient Weight Binarization for LLMs â€£ 3 Method â€£ BiLLM: Pushing
    the Limit of Post-Training Quantization for LLMs"). Then, we elaborate on the
    distribution-based binarization strategy for non-salient weights in Section [3.2](#S3.SS2
    "3.2 Bell-shaped Distribution Splitting for Binarization â€£ 3 Method â€£ BiLLM: Pushing
    the Limit of Post-Training Quantization for LLMs").'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¸ºäº†å®ç°LLMsçš„å‡†ç¡®äºŒå€¼åŒ–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¯ä¸ºæ˜¾è‘—å’Œéæ˜¾è‘—æƒé‡è®¾è®¡ä¸åŒçš„äºŒå€¼åŒ–ç­–ç•¥ã€‚æˆ‘ä»¬é¦–å…ˆåœ¨ç¬¬[3.1èŠ‚](#S3.SS1 "3.1 Salient
    Weight Binarization for LLMs â€£ 3 Method â€£ BiLLM: Pushing the Limit of Post-Training
    Quantization for LLMs")ä»‹ç»æ˜¾è‘—æƒé‡çš„é€‰æ‹©è§„åˆ™åŠå…¶äºŒå€¼åŒ–ç­–ç•¥ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨ç¬¬[3.2èŠ‚](#S3.SS2 "3.2 Bell-shaped
    Distribution Splitting for Binarization â€£ 3 Method â€£ BiLLM: Pushing the Limit
    of Post-Training Quantization for LLMs")è¯¦ç»†è¯´æ˜åŸºäºåˆ†å¸ƒçš„éæ˜¾è‘—æƒé‡äºŒå€¼åŒ–ç­–ç•¥ã€‚'
- en: 3.1 Salient Weight Binarization for LLMs
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 LLMsçš„æ˜¾è‘—æƒé‡äºŒå€¼åŒ–
- en: 'In deep neural networks, not all parameters carry equal significance. Utilizing
    solely the magnitude of the weights can not fully capture the impact of each element
    on the modelâ€™s performance. The Hessian metric serves as a common benchmark for
    detecting parameter sensitivityÂ (Dong etÂ al., [2019](#bib.bib12); Dettmers etÂ al.,
    [2023b](#bib.bib11), [2022](#bib.bib9)). We thus leverage the Hessian matrix to
    assess the salience of parameters in each under-binarized layer. We implement
    an optimized computation process to derive weight sensitivity, which allows us
    to obtain the importance metric of parameters without compromising efficiency:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ·±åº¦ç¥ç»ç½‘ç»œä¸­ï¼Œå¹¶éæ‰€æœ‰å‚æ•°éƒ½å…·æœ‰ç›¸åŒçš„é‡è¦æ€§ã€‚ä»…åˆ©ç”¨æƒé‡çš„å¹…åº¦ä¸èƒ½å®Œå…¨æ•æ‰æ¯ä¸ªå…ƒç´ å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚Hessianåº¦é‡ä½œä¸ºæ£€æµ‹å‚æ•°æ•æ„Ÿåº¦çš„å¸¸è§åŸºå‡†ï¼ˆDong
    et al., [2019](#bib.bib12); Dettmers et al., [2023b](#bib.bib11), [2022](#bib.bib9)ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åˆ©ç”¨HessiançŸ©é˜µè¯„ä¼°æ¯ä¸ªæœªäºŒå€¼åŒ–å±‚ä¸­å‚æ•°çš„é‡è¦æ€§ã€‚æˆ‘ä»¬å®æ–½äº†ä¼˜åŒ–çš„è®¡ç®—è¿‡ç¨‹æ¥å¯¼å‡ºæƒé‡æ•æ„Ÿåº¦ï¼Œä»è€Œåœ¨ä¸å½±å“æ•ˆç‡çš„æƒ…å†µä¸‹è·å–å‚æ•°çš„é‡è¦æ€§åº¦é‡ï¼š
- en: '|  | $s_{i}=\frac{w_{i}^{2}}{[\mathbf{H}^{-1}]_{ii}^{2}},$ |  | (3) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | $s_{i}=\frac{w_{i}^{2}}{[\mathbf{H}^{-1}]_{ii}^{2}},$ |  | (3) |'
- en: where $\mathbf{H}$ serves as a criterion for assessing the significance of weight
    elements and is used as a feature indicator for structured selection.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­$\mathbf{H}$ä½œä¸ºè¯„ä¼°æƒé‡å…ƒç´ é‡è¦æ€§çš„æ ‡å‡†ï¼Œå¹¶ç”¨ä½œç»“æ„é€‰æ‹©çš„ç‰¹å¾æŒ‡ç¤ºç¬¦ã€‚
- en: 'Structural Searching Selection. Utilizing an unstructured selection enables
    the coverage of all salient elements. However, it requires the implementation
    of an additional 1-bit bitmap indexÂ (Chan & Ioannidis, [1998](#bib.bib4)), leading
    to increased average bit-width. This balance is inefficient, especially for Hessian
    outlier weights that constitute a mere 1-5% of the totalÂ (Yao etÂ al., [2023](#bib.bib48)).
    In our analysis of sensitivity distribution within LLMs, we discovered that the
    majority of the weightsâ€™ sensitive Hessian values are predominantly concentrated
    in specific columns or rows (Appendix [G](#A7 "Appendix G Magnitude and Hessian
    Distribution of LLMs â€£ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs")). This pattern is attributed to the convergence effects inherent in
    the multi-head self-attention mechanism of these models and further motivates
    us to implement a structured approach for selecting salient weights, for reducing
    the additional bitmap. Given that BiLLM employs a per-channel (or per-row) type
    of binarization, we determine salience through a per-column segmentation on the
    whole weight matrix.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æ„åŒ–æœç´¢é€‰æ‹©ã€‚åˆ©ç”¨éç»“æ„åŒ–é€‰æ‹©å¯ä»¥è¦†ç›–æ‰€æœ‰æ˜¾è‘—å…ƒç´ ã€‚ç„¶è€Œï¼Œè¿™éœ€è¦å®ç°é¢å¤–çš„ 1 ä½ä½å›¾ç´¢å¼•ï¼ˆChan & Ioannidis, [1998](#bib.bib4)ï¼‰ï¼Œå¯¼è‡´å¹³å‡ä½å®½å¢åŠ ã€‚è¿™ç§å¹³è¡¡æ•ˆç‡ä½ä¸‹ï¼Œç‰¹åˆ«æ˜¯å¯¹äºæ„æˆæ€»æ•°ä»…
    1-5% çš„ Hessian ç¦»ç¾¤æƒé‡ï¼ˆYao et al., [2023](#bib.bib48)ï¼‰ã€‚åœ¨å¯¹ LLMs çš„æ•æ„Ÿæ€§åˆ†å¸ƒåˆ†æä¸­ï¼Œæˆ‘ä»¬å‘ç°å¤§å¤šæ•°æƒé‡çš„æ•æ„Ÿ
    Hessian å€¼ä¸»è¦é›†ä¸­åœ¨ç‰¹å®šçš„åˆ—æˆ–è¡Œï¼ˆé™„å½• [G](#A7 "é™„å½• G LLMs çš„å¹…åº¦å’Œ Hessian åˆ†å¸ƒ â€£ BiLLMï¼šæ¨åŠ¨ LLMs åè®­ç»ƒé‡åŒ–çš„æé™")ï¼‰ã€‚è¿™ç§æ¨¡å¼å½’å› äºè¿™äº›æ¨¡å‹çš„å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­å›ºæœ‰çš„æ”¶æ•›æ•ˆåº”ï¼Œå¹¶è¿›ä¸€æ­¥æ¿€åŠ±æˆ‘ä»¬å®æ–½ç»“æ„åŒ–æ–¹æ³•æ¥é€‰æ‹©æ˜¾è‘—æƒé‡ï¼Œä»¥å‡å°‘é¢å¤–çš„ä½å›¾ã€‚é‰´äº
    BiLLM é‡‡ç”¨çš„æ˜¯æ¯é€šé“ï¼ˆæˆ–æ¯è¡Œï¼‰ç±»å‹çš„äºŒå€¼åŒ–ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹æ•´ä¸ªæƒé‡çŸ©é˜µè¿›è¡ŒæŒ‰åˆ—åˆ†å‰²æ¥ç¡®å®šæ˜¾è‘—æ€§ã€‚
- en: 'We organize the column salience in descending order and introduce an optimized
    search algorithm aimed at minimizing quantization error, which in turn determines
    the number of columns within the salient group. To elaborate on this methodology,
    we initially define the objective of binarization quantization, grounded on EquationÂ ([1](#S2.E1
    "Equation 1 â€£ 2.2 Network Binarization â€£ 2 Related Works â€£ BiLLM: Pushing the
    Limit of Post-Training Quantization for LLMs")):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åˆ—æ˜¾è‘—æ€§æŒ‰é™åºæ’åˆ—ï¼Œå¹¶å¼•å…¥ä¸€ç§ä¼˜åŒ–æœç´¢ç®—æ³•ï¼Œæ—¨åœ¨æœ€å°åŒ–é‡åŒ–è¯¯å·®ï¼Œä»è€Œç¡®å®šæ˜¾è‘—ç»„ä¸­çš„åˆ—æ•°ã€‚ä¸ºäº†è¯¦ç»†è¯´æ˜è¿™ä¸€æ–¹æ³•ï¼Œæˆ‘ä»¬é¦–å…ˆåŸºäºæ–¹ç¨‹å¼ ([1](#S2.E1
    "æ–¹ç¨‹ 1 â€£ 2.2 ç½‘ç»œäºŒå€¼åŒ– â€£ 2 ç›¸å…³å·¥ä½œ â€£ BiLLMï¼šæ¨åŠ¨ LLMs åè®­ç»ƒé‡åŒ–çš„æé™")) å®šä¹‰äºŒå€¼åŒ–é‡åŒ–çš„ç›®æ ‡ï¼š
- en: '|  | $\mathop{\arg\min}\limits_{\alpha,\mathbf{B}}&#124;&#124;\mathbf{W}-\alpha\mathbf{B}&#124;&#124;^{2},$
    |  | (4) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathop{\arg\min}\limits_{\alpha,\mathbf{B}}&#124;&#124;\mathbf{W}-\alpha\mathbf{B}&#124;&#124;^{2},$
    |  | (4) |'
- en: 'where $\mathbf{B}\in\{-1,+1\}^{k\times m}$. Then, the optimization function
    for selecting salient columns is defined as:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\mathbf{B}\in\{-1,+1\}^{k\times m}$ã€‚ç„¶åï¼Œé€‰æ‹©æ˜¾è‘—åˆ—çš„ä¼˜åŒ–å‡½æ•°å®šä¹‰ä¸ºï¼š
- en: '|  | $\mathop{\arg\min}\limits_{\mathbf{W}_{\text{uns}}}&#124;&#124;\mathbf{W}-(\alpha_{\text{sal}}\operatorname{sign}(\mathbf{W}_{\text{sal}})\cup\alpha_{\text{uns}}\operatorname{sign}(\mathbf{W}_{\text{uns}}))&#124;&#124;^{2},$
    |  | (5) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathop{\arg\min}\limits_{\mathbf{W}_{\text{uns}}}&#124;&#124;\mathbf{W}-(\alpha_{\text{sal}}\operatorname{sign}(\mathbf{W}_{\text{sal}})\cup\alpha_{\text{uns}}\operatorname{sign}(\mathbf{W}_{\text{uns}}))&#124;&#124;^{2},$
    |  | (5) |'
- en: where $\mathbf{W}_{\text{sal}}$.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\mathbf{W}_{\text{sal}}$ã€‚
- en: '![Refer to caption](img/580b8c3fe4a9d06096e46c60f31deb0b.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/580b8c3fe4a9d06096e46c60f31deb0b.png)'
- en: 'Figure 4: Illustration of salient weight binarization. The $\mathbf{B}_{1}$.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4ï¼šæ˜¾è‘—æƒé‡äºŒå€¼åŒ–çš„ç¤ºæ„å›¾ã€‚$\mathbf{B}_{1}$ã€‚
- en: 'Binary Residual Approximation. Salient weights are limited in quantity, yet
    exhibit significant variance when aggregated. Direct preservation of these weights
    in INT8 or FP16 formats leads to an increase in the average weight bits, undermining
    the compressive benefits of binarization. Traditional binarization methods for
    salient weights, however, result in substantial quantization errors. To that end,
    we develop a residual approximation approach for binarizing salient weights. Contrary
    to the comprehensive high-order quantizationÂ (Li etÂ al., [2017](#bib.bib24)) applied
    to the entire weight matrix, our technique minimizes binarization error through
    a second-order approximation of merely a select subset of salient weights. This
    method guarantees the precision of salient weights while simultaneously decreasing
    bit-width overhead. As illustrated in FigureÂ [4](#S3.F4 "Figure 4 â€£ 3.1 Salient
    Weight Binarization for LLMs â€£ 3 Method â€£ BiLLM: Pushing the Limit of Post-Training
    Quantization for LLMs"), this approach incorporates a recursive computation strategy
    for weight binarization compensation, applying a subsequent binarization process
    to the residuals remaining after the initial binary process. Building upon EquationÂ ([4](#S3.E4
    "Equation 4 â€£ 3.1 Salient Weight Binarization for LLMs â€£ 3 Method â€£ BiLLM: Pushing
    the Limit of Post-Training Quantization for LLMs")), we propose a redesigned residual
    approximation optimization specifically for salient weights, which is defined
    as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 'äºŒè¿›åˆ¶æ®‹å·®è¿‘ä¼¼ã€‚æ˜¾è‘—æƒé‡çš„æ•°é‡æœ‰é™ï¼Œä½†åœ¨èšåˆæ—¶è¡¨ç°å‡ºæ˜¾è‘—çš„æ–¹å·®ã€‚ç›´æ¥åœ¨INT8æˆ–FP16æ ¼å¼ä¸­ä¿å­˜è¿™äº›æƒé‡ä¼šå¢åŠ å¹³å‡æƒé‡ä½æ•°ï¼Œå‰Šå¼±äºŒå€¼åŒ–çš„å‹ç¼©æ•ˆç›Šã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„æ˜¾è‘—æƒé‡äºŒå€¼åŒ–æ–¹æ³•ä¼šå¯¼è‡´æ˜¾è‘—çš„é‡åŒ–è¯¯å·®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ç”¨äºäºŒå€¼åŒ–æ˜¾è‘—æƒé‡çš„æ®‹å·®è¿‘ä¼¼æ–¹æ³•ã€‚ä¸åº”ç”¨äºæ•´ä¸ªæƒé‡çŸ©é˜µçš„å…¨é¢é«˜é˜¶é‡åŒ–ï¼ˆLi
    et al., [2017](#bib.bib24)ï¼‰ä¸åŒï¼Œæˆ‘ä»¬çš„æŠ€æœ¯é€šè¿‡å¯¹ä»…é€‰æ‹©çš„æ˜¾è‘—æƒé‡å­é›†è¿›è¡ŒäºŒé˜¶è¿‘ä¼¼æ¥æœ€å°åŒ–äºŒå€¼åŒ–è¯¯å·®ã€‚è¿™ç§æ–¹æ³•åœ¨å‡å°‘æ¯”ç‰¹å®½åº¦å¼€é”€çš„åŒæ—¶ï¼Œä¿è¯äº†æ˜¾è‘—æƒé‡çš„ç²¾åº¦ã€‚å¦‚å›¾
    [4](#S3.F4 "Figure 4 â€£ 3.1 Salient Weight Binarization for LLMs â€£ 3 Method â€£ BiLLM:
    Pushing the Limit of Post-Training Quantization for LLMs") æ‰€ç¤ºï¼Œè¿™ç§æ–¹æ³•åŒ…æ‹¬äº†ä¸€ç§é€’å½’è®¡ç®—ç­–ç•¥ç”¨äºæƒé‡äºŒå€¼åŒ–è¡¥å¿ï¼Œåœ¨åˆå§‹äºŒè¿›åˆ¶è¿‡ç¨‹ä¹‹åï¼Œå¯¹å‰©ä½™çš„æ®‹å·®è¿›è¡Œåç»­çš„äºŒå€¼åŒ–å¤„ç†ã€‚åŸºäºæ–¹ç¨‹
    ([4](#S3.E4 "Equation 4 â€£ 3.1 Salient Weight Binarization for LLMs â€£ 3 Method
    â€£ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs"))ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸“é—¨é’ˆå¯¹æ˜¾è‘—æƒé‡çš„é‡æ–°è®¾è®¡çš„æ®‹å·®è¿‘ä¼¼ä¼˜åŒ–ï¼Œå…¶å®šä¹‰å¦‚ä¸‹ï¼š'
- en: '|  | $$\left\{\begin{array}[]{lr}\alpha_{o}^{*},\mathbf{B}_{o}^{*}=\mathop{\arg\min}\limits_{\alpha_{o},\mathbf{B}_{o}}&#124;&#124;\mathbf{W}-\alpha_{o}\mathbf{B}_{o}&#124;&#124;^{2}),\\
    \alpha_{r}^{*},\mathbf{B}_{r}^{*}=\mathop{\arg\min}\limits_{\alpha_{r},\mathbf{B}_{r}}&#124;&#124;(\mathbf{W}-\alpha_{o}^{*}\mathbf{B}_{o}^{*})-\alpha_{r}\mathbf{B}_{r}&#124;&#124;^{2}),\\'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\left\{\begin{array}[]{lr}\alpha_{o}^{*},\mathbf{B}_{o}^{*}=\mathop{\arg\min}\limits_{\alpha_{o},\mathbf{B}_{o}}&#124;&#124;\mathbf{W}-\alpha_{o}\mathbf{B}_{o}&#124;&#124;^{2}),\\
    \alpha_{r}^{*},\mathbf{B}_{r}^{*}=\mathop{\arg\min}\limits_{\alpha_{r},\mathbf{B}_{r}}&#124;&#124;(\mathbf{W}-\alpha_{o}^{*}\mathbf{B}_{o}^{*})-\alpha_{r}\mathbf{B}_{r}&#124;&#124;^{2}),\\'
- en: \end{array}\right.$$ |  | (6) |
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: \end{array}\right.$$ |  | (6) |
- en: 'where $\mathbf{B}_{o}$. We efficiently solve for the two binarized optimization
    objectives using the same solution method as in EquationÂ ([4](#S3.E4 "Equation
    4 â€£ 3.1 Salient Weight Binarization for LLMs â€£ 3 Method â€£ BiLLM: Pushing the Limit
    of Post-Training Quantization for LLMs")). Ultimately, we arrive at the following
    approximation:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ $\mathbf{B}_{o}$ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸æ–¹ç¨‹ ([4](#S3.E4 "Equation 4 â€£ 3.1 Salient Weight Binarization
    for LLMs â€£ 3 Method â€£ BiLLM: Pushing the Limit of Post-Training Quantization for
    LLMs")) ç›¸åŒçš„è§£æ³•æ¥æœ‰æ•ˆåœ°æ±‚è§£ä¸¤ä¸ªäºŒå€¼åŒ–ä¼˜åŒ–ç›®æ ‡ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å¾—å‡ºäº†ä»¥ä¸‹è¿‘ä¼¼å€¼ï¼š'
- en: '|  | $\mathbf{W}\approx\alpha_{o}^{*}\mathbf{B}_{o}^{*}+\alpha_{r}^{*}\mathbf{B}_{r}^{*}.$
    |  | (7) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{W}\approx\alpha_{o}^{*}\mathbf{B}_{o}^{*}+\alpha_{r}^{*}\mathbf{B}_{r}^{*}.$
    |  | (7) |'
- en: 'It can be easily proven that the residual approach of EquationÂ ([7](#S3.E7
    "Equation 7 â€£ 3.1 Salient Weight Binarization for LLMs â€£ 3 Method â€£ BiLLM: Pushing
    the Limit of Post-Training Quantization for LLMs")) has a lower quantization error
    than the direct one of EquationÂ ([4](#S3.E4 "Equation 4 â€£ 3.1 Salient Weight Binarization
    for LLMs â€£ 3 Method â€£ BiLLM: Pushing the Limit of Post-Training Quantization for
    LLMs")). We define the residual binarization error $\mathcal{E}$:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¯ä»¥å¾ˆå®¹æ˜“è¯æ˜æ–¹ç¨‹ ([7](#S3.E7 "Equation 7 â€£ 3.1 Salient Weight Binarization for LLMs
    â€£ 3 Method â€£ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs"))
    çš„æ®‹å·®æ–¹æ³•å…·æœ‰æ¯”æ–¹ç¨‹ ([4](#S3.E4 "Equation 4 â€£ 3.1 Salient Weight Binarization for LLMs
    â€£ 3 Method â€£ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs"))
    ç›´æ¥æ–¹æ³•æ›´ä½çš„é‡åŒ–è¯¯å·®ã€‚æˆ‘ä»¬å®šä¹‰æ®‹å·®äºŒå€¼åŒ–è¯¯å·® $\mathcal{E}$ï¼š'
- en: '|  | $\mathcal{E}_{rb}=&#124;&#124;\mathbf{W}-\alpha_{o}^{*}\mathbf{B}_{o}^{*}-\alpha_{r}^{*}\mathbf{B}_{r}^{*}&#124;&#124;^{2}.$
    |  | (8) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{E}_{rb}=&#124;&#124;\mathbf{W}-\alpha_{o}^{*}\mathbf{B}_{o}^{*}-\alpha_{r}^{*}\mathbf{B}_{r}^{*}&#124;&#124;^{2}.$
    |  | (8) |'
- en: The original binarized quantization error is calculatde as $||\mathbf{W}-\alpha_{o}^{*}\mathbf{B}_{o}^{*}||^{2}$.
    Therefore, through the method of residual approximation, we are able to further
    reduce the binary quantization error of salient weights with ultra-low bit-width
    storage compared to retaining salient weights at 8 or 16 bits.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹çš„äºŒå€¼åŒ–é‡åŒ–è¯¯å·®è®¡ç®—ä¸º $||\mathbf{W}-\alpha_{o}^{*}\mathbf{B}_{o}^{*}||^{2}$ã€‚å› æ­¤ï¼Œé€šè¿‡æ®‹å·®è¿‘ä¼¼æ³•ï¼Œæˆ‘ä»¬èƒ½å¤Ÿè¿›ä¸€æ­¥å‡å°‘é‡è¦æƒé‡çš„äºŒå€¼åŒ–é‡åŒ–è¯¯å·®ï¼Œç›¸æ¯”äºå°†é‡è¦æƒé‡ä¿ç•™ä¸º
    8 ä½æˆ– 16 ä½ã€‚
- en: '![Refer to caption](img/2b3002317f466872f8854396925d9aba.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/2b3002317f466872f8854396925d9aba.png)'
- en: 'Figure 5: Distribution and splitting schematic of the $4^{th}$ projection layer
    in LLaMA2-7B. The top 5% of the Hessian elements are orange, and the optimal break-point
    divides the non-salient weights into sparse and concentrated areas.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5ï¼šLLaMA2-7B ä¸­ç¬¬ $4$ å±‚æŠ•å½±çš„åˆ†å¸ƒå’Œæ‹†åˆ†ç¤ºæ„å›¾ã€‚Hessian å…ƒç´ çš„å‰ 5% æ˜¯æ©™è‰²çš„ï¼Œæœ€ä½³æ–­ç‚¹å°†éé‡è¦æƒé‡åˆ’åˆ†ä¸ºç¨€ç–åŒºå’Œé›†ä¸­åŒºã€‚
- en: 3.2 Bell-shaped Distribution Splitting for Binarization
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 é’Ÿå½¢åˆ†å¸ƒæ‹†åˆ†ç”¨äºäºŒå€¼åŒ–
- en: 'Following the removal of salient weights, the remaining weights maintain a
    bell-shaped distribution, which becomes closer to symmetric with the exclusion
    of salient weightsâ€™ impact, as depicted in FigureÂ [5](#S3.F5 "Figure 5 â€£ 3.1 Salient
    Weight Binarization for LLMs â€£ 3 Method â€£ BiLLM: Pushing the Limit of Post-Training
    Quantization for LLMs"). Binary quantization, representing an extreme form of
    uniform quantization, encounters more loss in the presence of non-uniform distributions.
    A practical approach involves the group-wise quantizationÂ (Park etÂ al., [2018](#bib.bib30);
    Fang etÂ al., [2020](#bib.bib13); Jain etÂ al., [2019](#bib.bib20)) of weights according
    to their distribution. Balancing between quantization accuracy and compression
    efficiency, we identify a single break-point within the distribution. As shown
    in FigureÂ [5](#S3.F5 "Figure 5 â€£ 3.1 Salient Weight Binarization for LLMs â€£ 3
    Method â€£ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs"), this
    partition divides the non-salient bell-shaped distribution into two categories:
    the sparse area and the concentrated area.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨å»é™¤é‡è¦æƒé‡åï¼Œå‰©ä½™æƒé‡ä¿æŒé’Ÿå½¢åˆ†å¸ƒï¼Œéšç€é‡è¦æƒé‡å½±å“çš„æ’é™¤ï¼Œè¿™ä¸€åˆ†å¸ƒå˜å¾—æ›´æ¥è¿‘å¯¹ç§°ï¼Œå¦‚å›¾Â [5](#S3.F5 "å›¾ 5 â€£ 3.1 é‡è¦æƒé‡äºŒå€¼åŒ–
    â€£ 3 æ–¹æ³• â€£ BiLLM: æ¨åŠ¨ LLM åè®­ç»ƒé‡åŒ–çš„æé™") æ‰€ç¤ºã€‚äºŒå€¼åŒ–ä½œä¸ºå‡åŒ€é‡åŒ–çš„ä¸€ç§æç«¯å½¢å¼ï¼Œåœ¨éå‡åŒ€åˆ†å¸ƒçš„æƒ…å†µä¸‹ä¼šé‡åˆ°æ›´å¤šæŸå¤±ã€‚å®é™…æ–¹æ³•æ¶‰åŠæ ¹æ®æƒé‡åˆ†å¸ƒè¿›è¡Œç»„å†…é‡åŒ–
    (Park et al., [2018](#bib.bib30); Fang et al., [2020](#bib.bib13); Jain et al.,
    [2019](#bib.bib20))ã€‚åœ¨é‡åŒ–ç²¾åº¦å’Œå‹ç¼©æ•ˆç‡ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œæˆ‘ä»¬åœ¨åˆ†å¸ƒä¸­è¯†åˆ«å‡ºä¸€ä¸ªæ–­ç‚¹ã€‚å¦‚å›¾Â [5](#S3.F5 "å›¾ 5 â€£ 3.1 é‡è¦æƒé‡äºŒå€¼åŒ–
    â€£ 3 æ–¹æ³• â€£ BiLLM: æ¨åŠ¨ LLM åè®­ç»ƒé‡åŒ–çš„æé™") æ‰€ç¤ºï¼Œè¿™ä¸€åˆ’åˆ†å°†éé‡è¦çš„é’Ÿå½¢åˆ†å¸ƒåˆ†ä¸ºä¸¤ç±»ï¼šç¨€ç–åŒºå’Œé›†ä¸­åŒºã€‚'
- en: 'The segmentation process identifies a break-point that categorizes non-salient
    weights into two groups: $A_{c}[-p,p]$. Then the mean squared quantization error
    of binarization is defined as:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†å‰²è¿‡ç¨‹è¯†åˆ«å‡ºä¸€ä¸ªæ–­ç‚¹ï¼Œå°†éé‡è¦æƒé‡åˆ†ä¸ºä¸¤ç»„ï¼š$A_{c}[-p,p]$ã€‚ç„¶åå®šä¹‰äº†äºŒå€¼åŒ–çš„å‡æ–¹é‡åŒ–è¯¯å·®ï¼š
- en: '|  | $1$2 |  | (9) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (9) |'
- en: 'Since $g(x)$ is a symmetric function, the above formula is simplified to:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äº $g(x)$ æ˜¯å¯¹ç§°å‡½æ•°ï¼Œä¸Šè¿°å…¬å¼ç®€åŒ–ä¸ºï¼š
- en: '|  | $\theta_{q}^{2}=2\int_{0}^{m}(\alpha-x)^{2}g(x)dx.$ |  | (10) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta_{q}^{2}=2\int_{0}^{m}(\alpha-x)^{2}g(x)dx.$ |  | (10) |'
- en: 'Then, the break-point $p$ divides the non-salient weights into two parts. According
    to the EquationÂ ([10](#S3.E10 "Equation 10 â€£ 3.2 Bell-shaped Distribution Splitting
    for Binarization â€£ 3 Method â€£ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs")), under the discontinuous weight distribution, we get a new binary
    quantization error:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç„¶åï¼Œæ–­ç‚¹ $p$ å°†éé‡è¦æƒé‡åˆ†ä¸ºä¸¤éƒ¨åˆ†ã€‚æ ¹æ®å…¬å¼Â ([10](#S3.E10 "å…¬å¼ 10 â€£ 3.2 é’Ÿå½¢åˆ†å¸ƒæ‹†åˆ† â€£ 3 æ–¹æ³• â€£ BiLLM:
    æ¨åŠ¨ LLM åè®­ç»ƒé‡åŒ–çš„æé™"))ï¼Œåœ¨ä¸è¿ç»­çš„æƒé‡åˆ†å¸ƒä¸‹ï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªæ–°çš„äºŒå€¼åŒ–é‡åŒ–è¯¯å·®ï¼š'
- en: '|  | $\theta_{q,p}^{2}=&#124;&#124;\mathbf{W}_{s}-\alpha_{s}\mathbf{B}_{s}&#124;&#124;^{2}+&#124;&#124;\mathbf{W}_{c}-\alpha_{c}\mathbf{B}_{c}&#124;&#124;^{2},$
    |  | (11) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta_{q,p}^{2}=&#124;&#124;\mathbf{W}_{s}-\alpha_{s}\mathbf{B}_{s}&#124;&#124;^{2}+&#124;&#124;\mathbf{W}_{c}-\alpha_{c}\mathbf{B}_{c}&#124;&#124;^{2},$
    |  | (11) |'
- en: 'where $\mathbf{W}_{s}$ are the binarization scales, determined by EquationÂ ([4](#S3.E4
    "Equation 4 â€£ 3.1 Salient Weight Binarization for LLMs â€£ 3 Method â€£ BiLLM: Pushing
    the Limit of Post-Training Quantization for LLMs")):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ $\mathbf{W}_{s}$ æ˜¯é€šè¿‡å…¬å¼Â ([4](#S3.E4 "å…¬å¼ 4 â€£ 3.1 é‡è¦æƒé‡äºŒå€¼åŒ– â€£ 3 æ–¹æ³• â€£ BiLLM: æ¨åŠ¨
    LLM åè®­ç»ƒé‡åŒ–çš„æé™")) ç¡®å®šçš„äºŒå€¼åŒ–å°ºåº¦ï¼š'
- en: '|  | $\alpha_{s}=\frac{1}{n_{s}}&#124;&#124;\mathbf{W}_{s}&#124;&#124;_{\ell
    1},\alpha_{c}=\frac{1}{n_{c}}&#124;&#124;\mathbf{W}_{c}&#124;&#124;_{\ell 1},$
    |  | (12) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $\alpha_{s}=\frac{1}{n_{s}}&#124;&#124;\mathbf{W}_{s}&#124;&#124;_{\ell
    1},\alpha_{c}=\frac{1}{n_{c}}&#124;&#124;\mathbf{W}_{c}&#124;&#124;_{\ell 1},$
    |  | (12) |'
- en: 'where $n$ can be defined as:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: $n$ å¯ä»¥å®šä¹‰ä¸ºï¼š
- en: '|  | $p^{*}=\mathop{\arg\min}_{p}(\theta_{q,p}^{2}).$ |  | (13) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $p^{*}=\mathop{\arg\min}_{p}(\theta_{q,p}^{2}).$ |  | (13) |'
- en: 'When the remaining weights follow an ideal Gaussian distribution, EquationÂ ([11](#S3.E11
    "Equation 11 â€£ 3.2 Bell-shaped Distribution Splitting for Binarization â€£ 3 Method
    â€£ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs")) is demonstrated
    to be a convex function with a global minimum, as evidenced in prior studiesÂ (Fang
    etÂ al., [2020](#bib.bib13); You, [2010](#bib.bib49)). Nonetheless, the actual
    distribution of non-salient weights, while bell-shaped, diverges from the ideal
    Gaussian model. Simultaneously, we retain the block-wise compensation strategies
    of GPTQÂ (Frantar etÂ al., [2022](#bib.bib17)) and OBCÂ (Frantar & Alistarh, [2022](#bib.bib15))
    to offset quantization errors, which could change the distribution of weights.
    In response, we employ a percentile search method to identify the optimal break-point
    based on the objective function outlined in EquationÂ ([13](#S3.E13 "Equation 13
    â€£ 3.2 Bell-shaped Distribution Splitting for Binarization â€£ 3 Method â€£ BiLLM:
    Pushing the Limit of Post-Training Quantization for LLMs")). This percentile search
    strategy is efficient and straightforward, completing the binarization process
    for a 7B LLM within merely 30 minutes. Furthermore, our findings indicate that
    despite the deviation of non-salient weights from the ideal Gaussian distribution,
    the error curve associated with the search process still exhibits convex properties
    (as detailed in Appendix [C](#A3 "Appendix C Searching Curve of Salient Column
    and Non-salient Distribution â€£ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs")), confirming the feasibility of pinpointing the optimal break-point.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 'å½“å‰©ä½™æƒé‡éµå¾ªç†æƒ³çš„é«˜æ–¯åˆ†å¸ƒæ—¶ï¼Œæ–¹ç¨‹ ([11](#S3.E11 "Equation 11 â€£ 3.2 Bell-shaped Distribution
    Splitting for Binarization â€£ 3 Method â€£ BiLLM: Pushing the Limit of Post-Training
    Quantization for LLMs")) è¢«è¯æ˜æ˜¯ä¸€ä¸ªå…·æœ‰å…¨å±€æœ€å°å€¼çš„å‡¸å‡½æ•°ï¼Œå¦‚ä¹‹å‰çš„ç ”ç©¶æ‰€ç¤º (Fang et al., [2020](#bib.bib13);
    You, [2010](#bib.bib49))ã€‚ç„¶è€Œï¼Œå°½ç®¡éæ˜¾è‘—æƒé‡çš„åˆ†å¸ƒå‘ˆé’Ÿå½¢ï¼Œä½†å®ƒåç¦»äº†ç†æƒ³çš„é«˜æ–¯æ¨¡å‹ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä¿ç•™äº† GPTQ (Frantar
    et al., [2022](#bib.bib17)) å’Œ OBC (Frantar & Alistarh, [2022](#bib.bib15)) çš„å—çº§è¡¥å¿ç­–ç•¥æ¥æŠµæ¶ˆé‡åŒ–è¯¯å·®ï¼Œè¿™å¯èƒ½ä¼šæ”¹å˜æƒé‡çš„åˆ†å¸ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨ç™¾åˆ†ä½æœç´¢æ–¹æ³•æ¥ç¡®å®šåŸºäºæ–¹ç¨‹
    ([13](#S3.E13 "Equation 13 â€£ 3.2 Bell-shaped Distribution Splitting for Binarization
    â€£ 3 Method â€£ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs"))
    ä¸­åˆ—å‡ºçš„ç›®æ ‡å‡½æ•°çš„æœ€ä½³æ–­ç‚¹ã€‚è¿™ç§ç™¾åˆ†ä½æœç´¢ç­–ç•¥é«˜æ•ˆä¸”ç®€å•ï¼Œåœ¨ä»… 30 åˆ†é’Ÿå†…å®Œæˆ 7B LLM çš„äºŒå€¼åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡éæ˜¾è‘—æƒé‡åç¦»äº†ç†æƒ³çš„é«˜æ–¯åˆ†å¸ƒï¼Œæœç´¢è¿‡ç¨‹ä¸­ç›¸å…³çš„è¯¯å·®æ›²çº¿ä»ç„¶æ˜¾ç¤ºå‡ºå‡¸æ€§ï¼ˆè¯¦è§é™„å½•
    [C](#A3 "Appendix C Searching Curve of Salient Column and Non-salient Distribution
    â€£ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs")ï¼‰ï¼Œç¡®è®¤äº†å®šä½æœ€ä½³æ–­ç‚¹çš„å¯è¡Œæ€§ã€‚'
- en: 3.3 Pipeline of BiLLM
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 BiLLM çš„æµç¨‹
- en: 'As depicted in FigureÂ [3](#S2.F3 "Figure 3 â€£ 2 Related Works â€£ BiLLM: Pushing
    the Limit of Post-Training Quantization for LLMs") left, BiLLM primarily performs
    binary quantization on all Linear weights within the Transformer blocks. This
    section introduces the detailed pipeline of BiLLM.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¦‚å›¾ [3](#S2.F3 "Figure 3 â€£ 2 Related Works â€£ BiLLM: Pushing the Limit of Post-Training
    Quantization for LLMs") å·¦ä¾§æ‰€ç¤ºï¼ŒBiLLM ä¸»è¦å¯¹ Transformer å—ä¸­çš„æ‰€æœ‰çº¿æ€§æƒé‡æ‰§è¡ŒäºŒå€¼åŒ–ã€‚æœ¬èŠ‚ä»‹ç»äº† BiLLM
    çš„è¯¦ç»†æµç¨‹ã€‚'
- en: 'Binarization Workflow. We first deploy the structural search of salient columns
    and a residual approximation binarization for salient columns. The process of
    salient columns incurs additional weight bits due to the search proportion and
    residual mechanism. Table [1](#S3.T1 "Table 1 â€£ 3.3 Pipeline of BiLLM â€£ 3 Method
    â€£ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs") presents the
    extra bits generated in some LLMsÂ (Zhang etÂ al., [2022](#bib.bib51); Touvron etÂ al.,
    [2023a](#bib.bib40), [b](#bib.bib41)). It can be observed that the searching and
    residuals bring only about 0.1 additional weight bits. Then, for these non-uniformly
    distributed weights, we use a split binarization strategy searching optimal $p^{*}$.
    The concentrated area and the sparse area are binarized separately. This part
    incurs the cost of an additional 1 bit for hardware group identification, but
    the computing parameters are still compressed to 1 bit. By retaining only block-wise
    compensation(Frantar etÂ al., [2022](#bib.bib17); Frantar & Alistarh, [2022](#bib.bib15))
    and eliminating column-wise quantization error compensation, we further enhance
    the efficiency of PTQ and ensure the effectiveness of distribution exploration.
    Algorithm [1](#alg1 "Algorithm 1 â€£ 3.3 Pipeline of BiLLM â€£ 3 Method â€£ BiLLM: Pushing
    the Limit of Post-Training Quantization for LLMs") illustrates the complete process
    of BiLLM, and detailed implementation of BiLLM is shown in Appendix [A](#A1 "Appendix
    A BiLLM Implementation â€£ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs").'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 'äºŒå€¼åŒ–å·¥ä½œæµç¨‹ã€‚æˆ‘ä»¬é¦–å…ˆéƒ¨ç½²äº†æ˜¾è‘—åˆ—çš„ç»“æ„æœç´¢å’Œæ˜¾è‘—åˆ—çš„æ®‹å·®è¿‘ä¼¼äºŒå€¼åŒ–ã€‚æ˜¾è‘—åˆ—çš„è¿‡ç¨‹ç”±äºæœç´¢æ¯”ä¾‹å’Œæ®‹å·®æœºåˆ¶å¼•å…¥äº†é¢å¤–çš„æƒé‡ä½ã€‚è¡¨[1](#S3.T1
    "Table 1 â€£ 3.3 Pipeline of BiLLM â€£ 3 Method â€£ BiLLM: Pushing the Limit of Post-Training
    Quantization for LLMs")å±•ç¤ºäº†åœ¨ä¸€äº›LLMsä¸­ç”Ÿæˆçš„é¢å¤–ä½ï¼ˆZhang et al., [2022](#bib.bib51)ï¼›Touvron
    et al., [2023a](#bib.bib40)ï¼Œ[b](#bib.bib41)ï¼‰ã€‚å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œæœç´¢å’Œæ®‹å·®ä»…å¸¦æ¥äº†å¤§çº¦0.1ä¸ªé¢å¤–çš„æƒé‡ä½ã€‚ç„¶åï¼Œå¯¹äºè¿™äº›ä¸å‡åŒ€åˆ†å¸ƒçš„æƒé‡ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æ‹†åˆ†äºŒå€¼åŒ–ç­–ç•¥æ¥æœç´¢æœ€ä¼˜çš„$p^{*}$ã€‚é›†ä¸­åŒºåŸŸå’Œç¨€ç–åŒºåŸŸåˆ†åˆ«è¿›è¡ŒäºŒå€¼åŒ–ã€‚è¿™éƒ¨åˆ†ä¼šå¢åŠ é¢å¤–çš„1ä¸ªä½ç”¨äºç¡¬ä»¶ç»„è¯†åˆ«ï¼Œä½†è®¡ç®—å‚æ•°ä»å‹ç¼©ä¸º1ä¸ªä½ã€‚é€šè¿‡ä»…ä¿ç•™å—çº§è¡¥å¿ï¼ˆFrantar
    et al., [2022](#bib.bib17)ï¼›Frantar & Alistarh, [2022](#bib.bib15)ï¼‰å¹¶æ¶ˆé™¤åˆ—çº§é‡åŒ–è¯¯å·®è¡¥å¿ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æé«˜äº†PTQçš„æ•ˆç‡ï¼Œç¡®ä¿äº†åˆ†å¸ƒæ¢ç´¢çš„æœ‰æ•ˆæ€§ã€‚ç®—æ³•[1](#alg1
    "Algorithm 1 â€£ 3.3 Pipeline of BiLLM â€£ 3 Method â€£ BiLLM: Pushing the Limit of
    Post-Training Quantization for LLMs")å±•ç¤ºäº†BiLLMçš„å®Œæ•´è¿‡ç¨‹ï¼ŒBiLLMçš„è¯¦ç»†å®ç°è§é™„å½•[A](#A1 "Appendix
    A BiLLM Implementation â€£ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs")ã€‚'
- en: '![Refer to caption](img/e9ee08fcb900c5da7bdbbdc830467085.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/e9ee08fcb900c5da7bdbbdc830467085.png)'
- en: 'Figure 6: Weights and hardware overhead changes on Llama-7B. The left picture
    shows the calculation parameters as a function of the significant weight ratio;
    the right picture shows the hardware overhead as a function of the block.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾6ï¼šLlama-7Bä¸Šçš„æƒé‡å’Œç¡¬ä»¶å¼€é”€å˜åŒ–ã€‚å·¦å›¾æ˜¾ç¤ºäº†æ˜¾è‘—æƒé‡æ¯”ç‡ä½œä¸ºå‡½æ•°çš„è®¡ç®—å‚æ•°ï¼›å³å›¾æ˜¾ç¤ºäº†ä½œä¸ºå—å‡½æ•°çš„ç¡¬ä»¶å¼€é”€ã€‚
- en: 'Table 1: Average bit results from structural searching and residual binarization
    of OPT, LLaMA, and LLaMA2 families.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨1ï¼šOPTã€LLaMAå’ŒLLaMA2ç³»åˆ—çš„ç»“æ„æœç´¢å’Œæ®‹å·®äºŒå€¼åŒ–çš„å¹³å‡ä½ç»“æœã€‚
- en: '| Model | 7B | 13B | 30B | 66B/65B/70B* |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| æ¨¡å‹ | 7B | 13B | 30B | 66B/65B/70B* |'
- en: '| OPT | 1.10 | 1.12 | 1.12 | 1.13 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| OPT | 1.10 | 1.12 | 1.12 | 1.13 |'
- en: '| LLaMA | 1.09 | 1.09 | 1.10 | 1.10 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA | 1.09 | 1.09 | 1.10 | 1.10 |'
- en: '| LLaMA2 | 1.07 | 1.08 | N/A | 1.09 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2 | 1.07 | 1.08 | N/A | 1.09 |'
- en: â€¢
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: '*: OPT-66B, LLaMA-65B and LLaMA2-70B.'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*: OPT-66Bã€LLaMA-65B å’Œ LLaMA2-70Bã€‚'
- en: 'Extra Storing Bits. The extra bits is acceptable under the binary weight quantization
    of BiLLM. The weight parameters and additional hardware overhead are as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: é¢å¤–å­˜å‚¨ä½ã€‚åœ¨BiLLMçš„äºŒå€¼æƒé‡é‡åŒ–ä¸‹ï¼Œé¢å¤–çš„ä½æ˜¯å¯ä»¥æ¥å—çš„ã€‚æƒé‡å‚æ•°å’Œé¢å¤–çš„ç¡¬ä»¶å¼€é”€å¦‚ä¸‹ï¼š
- en: '|  | $$\left\{\begin{array}[]{lr}N_{\text{param}}=2\times r_{\text{salient}}+1\times(1-r_{\text{salient}}),\\
    N_{\text{storing}}=1+\dfrac{1}{b_{\text{size}}},\\'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\left\{\begin{array}[]{lr}N_{\text{param}}=2\times r_{\text{salient}}+1\times(1-r_{\text{salient}}),\\
    N_{\text{storing}}=1+\dfrac{1}{b_{\text{size}}},\\'
- en: \end{array}\right.$$ |  | (14) |
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: \end{array}\right.$$ |  | (14) |
- en: 'where $r_{salient}$ represents the identifier for the structured column of
    salient weights. For example, a 10% structural selection along with an OBC compensation
    of size 128 was employed. This results in a weight parameter bit-width of 1.1
    bits and a hardware flag bit-width of 1.008 bits. FigureÂ [6](#S3.F6 "Figure 6
    â€£ 3.3 Pipeline of BiLLM â€£ 3 Method â€£ BiLLM: Pushing the Limit of Post-Training
    Quantization for LLMs") illustrates the weight overhead for different proportions
    and block sizes. It is important to note that flag weights do not participate
    in the computation; actual calculations are executed solely with parameter weights.
    Therefore, additional hardware identification bits do not affect the acceleration
    effect of binary quantization.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $r_{salient}$ ä»£è¡¨æ˜¾è‘—æƒé‡çš„ç»“æ„åŒ–åˆ—æ ‡è¯†ç¬¦ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨äº†10%çš„ç»“æ„é€‰æ‹©ä»¥åŠå¤§å°ä¸º128çš„OBCè¡¥å¿ã€‚è¿™å¯¼è‡´æƒé‡å‚æ•°çš„ä½å®½ä¸º1.1ä½ï¼Œç¡¬ä»¶æ ‡å¿—ä½å®½ä¸º1.008ä½ã€‚å›¾Â [6](#S3.F6
    "å›¾ 6 â€£ 3.3 BiLLM æµç¨‹ â€£ 3 æ–¹æ³• â€£ BiLLMï¼šæ¨åŠ¨åè®­ç»ƒé‡åŒ–çš„æé™") è¯´æ˜äº†ä¸åŒæ¯”ä¾‹å’Œå—å¤§å°çš„æƒé‡å¼€é”€ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ ‡å¿—æƒé‡ä¸å‚ä¸è®¡ç®—ï¼›å®é™…è®¡ç®—ä»…ä½¿ç”¨å‚æ•°æƒé‡ã€‚å› æ­¤ï¼Œé¢å¤–çš„ç¡¬ä»¶è¯†åˆ«ä½ä¸ä¼šå½±å“äºŒè¿›åˆ¶é‡åŒ–çš„åŠ é€Ÿæ•ˆæœã€‚
- en: 'Algorithm 1 Main Framework of BiLLM: Inner details of each function are shown
    in Algorithm [2](#alg2 "Algorithm 2 â€£ Appendix A BiLLM Implementation â€£ BiLLM:
    Pushing the Limit of Post-Training Quantization for LLMs")'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ç®—æ³• 1 BiLLM çš„ä¸»è¦æ¡†æ¶ï¼šæ¯ä¸ªå‡½æ•°çš„å†…éƒ¨ç»†èŠ‚åœ¨ç®—æ³• [2](#alg2 "ç®—æ³• 2 â€£ é™„å½• A BiLLM å®ç° â€£ BiLLMï¼šæ¨åŠ¨åè®­ç»ƒé‡åŒ–çš„æé™")
    ä¸­å±•ç¤º
- en: func $\operatorname{BinaryLLM}$)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: func $\operatorname{BinaryLLM}$)
- en: 'Input: $\mathbf{W}\in\mathbb{R}^{n\times m}$ - weight matrix'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¾“å…¥: $\mathbf{W}\in\mathbb{R}^{n\times m}$ - æƒé‡çŸ©é˜µ'
- en: $\mathbf{X}\in\mathbb{R}^{r\times d}$ - calibration data
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathbf{X}\in\mathbb{R}^{r\times d}$ - æ ¡å‡†æ•°æ®
- en: $\beta$ - block size
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: $\beta$ - å—å¤§å°
- en: $\lambda$ - hessian regularizer
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: $\lambda$ - Hessian æ­£åˆ™åŒ–å™¨
- en: 'Output: $\mathbf{B}$ - binarized weights'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¾“å‡º: $\mathbf{B}$ - äºŒå€¼åŒ–æƒé‡'
- en: 1:Â Â $\mathbf{H}\coloneqq 2\mathbf{X}\mathbf{X}^{\top}\ $
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '1:  $\mathbf{H}\coloneqq 2\mathbf{X}\mathbf{X}^{\top}\ $'
- en: 4 Experiments
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 å®éªŒ
- en: 4.1 Setup
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 è®¾ç½®
- en: We deploy BiLLM within the PytorchÂ (Paszke etÂ al., [2019](#bib.bib31))-Huggingface
    librariesÂ (Wolf etÂ al., [2019](#bib.bib45)). All the binarization processes and
    experiments are conducted on a single 80 GB NVIDIA A100\. Given that BiLLM is
    an efficient PTQ framework, it eliminates the need for any fine-tuning, allowing
    for completion through a single quantization process.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨ Pytorch (Paszke et al., [2019](#bib.bib31))-Huggingface åº“ (Wolf et al.,
    [2019](#bib.bib45)) ä¸­éƒ¨ç½²äº† BiLLMã€‚æ‰€æœ‰çš„äºŒå€¼åŒ–è¿‡ç¨‹å’Œå®éªŒéƒ½åœ¨ä¸€å°80 GB NVIDIA A100ä¸Šè¿›è¡Œã€‚ç”±äº BiLLM æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„
    PTQ æ¡†æ¶ï¼Œå®ƒæ¶ˆé™¤äº†ä»»ä½•å¾®è°ƒçš„éœ€è¦ï¼Œé€šè¿‡ä¸€æ¬¡é‡åŒ–è¿‡ç¨‹å³å¯å®Œæˆã€‚
- en: 'Table 2: Perplexity of RTN, GPTQ, PB-LLM, and BiLLM on OPT Family. The columns
    represent the perplexity results on Wikitext2 datasets with different model sizes.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¡¨ 2: RTNã€GPTQã€PB-LLM å’Œ BiLLM åœ¨ OPT ç³»åˆ—ä¸Šçš„å›°æƒ‘åº¦ã€‚åˆ—è¡¨ç¤ºä¸åŒæ¨¡å‹å¤§å°çš„ Wikitext2 æ•°æ®é›†ä¸Šçš„å›°æƒ‘åº¦ç»“æœã€‚'
- en: '| Method | Block Size | Weight Bits | 1.3B | 2.7B | 6.7B | 13B | 30B | 66B
    |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| æ–¹æ³• | å—å¤§å° | æƒé‡ä½ | 1.3B | 2.7B | 6.7B | 13B | 30B | 66B |'
- en: '| Full Precision | - | 16.00 | 14.62 | 12.47 | 10.86 | 10.13 | 9.56 | 9.34
    |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| å…¨ç²¾åº¦ | - | 16.00 | 14.62 | 12.47 | 10.86 | 10.13 | 9.56 | 9.34 |'
- en: '| RTN | - | 3.00 | 13337.38 | 15594.72 | 5797.32 | 3357.01 | 1566.00 | 6126.09
    |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| RTN | - | 3.00 | 13337.38 | 15594.72 | 5797.32 | 3357.01 | 1566.00 | 6126.09
    |'
- en: '| GPTQ | 128 | 3.00 | 20.97 | 16.88 | 14.86 | 11.61 | 10.27 | 10.51 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 128 | 3.00 | 20.97 | 16.88 | 14.86 | 11.61 | 10.27 | 10.51 |'
- en: '| \hdashlineRTN | - | 2.00 | 11272.65 | 9505.76 | 28363.14 | 194086.78 | 169616.47
    | 1165864.25 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| \hdashlineRTN | - | 2.00 | 11272.65 | 9505.76 | 28363.14 | 194086.78 | 169616.47
    | 1165864.25 |'
- en: '| GPTQ | 128 | 2.00 | 115.17 | 61.59 | 50.19 | 21.36 | 15.71 | 82.10 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 128 | 2.00 | 115.17 | 61.59 | 50.19 | 21.36 | 15.71 | 82.10 |'
- en: '| RTN | - | 1.00 | 17165.72 | 36516.69 | 11550.91 | 6986.35 | 6485.99 | 184796.30
    |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| RTN | - | 1.00 | 17165.72 | 36516.69 | 11550.91 | 6986.35 | 6485.99 | 184796.30
    |'
- en: '| GPTQ | 128 | 1.00 | 14884.73 | 14144.58 | 10622.81 | 15196.96 | 12478.37
    | 13106.45 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 128 | 1.00 | 14884.73 | 14144.58 | 10622.81 | 15196.96 | 12478.37
    | 13106.45 |'
- en: '| PB-LLM $\dagger$ | 128 | 1.70 | 265.52 | 124.35 | 105.16 | 81.92 | 25.14
    | 29.09 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| PB-LLM $\dagger$ | 128 | 1.70 | 265.52 | 124.35 | 105.16 | 81.92 | 25.14
    | 29.09 |'
- en: '| BiLLM  $\ddagger$ | 128 | 1.11 | 69.97 | 49.55 | 35.36 | 18.82 | 12.71 |
    12.06 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| BiLLM  $\ddagger$ | 128 | 1.11 | 69.97 | 49.55 | 35.36 | 18.82 | 12.71 |
    12.06 |'
- en: â€¢
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: '-: Vanilla RTN conducts layer-wise quantization. $\dagger$: BiLLM uses structural
    searching for salient weights. The table gives the average bit-width of the OPT
    family.'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '-: Vanilla RTN æ‰§è¡Œé€å±‚é‡åŒ–ã€‚ $\dagger$: BiLLM ä½¿ç”¨ç»“æ„åŒ–æœç´¢æ˜¾è‘—æƒé‡ã€‚è¡¨æ ¼æ˜¾ç¤ºäº† OPT ç³»åˆ—çš„å¹³å‡ä½å®½ã€‚'
- en: 'Table 3: Perplexity of RTN, GPTQ, PB-LLM, BiLLM on LLaMA Family. The columns
    represent the perplexity results on Wikitext2 datasets with different model sizes.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¡¨ 3: RTNã€GPTQã€PB-LLMã€BiLLM åœ¨ LLaMA ç³»åˆ—ä¸Šçš„å›°æƒ‘åº¦ã€‚åˆ—è¡¨ç¤ºä¸åŒæ¨¡å‹å¤§å°çš„ Wikitext2 æ•°æ®é›†ä¸Šçš„å›°æƒ‘åº¦ç»“æœã€‚'
- en: '| Model | Method | Block Size | Weight Bits | 7B | 13B | 30B | 65B/70B* |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| æ¨¡å‹ | æ–¹æ³• | å—å¤§å° | æƒé‡ä½æ•° | 7B | 13B | 30B | 65B/70B* |'
- en: '|  | Full Precision | - | 16.00 | 5.68 | 5.09 | 4.10 | 3.53 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  | å…¨ç²¾åº¦ | - | 16.00 | 5.68 | 5.09 | 4.10 | 3.53 |'
- en: '|  | RTN | - | 2.00 | 106767.34 | 57409.93 | 26704.36 | 19832.87 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | RTN | - | 2.00 | 106767.34 | 57409.93 | 26704.36 | 19832.87 |'
- en: '|  | GPTQ | 128 | 2.00 | 152.31 | 20.44 | 13.01 | 8.78 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 128 | 2.00 | 152.31 | 20.44 | 13.01 | 8.78 |'
- en: '| LLaMA | RTN | - | 1.00 | 168388.00 | 1412020.25 | 14681.76 | 65253.24 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA | RTN | - | 1.00 | 168388.00 | 1412020.25 | 14681.76 | 65253.24 |'
- en: '|  | GPTQ | 128 | 1.00 | 267001.72 | 113894.12 | 67093.73 | 25082.88 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 128 | 1.00 | 267001.72 | 113894.12 | 67093.73 | 25082.88 |'
- en: '|  | PB-LLM $\dagger$ | 128 | 1.70 | 102.36 | 36.60 | 33.67 | 12.53 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  | PB-LLM $\dagger$ | 128 | 1.70 | 102.36 | 36.60 | 33.67 | 12.53 |'
- en: '|  | BiLLM  $\ddagger$ | 128 | 1.09 | 35.04 | 15.14 | 10.52 | 8.49 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | BiLLM  $\ddagger$ | 128 | 1.09 | 35.04 | 15.14 | 10.52 | 8.49 |'
- en: '|  | Full Precision | - | 16.00 | 5.47 | 4.88 | N/A | 3.32 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  | å…¨ç²¾åº¦ | - | 16.00 | 5.47 | 4.88 | ä¸é€‚ç”¨ | 3.32 |'
- en: '|  | RTN | - | 2.00 | 17788.93 | 51145.61 | N/A | 26066.13 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | RTN | - | 2.00 | 17788.93 | 51145.61 | ä¸é€‚ç”¨ | 26066.13 |'
- en: '|  | GPTQ | 128 | 2.00 | 60.45 | 19.70 | N/A | 9.12 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 128 | 2.00 | 60.45 | 19.70 | ä¸é€‚ç”¨ | 9.12 |'
- en: '| LLaMA2 | RTN | - | 1.00 | 157058.34 | 47902.32 | N/A | 160389.91 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2 | RTN | - | 1.00 | 157058.34 | 47902.32 | ä¸é€‚ç”¨ | 160389.91 |'
- en: '|  | GPTQ | 128 | 1.00 | 115905.67 | 9387.80 | N/A | 74395.42 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 128 | 1.00 | 115905.67 | 9387.80 | ä¸é€‚ç”¨ | 74395.42 |'
- en: '|  | PB-LLM $\dagger$ | 128 | 1.70 | 69.20 | 151.09 | N/A | 28.37 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | PB-LLM $\dagger$ | 128 | 1.70 | 69.20 | 151.09 | ä¸é€‚ç”¨ | 28.37 |'
- en: '|  | BiLLM  $\ddagger$ | 128 | 1.08 | 32.48 | 16.77 | N/A | 8.41 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  | BiLLM  $\ddagger$ | 128 | 1.08 | 32.48 | 16.77 | ä¸é€‚ç”¨ | 8.41 |'
- en: â€¢
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'The table gives the average bit-width of the LLaMA family. N/A: LLaMA2 do not
    have 30B version. *: LLaMA has 65B version and LLaMA2 has 70B version.'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¡¨æ ¼å±•ç¤ºäº†LLaMAå®¶æ—çš„å¹³å‡ä½å®½ã€‚ä¸é€‚ç”¨ï¼šLLaMA2æ²¡æœ‰30Bç‰ˆæœ¬ã€‚*ï¼šLLaMAæœ‰65Bç‰ˆæœ¬ï¼ŒLLaMA2æœ‰70Bç‰ˆæœ¬ã€‚
- en: 'Models and Datasets. We facilitate our method on the OPTÂ (Zhang etÂ al., [2022](#bib.bib51))
    and LLaMAÂ (Touvron etÂ al., [2023a](#bib.bib40), [b](#bib.bib41)) families. Additionally,
    considering the customary need for instruction-based fine-tuning of LLMs to adapt
    to varying contexts, we also conducted experiments on VicunaÂ (Chiang etÂ al., [2023](#bib.bib5)).
    In terms of evaluation metrics, we mainly focused on the perplexity of LLMsâ€™ outputs,
    which is widely acknowledged in prior studies as a challenging yet stable indicator
    of LLM capabilities, particularly apt for network compression Â ([Yao etÂ al.,](#bib.bib47)
    ; Frantar etÂ al., [2022](#bib.bib17); Frantar & Alistarh, [2023](#bib.bib16);
    Xiao etÂ al., [2023](#bib.bib46)). We consider the test of WikiText2Â (Merity etÂ al.,
    [2016](#bib.bib28)), PTBÂ (Marcus etÂ al., [1994](#bib.bib27)), as well as a part
    of the C4Â (Raffel etÂ al., [2020](#bib.bib35)) data. Then, we further conduct the
    experiments on seven zero-shot evaluation tasks (PIQAÂ (Bisk etÂ al., [2020](#bib.bib2)),
    BoolQÂ (Clark etÂ al., [2019](#bib.bib6)), OBQAÂ (Mihaylov etÂ al., [2018](#bib.bib29)),
    WinograndeÂ (Sakaguchi etÂ al., [2021](#bib.bib37)), ARC-eÂ (Clark etÂ al., [2018](#bib.bib7)),
    ARC-cÂ (Clark etÂ al., [2018](#bib.bib7)) HellaswagÂ (Zellers etÂ al., [2019](#bib.bib50)))
    in the Appendix [D](#A4 "Appendix D Multi-evaluation Comparisons â€£ BiLLM: Pushing
    the Limit of Post-Training Quantization for LLMs"), further verifying the robustness
    of our proposed BiLLM to the binarization of LLMs.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å’Œæ•°æ®é›†ã€‚æˆ‘ä»¬åœ¨OPTï¼ˆZhang et al., [2022](#bib.bib51)ï¼‰å’ŒLLaMAï¼ˆTouvron et al., [2023a](#bib.bib40)ï¼Œ[b](#bib.bib41)ï¼‰å®¶æ—ä¸Šåº”ç”¨äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè€ƒè™‘åˆ°LLMsåœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­éœ€è¦åŸºäºæŒ‡ä»¤çš„å¾®è°ƒï¼Œæˆ‘ä»¬è¿˜åœ¨Vicunaï¼ˆChiang
    et al., [2023](#bib.bib5)ï¼‰ä¸Šè¿›è¡Œäº†å®éªŒã€‚åœ¨è¯„ä¼°æŒ‡æ ‡æ–¹é¢ï¼Œæˆ‘ä»¬ä¸»è¦å…³æ³¨LLMsè¾“å‡ºçš„å›°æƒ‘åº¦ï¼Œè¿™åœ¨ä¹‹å‰çš„ç ”ç©¶ä¸­è¢«å¹¿æ³›è®¤å¯ä¸ºLLMèƒ½åŠ›çš„å…·æœ‰æŒ‘æˆ˜æ€§ä½†ç¨³å®šçš„æŒ‡æ ‡ï¼Œç‰¹åˆ«é€‚ç”¨äºç½‘ç»œå‹ç¼©ï¼ˆ[Yao
    et al.,](#bib.bib47)ï¼›Frantar et al., [2022](#bib.bib17)ï¼›Frantar & Alistarh, [2023](#bib.bib16)ï¼›Xiao
    et al., [2023](#bib.bib46)ï¼‰ã€‚æˆ‘ä»¬è€ƒè™‘äº†WikiText2ï¼ˆMerity et al., [2016](#bib.bib28)ï¼‰ã€PTBï¼ˆMarcus
    et al., [1994](#bib.bib27)ï¼‰ä»¥åŠéƒ¨åˆ†C4ï¼ˆRaffel et al., [2020](#bib.bib35)ï¼‰æ•°æ®ã€‚ç„¶åï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥åœ¨ä¸ƒä¸ªé›¶æ ·æœ¬è¯„ä¼°ä»»åŠ¡ï¼ˆPIQAï¼ˆBisk
    et al., [2020](#bib.bib2)ï¼‰ã€BoolQï¼ˆClark et al., [2019](#bib.bib6)ï¼‰ã€OBQAï¼ˆMihaylov
    et al., [2018](#bib.bib29)ï¼‰ã€Winograndeï¼ˆSakaguchi et al., [2021](#bib.bib37)ï¼‰ã€ARC-eï¼ˆClark
    et al., [2018](#bib.bib7)ï¼‰ã€ARC-cï¼ˆClark et al., [2018](#bib.bib7)ï¼‰Hellaswagï¼ˆZellers
    et al., [2019](#bib.bib50)ï¼‰ï¼‰ä¸Šè¿›è¡Œäº†å®éªŒï¼Œè¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬æå‡ºçš„BiLLMå¯¹LLMsäºŒå€¼åŒ–çš„é²æ£’æ€§ã€‚
- en: Baseline. Our primary baseline is PB-LLMÂ (Shang etÂ al., [2023](#bib.bib39)),
    the most recent PTQ approach on binary LLMs. GPTQÂ (Frantar etÂ al., [2022](#bib.bib17))
    and vanilla RTN are also selected. GPTQ is currently the advanced technology in
    PTQ, and many works(Lin etÂ al., [2023](#bib.bib25); Dettmers etÂ al., [2023b](#bib.bib11);
    Shang etÂ al., [2023](#bib.bib39)) choose it as the baseline. Other methods oriented
    towards 8-bit and 4-bit quantization are deemed unsuitable for binarization and
    were thus not considered.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºçº¿ã€‚æˆ‘ä»¬çš„ä¸»è¦åŸºçº¿æ˜¯ PB-LLMï¼ˆShang ç­‰ï¼Œ[2023](#bib.bib39)ï¼‰ï¼Œè¿™æ˜¯æœ€æ–°çš„äºŒè¿›åˆ¶ LLM PTQ æ–¹æ³•ã€‚GPTQï¼ˆFrantar
    ç­‰ï¼Œ[2022](#bib.bib17)ï¼‰å’ŒåŸå§‹ RTN ä¹Ÿè¢«é€‰æ‹©ã€‚GPTQ ç›®å‰æ˜¯ PTQ çš„å…ˆè¿›æŠ€æœ¯ï¼Œè®¸å¤šå·¥ä½œï¼ˆLin ç­‰ï¼Œ[2023](#bib.bib25)ï¼›Dettmers
    ç­‰ï¼Œ[2023b](#bib.bib11)ï¼›Shang ç­‰ï¼Œ[2023](#bib.bib39)ï¼‰å°†å…¶ä½œä¸ºåŸºçº¿ã€‚å…¶ä»–é¢å‘ 8 ä½å’Œ 4 ä½é‡åŒ–çš„æ–¹æ³•è¢«è®¤ä¸ºä¸é€‚åˆäºŒå€¼åŒ–ï¼Œå› æ­¤æœªäºˆè€ƒè™‘ã€‚
- en: 4.2 Results
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 ç»“æœ
- en: 'Comparison results. We conduct a meticulous comparison of the binary performance
    of different LLMs across various model sizes. We deploy the BiLLM on the OPT modelsÂ (Zhang
    etÂ al., [2022](#bib.bib51)) under the condition of a block size equal to 128\.
    As seen in Table [2](#S4.T2 "Table 2 â€£ 4.1 Setup â€£ 4 Experiments â€£ BiLLM: Pushing
    the Limit of Post-Training Quantization for LLMs"), the model outputs under the
    RTN and GPTQ methods have already collapsed at 1-bit weights, whereas BiLLM still
    maintains reasonable linguistic output capabilities with an average weight of
    1.1 bits. In comparison with PB-LLM at 1.7 bits, our method achieves a 35% reduction
    in weight bit-width while enhancing the performance of different sizes of the
    OPT model by 49.4% to 77.0%. It is noteworthy that when the parameter size exceeds
    30B, BiLLM can achieve performance nearly equivalent to that of GPTQ with 3-bit
    quantization.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ¯”è¾ƒç»“æœã€‚æˆ‘ä»¬å¯¹ä¸åŒ LLM åœ¨å„ç§æ¨¡å‹å°ºå¯¸ä¸‹çš„äºŒè¿›åˆ¶æ€§èƒ½è¿›è¡Œäº†ç»†è‡´æ¯”è¾ƒã€‚æˆ‘ä»¬åœ¨å—å¤§å°ä¸º 128 çš„æ¡ä»¶ä¸‹å°† BiLLM éƒ¨ç½²åœ¨ OPT æ¨¡å‹ä¸Šï¼ˆZhang
    ç­‰ï¼Œ[2022](#bib.bib51)ï¼‰ã€‚å¦‚è¡¨ [2](#S4.T2 "Table 2 â€£ 4.1 Setup â€£ 4 Experiments â€£ BiLLM:
    Pushing the Limit of Post-Training Quantization for LLMs") æ‰€ç¤ºï¼ŒRTN å’Œ GPTQ æ–¹æ³•ä¸‹çš„æ¨¡å‹è¾“å‡ºåœ¨
    1 ä½æƒé‡æ—¶å·²ç»å´©æºƒï¼Œè€Œ BiLLM ä»ç„¶ä»¥å¹³å‡ 1.1 ä½çš„æƒé‡ä¿æŒäº†åˆç†çš„è¯­è¨€è¾“å‡ºèƒ½åŠ›ã€‚ä¸ 1.7 ä½çš„ PB-LLM ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é™ä½ 35%
    æƒé‡ä½å®½çš„åŒæ—¶ï¼Œå°†ä¸åŒå°ºå¯¸çš„ OPT æ¨¡å‹æ€§èƒ½æé«˜äº† 49.4% è‡³ 77.0%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå½“å‚æ•°å¤§å°è¶…è¿‡ 30B æ—¶ï¼ŒBiLLM çš„æ€§èƒ½å‡ ä¹å¯ä»¥ä¸ 3
    ä½é‡åŒ–çš„ GPTQ ç›¸åª²ç¾ã€‚'
- en: '![Refer to caption](img/1b91bf301c37bded51868bfe7f6d0e76.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/1b91bf301c37bded51868bfe7f6d0e76.png)'
- en: 'Figure 7: GPTQ, PB-LLM, BiLLM performed on the PTB and c4 datasets, mainly
    on LLaMA-7B, LLaMA2-7B, and OPT-6.7B, and we found that BiLLM performed relatively
    well.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 7ï¼šGPTQã€PB-LLMã€BiLLM åœ¨ PTB å’Œ c4 æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œä¸»è¦åœ¨ LLaMA-7Bã€LLaMA2-7B å’Œ OPT-6.7B ä¸Šè¿›è¡Œï¼Œæˆ‘ä»¬å‘ç°
    BiLLM çš„è¡¨ç°ç›¸å¯¹è¾ƒå¥½ã€‚
- en: 'Table 4: Perplexity of BiLLM on Vicuna-7B and Vicuna-13B. The columns of different
    models represent the perplexity results on Wikitext2, PTB, and C4 datasets. The
    block size is set to 128.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 4ï¼šBiLLM åœ¨ Vicuna-7B å’Œ Vicuna-13B ä¸Šçš„å›°æƒ‘åº¦ã€‚ä¸åŒæ¨¡å‹çš„åˆ—è¡¨ç¤ºäº†åœ¨ Wikitext2ã€PTB å’Œ C4 æ•°æ®é›†ä¸Šçš„å›°æƒ‘åº¦ç»“æœã€‚å—å¤§å°è®¾ç½®ä¸º
    128ã€‚
- en: '| Model | Method | Weight Bits | Wiki -text2 $\downarrow$ |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| æ¨¡å‹ | æ–¹æ³• | æƒé‡ä½æ•° | Wiki -text2 $\downarrow$ |'
- en: '|  | GPTQ | 2.00 | 109.56 | 6227.73 | 64.28 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 2.00 | 109.56 | 6227.73 | 64.28 |'
- en: '| Vicuna-7B | PB-LLM | 1.70 | 68.01 | 477.52 | 67.23 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-7B | PB-LLM | 1.70 | 68.01 | 477.52 | 67.23 |'
- en: '|  | BiLLM | 1.08 | 33.00 | 332.17 | 36.24 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | BiLLM | 1.08 | 33.00 | 332.17 | 36.24 |'
- en: '|  | GPTQ | 2.00 | 41.75 | 465.94 | 40.57 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 2.00 | 41.75 | 465.94 | 40.57 |'
- en: '| Vicuna-13B | PB-LLM | 1.70 | 362.17 | 772.44 | 346.16 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-13B | PB-LLM | 1.70 | 362.17 | 772.44 | 346.16 |'
- en: '|  | BiLLM | 1.08 | 36.57 | 300.31 | 28.76 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | BiLLM | 1.08 | 36.57 | 300.31 | 28.76 |'
- en: 'Due to the exceptional performance of the LLaMAÂ (Touvron etÂ al., [2023a](#bib.bib40),
    [b](#bib.bib41)) series, they have become the foundation for many open-source
    modelsÂ (Chiang etÂ al., [2023](#bib.bib5)). Then, in Table [3](#S4.T3 "Table 3
    â€£ 4.1 Setup â€£ 4 Experiments â€£ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs"), we evaluate the perplexity of outputs from the LLaMA series models
    using different methods. It can be observed that, even at ultra-low weight bit-width,
    BiLLM consistently outperforms the 2-bit RTN and GPTQ methods. And 1.08 bits BiLLM
    for LLaMA-65B and LLaMA2-70B even surpasses the output of the full-precision OPT-66B
    model, which demonstrates the further binary potential of the LLaMA family. We
    extend perplexity evaluation to the PTB and C4 datasets. FigureÂ [7](#S4.F7 "Figure
    7 â€£ 4.2 Results â€£ 4 Experiments â€£ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs") illustrates the performance of the 7B parameter LLaMA series as well
    as the 6.7B OPT models. BiLLM continues to achieve a leading edge in performance
    compared to other methods (more additional comparisons are discussed in Appendix
    [D](#A4 "Appendix D Multi-evaluation Comparisons â€£ BiLLM: Pushing the Limit of
    Post-Training Quantization for LLMs")).'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç”±äº LLaMAÂ (Touvron etÂ al., [2023a](#bib.bib40), [b](#bib.bib41)) ç³»åˆ—çš„å“è¶Šè¡¨ç°ï¼Œå®ƒä»¬å·²æˆä¸ºè®¸å¤šå¼€æºæ¨¡å‹çš„åŸºç¡€
    (Chiang etÂ al., [2023](#bib.bib5))ã€‚åœ¨è¡¨æ ¼ [3](#S4.T3 "Table 3 â€£ 4.1 Setup â€£ 4 Experiments
    â€£ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs") ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸åŒçš„æ–¹æ³•è¯„ä¼°äº†
    LLaMA ç³»åˆ—æ¨¡å‹çš„å›°æƒ‘åº¦ã€‚å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œå³ä½¿åœ¨è¶…ä½æƒé‡é‡ä½å®½ä¸‹ï¼ŒBiLLM ä¹Ÿå§‹ç»ˆä¼˜äº 2 ä½ RTN å’Œ GPTQ æ–¹æ³•ã€‚è€Œ 1.08 ä½ BiLLM
    å¯¹äº LLaMA-65B å’Œ LLaMA2-70B ç”šè‡³è¶…è¿‡äº†å…¨ç²¾åº¦ OPT-66B æ¨¡å‹çš„è¾“å‡ºï¼Œè¿™å±•ç¤ºäº† LLaMA ç³»åˆ—çš„è¿›ä¸€æ­¥äºŒå€¼åŒ–æ½œåŠ›ã€‚æˆ‘ä»¬å°†å›°æƒ‘åº¦è¯„ä¼°æ‰©å±•åˆ°
    PTB å’Œ C4 æ•°æ®é›†ã€‚å›¾ [7](#S4.F7 "Figure 7 â€£ 4.2 Results â€£ 4 Experiments â€£ BiLLM: Pushing
    the Limit of Post-Training Quantization for LLMs") å±•ç¤ºäº† 7B å‚æ•° LLaMA ç³»åˆ—ä»¥åŠ 6.7B OPT
    æ¨¡å‹çš„è¡¨ç°ã€‚ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒBiLLM ç»§ç»­åœ¨æ€§èƒ½ä¸Šä¿æŒé¢†å…ˆ (æ›´å¤šé™„åŠ æ¯”è¾ƒè§é™„å½• [D](#A4 "Appendix D Multi-evaluation
    Comparisons â€£ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs"))ã€‚'
- en: 'Experiments of instruction-tuned models. Instruction fine-tuning can significantly
    improve the application capabilities of the model and has become a necessary process
    for LLMs deployment in different scenariosÂ (Wei etÂ al., [2021](#bib.bib44); Sanh
    etÂ al., [2021](#bib.bib38); Chiang etÂ al., [2023](#bib.bib5)). We also deployed
    BiLLM on the recently popular fine-tuning instruction model Vicuna for benchmark
    testing. As shown in Table [4](#S4.T4 "Table 4 â€£ 4.2 Results â€£ 4 Experiments â€£
    BiLLM: Pushing the Limit of Post-Training Quantization for LLMs"), the perplexity
    performance of GPTQ and PB-LLM are compared on Vicuna-7B and Vicuna-13B with three
    evaluations. BiLLM can achieve better performance at an average weight bit of
    1.08, which further proves that BiLLMâ€™s universal LLMs binarization potential.
    We also provide dialogue examples of binary models in Appeandix [F](#A6 "Appendix
    F Dialog Examples â€£ BiLLM: Pushing the Limit of Post-Training Quantization for
    LLMs").'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 'æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹çš„å®éªŒã€‚æŒ‡ä»¤å¾®è°ƒå¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹çš„åº”ç”¨èƒ½åŠ›ï¼Œå·²æˆä¸º LLM åœ¨ä¸åŒåœºæ™¯ä¸­éƒ¨ç½²çš„å¿…è¦è¿‡ç¨‹ (Wei etÂ al., [2021](#bib.bib44);
    Sanh etÂ al., [2021](#bib.bib38); Chiang etÂ al., [2023](#bib.bib5))ã€‚æˆ‘ä»¬è¿˜åœ¨æœ€è¿‘æµè¡Œçš„å¾®è°ƒæŒ‡ä»¤æ¨¡å‹
    Vicuna ä¸Šéƒ¨ç½²äº† BiLLM è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚å¦‚è¡¨æ ¼ [4](#S4.T4 "Table 4 â€£ 4.2 Results â€£ 4 Experiments
    â€£ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs") æ‰€ç¤ºï¼Œå¯¹ Vicuna-7B
    å’Œ Vicuna-13B ä¸Šçš„ GPTQ å’Œ PB-LLM çš„å›°æƒ‘åº¦æ€§èƒ½è¿›è¡Œäº†ä¸‰æ¬¡è¯„ä¼°æ¯”è¾ƒã€‚BiLLM åœ¨å¹³å‡æƒé‡é‡ä¸º 1.08 çš„æƒ…å†µä¸‹èƒ½å®ç°æ›´å¥½çš„æ€§èƒ½ï¼Œè¿™è¿›ä¸€æ­¥è¯æ˜äº†
    BiLLM åœ¨é€šç”¨ LLMs äºŒå€¼åŒ–çš„æ½œåŠ›ã€‚æˆ‘ä»¬è¿˜åœ¨é™„å½• [F](#A6 "Appendix F Dialog Examples â€£ BiLLM: Pushing
    the Limit of Post-Training Quantization for LLMs") æä¾›äº†äºŒå€¼æ¨¡å‹çš„å¯¹è¯ç¤ºä¾‹ã€‚'
- en: 'Zero-Shot results. To conduct a more comprehensive evaluation of binary LLMs,
    we extend our experiments to 7 zero-shot datasets. Appendix [D](#A4 "Appendix
    D Multi-evaluation Comparisons â€£ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs") provides detailed results of our approach compared to previous methods
    in ultra-low bit quantization, further showing the outlier of BiLLM.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 'Zero-Shot ç»“æœã€‚ä¸ºäº†å¯¹äºŒå€¼ LLMs è¿›è¡Œæ›´å…¨é¢çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å°†å®éªŒæ‰©å±•åˆ° 7 ä¸ªé›¶æ ·æœ¬æ•°æ®é›†ã€‚é™„å½• [D](#A4 "Appendix D
    Multi-evaluation Comparisons â€£ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs") æä¾›äº†æˆ‘ä»¬æ–¹æ³•ä¸ä¹‹å‰è¶…ä½ä½é‡åŒ–æ–¹æ³•çš„è¯¦ç»†æ¯”è¾ƒç»“æœï¼Œè¿›ä¸€æ­¥å±•ç¤ºäº† BiLLM çš„å¼‚ç±»è¡¨ç°ã€‚'
- en: '![Refer to caption](img/5cd60a51a968b07e1c4ee175174159b6.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/5cd60a51a968b07e1c4ee175174159b6.png)'
- en: 'Figure 8: Ablation results of salient-only and splitting-only methods on OPT
    and LLaMA.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 8: OPT å’Œ LLaMA ä¸Šä»…æœ‰æ˜¾è‘—æ€§å’Œä»…æœ‰åˆ†å‰²æ–¹æ³•çš„æ¶ˆèç»“æœã€‚'
- en: 'Ablation results. BiLLM enhances binarization precision through two primary
    methods: structured salient binarization via residual approximation, and non-salient
    weight binarization via optimal splitting. To examine the effects of these strategies,
    we conducted decomposition experiments. As shown in FigureÂ [8](#S4.F8 "Figure
    8 â€£ 4.2 Results â€£ 4 Experiments â€£ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs"), both approaches significantly improve binary performance. Notably,
    we found that OPT-6.7B exhibits greater sensitivity to the splitting of non-salient
    weights (the blue line is lower than the green line), whereas LLaMA-7B is more
    responsive to salient weightsâ€™ residual approximation (the green line is lower
    than the blue line). This further indicates that different LLMs exhibit varying
    responses to distinct binarization optimization strategies, showing that the two
    binarization strategies proposed by BiLLM are efficient to various LLMs. We further
    discuss details on the block-size ablation results in Appendix [E](#A5 "Appendix
    E Ablation of BiLLM with different block size â€£ BiLLM: Pushing the Limit of Post-Training
    Quantization for LLMs").'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ¶ˆèç»“æœã€‚BiLLM é€šè¿‡ä¸¤ç§ä¸»è¦æ–¹æ³•å¢å¼ºäº†äºŒå€¼åŒ–ç²¾åº¦ï¼šé€šè¿‡æ®‹å·®é€¼è¿‘è¿›è¡Œç»“æ„åŒ–æ˜¾è‘—äºŒå€¼åŒ–ï¼Œä»¥åŠé€šè¿‡æœ€ä¼˜åˆ†å‰²è¿›è¡Œéæ˜¾è‘—æƒé‡çš„äºŒå€¼åŒ–ã€‚ä¸ºäº†æ£€æŸ¥è¿™äº›ç­–ç•¥çš„æ•ˆæœï¼Œæˆ‘ä»¬è¿›è¡Œäº†åˆ†è§£å®éªŒã€‚å¦‚å›¾
    [8](#S4.F8 "Figure 8 â€£ 4.2 Results â€£ 4 Experiments â€£ BiLLM: Pushing the Limit
    of Post-Training Quantization for LLMs") æ‰€ç¤ºï¼Œè¿™ä¸¤ç§æ–¹æ³•æ˜¾è‘—æ”¹å–„äº†äºŒè¿›åˆ¶æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç° OPT-6.7B
    å¯¹éæ˜¾è‘—æƒé‡çš„åˆ†å‰²æ›´ä¸ºæ•æ„Ÿï¼ˆè“çº¿ä½äºç»¿çº¿ï¼‰ï¼Œè€Œ LLaMA-7B å¯¹æ˜¾è‘—æƒé‡çš„æ®‹å·®é€¼è¿‘æ›´ä¸ºå“åº”ï¼ˆç»¿çº¿ä½äºè“çº¿ï¼‰ã€‚è¿™è¿›ä¸€æ­¥è¡¨æ˜ï¼Œä¸åŒçš„ LLM å¯¹ä¸åŒçš„äºŒå€¼åŒ–ä¼˜åŒ–ç­–ç•¥æœ‰ä¸åŒçš„ååº”ï¼Œæ˜¾ç¤ºå‡º
    BiLLM æå‡ºçš„ä¸¤ç§äºŒå€¼åŒ–ç­–ç•¥å¯¹å„ç§ LLM éƒ½æ˜¯æœ‰æ•ˆçš„ã€‚æˆ‘ä»¬åœ¨é™„å½• [E](#A5 "Appendix E Ablation of BiLLM with
    different block size â€£ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs") ä¸­è¿›ä¸€æ­¥è®¨è®ºäº†å—å¤§å°æ¶ˆèç»“æœçš„ç»†èŠ‚ã€‚'
- en: 5 Conclusions
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 ç»“è®º
- en: This work proposed a novel post-training binary quantization method named BiLLM,
    specifically tailored for compressing pre-trained LLMs. Inspired by the characteristics
    of weightâ€™s value and Hessian distributions, we adopted a binary residual approximation
    for structurally salient weights to preserve their capabilities at ultra-low bits.
    For non-salient weights, we employed optimal segmentation for grouped binarization.
    Our results demonstrate that LLMs can undergo a one-time weight quantization at
    ultra-low bits without substantial loss of precision. BiLLM has pioneered the
    achievement of LLM performance guarantees at an average bit rate close to 1 bit.
    We validated the binarization performance of BiLLM across multiple open-source
    LLM families and conducted generalization tests on a fine-tuned instruction model.
    BiLLM advances the bit-width quantization frontier of LLMs, promising to facilitate
    the deployment of LLMs in edge scenarios and resource-constrained devices, and
    encourages further exploration in LLMs compression.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§æ–°é¢–çš„è®­ç»ƒåäºŒå€¼é‡åŒ–æ–¹æ³•ï¼Œåä¸º BiLLMï¼Œä¸“é—¨é’ˆå¯¹å‹ç¼©é¢„è®­ç»ƒçš„ LLMs è¿›è¡Œè®¾è®¡ã€‚å—åˆ°æƒé‡å€¼å’Œ Hessian åˆ†å¸ƒç‰¹å¾çš„å¯å‘ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å¯¹ç»“æ„ä¸Šæ˜¾è‘—çš„æƒé‡è¿›è¡ŒäºŒå€¼æ®‹å·®é€¼è¿‘çš„æ–¹æ³•ï¼Œä»¥åœ¨è¶…ä½ä½ä¸‹ä¿ç•™å…¶èƒ½åŠ›ã€‚å¯¹äºéæ˜¾è‘—æƒé‡ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†åˆ†ç»„äºŒå€¼åŒ–çš„æœ€ä¼˜åˆ†å‰²æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒLLMs
    å¯ä»¥åœ¨è¶…ä½ä½ä¸‹è¿›è¡Œä¸€æ¬¡æƒé‡é‡åŒ–ï¼Œè€Œä¸ä¼šé€ æˆæ˜¾è‘—çš„ç²¾åº¦æŸå¤±ã€‚BiLLM åœ¨æ¥è¿‘ 1 ä½çš„å¹³å‡æ¯”ç‰¹ç‡ä¸‹ï¼Œé¦–æ¬¡å®ç°äº† LLM æ€§èƒ½ä¿è¯ã€‚æˆ‘ä»¬éªŒè¯äº† BiLLM åœ¨å¤šä¸ªå¼€æº
    LLM ç³»åˆ—ä¸Šçš„äºŒå€¼åŒ–æ€§èƒ½ï¼Œå¹¶å¯¹ç»è¿‡å¾®è°ƒçš„æŒ‡ä»¤æ¨¡å‹è¿›è¡Œäº†æ³›åŒ–æµ‹è¯•ã€‚BiLLM æ¨è¿›äº† LLM çš„æ¯”ç‰¹å®½åº¦é‡åŒ–å‰æ²¿ï¼Œæœ‰æœ›ä¿ƒè¿› LLM åœ¨è¾¹ç¼˜åœºæ™¯å’Œèµ„æºå—é™è®¾å¤‡ä¸Šçš„éƒ¨ç½²ï¼Œå¹¶é¼“åŠ±è¿›ä¸€æ­¥æ¢ç´¢
    LLM å‹ç¼©ã€‚
- en: 6 Impact Statements
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 å½±å“å£°æ˜
- en: This paper presents work whose goal is to advance the field of Machine Learning.
    There are many potential societal consequences of our work, none which we feel
    must be specifically highlighted here.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡å±•ç¤ºäº†æ—¨åœ¨æ¨åŠ¨æœºå™¨å­¦ä¹ é¢†åŸŸè¿›å±•çš„å·¥ä½œã€‚æˆ‘ä»¬çš„å·¥ä½œæœ‰è®¸å¤šæ½œåœ¨çš„ç¤¾ä¼šå½±å“ï¼Œä½†æˆ‘ä»¬è®¤ä¸ºè¿™é‡Œæ²¡æœ‰å¿…è¦ç‰¹åˆ«å¼ºè°ƒè¿™äº›å½±å“ã€‚
- en: References
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: Bengio etÂ al. (2013) Bengio, Y., LÃ©onard, N., and Courville, A. Estimating or
    propagating gradients through stochastic neurons for conditional computation.
    *arXiv preprint arXiv:1308.3432*, 2013.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio ç­‰ (2013) Bengio, Y., LÃ©onard, N., å’Œ Courville, A. é€šè¿‡éšæœºç¥ç»å…ƒè¿›è¡Œæ¡ä»¶è®¡ç®—çš„æ¢¯åº¦ä¼°è®¡æˆ–ä¼ æ’­ã€‚*arXiv
    é¢„å°æœ¬ arXiv:1308.3432*ï¼Œ2013ã€‚
- en: 'Bisk etÂ al. (2020) Bisk, Y., Zellers, R., Gao, J., Choi, Y., etÂ al. Piqa: Reasoning
    about physical commonsense in natural language. In *Proceedings of the AAAI conference
    on artificial intelligence*, volumeÂ 34, pp.Â  7432â€“7439, 2020.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bisk ç­‰ (2020) Bisk, Y., Zellers, R., Gao, J., Choi, Y., ç­‰ã€‚Piqaï¼šè‡ªç„¶è¯­è¨€ä¸­çš„ç‰©ç†å¸¸è¯†æ¨ç†ã€‚å‘è¡¨äº
    *AAAI äººå·¥æ™ºèƒ½ä¼šè®®è®ºæ–‡é›†*ï¼Œç¬¬ 34 å·ï¼Œé¡µç  7432â€“7439ï¼Œ2020ã€‚
- en: Blundell etÂ al. (2015) Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra,
    D. Weight uncertainty in neural network. In *International conference on machine
    learning*, pp.Â  1613â€“1622\. PMLR, 2015.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blundell et al. (2015) Blundell, C., Cornebise, J., Kavukcuoglu, K., å’Œ Wierstra,
    D. ç¥ç»ç½‘ç»œä¸­çš„æƒé‡ä¸ç¡®å®šæ€§ã€‚è§äº*å›½é™…æœºå™¨å­¦ä¹ ä¼šè®®*ï¼Œç¬¬1613â€“1622é¡µã€‚PMLRï¼Œ2015å¹´ã€‚
- en: Chan & Ioannidis (1998) Chan, C.-Y. and Ioannidis, Y.Â E. Bitmap index design
    and evaluation. In *Proceedings of the 1998 ACM SIGMOD international conference
    on Management of data*, pp.Â  355â€“366, 1998.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chan & Ioannidis (1998) Chan, C.-Y. å’Œ Ioannidis, Y. E. ä½å›¾ç´¢å¼•è®¾è®¡ä¸è¯„ä¼°ã€‚è§äº*1998å¹´ACM
    SIGMODå›½é™…æ•°æ®ç®¡ç†ä¼šè®®è®ºæ–‡é›†*ï¼Œç¬¬355â€“366é¡µï¼Œ1998å¹´ã€‚
- en: 'Chiang etÂ al. (2023) Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,
    H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J.Â E., etÂ al. Vicuna: An open-source
    chatbot impressing gpt-4 with 90%* chatgpt quality. *See https://vicuna. lmsys.
    org (accessed 14 April 2023)*, 2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chiang et al. (2023) Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,
    H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E., ç­‰äººã€‚Vicunaï¼šä¸€ä¸ªå¼€æºèŠå¤©æœºå™¨äººï¼Œå…¶GPT-4è¡¨ç°è¾¾åˆ°90%*ChatGPTè´¨é‡*ã€‚*å‚è§
    https://vicuna.lmsys.orgï¼ˆè®¿é—®æ—¥æœŸï¼š2023å¹´4æœˆ14æ—¥ï¼‰*ï¼Œ2023å¹´ã€‚
- en: 'Clark etÂ al. (2019) Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins,
    M., and Toutanova, K. Boolq: Exploring the surprising difficulty of natural yes/no
    questions. *arXiv preprint arXiv:1905.10044*, 2019.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark et al. (2019) Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins,
    M., å’Œ Toutanova, K. Boolqï¼šæ¢ç´¢è‡ªç„¶æ˜¯/å¦é—®é¢˜çš„æ„å¤–éš¾åº¦ã€‚*arXivé¢„å°æœ¬ arXiv:1905.10044*ï¼Œ2019å¹´ã€‚
- en: Clark etÂ al. (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal,
    A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try
    arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*, 2018.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark et al. (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal,
    A., Schoenick, C., å’Œ Tafjord, O. è®¤ä¸ºä½ å·²ç»è§£å†³äº†é—®ç­”é—®é¢˜ï¼Ÿå°è¯•ARCï¼ŒAI2æ¨ç†æŒ‘æˆ˜ã€‚*arXivé¢„å°æœ¬ arXiv:1803.05457*ï¼Œ2018å¹´ã€‚
- en: 'Courbariaux etÂ al. (2016) Courbariaux, M., Hubara, I., Soudry, D., El-Yaniv,
    R., and Bengio, Y. Binarized neural networks: Training deep neural networks with
    weights and activations constrained to+ 1 or-1. *arXiv preprint arXiv:1602.02830*,
    2016.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Courbariaux et al. (2016) Courbariaux, M., Hubara, I., Soudry, D., El-Yaniv,
    R., å’Œ Bengio, Y. äºŒå€¼ç¥ç»ç½‘ç»œï¼šè®­ç»ƒå…·æœ‰æƒé‡å’Œæ¿€æ´»å€¼è¢«é™åˆ¶ä¸º+1æˆ–-1çš„æ·±åº¦ç¥ç»ç½‘ç»œã€‚*arXivé¢„å°æœ¬ arXiv:1602.02830*ï¼Œ2016å¹´ã€‚
- en: 'Dettmers etÂ al. (2022) Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer,
    L. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. *arXiv
    preprint arXiv:2208.07339*, 2022.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers et al. (2022) Dettmers, T., Lewis, M., Belkada, Y., å’Œ Zettlemoyer,
    L. LLM. int8()ï¼šå¤§è§„æ¨¡å˜æ¢å™¨çš„8ä½çŸ©é˜µä¹˜æ³•ã€‚*arXivé¢„å°æœ¬ arXiv:2208.07339*ï¼Œ2022å¹´ã€‚
- en: 'Dettmers etÂ al. (2023a) Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer,
    L. Qlora: Efficient finetuning of quantized llms. *arXiv preprint arXiv:2305.14314*,
    2023a.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers et al. (2023a) Dettmers, T., Pagnoni, A., Holtzman, A., å’Œ Zettlemoyer,
    L. Qloraï¼šé«˜æ•ˆå¾®è°ƒé‡åŒ–LLMsã€‚*arXivé¢„å°æœ¬ arXiv:2305.14314*ï¼Œ2023aã€‚
- en: 'Dettmers etÂ al. (2023b) Dettmers, T., Svirschevski, R., Egiazarian, V., Kuznedelev,
    D., Frantar, E., Ashkboos, S., Borzunov, A., Hoefler, T., and Alistarh, D. Spqr:
    A sparse-quantized representation for near-lossless llm weight compression. *arXiv
    preprint arXiv:2306.03078*, 2023b.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers et al. (2023b) Dettmers, T., Svirschevski, R., Egiazarian, V., Kuznedelev,
    D., Frantar, E., Ashkboos, S., Borzunov, A., Hoefler, T., å’Œ Alistarh, D. Spqrï¼šä¸€ç§ç¨€ç–é‡åŒ–è¡¨ç¤ºï¼Œç”¨äºè¿‘æ— æŸçš„LLMæƒé‡å‹ç¼©ã€‚*arXivé¢„å°æœ¬
    arXiv:2306.03078*ï¼Œ2023bã€‚
- en: 'Dong etÂ al. (2019) Dong, Z., Yao, Z., Gholami, A., Mahoney, M.Â W., and Keutzer,
    K. Hawq: Hessian aware quantization of neural networks with mixed-precision. In
    *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pp.Â 
    293â€“302, 2019.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong et al. (2019) Dong, Z., Yao, Z., Gholami, A., Mahoney, M. W., å’Œ Keutzer,
    K. Hawqï¼šå…·æœ‰æ··åˆç²¾åº¦çš„ç¥ç»ç½‘ç»œæµ·æ£®çŸ©é˜µæ„ŸçŸ¥é‡åŒ–ã€‚è§äº*IEEE/CVFå›½é™…è®¡ç®—æœºè§†è§‰ä¼šè®®è®ºæ–‡é›†*ï¼Œç¬¬293â€“302é¡µï¼Œ2019å¹´ã€‚
- en: 'Fang etÂ al. (2020) Fang, J., Shafiee, A., Abdel-Aziz, H., Thorsley, D., Georgiadis,
    G., and Hassoun, J.Â H. Post-training piecewise linear quantization for deep neural
    networks. In *Computer Visionâ€“ECCV 2020: 16th European Conference, Glasgow, UK,
    August 23â€“28, 2020, Proceedings, Part II 16*, pp.Â  69â€“86\. Springer, 2020.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang et al. (2020) Fang, J., Shafiee, A., Abdel-Aziz, H., Thorsley, D., Georgiadis,
    G., å’Œ Hassoun, J. H. è®­ç»ƒååˆ†æ®µçº¿æ€§é‡åŒ–æ·±åº¦ç¥ç»ç½‘ç»œã€‚è§äº*è®¡ç®—æœºè§†è§‰â€“ECCV 2020ï¼šç¬¬16å±Šæ¬§æ´²ä¼šè®®ï¼Œè‹±å›½æ ¼æ‹‰æ–¯å“¥ï¼Œ2020å¹´8æœˆ23â€“28æ—¥ï¼Œè®ºæ–‡é›†ï¼Œç¬¬IIéƒ¨åˆ†16*ï¼Œç¬¬69â€“86é¡µã€‚Springerï¼Œ2020å¹´ã€‚
- en: 'Faraone etÂ al. (2018) Faraone, J., Fraser, N., Blott, M., and Leong, P.Â H.
    Syq: Learning symmetric quantization for efficient deep neural networks. In *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, pp.Â  4300â€“4309,
    2018.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Faraone et al. (2018) Faraone, J., Fraser, N., Blott, M., å’Œ Leong, P. H. Syqï¼šå­¦ä¹ å¯¹ç§°é‡åŒ–ä»¥æé«˜æ·±åº¦ç¥ç»ç½‘ç»œçš„æ•ˆç‡ã€‚è§äº*IEEEè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†*ï¼Œç¬¬4300â€“4309é¡µï¼Œ2018å¹´ã€‚
- en: 'Frantar & Alistarh (2022) Frantar, E. and Alistarh, D. Optimal brain compression:
    A framework for accurate post-training quantization and pruning. *Advances in
    Neural Information Processing Systems*, 35:4475â€“4488, 2022.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar & Alistarhï¼ˆ2022ï¼‰Frantar, E. å’Œ Alistarh, D. æœ€ä¼˜è„‘å‹ç¼©: ä¸€ä¸ªå‡†ç¡®çš„åè®­ç»ƒé‡åŒ–å’Œå‰ªææ¡†æ¶ã€‚*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•*ï¼Œ35:4475â€“4488ï¼Œ2022ã€‚'
- en: 'Frantar & Alistarh (2023) Frantar, E. and Alistarh, D. Sparsegpt: Massive language
    models can be accurately pruned in one-shot. In *International Conference on Machine
    Learning*, pp.Â  10323â€“10337\. PMLR, 2023.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar & Alistarhï¼ˆ2023ï¼‰Frantar, E. å’Œ Alistarh, D. Sparsegpt: å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹å¯ä»¥åœ¨ä¸€æ¬¡å‰ªæä¸­å‡†ç¡®åœ°è¢«å‰ªæã€‚æ”¶å½•äº
    *å›½é™…æœºå™¨å­¦ä¹ ä¼šè®®*ï¼Œç¬¬10323â€“10337é¡µï¼ŒPMLRï¼Œ2023ã€‚'
- en: 'Frantar etÂ al. (2022) Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh,
    D. Gptq: Accurate post-training quantization for generative pre-trained transformers.
    *arXiv preprint arXiv:2210.17323*, 2022.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar ç­‰ï¼ˆ2022ï¼‰Frantar, E., Ashkboos, S., Hoefler, T., å’Œ Alistarh, D. Gptq:
    é’ˆå¯¹ç”Ÿæˆé¢„è®­ç»ƒå˜æ¢å™¨çš„å‡†ç¡®åè®­ç»ƒé‡åŒ–ã€‚*arXiv é¢„å°æœ¬ arXiv:2210.17323*ï¼Œ2022ã€‚'
- en: 'Helwegen etÂ al. (2019) Helwegen, K., Widdicombe, J., Geiger, L., Liu, Z., Cheng,
    K.-T., and Nusselder, R. Latent weights do not exist: Rethinking binarized neural
    network optimization. *Advances in neural information processing systems*, 32,
    2019.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Helwegen ç­‰ï¼ˆ2019ï¼‰Helwegen, K., Widdicombe, J., Geiger, L., Liu, Z., Cheng, K.-T.,
    å’Œ Nusselder, R. æ½œåœ¨æƒé‡ä¸å­˜åœ¨: é‡æ–°æ€è€ƒäºŒå€¼ç¥ç»ç½‘ç»œä¼˜åŒ–ã€‚*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•*ï¼Œ32ï¼Œ2019ã€‚'
- en: Jacob etÂ al. (2018) Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard,
    A., Adam, H., and Kalenichenko, D. Quantization and training of neural networks
    for efficient integer-arithmetic-only inference. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, pp.Â  2704â€“2713, 2018.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jacob ç­‰ï¼ˆ2018ï¼‰Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A.,
    Adam, H., å’Œ Kalenichenko, D. é‡åŒ–å’Œè®­ç»ƒç¥ç»ç½‘ç»œä»¥å®ç°é«˜æ•ˆçš„æ•´æ•°ç®—æœ¯æ¨ç†ã€‚æ”¶å½•äº *IEEE è®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†*ï¼Œç¬¬2704â€“2713é¡µï¼Œ2018ã€‚
- en: 'Jain etÂ al. (2019) Jain, S., Venkataramani, S., Srinivasan, V., Choi, J., Gopalakrishnan,
    K., and Chang, L. Biscaled-dnn: Quantizing long-tailed datastructures with two
    scale factors for deep neural networks. In *Proceedings of the 56th Annual Design
    Automation Conference 2019*, pp.Â  1â€“6, 2019.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jain ç­‰ï¼ˆ2019ï¼‰Jain, S., Venkataramani, S., Srinivasan, V., Choi, J., Gopalakrishnan,
    K., å’Œ Chang, L. Biscaled-dnn: é€šè¿‡ä¸¤ä¸ªå°ºåº¦å› å­å¯¹æ·±åº¦ç¥ç»ç½‘ç»œä¸­çš„é•¿å°¾æ•°æ®ç»“æ„è¿›è¡Œé‡åŒ–ã€‚æ”¶å½•äº *ç¬¬56å±Šå¹´åº¦è®¾è®¡è‡ªåŠ¨åŒ–ä¼šè®®è®ºæ–‡é›†
    2019*ï¼Œç¬¬1â€“6é¡µï¼Œ2019ã€‚'
- en: LeCun etÂ al. (1989) LeCun, Y., Denker, J., and Solla, S. Optimal brain damage.
    *Advances in neural information processing systems*, 2, 1989.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun ç­‰ï¼ˆ1989ï¼‰LeCun, Y., Denker, J., å’Œ Solla, S. æœ€ä¼˜è„‘æŸä¼¤ã€‚*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•*ï¼Œ2ï¼Œ1989ã€‚
- en: 'Lee etÂ al. (2023) Lee, C., Jin, J., Kim, T., Kim, H., and Park, E. Owq: Lessons
    learned from activation outliers for weight quantization in large language models.
    *arXiv preprint arXiv:2306.02272*, 2023.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lee ç­‰ï¼ˆ2023ï¼‰Lee, C., Jin, J., Kim, T., Kim, H., å’Œ Park, E. Owq: ä»æ¿€æ´»å¼‚å¸¸å€¼ä¸­è·å¾—çš„æ•™è®­ï¼Œä»¥è¿›è¡Œå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„æƒé‡é‡åŒ–ã€‚*arXiv
    é¢„å°æœ¬ arXiv:2306.02272*ï¼Œ2023ã€‚'
- en: 'Li etÂ al. (2021) Li, Y., Gong, R., Tan, X., Yang, Y., Hu, P., Zhang, Q., Yu,
    F., Wang, W., and Gu, S. Brecq: Pushing the limit of post-training quantization
    by block reconstruction. *arXiv preprint arXiv:2102.05426*, 2021.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li ç­‰ï¼ˆ2021ï¼‰Li, Y., Gong, R., Tan, X., Yang, Y., Hu, P., Zhang, Q., Yu, F., Wang,
    W., å’Œ Gu, S. Brecq: é€šè¿‡å—é‡æ„æ¨åŠ¨åè®­ç»ƒé‡åŒ–çš„æé™ã€‚*arXiv é¢„å°æœ¬ arXiv:2102.05426*ï¼Œ2021ã€‚'
- en: Li etÂ al. (2017) Li, Z., Ni, B., Zhang, W., Yang, X., and Gao, W. Performance
    guaranteed network acceleration via high-order residual quantization. In *Proceedings
    of the IEEE international conference on computer vision*, pp.Â  2584â€“2592, 2017.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li ç­‰ï¼ˆ2017ï¼‰Li, Z., Ni, B., Zhang, W., Yang, X., å’Œ Gao, W. é€šè¿‡é«˜é˜¶æ®‹å·®é‡åŒ–ä¿è¯æ€§èƒ½çš„ç½‘ç»œåŠ é€Ÿã€‚æ”¶å½•äº
    *IEEE å›½é™…è®¡ç®—æœºè§†è§‰ä¼šè®®è®ºæ–‡é›†*ï¼Œç¬¬2584â€“2592é¡µï¼Œ2017ã€‚
- en: 'Lin etÂ al. (2023) Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., and Han,
    S. Awq: Activation-aware weight quantization for llm compression and acceleration.
    *arXiv preprint arXiv:2306.00978*, 2023.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin ç­‰ï¼ˆ2023ï¼‰Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., å’Œ Han, S. Awq:
    é’ˆå¯¹ LLM å‹ç¼©å’ŒåŠ é€Ÿçš„æ¿€æ´»æ„ŸçŸ¥æƒé‡é‡åŒ–ã€‚*arXiv é¢„å°æœ¬ arXiv:2306.00978*ï¼Œ2023ã€‚'
- en: 'Liu etÂ al. (2023) Liu, Z., Oguz, B., Zhao, C., Chang, E., Stock, P., Mehdad,
    Y., Shi, Y., Krishnamoorthi, R., and Chandra, V. Llm-qat: Data-free quantization
    aware training for large language models. *arXiv preprint arXiv:2305.17888*, 2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu ç­‰ï¼ˆ2023ï¼‰Liu, Z., Oguz, B., Zhao, C., Chang, E., Stock, P., Mehdad, Y., Shi,
    Y., Krishnamoorthi, R., å’Œ Chandra, V. Llm-qat: é’ˆå¯¹å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„æ•°æ®æ— å…³é‡åŒ–æ„ŸçŸ¥è®­ç»ƒã€‚*arXiv é¢„å°æœ¬
    arXiv:2305.17888*ï¼Œ2023ã€‚'
- en: 'Marcus etÂ al. (1994) Marcus, M., Kim, G., Marcinkiewicz, M.Â A., MacIntyre,
    R., Bies, A., Ferguson, M., Katz, K., and Schasberger, B. The penn treebank: Annotating
    predicate argument structure. In *Human Language Technology: Proceedings of a
    Workshop held at Plainsboro, New Jersey, March 8-11, 1994*, 1994.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Marcusç­‰äººï¼ˆ1994ï¼‰Marcus, M., Kim, G., Marcinkiewicz, M. A., MacIntyre, R., Bies,
    A., Ferguson, M., Katz, K., å’Œ Schasberger, B. Penn Treebank: æ³¨é‡Šè°“è¯å‚æ•°ç»“æ„ã€‚è§äº*äººç±»è¯­è¨€æŠ€æœ¯ï¼šåœ¨æ–°æ³½è¥¿å·æ™®è±æ©æ–¯ä¼¯å‹’ä¸¾è¡Œçš„ç ”è®¨ä¼šè®°å½•ï¼Œ1994å¹´3æœˆ8-11æ—¥*ï¼Œ1994å¹´ã€‚'
- en: Merity etÂ al. (2016) Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer
    sentinel mixture models. *arXiv preprint arXiv:1609.07843*, 2016.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merityç­‰äººï¼ˆ2016ï¼‰Merity, S., Xiong, C., Bradbury, J., å’Œ Socher, R. æŒ‡é’ˆå®ˆå«æ··åˆæ¨¡å‹ã€‚*arXivé¢„å°æœ¬
    arXiv:1609.07843*ï¼Œ2016å¹´ã€‚
- en: Mihaylov etÂ al. (2018) Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A.
    Can a suit of armor conduct electricity? a new dataset for open book question
    answering. *arXiv preprint arXiv:1809.02789*, 2018.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mihaylovç­‰äººï¼ˆ2018ï¼‰Mihaylov, T., Clark, P., Khot, T., å’Œ Sabharwal, A. ä¸€å¥—ç›”ç”²èƒ½å¯¼ç”µå—ï¼Ÿä¸€ä¸ªç”¨äºå¼€æ”¾ä¹¦ç±é—®ç­”çš„æ–°æ•°æ®é›†ã€‚*arXivé¢„å°æœ¬
    arXiv:1809.02789*ï¼Œ2018å¹´ã€‚
- en: Park etÂ al. (2018) Park, E., Yoo, S., and Vajda, P. Value-aware quantization
    for training and inference of neural networks. In *Proceedings of the European
    Conference on Computer Vision (ECCV)*, pp.Â  580â€“595, 2018.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parkç­‰äººï¼ˆ2018ï¼‰Park, E., Yoo, S., å’Œ Vajda, P. ä»·å€¼æ„ŸçŸ¥é‡åŒ–ç”¨äºç¥ç»ç½‘ç»œçš„è®­ç»ƒå’Œæ¨ç†ã€‚è§äº*æ¬§æ´²è®¡ç®—æœºè§†è§‰å¤§ä¼šï¼ˆECCVï¼‰ä¼šè®®è®°å½•*ï¼Œç¬¬580â€“595é¡µï¼Œ2018å¹´ã€‚
- en: 'Paszke etÂ al. (2019) Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury,
    J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., etÂ al. Pytorch:
    An imperative style, high-performance deep learning library. *Advances in neural
    information processing systems*, 32, 2019.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Paszkeç­‰äººï¼ˆ2019ï¼‰Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan,
    G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., ç­‰äººã€‚Pytorch: ä¸€ç§å‘½ä»¤å¼é£æ ¼çš„é«˜æ€§èƒ½æ·±åº¦å­¦ä¹ åº“ã€‚*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•*ï¼Œ32ï¼Œ2019å¹´ã€‚'
- en: Qin etÂ al. (2020) Qin, H., Gong, R., Liu, X., Shen, M., Wei, Z., Yu, F., and
    Song, J. Forward and backward information retention for accurate binary neural
    networks. In *Proceedings of the IEEE/CVF conference on computer vision and pattern
    recognition*, pp.Â  2250â€“2259, 2020.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qinç­‰äººï¼ˆ2020ï¼‰Qin, H., Gong, R., Liu, X., Shen, M., Wei, Z., Yu, F., å’Œ Song, J.
    ç²¾ç¡®äºŒå€¼ç¥ç»ç½‘ç»œçš„å‰å‘å’Œåå‘ä¿¡æ¯ä¿ç•™ã€‚è§äº*IEEE/CVFè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®è®°å½•*ï¼Œç¬¬2250â€“2259é¡µï¼Œ2020å¹´ã€‚
- en: 'Qin etÂ al. (2022) Qin, H., Ding, Y., Zhang, M., Yan, Q., Liu, A., Dang, Q.,
    Liu, Z., and Liu, X. Bibert: Accurate fully binarized bert. *arXiv preprint arXiv:2203.06390*,
    2022.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qinç­‰äººï¼ˆ2022ï¼‰Qin, H., Ding, Y., Zhang, M., Yan, Q., Liu, A., Dang, Q., Liu, Z.,
    å’Œ Liu, X. Bibert: ç²¾ç¡®çš„å®Œå…¨äºŒå€¼åŒ–BERTã€‚*arXivé¢„å°æœ¬ arXiv:2203.06390*ï¼Œ2022å¹´ã€‚'
- en: 'Qin etÂ al. (2023) Qin, H., Zhang, M., Ding, Y., Li, A., Cai, Z., Liu, Z., Yu,
    F., and Liu, X. Bibench: Benchmarking and analyzing network binarization. *arXiv
    preprint arXiv:2301.11233*, 2023.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qinç­‰äººï¼ˆ2023ï¼‰Qin, H., Zhang, M., Ding, Y., Li, A., Cai, Z., Liu, Z., Yu, F.,
    å’Œ Liu, X. Bibench: ç½‘ç»œäºŒå€¼åŒ–çš„åŸºå‡†æµ‹è¯•ä¸åˆ†æã€‚*arXivé¢„å°æœ¬ arXiv:2301.11233*ï¼Œ2023å¹´ã€‚'
- en: Raffel etÂ al. (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang,
    S., Matena, M., Zhou, Y., Li, W., and Liu, P.Â J. Exploring the limits of transfer
    learning with a unified text-to-text transformer. *The Journal of Machine Learning
    Research*, 21(1):5485â€“5551, 2020.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffelç­‰äººï¼ˆ2020ï¼‰Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena,
    M., Zhou, Y., Li, W., å’Œ Liu, P. J. æ¢ç´¢ç»Ÿä¸€æ–‡æœ¬åˆ°æ–‡æœ¬å˜æ¢å™¨çš„è¿ç§»å­¦ä¹ æé™ã€‚*æœºå™¨å­¦ä¹ ç ”ç©¶æ‚å¿—*ï¼Œ21(1):5485â€“5551ï¼Œ2020å¹´ã€‚
- en: 'Rastegari etÂ al. (2016) Rastegari, M., Ordonez, V., Redmon, J., and Farhadi,
    A. Xnor-net: Imagenet classification using binary convolutional neural networks.
    In *European conference on computer vision*, pp.Â  525â€“542\. Springer, 2016.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rastegariç­‰äººï¼ˆ2016ï¼‰Rastegari, M., Ordonez, V., Redmon, J., å’Œ Farhadi, A. Xnor-net:
    ä½¿ç”¨äºŒè¿›åˆ¶å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡ŒImageNetåˆ†ç±»ã€‚è§äº*æ¬§æ´²è®¡ç®—æœºè§†è§‰å¤§ä¼š*ï¼Œç¬¬525â€“542é¡µï¼ŒSpringerï¼Œ2016å¹´ã€‚'
- en: 'Sakaguchi etÂ al. (2021) Sakaguchi, K., Bras, R.Â L., Bhagavatula, C., and Choi,
    Y. Winogrande: An adversarial winograd schema challenge at scale. *Communications
    of the ACM*, 64(9):99â€“106, 2021.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sakaguchiç­‰äººï¼ˆ2021ï¼‰Sakaguchi, K., Bras, R. L., Bhagavatula, C., å’Œ Choi, Y. Winogrande:
    å¤§è§„æ¨¡çš„å¯¹æŠ—æ€§Winogradæ¨¡å¼æŒ‘æˆ˜ã€‚*ACMé€šè®¯*ï¼Œ64(9):99â€“106ï¼Œ2021å¹´ã€‚'
- en: Sanh etÂ al. (2021) Sanh, V., Webson, A., Raffel, C., Bach, S.Â H., Sutawika,
    L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T.Â L., Raja, A., etÂ al. Multitask
    prompted training enables zero-shot task generalization. *arXiv preprint arXiv:2110.08207*,
    2021.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanhç­‰äººï¼ˆ2021ï¼‰Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai,
    Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., ç­‰äººã€‚å¤šä»»åŠ¡æç¤ºè®­ç»ƒä½¿é›¶æ ·æœ¬ä»»åŠ¡æ³›åŒ–æˆä¸ºå¯èƒ½ã€‚*arXivé¢„å°æœ¬
    arXiv:2110.08207*ï¼Œ2021å¹´ã€‚
- en: 'Shang etÂ al. (2023) Shang, Y., Yuan, Z., Wu, Q., and Dong, Z. Pb-llm: Partially
    binarized large language models. *arXiv preprint arXiv:2310.00034*, 2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shangç­‰äººï¼ˆ2023ï¼‰Shang, Y., Yuan, Z., Wu, Q., å’Œ Dong, Z. Pb-llm: éƒ¨åˆ†äºŒå€¼åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚*arXivé¢„å°æœ¬
    arXiv:2310.00034*ï¼Œ2023å¹´ã€‚'
- en: 'Touvron etÂ al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X.,
    Lachaux, M.-A., Lacroix, T., RoziÃ¨re, B., Goyal, N., Hambro, E., Azhar, F., etÂ al.
    Llama: Open and efficient foundation language models. *arXiv preprint arXiv:2302.13971*,
    2023a.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X.,
    Lachaux, M.-A., Lacroix, T., RoziÃ¨re, B., Goyal, N., Hambro, E., Azhar, F., ç­‰ã€‚Llama:
    å¼€æ”¾è€Œé«˜æ•ˆçš„åŸºç¡€è¯­è¨€æ¨¡å‹ã€‚*arXiv é¢„å°æœ¬ arXiv:2302.13971*ï¼Œ2023aã€‚'
- en: 'Touvron etÂ al. (2023b) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., etÂ al. Llama
    2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*,
    2023b.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023b) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., ç­‰ã€‚Llama 2:
    å¼€æ”¾åŸºç¡€å’Œå¾®è°ƒèŠå¤©æ¨¡å‹ã€‚*arXiv é¢„å°æœ¬ arXiv:2307.09288*ï¼Œ2023bã€‚'
- en: Vaswani etÂ al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A.Â N., Kaiser, Å., and Polosukhin, I. Attention is all you need. *Advances
    in neural information processing systems*, 30, 2017.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A. N., Kaiser, Å., å’Œ Polosukhin, I. æ³¨æ„åŠ›æœºåˆ¶æ‰æ˜¯ä½ æ‰€éœ€è¦çš„ä¸€åˆ‡ã€‚*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•*ï¼Œ30ï¼Œ2017ã€‚
- en: 'Wang etÂ al. (2023) Wang, H., Ma, S., Dong, L., Huang, S., Wang, H., Ma, L.,
    Yang, F., Wang, R., Wu, Y., and Wei, F. Bitnet: Scaling 1-bit transformers for
    large language models. *arXiv preprint arXiv:2310.11453*, 2023.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2023) Wang, H., Ma, S., Dong, L., Huang, S., Wang, H., Ma, L.,
    Yang, F., Wang, R., Wu, Y., å’Œ Wei, F. Bitnet: ä¸ºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æ‰©å±• 1 ä½å˜æ¢å™¨ã€‚*arXiv é¢„å°æœ¬ arXiv:2310.11453*ï¼Œ2023ã€‚'
- en: Wei etÂ al. (2021) Wei, J., Bosma, M., Zhao, V.Â Y., Guu, K., Yu, A.Â W., Lester,
    B., Du, N., Dai, A.Â M., and Le, Q.Â V. Finetuned language models are zero-shot
    learners. *arXiv preprint arXiv:2109.01652*, 2021.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2021) Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester,
    B., Du, N., Dai, A. M., å’Œ Le, Q. V. å¾®è°ƒè¯­è¨€æ¨¡å‹æ˜¯é›¶æ ·æœ¬å­¦ä¹ è€…ã€‚*arXiv é¢„å°æœ¬ arXiv:2109.01652*ï¼Œ2021ã€‚
- en: 'Wolf etÂ al. (2019) Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C.,
    Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., etÂ al. Huggingfaceâ€™s
    transformers: State-of-the-art natural language processing. *arXiv preprint arXiv:1910.03771*,
    2019.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wolf et al. (2019) Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C.,
    Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., ç­‰ã€‚Huggingface çš„å˜æ¢å™¨ï¼šæœ€å…ˆè¿›çš„è‡ªç„¶è¯­è¨€å¤„ç†ã€‚*arXiv
    é¢„å°æœ¬ arXiv:1910.03771*ï¼Œ2019ã€‚
- en: 'Xiao etÂ al. (2023) Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and
    Han, S. Smoothquant: Accurate and efficient post-training quantization for large
    language models. In *International Conference on Machine Learning*, pp.Â  38087â€“38099\.
    PMLR, 2023.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao et al. (2023) Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., å’Œ Han,
    S. Smoothquant: ç²¾ç¡®ä¸”é«˜æ•ˆçš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åè®­ç»ƒé‡åŒ–ã€‚åœ¨*å›½é™…æœºå™¨å­¦ä¹ ä¼šè®®*ï¼Œç¬¬ 38087â€“38099 é¡µã€‚PMLRï¼Œ2023ã€‚'
- en: (47) Yao, Z., Aminabadi, R., Zhang, M., Wu, X., Li, C., and He, Y.Â Z. Efficient
    and affordable post-training quantization for large-scale transformers, 2022.
    *URL https://arxiv. org/abs/2206.01861*.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (47) Yao, Z., Aminabadi, R., Zhang, M., Wu, X., Li, C., å’Œ He, Y. Z. å¤§è§„æ¨¡å˜æ¢å™¨çš„é«˜æ•ˆä¸”ç»æµçš„åè®­ç»ƒé‡åŒ–ï¼Œ2022ã€‚*URL
    https://arxiv.org/abs/2206.01861*ã€‚
- en: Yao etÂ al. (2023) Yao, Z., Li, C., Wu, X., Youn, S., and He, Y. A comprehensive
    study on post-training quantization for large language models. *arXiv preprint
    arXiv:2303.08302*, 2023.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. (2023) Yao, Z., Li, C., Wu, X., Youn, S., å’Œ He, Y. å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„åè®­ç»ƒé‡åŒ–ç»¼åˆç ”ç©¶ã€‚*arXiv
    é¢„å°æœ¬ arXiv:2303.08302*ï¼Œ2023ã€‚
- en: 'You (2010) You, Y. *Audio coding: theory and applications*. Springer Science
    & Business Media, 2010.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: You (2010) You, Y. *éŸ³é¢‘ç¼–ç ï¼šç†è®ºä¸åº”ç”¨*ã€‚Springer Science & Business Mediaï¼Œ2010ã€‚
- en: 'Zellers etÂ al. (2019) Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and
    Choi, Y. Hellaswag: Can a machine really finish your sentence? *arXiv preprint
    arXiv:1905.07830*, 2019.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zellers et al. (2019) Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., å’Œ Choi,
    Y. Hellaswag: æœºå™¨çœŸçš„èƒ½å®Œæˆä½ çš„å¥å­å—ï¼Ÿ*arXiv é¢„å°æœ¬ arXiv:1905.07830*ï¼Œ2019ã€‚'
- en: 'Zhang etÂ al. (2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
    Chen, S., Dewan, C., Diab, M., Li, X., Lin, X.Â V., etÂ al. Opt: Open pre-trained
    transformer language models. *arXiv preprint arXiv:2205.01068*, 2022.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
    Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., ç­‰ã€‚Opt: å¼€æ”¾é¢„è®­ç»ƒçš„å˜æ¢å™¨è¯­è¨€æ¨¡å‹ã€‚*arXiv
    é¢„å°æœ¬ arXiv:2205.01068*ï¼Œ2022ã€‚'
- en: 'Zhou etÂ al. (2016) Zhou, S., Wu, Y., Ni, Z., Zhou, X., Wen, H., and Zou, Y.
    Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth
    gradients. *arXiv preprint arXiv:1606.06160*, 2016.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou et al. (2016) Zhou, S., Wu, Y., Ni, Z., Zhou, X., Wen, H., å’Œ Zou, Y. Dorefa-net:
    ç”¨ä½ä½å®½æ¢¯åº¦è®­ç»ƒä½ä½å®½å·ç§¯ç¥ç»ç½‘ç»œã€‚*arXiv é¢„å°æœ¬ arXiv:1606.06160*ï¼Œ2016ã€‚'
- en: Zhu etÂ al. (2023) Zhu, X., Li, J., Liu, Y., Ma, C., and Wang, W. A survey on
    model compression for large language models. *arXiv preprint arXiv:2308.07633*,
    2023.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. (2023) Zhu, X., Li, J., Liu, Y., Ma, C., å’Œ Wang, W. å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„æ¨¡å‹å‹ç¼©è°ƒæŸ¥ã€‚*arXiv
    é¢„å°æœ¬ arXiv:2308.07633*ï¼Œ2023ã€‚
- en: Appendix A BiLLM Implementation
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é™„å½• A BiLLM å®ç°
- en: 'Algorithm 2 BiLLM: Detailed functions process'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç®—æ³• 2 BiLLM: è¯¦ç»†å‡½æ•°è¿‡ç¨‹'
- en: func $\operatorname{salient}$
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: å‡½æ•° $\operatorname{salient}$
- en: 1:Â Â $\mathbf{S}\coloneqq\mathbf{W}^{2}/[\mathbf{H}^{c}_{b:b+\beta b:b+\beta}]^{2}\
    $
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 1:Â Â $\mathbf{S}\coloneqq\mathbf{W}^{2}/[\mathbf{H}^{c}_{b:b+\beta b:b+\beta}]^{2}\
    $
- en: func $\operatorname{binary}$
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: å‡½æ•° $\operatorname{binary}$
- en: 1:Â Â $\alpha\coloneqq\dfrac{||\mathbf{W}||_{\ell 1}}{m}$
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 1:Â Â $\alpha\coloneqq\dfrac{||\mathbf{W}||_{\ell 1}}{m}$
- en: func $\operatorname{res\_approximation}$
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: å‡½æ•° $\operatorname{res\_approximation}$
- en: 1:Â Â $\mathbf{B}_{1}\coloneqq\operatorname{binary}(\mathbf{W})$
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 1:Â Â $\mathbf{B}_{1}\coloneqq\operatorname{binary}(\mathbf{W})$
- en: func $\operatorname{seg\_search}$
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: å‡½æ•° $\operatorname{seg\_search}$
- en: 1:Â Â $e=\operatorname{inf}$
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 1:Â Â $e=\operatorname{inf}$
- en: 'BiLLM necessitates the structured selection of salient rows and their subsequent
    quantization through residual approximation binarization. This is followed by
    dividing the non-salient weights, which exhibit a bell-shaped distribution, into
    a sparse area and a concentrated area. The division requires the optimization
    of the segmentation point $p^{*}$ by minimizing quantization loss. Ultimately,
    the two regions of non-salient weights are binarized separately to derive the
    final binary weights for LLMs. The implementation details of the aforementioned
    function are enumerated in Algorithm [2](#alg2 "Algorithm 2 â€£ Appendix A BiLLM
    Implementation â€£ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs").'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 'BiLLM éœ€è¦ç»“æ„åŒ–é€‰æ‹©æ˜¾è‘—è¡Œï¼Œå¹¶é€šè¿‡æ®‹å·®é€¼è¿‘äºŒå€¼åŒ–è¿›è¡Œé‡åŒ–ã€‚ç„¶åå°†è¡¨ç°ä¸ºé’Ÿå½¢åˆ†å¸ƒçš„éæ˜¾è‘—æƒé‡åˆ†ä¸ºç¨€ç–åŒºåŸŸå’Œé›†ä¸­åŒºåŸŸã€‚æ­¤åˆ’åˆ†éœ€è¦é€šè¿‡æœ€å°åŒ–é‡åŒ–æŸå¤±æ¥ä¼˜åŒ–åˆ†å‰²ç‚¹
    $p^{*}$ã€‚æœ€ç»ˆï¼Œä¸¤ä¸ªéæ˜¾è‘—æƒé‡åŒºåŸŸåˆ†åˆ«è¿›è¡ŒäºŒå€¼åŒ–ï¼Œä»¥å¾—å‡º LLM çš„æœ€ç»ˆäºŒè¿›åˆ¶æƒé‡ã€‚ä¸Šè¿°å‡½æ•°çš„å®ç°ç»†èŠ‚åœ¨ç®—æ³• [2](#alg2 "ç®—æ³• 2 â€£ é™„å½•
    A BiLLM å®ç° â€£ BiLLM: æ¨åŠ¨ LLM åè®­ç»ƒé‡åŒ–çš„æé™") ä¸­åˆ—å‡ºã€‚'
- en: Appendix B Quantization Error
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é™„å½• B é‡åŒ–è¯¯å·®
- en: 'Quantization error definition for weight distribution The numerical range covered
    by the uniform quantizer spans from $[X_{min},X_{max}]$ represents the target
    bit-width of quantization. So the quantization step size is:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: æƒé‡åˆ†å¸ƒçš„é‡åŒ–è¯¯å·®å®šä¹‰ å‡åŒ€é‡åŒ–å™¨è¦†ç›–çš„æ•°å€¼èŒƒå›´ä» $[X_{min},X_{max}]$ ä»£è¡¨é‡åŒ–çš„ç›®æ ‡ä½å®½ã€‚å› æ­¤é‡åŒ–æ­¥é•¿ä¸ºï¼š
- en: '|  | $\Delta=\frac{X_{\text{max}}-X_{\text{min}}}{M}$ |  | (15) |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Delta=\frac{X_{\text{max}}-X_{\text{min}}}{M}$ |  | (15) |'
- en: 'The boundaries can be calculated as:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: è¾¹ç•Œå¯ä»¥è®¡ç®—ä¸ºï¼š
- en: '|  | $b_{q}=X_{\text{min}}+\Delta\cdot l$ |  | (16) |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '|  | $b_{q}=X_{\text{min}}+\Delta\cdot l$ |  | (16) |'
- en: 'where $l\in{0,1,...,M}$ under binarization. Then we give the mean of each interval:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $l\in{0,1,...,M}$ åœ¨äºŒå€¼åŒ–ä¸‹ã€‚ç„¶åæˆ‘ä»¬ç»™å‡ºæ¯ä¸ªåŒºé—´çš„å‡å€¼ï¼š
- en: '|  | $x_{q}=X_{\text{min}}+\Delta\cdot l-0.5\Delta$ |  | (17) |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  | $x_{q}=X_{\text{min}}+\Delta\cdot l-0.5\Delta$ |  | (17) |'
- en: 'where $l\in{1,...,M}$. In this quantization scheme, we can get the MSQE from
    Â (You, [2010](#bib.bib49)):'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $l\in{1,...,M}$ã€‚åœ¨æ­¤é‡åŒ–æ–¹æ¡ˆä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä» (You, [2010](#bib.bib49)) å¾—åˆ° MSQEï¼š
- en: '|  | $1$2 |  | (18) |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (18) |'
- en: 'then we let the $y$ part, so the EquationÂ ([18](#A2.E18 "Equation 18 â€£ Appendix
    B Quantization Error â€£ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs")) becomes:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç„¶åæˆ‘ä»¬è€ƒè™‘ $y$ éƒ¨åˆ†ï¼Œå› æ­¤æ–¹ç¨‹Â ([18](#A2.E18 "æ–¹ç¨‹ 18 â€£ é™„å½• B é‡åŒ–è¯¯å·® â€£ BiLLM: æ¨åŠ¨ LLM åè®­ç»ƒé‡åŒ–çš„æé™"))
    å˜ä¸ºï¼š'
- en: '|  | $1$2 |  | (19) |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (19) |'
- en: 'consider the EquationÂ ([16](#A2.E16 "Equation 16 â€£ Appendix B Quantization
    Error â€£ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs")) and
    EquationÂ ([17](#A2.E17 "Equation 17 â€£ Appendix B Quantization Error â€£ BiLLM: Pushing
    the Limit of Post-Training Quantization for LLMs")), the above equation becomes:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 'è€ƒè™‘æ–¹ç¨‹Â ([16](#A2.E16 "æ–¹ç¨‹ 16 â€£ é™„å½• B é‡åŒ–è¯¯å·® â€£ BiLLM: æ¨åŠ¨ LLM åè®­ç»ƒé‡åŒ–çš„æé™")) å’Œæ–¹ç¨‹Â ([17](#A2.E17
    "æ–¹ç¨‹ 17 â€£ é™„å½• B é‡åŒ–è¯¯å·® â€£ BiLLM: æ¨åŠ¨ LLM åè®­ç»ƒé‡åŒ–çš„æé™"))ï¼Œä¸Šè¿°æ–¹ç¨‹å˜ä¸ºï¼š'
- en: '|  | $\theta^{2}=\sum_{l=1}^{M}\int_{-0.5\Delta}^{0.5\Delta}x^{2}f(x_{p}-x)dx$
    |  | (20) |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta^{2}=\sum_{l=1}^{M}\int_{-0.5\Delta}^{0.5\Delta}x^{2}f(x_{p}-x)dx$
    |  | (20) |'
- en: 'The aforementioned reasoning indicates that the MSQE of a uniform quantizer
    depends on the PDF and the quantization bit-width. Due to previous observations
    of the weights in pretrained LLMs, we have eliminated the salient weights. The
    remaining distribution of non-salient weightsâ€™ $g(x)$ into EquationÂ ([18](#A2.E18
    "Equation 18 â€£ Appendix B Quantization Error â€£ BiLLM: Pushing the Limit of Post-Training
    Quantization for LLMs")), resulting in:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¸Šè¿°æ¨ç†è¡¨æ˜ï¼Œå‡åŒ€é‡åŒ–å™¨çš„ MSQE ä¾èµ–äº PDF å’Œé‡åŒ–ä½å®½ã€‚ç”±äºä¹‹å‰å¯¹é¢„è®­ç»ƒ LLM ä¸­æƒé‡çš„è§‚å¯Ÿï¼Œæˆ‘ä»¬å·²ç»æ¶ˆé™¤äº†æ˜¾è‘—æƒé‡ã€‚å‰©ä½™çš„éæ˜¾è‘—æƒé‡çš„åˆ†å¸ƒ
    $g(x)$ ä»£å…¥æ–¹ç¨‹Â ([18](#A2.E18 "æ–¹ç¨‹ 18 â€£ é™„å½• B é‡åŒ–è¯¯å·® â€£ BiLLM: æ¨åŠ¨ LLM åè®­ç»ƒé‡åŒ–çš„æé™"))ï¼Œç»“æœä¸ºï¼š'
- en: '|  | $\displaystyle\theta^{2}$ |  | (21) |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\theta^{2}$ |  | (21) |'
- en: '|  |  | $\displaystyle=$ |  |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=$ |  |'
- en: Appendix C Searching Curve of Salient Column and Non-salient Distribution
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é™„å½•C æ˜¾è‘—åˆ—ä¸éæ˜¾è‘—åˆ†å¸ƒçš„æœç´¢æ›²çº¿
- en: '![Refer to caption](img/4c8f987f1ac22b90d1aa19d7164e81d8.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/4c8f987f1ac22b90d1aa19d7164e81d8.png)'
- en: 'Figure 9: Block-wise searching curve of salient columns in OPT-6.7B. The majority
    of the curves indicate that the minimal quantization error can be achieved at
    the block level by considering only a few columns as salient. The Out Projection
    layer has a larger number of salient columns, hence varying coverage for each
    block. The distribution in the FC layer is more dispersed. After optimal searching,
    the overall average weight bit is merely 1.1 bits.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9ï¼šOPT-6.7Bä¸­æ˜¾è‘—åˆ—çš„å—çº§æœç´¢æ›²çº¿ã€‚å¤§å¤šæ•°æ›²çº¿è¡¨æ˜ï¼Œé€šè¿‡ä»…è€ƒè™‘å°‘é‡åˆ—ä½œä¸ºæ˜¾è‘—åˆ—ï¼Œåœ¨å—çº§åˆ«å¯ä»¥å®ç°æœ€å°é‡åŒ–è¯¯å·®ã€‚Out Projectionå±‚çš„æ˜¾è‘—åˆ—æ•°é‡è¾ƒå¤šï¼Œå› æ­¤æ¯ä¸ªå—çš„è¦†ç›–èŒƒå›´å„ä¸ç›¸åŒã€‚FCå±‚çš„åˆ†å¸ƒæ›´ä¸ºåˆ†æ•£ã€‚ç»è¿‡ä¼˜åŒ–æœç´¢åï¼Œæ•´ä½“å¹³å‡æƒé‡æ¯”ç‰¹æ•°ä»…ä¸º1.1æ¯”ç‰¹ã€‚
- en: 'We implemented a column-level segmentation and formulated a minimal-error column
    number search, as delineated in EquationÂ ([5](#S3.E5 "Equation 5 â€£ 3.1 Salient
    Weight Binarization for LLMs â€£ 3 Method â€£ BiLLM: Pushing the Limit of Post-Training
    Quantization for LLMs")). The identification of the optimal count of salient column
    groups commences with the column exhibiting the highest salience. To mitigate
    the increase in bit-width resulting from residual approximation, we confined the
    search range to between 3 to 30 columns. FigureÂ [9](#A3.F9 "Figure 9 â€£ Appendix
    C Searching Curve of Salient Column and Non-salient Distribution â€£ BiLLM: Pushing
    the Limit of Post-Training Quantization for LLMs") illustrates the search curve
    pertinent to the inaugural Transformer block within the OPT6.7B model. It includes
    six layers of operators (Q, K, V, Out Projection, FC1, and FC2), with each layer
    showing the search curves for the first five blocks. FigureÂ [15](#A7.F15 "Figure
    15 â€£ Appendix G Magnitude and Hessian Distribution of LLMs â€£ BiLLM: Pushing the
    Limit of Post-Training Quantization for LLMs") elucidates the clustering of salient
    weights, suggesting that a majority of the layers and blocks are capable of attaining
    minimal quantization errors with a limited number of salient columns. The block-wise
    changes in weight distribution brought about by OBCÂ (Frantar & Alistarh, [2022](#bib.bib15))
    introduce fluctuations in the search curve; however, the structured selection
    still manages to encompass the majority of salient weights. In the Feedforward
    layer, where salient weight distribution is more scattered, the search curve leans
    towards employing residual approximation across an increased number of columns.
    Nonetheless, Table [1](#S3.T1 "Table 1 â€£ 3.3 Pipeline of BiLLM â€£ 3 Method â€£ BiLLM:
    Pushing the Limit of Post-Training Quantization for LLMs"), displaying the average
    weight bit numbers across various LLMs, confirms that this search strategy effectively
    maintains weight compression at approximately 1.1 bits.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 'æˆ‘ä»¬å®ç°äº†ä¸€ä¸ªåˆ—çº§åˆ«çš„åˆ†æ®µï¼Œå¹¶åˆ¶å®šäº†ä¸€ä¸ªæœ€å°è¯¯å·®çš„åˆ—æ•°æœç´¢æ–¹æ³•ï¼Œå¦‚å…¬å¼Â ([5](#S3.E5 "Equation 5 â€£ 3.1 Salient Weight
    Binarization for LLMs â€£ 3 Method â€£ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs")) æ‰€è¿°ã€‚ç¡®å®šæœ€ä½³æ˜¾è‘—åˆ—ç»„çš„æ•°é‡ä»æ˜¾è‘—æ€§æœ€é«˜çš„åˆ—å¼€å§‹ã€‚ä¸ºäº†å‡å°‘ç”±äºæ®‹å·®è¿‘ä¼¼å¼•èµ·çš„æ¯”ç‰¹å®½åº¦å¢åŠ ï¼Œæˆ‘ä»¬å°†æœç´¢èŒƒå›´é™åˆ¶åœ¨3åˆ°30åˆ—ä¹‹é—´ã€‚å›¾Â [9](#A3.F9
    "Figure 9 â€£ Appendix C Searching Curve of Salient Column and Non-salient Distribution
    â€£ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs") è¯´æ˜äº†ä¸OPT6.7Bæ¨¡å‹ä¸­çš„é¦–ä¸ªTransformerå—ç›¸å…³çš„æœç´¢æ›²çº¿ã€‚å®ƒåŒ…æ‹¬å…­å±‚æ“ä½œç¬¦ï¼ˆQã€Kã€Vã€Out
    Projectionã€FC1 å’Œ FC2ï¼‰ï¼Œæ¯ä¸€å±‚æ˜¾ç¤ºäº†å‰äº”ä¸ªå—çš„æœç´¢æ›²çº¿ã€‚å›¾Â [15](#A7.F15 "Figure 15 â€£ Appendix G Magnitude
    and Hessian Distribution of LLMs â€£ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs") é˜æ˜äº†æ˜¾è‘—æƒé‡çš„èšç±»ï¼Œè¡¨æ˜å¤§å¤šæ•°å±‚å’Œå—å¯ä»¥é€šè¿‡æœ‰é™æ•°é‡çš„æ˜¾è‘—åˆ—æ¥è¾¾åˆ°æœ€å°é‡åŒ–è¯¯å·®ã€‚OBCÂ (Frantar & Alistarh, [2022](#bib.bib15))
    å¼•å…¥çš„å—çº§æƒé‡åˆ†å¸ƒå˜åŒ–ä¼šå¯¼è‡´æœç´¢æ›²çº¿çš„æ³¢åŠ¨ï¼›ç„¶è€Œï¼Œç»“æ„åŒ–é€‰æ‹©ä»ç„¶èƒ½å¤Ÿæ¶µç›–å¤§å¤šæ•°æ˜¾è‘—æƒé‡ã€‚åœ¨æ˜¾è‘—æƒé‡åˆ†å¸ƒæ›´ä¸ºåˆ†æ•£çš„å‰é¦ˆå±‚ä¸­ï¼Œæœç´¢æ›²çº¿å€¾å‘äºé‡‡ç”¨åœ¨æ›´å¤šåˆ—ä¸Šè¿›è¡Œæ®‹å·®è¿‘ä¼¼ã€‚å°½ç®¡å¦‚æ­¤ï¼Œè¡¨
    [1](#S3.T1 "Table 1 â€£ 3.3 Pipeline of BiLLM â€£ 3 Method â€£ BiLLM: Pushing the Limit
    of Post-Training Quantization for LLMs") æ˜¾ç¤ºäº†å„ç§LLMä¸­å¹³å‡æƒé‡æ¯”ç‰¹æ•°ï¼Œç¡®è®¤äº†è¿™ç§æœç´¢ç­–ç•¥æœ‰æ•ˆåœ°ä¿æŒäº†å¤§çº¦1.1æ¯”ç‰¹çš„æƒé‡å‹ç¼©ã€‚'
- en: 'FigureÂ [10](#A3.F10 "Figure 10 â€£ Appendix C Searching Curve of Salient Column
    and Non-salient Distribution â€£ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs") shows the unstructured search curve for the non-salient weights in
    the OPT6.7B model, with the same composition as that in FigureÂ [9](#A3.F9 "Figure
    9 â€£ Appendix C Searching Curve of Salient Column and Non-salient Distribution
    â€£ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs"). The horizontal
    axis represents the ratio between $p$. This phenomenon demonstrates that the non-salient
    weights exhibit characteristics closely resembling an ideal Gaussian or Laplacian
    distributionÂ (You, [2010](#bib.bib49); Fang etÂ al., [2020](#bib.bib13)).'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾Â [10](#A3.F10 "å›¾ 10 â€£ é™„å½• C æ˜¾è‘—åˆ—å’Œéæ˜¾è‘—åˆ†å¸ƒçš„æœç´¢æ›²çº¿ â€£ BiLLMï¼šæ¨åŠ¨ LLM åè®­ç»ƒé‡åŒ–çš„æé™") å±•ç¤ºäº† OPT6.7B
    æ¨¡å‹ä¸­éæ˜¾è‘—æƒé‡çš„æ— ç»“æ„æœç´¢æ›²çº¿ï¼Œå…¶ç»„æˆä¸å›¾Â [9](#A3.F9 "å›¾ 9 â€£ é™„å½• C æ˜¾è‘—åˆ—å’Œéæ˜¾è‘—åˆ†å¸ƒçš„æœç´¢æ›²çº¿ â€£ BiLLMï¼šæ¨åŠ¨ LLM åè®­ç»ƒé‡åŒ–çš„æé™")
    ä¸­ç›¸åŒã€‚æ¨ªè½´è¡¨ç¤º $p$ çš„æ¯”ç‡ã€‚è¿™ä¸€ç°è±¡è¡¨æ˜ï¼Œéæ˜¾è‘—æƒé‡è¡¨ç°å‡ºä¸ç†æƒ³çš„é«˜æ–¯æˆ–æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒç´§å¯†ç›¸ä¼¼çš„ç‰¹å¾ï¼ˆYouï¼Œ[2010](#bib.bib49)ï¼›Fang
    ç­‰ï¼Œ[2020](#bib.bib13)ï¼‰ã€‚
- en: '![Refer to caption](img/885063feb7f178ce01dce16d619179c7.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/885063feb7f178ce01dce16d619179c7.png)'
- en: 'Figure 10: Block-wise splitting curve of bell-shaped distribution in OPT6.7B.
    The overall presentation exhibits the characteristics of a convex function, fundamentally
    aligning with the theoretical optimal point in terms of theoretical basis.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 10ï¼šOPT6.7B ä¸­é’Ÿå½¢åˆ†å¸ƒçš„å—çŠ¶åˆ†å‰²æ›²çº¿ã€‚æ•´ä½“å±•ç¤ºäº†å‡¸å‡½æ•°çš„ç‰¹å¾ï¼Œä»ç†è®ºåŸºç¡€ä¸ŠåŸºæœ¬ç¬¦åˆç†è®ºæœ€ä¼˜ç‚¹ã€‚
- en: Appendix D Multi-evaluation Comparisons
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é™„å½• D å¤šé‡è¯„ä¼°æ¯”è¾ƒ
- en: '![Refer to caption](img/cf4e0650fe8a94019d5f78a37c584c4b.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/cf4e0650fe8a94019d5f78a37c584c4b.png)'
- en: 'Figure 11: GPTQ, PB-LLM, BiLLM performed on the PTB and C4 datasets, mainly
    on LLaMA-13B, LLaMA2-13B, OPT-13B, and so on. The results showed that BiLLM performed
    relatively well.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 11ï¼šGPTQã€PB-LLMã€BiLLM åœ¨ PTB å’Œ C4 æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œä¸»è¦æ¶‰åŠ LLaMA-13Bã€LLaMA2-13Bã€OPT-13B
    ç­‰ã€‚ç»“æœæ˜¾ç¤ºï¼ŒBiLLM çš„è¡¨ç°ç›¸å¯¹è¾ƒå¥½ã€‚
- en: Perplexity results on PTB and C4.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: PTB å’Œ C4 çš„å›°æƒ‘åº¦ç»“æœã€‚
- en: We use tables in the main text to show the perplexity of the three methods GPTQ,
    PB-LLM, and BiLLM on the Wikitext2 dataset, and bar charts to show the perplexity
    results for LLaMA-7B, LLaMA2-7B, and OPT-6.7B on the PTB and C4 datasets. In the
    appendix, we show the quantitative comparison results for models of other sizes
    on the PTB and C4 datasets with more images.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨æ­£æ–‡ä¸­ä½¿ç”¨è¡¨æ ¼å±•ç¤ºäº† GPTQã€PB-LLM å’Œ BiLLM åœ¨ Wikitext2 æ•°æ®é›†ä¸Šçš„å›°æƒ‘åº¦ï¼Œä»¥åŠæ¡å½¢å›¾å±•ç¤ºäº† LLaMA-7Bã€LLaMA2-7B
    å’Œ OPT-6.7B åœ¨ PTB å’Œ C4 æ•°æ®é›†ä¸Šçš„å›°æƒ‘åº¦ç»“æœã€‚åœ¨é™„å½•ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å…¶ä»–å°ºå¯¸æ¨¡å‹åœ¨ PTB å’Œ C4 æ•°æ®é›†ä¸Šçš„å®šé‡æ¯”è¾ƒç»“æœï¼Œå¹¶é™„æœ‰æ›´å¤šå›¾åƒã€‚
- en: 'In FigureÂ [11](#A4.F11 "Figure 11 â€£ Appendix D Multi-evaluation Comparisons
    â€£ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs"), we find that
    although different models have different perplexity results, they still roughly
    follow the law that the larger the model, the lower the perplexity. BiLLM is generally
    still relatively better than the GPTQ and PB-LLM results in terms of perplexity
    with a lower bit-width configuration, while PB-LLM and GPTQ are higher or lower
    than each other, with slightly inferior results at very low bits.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å›¾Â [11](#A4.F11 "å›¾ 11 â€£ é™„å½• D å¤šé‡è¯„ä¼°æ¯”è¾ƒ â€£ BiLLMï¼šæ¨åŠ¨ LLM åè®­ç»ƒé‡åŒ–çš„æé™") ä¸­ï¼Œæˆ‘ä»¬å‘ç°è™½ç„¶ä¸åŒæ¨¡å‹çš„å›°æƒ‘åº¦ç»“æœä¸åŒï¼Œä½†å®ƒä»¬å¤§è‡´ä»ç„¶éµå¾ªæ¨¡å‹è¶Šå¤§ï¼Œå›°æƒ‘åº¦è¶Šä½çš„è§„å¾‹ã€‚BiLLM
    åœ¨è¾ƒä½æ¯”ç‰¹å®½åº¦é…ç½®ä¸‹çš„å›°æƒ‘åº¦é€šå¸¸ä»ç„¶ä¼˜äº GPTQ å’Œ PB-LLM çš„ç»“æœï¼Œè€Œ PB-LLM å’Œ GPTQ åœ¨æä½æ¯”ç‰¹æ—¶è¡¨ç°è¾ƒå·®ã€‚
- en: Zero-shot results
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: é›¶-shot ç»“æœ
- en: 'For completeness of testing, we have also tested and compared metrics such
    as the accuracy of GPTQ, PB-LLM, and BiLLM on datasets such as PIQA and BoolQ,
    all using Zero Shotâ€™s experimental setup. From Table [5](#A4.T5 "Table 5 â€£ Appendix
    D Multi-evaluation Comparisons â€£ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs"), We find that despite the loss in quantification, a side-by-side comparison
    between the three methods still shows BiLLM to be superior overall, testing one
    level higher on some datasets, while the effect of some random perturbations,
    although present, does not pull down BiLLMâ€™s performance across the board. This
    suggests that BiLLMâ€™s quantization results have significantly improved performance
    at very low bits, and further validates the conclusions.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¸ºäº†æµ‹è¯•çš„å®Œæ•´æ€§ï¼Œæˆ‘ä»¬è¿˜æµ‹è¯•å¹¶æ¯”è¾ƒäº†GPTQã€PB-LLMå’ŒBiLLMåœ¨PIQAå’ŒBoolQç­‰æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ï¼Œæ‰€æœ‰å®éªŒéƒ½ä½¿ç”¨äº†Zero Shotçš„å®éªŒè®¾ç½®ã€‚ä»è¡¨æ ¼
    [5](#A4.T5 "Table 5 â€£ Appendix D Multi-evaluation Comparisons â€£ BiLLM: Pushing
    the Limit of Post-Training Quantization for LLMs")ä¸­ï¼Œæˆ‘ä»¬å‘ç°å°½ç®¡é‡åŒ–æœ‰æŸå¤±ï¼Œä½†ä¸‰ç§æ–¹æ³•çš„å¹¶åˆ—æ¯”è¾ƒä»æ˜¾ç¤ºBiLLMæ€»ä½“ä¸Šä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œåœ¨æŸäº›æ•°æ®é›†ä¸Šæµ‹è¯•çš„æ°´å¹³æ›´é«˜ï¼Œè™½ç„¶ä¸€äº›éšæœºæ‰°åŠ¨å­˜åœ¨ï¼Œä½†å¹¶æœªæ˜¾è‘—é™ä½BiLLMçš„æ•´ä½“è¡¨ç°ã€‚è¿™è¡¨æ˜BiLLMçš„é‡åŒ–ç»“æœåœ¨éå¸¸ä½çš„ä½æ•°ä¸‹æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå¹¶è¿›ä¸€æ­¥éªŒè¯äº†ç»“è®ºã€‚'
- en: 'Table 5: Accuracy on 7 data sets, from binarization LLaMA, LLaMA2, and OPT,
    and we also compare the results among GPTQ, PB-LLM, and BiLLM to validate the
    quantization effect.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨5ï¼šåœ¨7ä¸ªæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ï¼Œæ¥æºäºäºŒå€¼åŒ–LLaMAã€LLaMA2å’ŒOPTï¼Œæˆ‘ä»¬è¿˜æ¯”è¾ƒäº†GPTQã€PB-LLMå’ŒBiLLMä¹‹é—´çš„ç»“æœï¼Œä»¥éªŒè¯é‡åŒ–æ•ˆæœã€‚
- en: '| Model | Method | Weight Bits | Block Size | PIQA  $\uparrow$ |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| æ¨¡å‹ | æ–¹æ³• | æƒé‡ä½æ•° | å—å¤§å° | PIQA  $\uparrow$ |'
- en: '|  | GPTQ | 2.00 | 128 | 52.8 | 50.0 | 28.2 | 49.3 | 26.6 | 29.5 | 26.3 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 2.00 | 128 | 52.8 | 50.0 | 28.2 | 49.3 | 26.6 | 29.5 | 26.3 |'
- en: '| LLaMA-7B | PB-LLM | 1.70 | 128 | 54.6 | 59.7 | 30.4 | 50.6 | 28.2 | 24.6
    | 28.7 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7B | PB-LLM | 1.70 | 128 | 54.6 | 59.7 | 30.4 | 50.6 | 28.2 | 24.6
    | 28.7 |'
- en: '|  | BiLLM | 1.09 | 128 | 61.2 | 62.7 | 31.8 | 51.1 | 36.0 | 25.7 | 36.8 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|  | BiLLM | 1.09 | 128 | 61.2 | 62.7 | 31.8 | 51.1 | 36.0 | 25.7 | 36.8 |'
- en: '|  | GPTQ | 2.00 | 128 | 51.1 | 43.9 | 29.0 | 50.8 | 26.6 | 28.5 | 26.3 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 2.00 | 128 | 51.1 | 43.9 | 29.0 | 50.8 | 26.6 | 28.5 | 26.3 |'
- en: '| LLaMA2-7B | PB-LLM | 1.70 | 128 | 53.8 | 62.3 | 30.2 | 49.3 | 28.0 | 25.0
    | 27.7 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7B | PB-LLM | 1.70 | 128 | 53.8 | 62.3 | 30.2 | 49.3 | 28.0 | 25.0
    | 27.7 |'
- en: '|  | BiLLM | 1.08 | 128 | 60.6 | 61.8 | 33.2 | 52.4 | 36.2 | 24.4 | 34.8 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|  | BiLLM | 1.08 | 128 | 60.6 | 61.8 | 33.2 | 52.4 | 36.2 | 24.4 | 34.8 |'
- en: '|  | GPTQ | 2.00 | 128 | 56.6 | 51.1 | 25.6 | 51.2 | 31.3 | 22.9 | 30.4 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 2.00 | 128 | 56.6 | 51.1 | 25.6 | 51.2 | 31.3 | 22.9 | 30.4 |'
- en: '| OPT-6.7B | PB-LLM | 1.70 | 128 | 57.6 | 55.5 | 24.2 | 47.7 | 33.2 | 21.0
    | 31.0 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7B | PB-LLM | 1.70 | 128 | 57.6 | 55.5 | 24.2 | 47.7 | 33.2 | 21.0
    | 31.0 |'
- en: '|  | BiLLM | 1.11 | 128 | 58.6 | 62.2 | 29.0 | 51.5 | 34.1 | 23.9 | 31.9 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '|  | BiLLM | 1.11 | 128 | 58.6 | 62.2 | 29.0 | 51.5 | 34.1 | 23.9 | 31.9 |'
- en: Appendix E Ablation of BiLLM with different block size
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é™„å½•Eï¼šä¸åŒå—å¤§å°ä¸‹çš„BiLLMæ¶ˆèå®éªŒ
- en: To explore the effect of different chunk sizes on the quantization effect of
    BiLLM, we set up block size settings including 32 columns and 64 columns up to
    512 columns and performed quantization experiments on them. The results show that
    the overall perplexity is lower as the chunk granularity becomes finer and the
    number of bits used becomes relatively smaller. We believe this is because the
    smaller the chunks, the finer the data representation, and the more scale is used,
    but increasing the diversity of quantization results also increases the weighting
    overhead. A block size of 128 can better balance the bit-width and quantization
    effect.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ¢ç©¶ä¸åŒå—å¤§å°å¯¹BiLLMé‡åŒ–æ•ˆæœçš„å½±å“ï¼Œæˆ‘ä»¬è®¾ç½®äº†32åˆ—ã€64åˆ—åˆ°512åˆ—çš„å—å¤§å°ï¼Œå¹¶åœ¨è¿™äº›è®¾ç½®ä¸‹è¿›è¡Œäº†é‡åŒ–å®éªŒã€‚ç»“æœæ˜¾ç¤ºï¼Œéšç€å—ç²’åº¦çš„å˜ç»†å’Œä½¿ç”¨ä½æ•°çš„å‡å°‘ï¼Œæ•´ä½“å›°æƒ‘åº¦è¾ƒä½ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™æ˜¯å› ä¸ºå—è¶Šå°ï¼Œæ•°æ®è¡¨ç¤ºè¶Šç²¾ç»†ï¼Œä½¿ç”¨çš„å°ºåº¦è¶Šå¤§ï¼Œä½†å¢åŠ äº†é‡åŒ–ç»“æœçš„å¤šæ ·æ€§ä¹Ÿå¢åŠ äº†åŠ æƒå¼€é”€ã€‚128çš„å—å¤§å°å¯ä»¥æ›´å¥½åœ°å¹³è¡¡ä½å®½å’Œé‡åŒ–æ•ˆæœã€‚
- en: 'Table 6: Perplexity on Wikitext2, PTB, and C4 with different block size settings
    on BiLLM.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨6ï¼šä¸åŒå—å¤§å°è®¾ç½®ä¸‹BiLLMåœ¨Wikitext2ã€PTBå’ŒC4ä¸Šçš„å›°æƒ‘åº¦ã€‚
- en: '| Â Â Â Â Â Â Â Â Â Model | Â Â Â Â Â Â Â Â Â  Block Size | Â Â Â Â Â Â Â Â Â Wikitext2 | Â Â Â Â Â Â Â Â Â PTB
    | Â Â Â Â Â Â Â Â Â C4 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â Â Â Â Â æ¨¡å‹ | Â Â Â Â Â Â Â Â Â  å—å¤§å° | Â Â Â Â Â Â Â Â Â Wikitext2 | Â Â Â Â Â Â Â Â Â PTB | Â Â Â Â Â Â Â Â Â C4
    |'
- en: '|  | Â Â Â Â Â Â Â Â Â 512 | Â Â Â Â Â Â Â Â Â 74.14 | Â Â Â Â Â Â Â Â Â 1078.90 | Â Â Â Â Â Â Â Â Â 81.76 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '|  | Â Â Â Â Â Â Â Â Â 512 | Â Â Â Â Â Â Â Â Â 74.14 | Â Â Â Â Â Â Â Â Â 1078.90 | Â Â Â Â Â Â Â Â Â 81.76 |'
- en: '|  | Â Â Â Â Â Â Â Â Â 256 | Â Â Â Â Â Â Â Â Â 48.91 | Â Â Â Â Â Â Â Â Â 574.34 | Â Â Â Â Â Â Â Â Â 57.60 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '|  | Â Â Â Â Â Â Â Â Â 256 | Â Â Â Â Â Â Â Â Â 48.91 | Â Â Â Â Â Â Â Â Â 574.34 | Â Â Â Â Â Â Â Â Â 57.60 |'
- en: '| Â Â Â Â Â Â Â Â Â LLaMA-7B | Â Â Â Â Â Â Â Â Â 128 | Â Â Â Â Â Â Â Â Â 35.04 | Â Â Â Â Â Â Â Â Â 421.27 | Â Â Â Â Â Â Â Â Â 39.59
    |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â Â Â Â Â LLaMA-7B | Â Â Â Â Â Â Â Â Â 128 | Â Â Â Â Â Â Â Â Â 35.04 | Â Â Â Â Â Â Â Â Â 421.27 | Â Â Â Â Â Â Â Â Â 39.59
    |'
- en: '|  | Â Â Â Â Â Â Â Â Â 64 | Â Â Â Â Â Â Â Â Â 27.23 | Â Â Â Â Â Â Â Â Â 399.81 | Â Â Â Â Â Â Â Â Â 27.74 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  | Â Â Â Â Â Â Â Â Â 64 | Â Â Â Â Â Â Â Â Â 27.23 | Â Â Â Â Â Â Â Â Â 399.81 | Â Â Â Â Â Â Â Â Â 27.74 |'
- en: '|  | Â Â Â Â Â Â Â Â Â 32 | Â Â Â Â Â Â Â Â Â 17.56 | Â Â Â Â Â Â Â Â Â 263.39 | Â Â Â Â Â Â Â Â Â 19.85 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  | Â Â Â Â Â Â Â Â Â 32 | Â Â Â Â Â Â Â Â Â 17.56 | Â Â Â Â Â Â Â Â Â 263.39 | Â Â Â Â Â Â Â Â Â 19.85 |'
- en: '|  | Â Â Â Â Â Â Â Â Â 512 | Â Â Â Â Â Â Â Â Â 52.90 | Â Â Â Â Â Â Â Â Â 267.82 | Â Â Â Â Â Â Â Â Â 43.86 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '|  | Â Â Â Â Â Â Â Â Â 512 | Â Â Â Â Â Â Â Â Â 52.90 | Â Â Â Â Â Â Â Â Â 267.82 | Â Â Â Â Â Â Â Â Â 43.86 |'
- en: '|  | Â Â Â Â Â Â Â Â Â 256 | Â Â Â Â Â Â Â Â Â 43.69 | Â Â Â Â Â Â Â Â Â 232.34 | Â Â Â Â Â Â Â Â Â 43.21 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|  | Â Â Â Â Â Â Â Â Â 256 | Â Â Â Â Â Â Â Â Â 43.69 | Â Â Â Â Â Â Â Â Â 232.34 | Â Â Â Â Â Â Â Â Â 43.21 |'
- en: '| Â Â Â Â Â Â Â Â Â LLaMA2-7B | Â Â Â Â Â Â Â Â Â 128 | Â Â Â Â Â Â Â Â Â 32.48 | Â Â Â Â Â Â Â Â Â 3877.38 | Â Â Â Â Â Â Â Â Â 40.52
    |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â Â Â Â Â LLaMA2-7B | Â Â Â Â Â Â Â Â Â 128 | Â Â Â Â Â Â Â Â Â 32.48 | Â Â Â Â Â Â Â Â Â 3877.38 | Â Â Â Â Â Â Â Â Â 40.52
    |'
- en: '|  | Â Â Â Â Â Â Â Â Â 64 | Â Â Â Â Â Â Â Â Â 20.12 | Â Â Â Â Â Â Â Â Â 830.36 | Â Â Â Â Â Â Â Â Â 24.46 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '|  | Â Â Â Â Â Â Â Â Â 64 | Â Â Â Â Â Â Â Â Â 20.12 | Â Â Â Â Â Â Â Â Â 830.36 | Â Â Â Â Â Â Â Â Â 24.46 |'
- en: '|  | Â Â Â Â Â Â Â Â Â 32 | Â Â Â Â Â Â Â Â Â 13.58 | Â Â Â Â Â Â Â Â Â 440.40 | Â Â Â Â Â Â Â Â Â 17.34 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '|  | Â Â Â Â Â Â Â Â Â 32 | Â Â Â Â Â Â Â Â Â 13.58 | Â Â Â Â Â Â Â Â Â 440.40 | Â Â Â Â Â Â Â Â Â 17.34 |'
- en: '|  | Â Â Â Â Â Â Â Â Â 512 | Â Â Â Â Â Â Â Â Â 151.81 | Â Â Â Â Â Â Â Â Â 257.22 | Â Â Â Â Â Â Â Â Â 101.96 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '|  | Â Â Â Â Â Â Â Â Â 512 | Â Â Â Â Â Â Â Â Â 151.81 | Â Â Â Â Â Â Â Â Â 257.22 | Â Â Â Â Â Â Â Â Â 101.96 |'
- en: '|  | Â Â Â Â Â Â Â Â Â 256 | Â Â Â Â Â Â Â Â Â 84.42 | Â Â Â Â Â Â Â Â Â 116.44 | Â Â Â Â Â Â Â Â Â 77.25 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|  | Â Â Â Â Â Â Â Â Â 256 | Â Â Â Â Â Â Â Â Â 84.42 | Â Â Â Â Â Â Â Â Â 116.44 | Â Â Â Â Â Â Â Â Â 77.25 |'
- en: '| Â Â Â Â Â Â Â Â Â OPT-6.7B | Â Â Â Â Â Â Â Â Â 128 | Â Â Â Â Â Â Â Â Â 35.36 | Â Â Â Â Â Â Â Â Â 73.63 | Â Â Â Â Â Â Â Â Â 43.16
    |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| Â Â Â Â Â Â Â Â Â OPT-6.7B | Â Â Â Â Â Â Â Â Â 128 | Â Â Â Â Â Â Â Â Â 35.36 | Â Â Â Â Â Â Â Â Â 73.63 | Â Â Â Â Â Â Â Â Â 43.16
    |'
- en: '|  | Â Â Â Â Â Â Â Â Â 64 | Â Â Â Â Â Â Â Â Â 33.36 | Â Â Â Â Â Â Â Â Â 48.16 | Â Â Â Â Â Â Â Â Â 31.94 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '|  | Â Â Â Â Â Â Â Â Â 64 | Â Â Â Â Â Â Â Â Â 33.36 | Â Â Â Â Â Â Â Â Â 48.16 | Â Â Â Â Â Â Â Â Â 31.94 |'
- en: '|  | Â Â Â Â Â Â Â Â Â 32 | Â Â Â Â Â Â Â Â Â 20.48 | Â Â Â Â Â Â Â Â Â 31.02 | Â Â Â Â Â Â Â Â Â 21.47 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '|  | Â Â Â Â Â Â Â Â Â 32 | Â Â Â Â Â Â Â Â Â 20.48 | Â Â Â Â Â Â Â Â Â 31.02 | Â Â Â Â Â Â Â Â Â 21.47 |'
- en: Appendix F Dialog Examples
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é™„å½• F å¯¹è¯ç¤ºä¾‹
- en: In this section, we show some dialogue examples of binarized LLaMA-13B and Vicuna-13B.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€äº›äºŒå€¼åŒ– LLaMA-13B å’Œ Vicuna-13B çš„å¯¹è¯ç¤ºä¾‹ã€‚
- en: '![Refer to caption](img/1cf11059c5895950ec6af5a245d22c11.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒæ ‡é¢˜](img/1cf11059c5895950ec6af5a245d22c11.png)'
- en: 'Figure 12: Some examples of conversations. LLaMA-13B and Vicuna-13B are chosen
    to show the case of language supplementary and Q&A ability. And PB-LLM (int 8,
    10%) is selected as the comparison. We color the text to show the reasonable or
    inappropriate responses.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 12ï¼šä¸€äº›å¯¹è¯ç¤ºä¾‹ã€‚é€‰æ‹©äº† LLaMA-13B å’Œ Vicuna-13B æ¥å±•ç¤ºè¯­è¨€è¡¥å……å’Œé—®ç­”èƒ½åŠ›çš„æƒ…å†µã€‚å¹¶é€‰æ‹©äº† PB-LLMï¼ˆint 8, 10%ï¼‰ä½œä¸ºå¯¹æ¯”ã€‚æˆ‘ä»¬é€šè¿‡ä¸ºæ–‡æœ¬ä¸Šè‰²æ¥æ˜¾ç¤ºåˆç†æˆ–ä¸æ°å½“çš„å“åº”ã€‚
- en: Appendix G Magnitude and Hessian Distribution of LLMs
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é™„å½• G LLMs çš„å¹…åº¦å’Œ Hessian åˆ†å¸ƒ
- en: 'FigureÂ [2](#S1.F2 "Figure 2 â€£ 1 Introduction â€£ BiLLM: Pushing the Limit of
    Post-Training Quantization for LLMs") displays the distribution characteristics
    of weights and Hessian in LLMs. In this section, we provide additional examples
    to illustrate the bell-shaped distribution of weight values and the long-tailed
    distribution of Hessian weights. FigureÂ [13](#A7.F13 "Figure 13 â€£ Appendix G Magnitude
    and Hessian Distribution of LLMs â€£ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs") depicts the distributions of four linear layers in the first Transformer
    block of the OPT-1.3B model, while FigureÂ [14](#A7.F14 "Figure 14 â€£ Appendix G
    Magnitude and Hessian Distribution of LLMs â€£ BiLLM: Pushing the Limit of Post-Training
    Quantization for LLMs") shows the distributions of seven linear layers in the
    sixth block of the LLaMA-7B model. The selection of these specific block positions
    is intended to demonstrate the universality of these distribution characteristics
    in LLMs.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾Â [2](#S1.F2 "å›¾ 2 â€£ 1 å¼•è¨€ â€£ BiLLMï¼šæ¨åŠ¨ LLMs åè®­ç»ƒé‡åŒ–çš„æé™") å±•ç¤ºäº† LLMs ä¸­æƒé‡å’Œ Hessian çš„åˆ†å¸ƒç‰¹å¾ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†é¢å¤–çš„ç¤ºä¾‹ä»¥è¯´æ˜æƒé‡å€¼çš„é’Ÿå½¢åˆ†å¸ƒå’Œ
    Hessian æƒé‡çš„é•¿å°¾åˆ†å¸ƒã€‚å›¾Â [13](#A7.F13 "å›¾ 13 â€£ é™„å½• G LLMs çš„å¹…åº¦å’Œ Hessian åˆ†å¸ƒ â€£ BiLLMï¼šæ¨åŠ¨ LLMs
    åè®­ç»ƒé‡åŒ–çš„æé™") æç»˜äº† OPT-1.3B æ¨¡å‹ç¬¬ä¸€ä¸ª Transformer å—ä¸­å››ä¸ªçº¿æ€§å±‚çš„åˆ†å¸ƒï¼Œè€Œå›¾Â [14](#A7.F14 "å›¾ 14 â€£ é™„å½•
    G LLMs çš„å¹…åº¦å’Œ Hessian åˆ†å¸ƒ â€£ BiLLMï¼šæ¨åŠ¨ LLMs åè®­ç»ƒé‡åŒ–çš„æé™") æ˜¾ç¤ºäº† LLaMA-7B æ¨¡å‹ç¬¬å…­ä¸ªå—ä¸­ä¸ƒä¸ªçº¿æ€§å±‚çš„åˆ†å¸ƒã€‚è¿™äº›ç‰¹å®šå—ä½ç½®çš„é€‰æ‹©æ—¨åœ¨å±•ç¤ºè¿™äº›åˆ†å¸ƒç‰¹å¾åœ¨
    LLMs ä¸­çš„æ™®éæ€§ã€‚
- en: 'FigureÂ [15](#A7.F15 "Figure 15 â€£ Appendix G Magnitude and Hessian Distribution
    of LLMs â€£ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs") displays
    the distribution of sensitive weights across 5 Transformer blocks within the OPT-1.3B
    model. We present the Hessian distribution results for both the attention and
    feedforward blocks, with the red portion indicating the top 10% of the most significant
    weight distribution. We observed that the salient weights of Q, K, and V in the
    OPT family tend to concentrate in some columns or rows. Moreover, we noticed that
    salient weights in the Out Projection layer of multi-head self-attention blocks
    are distinctly concentrated in specific columns, supporting our structured selection
    approach discussed in the main text. In contrast, the distribution of salient
    weights in the feedforward layers is more dispersed. Based on these observations,
    we adopt a sensitivity-based structured search method to identify salient columns.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾[15](#A7.F15 "å›¾ 15 â€£ é™„å½• G LLMçš„å¹…åº¦å’ŒHessianåˆ†å¸ƒ â€£ BiLLMï¼šæ¨åŠ¨LLMsåè®­ç»ƒé‡åŒ–çš„æé™")å±•ç¤ºäº†OPT-1.3Bæ¨¡å‹ä¸­5ä¸ªTransformerå—çš„æ•æ„Ÿæƒé‡åˆ†å¸ƒã€‚æˆ‘ä»¬å‘ˆç°äº†æ³¨æ„åŠ›å’Œå‰é¦ˆå—çš„Hessianåˆ†å¸ƒç»“æœï¼Œå…¶ä¸­çº¢è‰²éƒ¨åˆ†è¡¨ç¤ºæœ€é‡è¦çš„å‰10%æƒé‡åˆ†å¸ƒã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼ŒOPTç³»åˆ—ä¸­Qã€Kå’ŒVçš„æ˜¾è‘—æƒé‡å€¾å‘äºé›†ä¸­åœ¨æŸäº›åˆ—æˆ–è¡Œä¸­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ³¨æ„åˆ°ï¼Œåœ¨å¤šå¤´è‡ªæ³¨æ„åŠ›å—çš„è¾“å‡ºæŠ•å½±å±‚ä¸­ï¼Œæ˜¾è‘—æƒé‡æ˜æ˜¾é›†ä¸­åœ¨ç‰¹å®šåˆ—ï¼Œè¿™æ”¯æŒäº†æˆ‘ä»¬åœ¨æ­£æ–‡ä¸­è®¨è®ºçš„ç»“æ„åŒ–é€‰æ‹©æ–¹æ³•ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå‰é¦ˆå±‚ä¸­çš„æ˜¾è‘—æƒé‡åˆ†å¸ƒåˆ™æ›´ä¸ºåˆ†æ•£ã€‚åŸºäºè¿™äº›è§‚å¯Ÿç»“æœï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºæ•æ„Ÿåº¦çš„ç»“æ„åŒ–æœç´¢æ–¹æ³•æ¥è¯†åˆ«æ˜¾è‘—åˆ—ã€‚
- en: '![Refer to caption](img/39e70a3c7f0ae501dea416b298be1326.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒå›¾æ³¨](img/39e70a3c7f0ae501dea416b298be1326.png)'
- en: 'Figure 13: Different layers weight density distribution (blue) and hessian
    density distribution (orange) of the $1^{st}$ Transformer block of the OPT-1.3B
    model'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾13ï¼šOPT-1.3Bæ¨¡å‹çš„ç¬¬$1^{st}$ä¸ªTransformerå—çš„ä¸åŒå±‚æƒé‡å¯†åº¦åˆ†å¸ƒï¼ˆè“è‰²ï¼‰å’ŒHessianå¯†åº¦åˆ†å¸ƒï¼ˆæ©™è‰²ï¼‰
- en: '![Refer to caption](img/ad6faeb860b4a40d544eb03818b49049.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒå›¾æ³¨](img/ad6faeb860b4a40d544eb03818b49049.png)'
- en: 'Figure 14: Different layers weight density distribution (blue) and hessian
    density distribution (orange) of the $6^{th}$ Transformer block of the LLaMA-7B
    model'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾14ï¼šLLaMA-7Bæ¨¡å‹çš„ç¬¬$6^{th}$ä¸ªTransformerå—çš„ä¸åŒå±‚æƒé‡å¯†åº¦åˆ†å¸ƒï¼ˆè“è‰²ï¼‰å’ŒHessianå¯†åº¦åˆ†å¸ƒï¼ˆæ©™è‰²ï¼‰
- en: '![Refer to caption](img/a3601a55e6629a51a80a1c46b8de371f.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒå›¾æ³¨](img/a3601a55e6629a51a80a1c46b8de371f.png)'
- en: 'Figure 15: Distribution of top 10% salient elements in Hessian matrix. The
    distribution of $1^{st}-5^{th}$ Transformer blocks in OPT-1.3B'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾15ï¼šHessiançŸ©é˜µä¸­å‰10%æ˜¾è‘—å…ƒç´ çš„åˆ†å¸ƒã€‚OPT-1.3Bæ¨¡å‹ä¸­ç¬¬$1^{st}-5^{th}$ä¸ªTransformerå—çš„åˆ†å¸ƒ
