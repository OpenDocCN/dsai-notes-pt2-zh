- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç±»åˆ«: æœªåˆ†ç±»'
- en: 'date: 2024-09-08 18:51:07'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ—¥æœŸ: 2024-09-08 18:51:07'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'QFT: é‡åŒ–å…¨å‚æ•°å¾®è°ƒçš„LLMsï¼Œèµ„æºå¯æ‰¿å—'
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2310.07147](https://ar5iv.labs.arxiv.org/html/2310.07147)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2310.07147](https://ar5iv.labs.arxiv.org/html/2310.07147)
- en: Zhikai Li^(1, 3)Â , Xiaoxuan LiuÂ²Â , Banghua ZhuÂ²Â , Zhen Dong^(2ğŸ–‚), Qingyi Gu^(1ğŸ–‚),
    Kurt KeutzerÂ²
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Zhikai Li^(1, 3)Â , Xiaoxuan LiuÂ²Â , Banghua ZhuÂ²Â , Zhen Dong^(2ğŸ–‚), Qingyi Gu^(1ğŸ–‚),
    Kurt KeutzerÂ²
- en: Â¹Institute of Automation, Chinese Academy of Sciences â€ƒÂ²University of California,
    Berkeley
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Â¹ä¸­å›½ç§‘å­¦é™¢è‡ªåŠ¨åŒ–ç ”ç©¶æ‰€ â€ƒÂ²åŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡
- en: Â³School of Artificial Intelligence, University of Chinese Academy of Sciences
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Â³ä¸­å›½ç§‘å­¦é™¢äººå·¥æ™ºèƒ½å­¦é™¢
- en: '{lizhikai2020,qingyi.gu}@ia.ac.cn'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{lizhikai2020,qingyi.gu}@ia.ac.cn'
- en: '{xiaoxuan_liu,banghua,zhendong,keutzer}@berkeley.edu ^(ğŸ–‚) Corresponding authors.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{xiaoxuan_liu,banghua,zhendong,keutzer}@berkeley.edu ^(ğŸ–‚) é€šè®¯ä½œè€…ã€‚'
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: 'Large Language Models (LLMs) have showcased remarkable impacts across a wide
    spectrum of natural language processing tasks. Fine-tuning these pre-trained models
    on downstream datasets provides further significant performance gains, but this
    process has been challenging due to its extraordinary resource requirements. To
    this end, existing efforts focus on parameter-efficient fine-tuning, which, unfortunately,
    fail to capitalize on the powerful potential of full-parameter fine-tuning. In
    this work, we propose QFT, a novel Quantized Full-parameter Tuning framework for
    LLMs that enables memory-efficient fine-tuning without harming performance. Our
    framework incorporates two novel ideas: (i) we adopt the efficient Lion optimizer,
    which only keeps track of the momentum and has consistent update magnitudes for
    each parameter, an inherent advantage for robust quantization; and (ii) we quantize
    all model states and store them as integer values, and present a gradient flow
    and parameter update scheme for the quantized weights. As a result, QFT reduces
    the model state memory to 21% of the standard solution while achieving comparable
    performance, e.g., tuning a LLaMA-7B model requires only $<$30GB of memory, satisfied
    by a single A6000 GPU.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å±•ç¤ºäº†æ˜¾è‘—çš„å½±å“ã€‚å¯¹è¿™äº›é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œä¸‹æ¸¸æ•°æ®é›†çš„å¾®è°ƒå¯ä»¥å¸¦æ¥è¿›ä¸€æ­¥çš„æ˜¾è‘—æ€§èƒ½æå‡ï¼Œä½†ç”±äºå…¶æå¤§çš„èµ„æºéœ€æ±‚ï¼Œè¿™ä¸€è¿‡ç¨‹å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºæ­¤ï¼Œç°æœ‰çš„åŠªåŠ›é›†ä¸­åœ¨å‚æ•°é«˜æ•ˆå¾®è°ƒä¸Šï¼Œä½†é—æ†¾çš„æ˜¯ï¼Œè¿™äº›æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨å…¨å‚æ•°å¾®è°ƒçš„å¼ºå¤§æ½œåŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†QFTï¼Œä¸€ç§æ–°é¢–çš„é‡åŒ–å…¨å‚æ•°å¾®è°ƒæ¡†æ¶ï¼Œç”¨äºLLMsï¼Œå®ç°äº†å†…å­˜é«˜æ•ˆçš„å¾®è°ƒè€Œä¸ä¼šæŸå®³æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ¡†æ¶åŒ…å«ä¸¤ä¸ªæ–°é¢–çš„ç†å¿µï¼šï¼ˆiï¼‰æˆ‘ä»¬é‡‡ç”¨é«˜æ•ˆçš„Lionä¼˜åŒ–å™¨ï¼Œå®ƒåªè·Ÿè¸ªåŠ¨é‡ï¼Œå¹¶ä¸”å¯¹æ¯ä¸ªå‚æ•°æœ‰ä¸€è‡´çš„æ›´æ–°å¹…åº¦ï¼Œè¿™æ˜¯é²æ£’é‡åŒ–çš„å†…åœ¨ä¼˜åŠ¿ï¼›ï¼ˆiiï¼‰æˆ‘ä»¬å¯¹æ‰€æœ‰æ¨¡å‹çŠ¶æ€è¿›è¡Œé‡åŒ–å¹¶å°†å…¶å­˜å‚¨ä¸ºæ•´æ•°å€¼ï¼ŒåŒæ—¶æå‡ºäº†ä¸€ä¸ªç”¨äºé‡åŒ–æƒé‡çš„æ¢¯åº¦æµå’Œå‚æ•°æ›´æ–°æ–¹æ¡ˆã€‚ç»“æœï¼ŒQFTå°†æ¨¡å‹çŠ¶æ€å†…å­˜å‡å°‘åˆ°æ ‡å‡†è§£å†³æ–¹æ¡ˆçš„21%ï¼ŒåŒæ—¶å®ç°äº†ç›¸å½“çš„æ€§èƒ½ï¼Œä¾‹å¦‚ï¼Œå¾®è°ƒLLaMA-7Bæ¨¡å‹åªéœ€<$30GBçš„å†…å­˜ï¼Œè¿™ä¸€è¦æ±‚ç”±å•ä¸ªA6000
    GPUæ»¡è¶³ã€‚
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 å¼•è¨€
- en: Large Language Models (LLMs), with up to hundreds of billions of parameters,
    have left an indelible mark on the landscape of natural language processing tasks,
    showcasing their remarkable impacts across a diverse spectrum of applications
    and domainsÂ (Touvron etÂ al., [2023a](#bib.bib41); [b](#bib.bib42); Brown etÂ al.,
    [2020](#bib.bib1); Zhang etÂ al., [2022](#bib.bib45)). Fine-tuning these pre-trained
    models on downstream datasets enhances their ability to understand and perform
    specific tasksÂ (Zhao etÂ al., [2023](#bib.bib46)). However, due to the enormous
    number of parameters, the fine-tuning process requires unprecedented resources.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œæ‹¥æœ‰å¤šè¾¾æ•°ç™¾äº¿ä¸ªå‚æ•°ï¼Œå·²ç»åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„é¢†åŸŸä¸­ç•™ä¸‹äº†ä¸å¯ç£¨ç­çš„å°è®°ï¼Œå±•ç¤ºäº†å®ƒä»¬åœ¨å¤šæ ·åŒ–åº”ç”¨å’Œé¢†åŸŸä¸­çš„æ˜¾è‘—å½±å“ï¼ˆTouvron
    ç­‰ï¼Œ[2023a](#bib.bib41); [b](#bib.bib42); Brown ç­‰ï¼Œ[2020](#bib.bib1); Zhang ç­‰ï¼Œ[2022](#bib.bib45)ï¼‰ã€‚å¯¹è¿™äº›é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œä¸‹æ¸¸æ•°æ®é›†çš„å¾®è°ƒå¯ä»¥æå‡å®ƒä»¬ç†è§£å’Œæ‰§è¡Œç‰¹å®šä»»åŠ¡çš„èƒ½åŠ›ï¼ˆZhao
    ç­‰ï¼Œ[2023](#bib.bib46)ï¼‰ã€‚ç„¶è€Œï¼Œç”±äºå‚æ•°æ•°é‡åºå¤§ï¼Œå¾®è°ƒè¿‡ç¨‹éœ€è¦å‰æ‰€æœªæœ‰çš„èµ„æºã€‚
- en: Parameter-efficient fine-tuning, involving the tuning of only selected parameters,
    is deemed a practical choice for low-resource situationsÂ (Ding etÂ al., [2022](#bib.bib8);
    Hu etÂ al., [2021](#bib.bib15); Li & Liang, [2021](#bib.bib26)). Regrettably, owing
    to the limited representational capacity of the smaller parameter set, the outcomes
    of this approach often fall short of expectationsÂ (Lv etÂ al., [2023](#bib.bib35)).
    Therefore, our emphasis is placed on full-parameter fine-tuning, with a keen interest
    in investigating memory optimization strategies to render it feasible on cost-effective
    resources.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°é«˜æ•ˆå¾®è°ƒï¼Œå³ä»…è°ƒæ•´é€‰æ‹©çš„å‚æ•°ï¼Œè¢«è®¤ä¸ºæ˜¯ä½èµ„æºæƒ…å†µä¸‹çš„å®é™…é€‰æ‹©ï¼ˆDing et al., [2022](#bib.bib8); Hu et al.,
    [2021](#bib.bib15); Li & Liang, [2021](#bib.bib26)ï¼‰ã€‚é—æ†¾çš„æ˜¯ï¼Œç”±äºè¾ƒå°å‚æ•°é›†çš„è¡¨ç¤ºèƒ½åŠ›æœ‰é™ï¼Œè¿™ç§æ–¹æ³•çš„ç»“æœå¾€å¾€æœªèƒ½è¾¾åˆ°é¢„æœŸï¼ˆLv
    et al., [2023](#bib.bib35)ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†é‡ç‚¹æ”¾åœ¨å…¨å‚æ•°å¾®è°ƒä¸Šï¼Œå¹¶å¯¹ç ”ç©¶å†…å­˜ä¼˜åŒ–ç­–ç•¥ä»¥ä½¿å…¶åœ¨æˆæœ¬æ•ˆç›Šèµ„æºä¸Šå¯è¡Œä¿æŒæµ“åšçš„å…´è¶£ã€‚
- en: 'We begin by examining the full spectrum of memory usage in full-parameter fine-tuning,
    which can be categorized into three components: model states, activation, and
    other temporary or unusable memory. Model states, which include the model parameters
    (weights), gradients, and optimizer states (such as momentum and variances in
    AdamÂ (Kingma & Ba, [2015](#bib.bib22))), are mandatory to store and consequently
    consume the majority of the memoryÂ (Rajbhandari etÂ al., [2020](#bib.bib38)). For
    instance, when employing the standard fp32 training settings with the Adam optimizer,
    the memory allocation for model parameters, gradients, momentum, and variances
    amounts to 4 times the number of parameters. As a result, tuning a LLaMA-7B model
    necessitates a minimum of 100.4GB of RAM, which presents a formidable challenge
    given the limitations of current GPU capacities.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆè€ƒå¯Ÿäº†å…¨å‚æ•°å¾®è°ƒä¸­å†…å­˜ä½¿ç”¨çš„å…¨è°±ï¼Œè¿™å¯ä»¥åˆ†ä¸ºä¸‰ä¸ªç»„æˆéƒ¨åˆ†ï¼šæ¨¡å‹çŠ¶æ€ã€æ¿€æ´»å’Œå…¶ä»–ä¸´æ—¶æˆ–ä¸å¯ç”¨çš„å†…å­˜ã€‚æ¨¡å‹çŠ¶æ€ï¼ŒåŒ…æ‹¬æ¨¡å‹å‚æ•°ï¼ˆæƒé‡ï¼‰ã€æ¢¯åº¦ä»¥åŠä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå¦‚Adamä¸­çš„åŠ¨é‡å’Œæ–¹å·®ï¼ˆKingma
    & Ba, [2015](#bib.bib22)ï¼‰ï¼‰ï¼Œæ˜¯å¿…éœ€å­˜å‚¨çš„ï¼Œå› æ­¤æ¶ˆè€—äº†å¤§éƒ¨åˆ†å†…å­˜ï¼ˆRajbhandari et al., [2020](#bib.bib38)ï¼‰ã€‚ä¾‹å¦‚ï¼Œåœ¨ä½¿ç”¨Adamä¼˜åŒ–å™¨çš„æ ‡å‡†fp32è®­ç»ƒè®¾ç½®ä¸‹ï¼Œæ¨¡å‹å‚æ•°ã€æ¢¯åº¦ã€åŠ¨é‡å’Œæ–¹å·®çš„å†…å­˜åˆ†é…æ€»é‡æ˜¯å‚æ•°æ•°é‡çš„4å€ã€‚å› æ­¤ï¼Œè°ƒæ•´LLaMA-7Bæ¨¡å‹éœ€è¦è‡³å°‘100.4GBçš„RAMï¼Œè¿™åœ¨å½“å‰GPUå®¹é‡çš„é™åˆ¶ä¸‹æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚
- en: In this work, we are motivated to reduce the memory usage of all model states
    through quantized low-precision representations. First, instead of resorting to
    straightforward quantization, we expect an optimizer that simplifies the computation
    to replace Adam. Fortunately, the Lion optimizerÂ (Chen etÂ al., [2023](#bib.bib3))
    aligns almost perfectly with our expectations, as it only keeps track of the momentum
    and naturally eliminates the memory usage of the variances. And more importantly,
    its update has the same magnitude for each parameter, thus mitigating potential
    imbalances or inaccuracies in weight updates introduced by limited representation
    precision. Afterwards, we develop lightweight yet accurate quantizers for each
    model state, notably the dense-and-sparse quantizerÂ (Kim etÂ al., [2023](#bib.bib21))
    for weight parameters, which are then stored in the quantized integer format.
    During computation, these quantized representations are dequantized on-the-fly
    into the floating-point format to dynamically perform high-precision arithmetic.
    Moreover, we present a novel gradient flow scheme for the quantized weights to
    ensure proper error propagation and parameter updates in training.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡é‡åŒ–ä½ç²¾åº¦è¡¨ç¤ºæ¥å‡å°‘æ‰€æœ‰æ¨¡å‹çŠ¶æ€çš„å†…å­˜ä½¿ç”¨ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æœŸæœ›ä½¿ç”¨ä¸€ç§ç®€åŒ–è®¡ç®—çš„ä¼˜åŒ–å™¨æ¥æ›¿ä»£Adamï¼Œè€Œä¸æ˜¯ç›´æ¥é‡‡ç”¨ç®€å•çš„é‡åŒ–ã€‚å¹¸è¿çš„æ˜¯ï¼ŒLionä¼˜åŒ–å™¨ï¼ˆChen
    et al., [2023](#bib.bib3)ï¼‰å‡ ä¹å®Œå…¨ç¬¦åˆæˆ‘ä»¬çš„æœŸæœ›ï¼Œå› ä¸ºå®ƒåªè·Ÿè¸ªåŠ¨é‡ï¼Œè‡ªç„¶æ¶ˆé™¤äº†æ–¹å·®çš„å†…å­˜ä½¿ç”¨ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå®ƒçš„æ›´æ–°å¯¹æ¯ä¸ªå‚æ•°çš„å¹…åº¦ç›¸åŒï¼Œä»è€Œå‡è½»äº†ç”±äºè¡¨ç¤ºç²¾åº¦æœ‰é™è€Œå¼•å…¥çš„æƒé‡æ›´æ–°æ½œåœ¨çš„ä¸å¹³è¡¡æˆ–ä¸å‡†ç¡®æ€§ã€‚ä¹‹åï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªæ¨¡å‹çŠ¶æ€å¼€å‘äº†è½»é‡ä½†å‡†ç¡®çš„é‡åŒ–å™¨ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹æƒé‡å‚æ•°çš„å¯†é›†-ç¨€ç–é‡åŒ–å™¨ï¼ˆKim
    et al., [2023](#bib.bib21)ï¼‰ï¼Œç„¶åå°†å…¶å­˜å‚¨åœ¨é‡åŒ–æ•´æ•°æ ¼å¼ä¸­ã€‚åœ¨è®¡ç®—è¿‡ç¨‹ä¸­ï¼Œè¿™äº›é‡åŒ–è¡¨ç¤ºä¼šå³æ—¶è§£é‡åŒ–ä¸ºæµ®ç‚¹æ ¼å¼ï¼Œä»¥åŠ¨æ€æ‰§è¡Œé«˜ç²¾åº¦ç®—æœ¯æ“ä½œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„é‡åŒ–æƒé‡æ¢¯åº¦æµæ–¹æ¡ˆï¼Œä»¥ç¡®ä¿è®­ç»ƒä¸­çš„é”™è¯¯ä¼ æ’­å’Œå‚æ•°æ›´æ–°çš„æ­£ç¡®æ€§ã€‚
- en: 'More specifically, our contribution can be summarized as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬çš„è´¡çŒ®å¯ä»¥æ€»ç»“å¦‚ä¸‹ï¼š
- en: â€¢
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: We propose QFT, a novel Quantized Full-parameter Tuning framework for LLMs,
    which leverages quantization to optimize memory usage in fine-tuning without sacrificing
    performance. QFT can be seamlessly integrated into mainstream LLM training tools
    with minor modifications to a few training units, and is well compatible with
    existing memory optimization methods.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æå‡ºäº†QFTï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„é‡åŒ–å…¨å‚æ•°è°ƒä¼˜æ¡†æ¶ï¼Œç”¨äºLLMsï¼Œåˆ©ç”¨é‡åŒ–ä¼˜åŒ–å¾®è°ƒä¸­çš„å†…å­˜ä½¿ç”¨ï¼Œè€Œä¸ç‰ºç‰²æ€§èƒ½ã€‚QFTå¯ä»¥é€šè¿‡å¯¹å°‘é‡è®­ç»ƒå•å…ƒè¿›è¡Œå°å¹…ä¿®æ”¹ï¼Œä¸ä¸»æµLLMè®­ç»ƒå·¥å…·æ— ç¼é›†æˆï¼Œå¹¶ä¸”ä¸ç°æœ‰çš„å†…å­˜ä¼˜åŒ–æ–¹æ³•å…¼å®¹ã€‚
- en: â€¢
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: We analyze the simplicity and memory efficiency of the Lion optimizer and confidently
    recommend it as the best choice for quantized fine-tuning. On this basis, we proceed
    to quantize all model states into the integer format, with each quantizer striking
    a balance between training accuracy and throughput. We also present a gradient
    flow scheme for the quantized weights.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åˆ†æäº†Lionä¼˜åŒ–å™¨çš„ç®€æ´æ€§å’Œå†…å­˜æ•ˆç‡ï¼Œå¹¶è‡ªä¿¡åœ°æ¨èå®ƒä½œä¸ºé‡åŒ–å¾®è°ƒçš„æœ€ä½³é€‰æ‹©ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å°†æ‰€æœ‰æ¨¡å‹çŠ¶æ€é‡åŒ–ä¸ºæ•´æ•°æ ¼å¼ï¼Œæ¯ä¸ªé‡åŒ–å™¨åœ¨è®­ç»ƒç²¾åº¦å’Œååé‡ä¹‹é—´å–å¾—å¹³è¡¡ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§é‡åŒ–æƒé‡çš„æ¢¯åº¦æµæ–¹æ¡ˆã€‚
- en: â€¢
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: We perform instruction tuning on the pre-trained LLaMA-2 models and extensively
    evaluate performance on various benchmarks. The results demonstrate that our QFT,
    with memory usage reduced to 21%, achieves comparable performance to standard
    floating-point training.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯¹é¢„è®­ç»ƒçš„LLaMA-2æ¨¡å‹è¿›è¡ŒæŒ‡ä»¤è°ƒä¼˜ï¼Œå¹¶åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šå¹¿æ³›è¯„ä¼°æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„QFTåœ¨å°†å†…å­˜ä½¿ç”¨å‡å°‘åˆ°21%çš„æƒ…å†µä¸‹ï¼Œè¡¨ç°å‡ºä¸æ ‡å‡†æµ®ç‚¹è®­ç»ƒç›¸å½“çš„æ€§èƒ½ã€‚
- en: 2 Related Works
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 ç›¸å…³å·¥ä½œ
- en: Efficient Optimizer
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: é«˜æ•ˆä¼˜åŒ–å™¨
- en: The primary optimizers employed for training transformer models are the Adam
    familyÂ (Kingma & Ba, [2015](#bib.bib22); Loshchilov & Hutter, [2017](#bib.bib34)).
    They maintain a rolling average of the previous gradients to promote stable convergence
    in training. However, their optimizer states (momentum and variances) imposes
    an extra memory overhead proportional to the number of model parameters, and this
    becomes a significant burden as LLMsâ€™ parameters increase. To overcome the memory
    challenges of model states, there are various memory-efficient schemes. LOMOÂ (Lv
    etÂ al., [2023](#bib.bib35)) utilizes a vanilla SGD optimizer for training LLMs,
    which unfortunately fails to ensure training performance due to the slow convergence
    and weak stability of SGDÂ (Li etÂ al., [2023](#bib.bib25)). Another imperfect solution
    is to utilize an Adafactor optimizerÂ (Shazeer & Stern, [2018](#bib.bib40)), which,
    despite storing only aggregated information, is also beset by instability issues.
    In this work, we adopt the Lion optimizerÂ (Chen etÂ al., [2023](#bib.bib3)), relying
    on its advantage of only keeping track of the momentum but achieving comparable
    convergence to Adam. More importantly, thanks to the sign operation, its update
    has the same magnitude for each parameter, which gives it a great potential for
    robust quantization of gradients and optimizer states.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºè®­ç»ƒå˜å‹å™¨æ¨¡å‹çš„ä¸»è¦ä¼˜åŒ–å™¨æ˜¯Adamå®¶æ—ï¼ˆKingma & Baï¼Œ[2015](#bib.bib22)ï¼›Loshchilov & Hutterï¼Œ[2017](#bib.bib34)ï¼‰ã€‚å®ƒä»¬ä¿æŒå…ˆå‰æ¢¯åº¦çš„æ»šåŠ¨å¹³å‡ï¼Œä»¥ä¿ƒè¿›è®­ç»ƒä¸­çš„ç¨³å®šæ”¶æ•›ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆåŠ¨é‡å’Œæ–¹å·®ï¼‰å¸¦æ¥äº†é¢å¤–çš„å†…å­˜å¼€é”€ï¼Œä¸æ¨¡å‹å‚æ•°çš„æ•°é‡æˆæ­£æ¯”ï¼Œéšç€LLMså‚æ•°çš„å¢åŠ ï¼Œè¿™æˆä¸ºäº†ä¸€ä¸ªæ˜¾è‘—çš„è´Ÿæ‹…ã€‚ä¸ºäº†å…‹æœæ¨¡å‹çŠ¶æ€çš„å†…å­˜æŒ‘æˆ˜ï¼Œæœ‰å„ç§å†…å­˜é«˜æ•ˆæ–¹æ¡ˆã€‚LOMOï¼ˆLvç­‰ï¼Œ[2023](#bib.bib35)ï¼‰åˆ©ç”¨æ™®é€šSGDä¼˜åŒ–å™¨è®­ç»ƒLLMsï¼Œä½†ç”±äºSGDçš„æ”¶æ•›æ…¢å’Œç¨³å®šæ€§å·®ï¼Œä¸å¹¸çš„æ˜¯æœªèƒ½ç¡®ä¿è®­ç»ƒæ€§èƒ½ã€‚å¦ä¸€ä¸ªä¸å®Œç¾çš„è§£å†³æ–¹æ¡ˆæ˜¯ä½¿ç”¨Adafactorä¼˜åŒ–å™¨ï¼ˆShazeer
    & Sternï¼Œ[2018](#bib.bib40)ï¼‰ï¼Œå°½ç®¡åªå­˜å‚¨èšåˆä¿¡æ¯ï¼Œä½†ä¹Ÿå­˜åœ¨ä¸ç¨³å®šé—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨Lionä¼˜åŒ–å™¨ï¼ˆChenç­‰ï¼Œ[2023](#bib.bib3)ï¼‰ï¼Œä¾é å…¶ä»…è·Ÿè¸ªåŠ¨é‡çš„ä¼˜åŠ¿ï¼Œä½†èƒ½è¾¾åˆ°ä¸Adamç›¸å½“çš„æ”¶æ•›æ€§ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œç”±äºç¬¦å·æ“ä½œï¼Œå…¶æ›´æ–°å¯¹æ¯ä¸ªå‚æ•°çš„å¹…åº¦ç›¸åŒï¼Œè¿™ä½¿å…¶åœ¨æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€çš„é²æ£’é‡åŒ–æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚
- en: Quantization for Memory Optimization
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: é‡åŒ–ç”¨äºå†…å­˜ä¼˜åŒ–
- en: Most existing quantization methods focus on inference efficiencyÂ (Gholami etÂ al.,
    [2022](#bib.bib13); Dong etÂ al., [2019](#bib.bib9); [2020](#bib.bib10); Kim etÂ al.,
    [2023](#bib.bib21); Li etÂ al., [2022a](#bib.bib28); [b](#bib.bib29); Li & Gu,
    [2022](#bib.bib27); Jacob etÂ al., [2018](#bib.bib19)), and recently, quantization
    is also believed to have great potential for optimizing training efficiency. Note
    that this research line is different from traditional quantization-aware training
    (QAT)Â (Jacob etÂ al., [2018](#bib.bib19); Liu etÂ al., [2023](#bib.bib33)). QAT
    inserts fake quantization nodes on weights and activations in training, where
    parameter arithmetic and storage retains the floating-point format, and thus training
    efficiency is not improved. As a comparison, quantization-based memory optimization
    methods, which attempt to utilize low-precision units to store parameters, can
    effectively reduce the memory budget in training, and thus have received increasing
    attention. BitsandbytesÂ (Dettmers etÂ al., [2021](#bib.bib6)) introduces a block-wise
    quantization method to compress the memory of optimizer states. QLoRAÂ (Dettmers
    etÂ al., [2023](#bib.bib7)) uses quantized values to store frozen pre-training
    weights, keeping only the adapters in the floating-point format. In this work,
    we propose a novel memory-efficient full-parameter fine-tuning framework for LLMs,
    in which all model states are stored as quantized integer values, enabling comprehensive
    memory compression without sacrificing fine-tuning performance.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰å¤§å¤šæ•°é‡åŒ–æ–¹æ³•å…³æ³¨äºæ¨ç†æ•ˆç‡ï¼ˆGholami et al., [2022](#bib.bib13); Dong et al., [2019](#bib.bib9);
    [2020](#bib.bib10); Kim et al., [2023](#bib.bib21); Li et al., [2022a](#bib.bib28);
    [b](#bib.bib29); Li & Gu, [2022](#bib.bib27); Jacob et al., [2018](#bib.bib19)ï¼‰ï¼Œæœ€è¿‘ï¼Œé‡åŒ–ä¹Ÿè¢«è®¤ä¸ºå…·æœ‰ä¼˜åŒ–è®­ç»ƒæ•ˆç‡çš„å·¨å¤§æ½œåŠ›ã€‚æ³¨æ„ï¼Œè¿™ä¸€ç ”ç©¶æ–¹å‘ä¸åŒäºä¼ ç»Ÿçš„é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰ï¼ˆJacob
    et al., [2018](#bib.bib19); Liu et al., [2023](#bib.bib33)ï¼‰ã€‚QAT åœ¨è®­ç»ƒä¸­åœ¨æƒé‡å’Œæ¿€æ´»ä¸Šæ’å…¥å‡é‡åŒ–èŠ‚ç‚¹ï¼Œå…¶ä¸­å‚æ•°ç®—æœ¯å’Œå­˜å‚¨ä¿æŒæµ®ç‚¹æ ¼å¼ï¼Œå› æ­¤è®­ç»ƒæ•ˆç‡æ²¡æœ‰å¾—åˆ°æé«˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸºäºé‡åŒ–çš„å†…å­˜ä¼˜åŒ–æ–¹æ³•ï¼Œå°è¯•åˆ©ç”¨ä½ç²¾åº¦å•å…ƒå­˜å‚¨å‚æ•°ï¼Œå¯ä»¥æœ‰æ•ˆå‡å°‘è®­ç»ƒä¸­çš„å†…å­˜é¢„ç®—ï¼Œå› æ­¤å—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚Bitsandbytesï¼ˆDettmers
    et al., [2021](#bib.bib6)ï¼‰å¼•å…¥äº†ä¸€ç§å—çº§é‡åŒ–æ–¹æ³•æ¥å‹ç¼©ä¼˜åŒ–å™¨çŠ¶æ€çš„å†…å­˜ã€‚QLoRAï¼ˆDettmers et al., [2023](#bib.bib7)ï¼‰ä½¿ç”¨é‡åŒ–å€¼å­˜å‚¨å†»ç»“çš„é¢„è®­ç»ƒæƒé‡ï¼Œä»…ä¿ç•™é€‚é…å™¨ä¸ºæµ®ç‚¹æ ¼å¼ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å†…å­˜é«˜æ•ˆå…¨å‚æ•°å¾®è°ƒæ¡†æ¶ï¼Œç”¨äºLLMsï¼Œå…¶ä¸­æ‰€æœ‰æ¨¡å‹çŠ¶æ€éƒ½å­˜å‚¨ä¸ºé‡åŒ–æ•´æ•°å€¼ï¼Œå®ç°å…¨é¢çš„å†…å­˜å‹ç¼©è€Œä¸ç‰ºç‰²å¾®è°ƒæ€§èƒ½ã€‚
- en: Other Memory Optimization Methods
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å…¶ä»–å†…å­˜ä¼˜åŒ–æ–¹æ³•
- en: Other prominent memory optimization methods include offloadingÂ (Huang etÂ al.,
    [2020](#bib.bib16); Wang etÂ al., [2018](#bib.bib43); Peng etÂ al., [2020](#bib.bib37))
    and gradient checkpointingÂ (Chen etÂ al., [2016](#bib.bib2); Kumar etÂ al., [2019](#bib.bib24);
    Jain etÂ al., [2020](#bib.bib20); Kirisame etÂ al., [2020](#bib.bib23)). Activation
    offloading offloads activation to external memory (e.g., CPU memory). It is worth
    noting that offloading comes at the cost of transferring data to another storage,
    which can increase execution time. Gradient checkpointing is a technique that
    discards activations in the forward pass and recomputes them in the backward pass
    as needed. This approach involves a trade-off between memory usage and computation
    cost. In addition, there are also customized schemes proposed for training LLMs.
    LOMOÂ (Lv etÂ al., [2023](#bib.bib35)) fuses the gradient computation and the parameter
    update in one step. This method can reduce the memory usage of gradient tensors
    to O(1); however, there is a potential caveat as it is incompatible with gradient
    accumulation for scaling batch sizes, limiting it to unstable training with small
    batch sizes. In contrast, our framework is orthogonal and well compatible with
    all the above methods.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä»–æ˜¾è‘—çš„å†…å­˜ä¼˜åŒ–æ–¹æ³•åŒ…æ‹¬å¸è½½ï¼ˆHuang et al., [2020](#bib.bib16); Wang et al., [2018](#bib.bib43);
    Peng et al., [2020](#bib.bib37)ï¼‰å’Œæ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆChen et al., [2016](#bib.bib2); Kumar et
    al., [2019](#bib.bib24); Jain et al., [2020](#bib.bib20); Kirisame et al., [2020](#bib.bib23)ï¼‰ã€‚æ¿€æ´»å¸è½½å°†æ¿€æ´»è½¬ç§»åˆ°å¤–éƒ¨å†…å­˜ï¼ˆä¾‹å¦‚ï¼ŒCPU
    å†…å­˜ï¼‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¸è½½ä¼šå¢åŠ å°†æ•°æ®ä¼ è¾“åˆ°å¦ä¸€ä¸ªå­˜å‚¨çš„æˆæœ¬ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ æ‰§è¡Œæ—¶é—´ã€‚æ¢¯åº¦æ£€æŸ¥ç‚¹æ˜¯ä¸€ç§åœ¨å‰å‘ä¼ æ’­ä¸­ä¸¢å¼ƒæ¿€æ´»å¹¶åœ¨åå‘ä¼ æ’­ä¸­æ ¹æ®éœ€è¦é‡æ–°è®¡ç®—å®ƒä»¬çš„æŠ€æœ¯ã€‚è¿™ç§æ–¹æ³•æ¶‰åŠå†…å­˜ä½¿ç”¨å’Œè®¡ç®—æˆæœ¬ä¹‹é—´çš„æƒè¡¡ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰ä¸ºè®­ç»ƒLLMsæå‡ºçš„å®šåˆ¶æ–¹æ¡ˆã€‚LOMOï¼ˆLv
    et al., [2023](#bib.bib35)ï¼‰å°†æ¢¯åº¦è®¡ç®—å’Œå‚æ•°æ›´æ–°åˆå¹¶ä¸ºä¸€æ­¥ã€‚è¿™ç§æ–¹æ³•å¯ä»¥å°†æ¢¯åº¦å¼ é‡çš„å†…å­˜ä½¿ç”¨é™ä½åˆ° O(1)ï¼›ç„¶è€Œï¼Œå®ƒæœ‰ä¸€ä¸ªæ½œåœ¨çš„è­¦å‘Šï¼Œå³å®ƒä¸æ¢¯åº¦ç´¯ç§¯ä¸å…¼å®¹ï¼Œä»¥æ‰©å¤§æ‰¹é‡å¤§å°ï¼Œé™åˆ¶äº†å…¶åœ¨å°æ‰¹é‡å¤§å°ä¸‹çš„ä¸ç¨³å®šè®­ç»ƒã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æ˜¯æ­£äº¤çš„ï¼Œå¹¶ä¸”ä¸æ‰€æœ‰ä¸Šè¿°æ–¹æ³•è‰¯å¥½å…¼å®¹ã€‚
- en: 3 Methodology
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 æ–¹æ³•è®º
- en: 3.1 Lion Optimizer
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 ç‹®å­ä¼˜åŒ–å™¨
- en: In a recent exploration of algorithm discovery through program search for neural
    network training, a novel optimization algorithm, Lion (EvoLved Sign Momentum),
    was conceivedÂ (Chen etÂ al., [2023](#bib.bib3)). The method explores an expansive
    program space while implementing program selection and simplification strategies.
    Lion stands out due to its simplicity and memory-efficiency, only tracking momentum,
    differing from adaptive optimizers by employing a consistent magnitude update
    for each parameter using the sign operation. Comparative studies with established
    optimizers, like AdamÂ (Kingma & Ba, [2015](#bib.bib22)) and AdafactorÂ (Shazeer
    & Stern, [2018](#bib.bib40)), underscored Lionâ€™s efficacy, leading to superior
    results in various domains, from image classification to language modeling. Particularly
    notable, Lion boosts the accuracy of Vision Transformers (ViT) on ImageNet, decreases
    pre-training compute on JFT, and surpasses Adam in training diffusion models.
    However, its advantages grow with increased training batch sizes and necessitate
    a lower learning rate than Adam, given the larger update norm resulting from the
    sign function.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ€è¿‘çš„ç¥ç»ç½‘ç»œè®­ç»ƒç¨‹åºæœç´¢ç®—æ³•å‘ç°ç ”ç©¶ä¸­ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¼˜åŒ–ç®—æ³•â€”â€”Lionï¼ˆEvoLved Sign Momentumï¼‰ï¼Œè¯¥ç®—æ³•ç”±é™ˆç­‰äººï¼ˆ[2023](#bib.bib3)ï¼‰æ„æƒ³ã€‚è¯¥æ–¹æ³•æ¢ç´¢äº†å¹¿æ³›çš„ç¨‹åºç©ºé—´ï¼ŒåŒæ—¶å®æ–½äº†ç¨‹åºé€‰æ‹©å’Œç®€åŒ–ç­–ç•¥ã€‚Lionå› å…¶ç®€å•æ€§å’Œå†…å­˜æ•ˆç‡è€Œè„±é¢–è€Œå‡ºï¼Œåªè·Ÿè¸ªåŠ¨é‡ï¼Œä¸è‡ªé€‚åº”ä¼˜åŒ–å™¨ä¸åŒçš„æ˜¯ï¼Œå®ƒé€šè¿‡ç¬¦å·æ“ä½œå¯¹æ¯ä¸ªå‚æ•°è¿›è¡Œä¸€è‡´çš„å¹…åº¦æ›´æ–°ã€‚ä¸å·²å»ºç«‹çš„ä¼˜åŒ–å™¨ï¼ˆå¦‚Adamï¼ˆKingma
    & Ba, [2015](#bib.bib22)ï¼‰å’ŒAdafactorï¼ˆShazeer & Stern, [2018](#bib.bib40)ï¼‰ï¼‰çš„æ¯”è¾ƒç ”ç©¶çªæ˜¾äº†Lionçš„æœ‰æ•ˆæ€§ï¼Œåœ¨ä»å›¾åƒåˆ†ç±»åˆ°è¯­è¨€å»ºæ¨¡ç­‰å„ä¸ªé¢†åŸŸéƒ½å–å¾—äº†ä¼˜è¶Šçš„ç»“æœã€‚ç‰¹åˆ«å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒLionæé«˜äº†Vision
    Transformersï¼ˆViTï¼‰åœ¨ImageNetä¸Šçš„å‡†ç¡®æ€§ï¼Œå‡å°‘äº†JFTä¸Šçš„é¢„è®­ç»ƒè®¡ç®—ï¼Œå¹¶åœ¨è®­ç»ƒæ‰©æ•£æ¨¡å‹æ—¶è¶…è¶Šäº†Adamã€‚ç„¶è€Œï¼Œç”±äºç¬¦å·å‡½æ•°äº§ç”Ÿäº†è¾ƒå¤§çš„æ›´æ–°èŒƒæ•°ï¼Œå…¶ä¼˜åŠ¿éšç€è®­ç»ƒæ‰¹æ¬¡å¤§å°çš„å¢åŠ è€Œå¢é•¿ï¼Œå¹¶ä¸”éœ€è¦æ¯”Adamæ›´ä½çš„å­¦ä¹ ç‡ã€‚
- en: 'Designing quantized fine-tuning algorithms involves working with limited-precision
    representations of parameters, gradients and momentum. This can lead to several
    challenges, including increased sensitivity to noise, potential accumulation of
    rounding errors, and other precision-related issues. We find Lion more suitable
    for the task of quantized fine-tuning, due to the following reasons:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾è®¡é‡åŒ–å¾®è°ƒç®—æ³•æ¶‰åŠå¤„ç†å‚æ•°ã€æ¢¯åº¦å’ŒåŠ¨é‡çš„æœ‰é™ç²¾åº¦è¡¨ç¤ºã€‚è¿™å¯èƒ½ä¼šå¯¼è‡´å‡ ä¸ªæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¯¹å™ªå£°çš„æ•æ„Ÿæ€§å¢åŠ ã€èˆå…¥è¯¯å·®çš„æ½œåœ¨ç´¯ç§¯ä»¥åŠå…¶ä»–ç²¾åº¦ç›¸å…³é—®é¢˜ã€‚æˆ‘ä»¬å‘ç°Lionæ›´é€‚åˆé‡åŒ–å¾®è°ƒä»»åŠ¡ï¼ŒåŸå› å¦‚ä¸‹ï¼š
- en: â€¢
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Simplicity: Lion is simpler and more memory-efficient since it only keeps track
    of the momentum. This reduced complexity might be beneficial when dealing with
    quantized values, where added algorithmic intricacies can amplify quantization
    errors.'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç®€å•æ€§ï¼šLionæ›´ç®€å•ã€æ›´èŠ‚çœå†…å­˜ï¼Œå› ä¸ºå®ƒåªè·Ÿè¸ªåŠ¨é‡ã€‚è¿™ç§é™ä½çš„å¤æ‚æ€§åœ¨å¤„ç†é‡åŒ–å€¼æ—¶å¯èƒ½æ˜¯æœ‰ç›Šçš„ï¼Œå› ä¸ºå¢åŠ çš„ç®—æ³•å¤æ‚æ€§ä¼šæ”¾å¤§é‡åŒ–è¯¯å·®ã€‚
- en: â€¢
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Consistent Update Magnitudes: Unlike adaptive optimizers, Lion ensures that
    updates have the same magnitude for each parameter, which is determined through
    the sign operation. In a quantized setting, this consistency can mitigate potential
    imbalances or inaccuracies in weight updates introduced by limited precision.'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä¸€è‡´æ›´æ–°å¹…åº¦ï¼šä¸è‡ªé€‚åº”ä¼˜åŒ–å™¨ä¸åŒï¼ŒLionç¡®ä¿æ¯ä¸ªå‚æ•°çš„æ›´æ–°å…·æœ‰ç›¸åŒçš„å¹…åº¦ï¼Œè¿™ä¸€å¹…åº¦é€šè¿‡ç¬¦å·æ“ä½œç¡®å®šã€‚åœ¨é‡åŒ–è®¾ç½®ä¸­ï¼Œè¿™ç§ä¸€è‡´æ€§å¯ä»¥ç¼“è§£ç”±æœ‰é™ç²¾åº¦å¼•å…¥çš„æ½œåœ¨ä¸å¹³è¡¡æˆ–ä¸å‡†ç¡®ã€‚
- en: â€¢
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Memory Efficiency: Memory usage is a common concern in quantized neural networks,
    especially when deploying on edge devices with constrained memory. Lionâ€™s memory
    efficiency (only tracking momentum) makes it a potentially better fit for such
    quantized settings than optimizers like Adam, which track more state variables.'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å†…å­˜æ•ˆç‡ï¼šåœ¨é‡åŒ–ç¥ç»ç½‘ç»œä¸­ï¼Œå†…å­˜ä½¿ç”¨æ˜¯ä¸€ä¸ªå¸¸è§çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å†…å­˜å—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²æ—¶ã€‚Lionçš„å†…å­˜æ•ˆç‡ï¼ˆåªè·Ÿè¸ªåŠ¨é‡ï¼‰ä½¿å…¶åœ¨è¿™ç§é‡åŒ–ç¯å¢ƒä¸‹æ¯”åƒAdamè¿™æ ·çš„ä¼˜åŒ–å™¨æ›´é€‚åˆï¼Œå› ä¸ºAdamè·Ÿè¸ªæ›´å¤šçš„çŠ¶æ€å˜é‡ã€‚
- en: 3.2 Quantization
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 é‡åŒ–
- en: The Lion optimizer simplifies the composition of model states, which consist
    only of model weights, gradients, and optimizer momentum, resulting in a 25% reduction
    in memory usage compared to the memory-intensive Adam optimizer. However, it is
    imperative to recognize that these model states are still retained in the original
    floating-point format, a characteristic that can introduce redundant representations
    and, consequently, contribute to memory inefficiency. In light of this consideration,
    quantization, which involves the use of reduced-precision formats such as INT8
    to represent neural networks, emerges as a compelling avenue for further memory
    optimization.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Lion ä¼˜åŒ–å™¨ç®€åŒ–äº†æ¨¡å‹çŠ¶æ€çš„ç»„æˆï¼Œè¿™äº›çŠ¶æ€ä»…åŒ…æ‹¬æ¨¡å‹æƒé‡ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨åŠ¨é‡ï¼Œç›¸æ¯”å†…å­˜å¯†é›†å‹çš„ Adam ä¼˜åŒ–å™¨ï¼Œå†…å­˜ä½¿ç”¨å‡å°‘äº† 25%ã€‚ç„¶è€Œï¼Œå¿…é¡»è®¤è¯†åˆ°è¿™äº›æ¨¡å‹çŠ¶æ€ä»ä»¥åŸå§‹æµ®ç‚¹æ ¼å¼ä¿ç•™ï¼Œè¿™ä¸€ç‰¹æ€§å¯èƒ½å¼•å…¥å†—ä½™è¡¨ç¤ºï¼Œä»è€Œå¯¼è‡´å†…å­˜ä½æ•ˆã€‚è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ï¼Œé‡åŒ–â€”â€”å³ä½¿ç”¨å¦‚
    INT8 ç­‰ä½ç²¾åº¦æ ¼å¼æ¥è¡¨ç¤ºç¥ç»ç½‘ç»œâ€”â€”æˆä¸ºè¿›ä¸€æ­¥å†…å­˜ä¼˜åŒ–çš„æœ‰åŠ›é€”å¾„ã€‚
- en: '![Refer to caption](img/3bef3b3d6d6148ea45902c9b35cd85dc.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/3bef3b3d6d6148ea45902c9b35cd85dc.png)'
- en: 'Figure 1: Comparison between our QFT and traditional QAT in the computation
    and update procedures of weights. QAT stores the weights in the floating-point
    format and adds fake quantization nodes to the computation. Conversely, in our
    QFT, the weights are stored in the low-precision integer format, which are de-quantized
    on-the-fly into the floating-point format for computation, resulting in a significant
    reduction in memory usage.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 1ï¼šæˆ‘ä»¬çš„ QFT ä¸ä¼ ç»Ÿ QAT åœ¨æƒé‡è®¡ç®—å’Œæ›´æ–°è¿‡ç¨‹ä¸­çš„æ¯”è¾ƒã€‚QAT å°†æƒé‡å­˜å‚¨ä¸ºæµ®ç‚¹æ ¼å¼ï¼Œå¹¶åœ¨è®¡ç®—ä¸­æ·»åŠ è™šå‡çš„é‡åŒ–èŠ‚ç‚¹ã€‚ç›¸åï¼Œåœ¨æˆ‘ä»¬çš„ QFT
    ä¸­ï¼Œæƒé‡ä»¥ä½ç²¾åº¦æ•´æ•°æ ¼å¼å­˜å‚¨ï¼Œè¿™äº›æƒé‡åœ¨è®¡ç®—æ—¶å³æ—¶è§£é‡åŒ–ä¸ºæµ®ç‚¹æ ¼å¼ï¼Œä»è€Œæ˜¾è‘—å‡å°‘äº†å†…å­˜ä½¿ç”¨ã€‚
- en: 'The field of quantization methods primarily emphasizes improving model inference
    efficiency, with limited attention paid to reducing training overheadÂ (Dettmers
    etÂ al., [2021](#bib.bib6)). Our approach stands out through a comprehensive training
    memory compression, which is accomplished by quantizing all model states within
    the Lion optimizer and storing them as integer values. This sets our approach
    apart from traditional QATÂ (Jacob etÂ al., [2018](#bib.bib19)). In our method,
    we initially store model parameters as quantized integers, whereas traditional
    QAT introduces fake quantization nodes to floating-point parameters. This distinction
    highlights the significance of our approach, as the latter method, with the reliance
    on fake quantization nodes, do not inherently enhance training efficiency. To
    more clearly demonstrate this difference, we present a comparison in Figure [1](#S3.F1
    "Figure 1 â€£ 3.2 Quantization â€£ 3 Methodology â€£ QFT: Quantized Full-parameter Tuning
    of LLMs with Affordable Resources").'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: é‡åŒ–æ–¹æ³•é¢†åŸŸä¸»è¦å¼ºè°ƒæé«˜æ¨¡å‹æ¨ç†æ•ˆç‡ï¼Œå¯¹å‡å°‘è®­ç»ƒå¼€é”€å…³æ³¨è¾ƒå°‘ï¼ˆDettmers ç­‰ï¼Œ [2021](#bib.bib6)ï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åœ¨ Lion
    ä¼˜åŒ–å™¨ä¸­å¯¹æ‰€æœ‰æ¨¡å‹çŠ¶æ€è¿›è¡Œé‡åŒ–å¹¶å°†å…¶å­˜å‚¨ä¸ºæ•´æ•°å€¼ï¼Œå®ç°äº†å…¨é¢çš„è®­ç»ƒå†…å­˜å‹ç¼©ã€‚è¿™ä½¿å¾—æˆ‘ä»¬çš„æ–¹æ³•ä¸ä¼ ç»Ÿ QATï¼ˆJacob ç­‰ï¼Œ [2018](#bib.bib19)ï¼‰æœ‰æ‰€ä¸åŒã€‚åœ¨æˆ‘ä»¬çš„æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬æœ€åˆå°†æ¨¡å‹å‚æ•°å­˜å‚¨ä¸ºé‡åŒ–æ•´æ•°ï¼Œè€Œä¼ ç»Ÿ
    QAT å‘æµ®ç‚¹å‚æ•°å¼•å…¥è™šå‡é‡åŒ–èŠ‚ç‚¹ã€‚è¿™ä¸€åŒºåˆ«å‡¸æ˜¾äº†æˆ‘ä»¬æ–¹æ³•çš„é‡è¦æ€§ï¼Œå› ä¸ºåè€…æ–¹æ³•ä¾èµ–è™šå‡é‡åŒ–èŠ‚ç‚¹ï¼Œä¸èƒ½æœ¬è´¨ä¸Šæé«˜è®­ç»ƒæ•ˆç‡ã€‚ä¸ºäº†æ›´æ¸…æ¥šåœ°å±•ç¤ºè¿™ç§å·®å¼‚ï¼Œæˆ‘ä»¬åœ¨å›¾
    [1](#S3.F1 "å›¾ 1 â€£ 3.2 é‡åŒ– â€£ 3 æ–¹æ³• â€£ QFTï¼šå…·æœ‰å¯æ‰¿å—èµ„æºçš„ LLM çš„é‡åŒ–å…¨å‚æ•°è°ƒæ•´") ä¸­è¿›è¡Œæ¯”è¾ƒã€‚
- en: '![Refer to caption](img/7ab4ae78129aaf703e321c250a80ae7e.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/7ab4ae78129aaf703e321c250a80ae7e.png)'
- en: 'Figure 2: Illustration of the model state distributions when training a LLaMA-2-7B
    model. The weight values are from the final down projection layer, and the gradient
    and momentum values are fetched on the 200th training step. The gradients and
    momentum show a canonical centralized distribution with few outliers, while the
    range of the weights increases by three orders of magnitude and exhibits extreme
    outliers, posing a significant challenge to quantization.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2ï¼šè®­ç»ƒ LLaMA-2-7B æ¨¡å‹æ—¶æ¨¡å‹çŠ¶æ€åˆ†å¸ƒçš„ç¤ºæ„å›¾ã€‚æƒé‡å€¼æ¥è‡ªæœ€ç»ˆçš„é™ç»´å±‚ï¼Œæ¢¯åº¦å’ŒåŠ¨é‡å€¼åœ¨ç¬¬ 200 æ­¥è®­ç»ƒæ—¶è·å–ã€‚æ¢¯åº¦å’ŒåŠ¨é‡æ˜¾ç¤ºå‡ºç»å…¸çš„é›†ä¸­åˆ†å¸ƒï¼Œåªæœ‰å°‘é‡å¼‚å¸¸å€¼ï¼Œè€Œæƒé‡çš„èŒƒå›´å¢åŠ äº†ä¸‰ä¸ªæ•°é‡çº§ï¼Œå¹¶ä¸”å‡ºç°äº†æç«¯çš„å¼‚å¸¸å€¼ï¼Œè¿™ç»™é‡åŒ–å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚
- en: 'We first perform an in-depth examination of the numerical distributions of
    the model weights, gradients and optimizer momentum, as shown in Figure [2](#S3.F2
    "Figure 2 â€£ 3.2 Quantization â€£ 3 Methodology â€£ QFT: Quantized Full-parameter Tuning
    of LLMs with Affordable Resources"). This comprehensive analysis forms the basis
    for designing appropriate quantization strategies. Remarkably, we prioritize lightweight
    quantizers to minimize the impact of de-quantization on the training throughput.
    In the following, we describe in detail the quantizers employed for different
    model states.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 'æˆ‘ä»¬é¦–å…ˆå¯¹æ¨¡å‹æƒé‡ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨åŠ¨é‡çš„æ•°å€¼åˆ†å¸ƒè¿›è¡Œæ·±å…¥åˆ†æï¼Œå¦‚å›¾ [2](#S3.F2 "Figure 2 â€£ 3.2 Quantization â€£
    3 Methodology â€£ QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources")
    æ‰€ç¤ºã€‚è¿™ä¸€å…¨é¢åˆ†æä¸ºè®¾è®¡åˆé€‚çš„é‡åŒ–ç­–ç•¥å¥ å®šäº†åŸºç¡€ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬ä¼˜å…ˆè€ƒè™‘è½»é‡çº§é‡åŒ–å™¨ï¼Œä»¥å‡å°‘å»é‡åŒ–å¯¹è®­ç»ƒååé‡çš„å½±å“ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è¯¦ç»†æè¿°äº†é’ˆå¯¹ä¸åŒæ¨¡å‹çŠ¶æ€ä½¿ç”¨çš„é‡åŒ–å™¨ã€‚'
- en: Uniform Quantizer for Gradients and Momentum
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ¢¯åº¦å’ŒåŠ¨é‡çš„å‡åŒ€é‡åŒ–å™¨
- en: 'The gradients and momentum values exhibit a central distribution with few outliers
    that deviate from the central range, allowing us to confidently utilize the uniform
    quantizer, which is regarded as the most fundamental quantization method. The
    uniform quantizer includes two essential procedures: quantization and de-quantization,
    which are defined as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦å’ŒåŠ¨é‡å€¼è¡¨ç°å‡ºä¸€ä¸ªä¸­å¿ƒåˆ†å¸ƒï¼Œåªæœ‰å°‘é‡å¼‚å¸¸å€¼åç¦»ä¸­å¿ƒèŒƒå›´ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿè‡ªä¿¡åœ°ä½¿ç”¨å‡åŒ€é‡åŒ–å™¨ï¼Œè¿™è¢«è®¤ä¸ºæ˜¯æœ€åŸºæœ¬çš„é‡åŒ–æ–¹æ³•ã€‚å‡åŒ€é‡åŒ–å™¨åŒ…æ‹¬ä¸¤ä¸ªåŸºæœ¬è¿‡ç¨‹ï¼šé‡åŒ–å’Œå»é‡åŒ–ï¼Œå®šä¹‰å¦‚ä¸‹ï¼š
- en: '|  | $Quant:\bm{X}^{(\mathbb{Z})}=\text{clip}\left(\left\lfloor\frac{\bm{X}}{s}\right\rceil+z,0,2^{b}-1\right)\quad
    De\text{-}quant:\hat{\bm{X}}=s\left(\bm{X}^{(\mathbb{Z})}-z\right)\approx\bm{X}$
    |  | (1) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $Quant:\bm{X}^{(\mathbb{Z})}=\text{clip}\left(\left\lfloor\frac{\bm{X}}{s}\right\rceil+z,0,2^{b}-1\right)\quad
    De\text{-}quant:\hat{\bm{X}}=s\left(\bm{X}^{(\mathbb{Z})}-z\right)\approx\bm{X}$
    |  | (1) |'
- en: 'where $\bm{X}$ as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\bm{X}$ å¦‚ä¸‹ï¼š
- en: '|  | $s=\frac{\max(\bm{X})-\min(\bm{X})}{2^{b}-1},\quad z=\left\lfloor-\frac{\min(\bm{X})}{s}\right\rceil$
    |  | (2) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $s=\frac{\max(\bm{X})-\min(\bm{X})}{2^{b}-1},\quad z=\left\lfloor-\frac{\min(\bm{X})}{s}\right\rceil$
    |  | (2) |'
- en: Dense-and-Sparse Quantizer for Weights
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç¨ å¯†ä¸ç¨€ç–æƒé‡é‡åŒ–å™¨
- en: In contrast to gradients and momentum, whose probability distributions lend
    themselves well to quantization, the weights present a distinct challenge. This
    challenge arises from their considerably broader range, which is approximately
    three orders of magnitude larger than that of momentum, as well as the presence
    of pronounced outliers. This combination of factors makes the accurate quantization
    of weights a particularly formidable taskÂ (Kim etÂ al., [2023](#bib.bib21); Frantar
    etÂ al., [2022](#bib.bib11); Lin etÂ al., [2023](#bib.bib31)).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸è¾ƒäºæ¢¯åº¦å’ŒåŠ¨é‡ï¼Œè¿™äº›çš„æ¦‚ç‡åˆ†å¸ƒé€‚åˆé‡åŒ–ï¼Œæƒé‡å´å‘ˆç°å‡ºç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚è¿™ç§æŒ‘æˆ˜æºäºæƒé‡èŒƒå›´çš„æ˜¾è‘—å¹¿æ³›ï¼Œå¤§çº¦æ˜¯åŠ¨é‡çš„ä¸‰å€ï¼Œä»¥åŠå­˜åœ¨æ˜æ˜¾çš„å¼‚å¸¸å€¼ã€‚è¿™äº›å› ç´ çš„ç»“åˆä½¿å¾—æƒé‡çš„å‡†ç¡®é‡åŒ–æˆä¸ºä¸€é¡¹ç‰¹åˆ«è‰°å·¨çš„ä»»åŠ¡Â (Kim
    etÂ al., [2023](#bib.bib21); Frantar etÂ al., [2022](#bib.bib11); Lin etÂ al., [2023](#bib.bib31))ã€‚
- en: 'Upon revisiting the weight distribution, we uncover an intriguing pattern:
    if we set aside the extreme outliers, the remaining parameters coalesce into a
    notably compact distribution. To elucidate, the initial expansive range is predominantly
    influenced by these extreme outliers, with a striking statistic that 99% of the
    values cluster within a mere 20% of the overall range. This revelation serves
    as the catalyst for our approach, drawing inspiration from the dense-and-sparse
    quantizer presented inÂ (Kim etÂ al., [2023](#bib.bib21)). This method effectively
    ameliorates the issue of outliers by decomposing the weights into two distinct
    matrices: one dense and the other sparse. Formally, the method is defined as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å½“é‡æ–°å®¡è§†æƒé‡åˆ†å¸ƒæ—¶ï¼Œæˆ‘ä»¬å‘ç°ä¸€ä¸ªæœ‰è¶£çš„æ¨¡å¼ï¼šå¦‚æœæ’é™¤æç«¯å¼‚å¸¸å€¼ï¼Œå…¶ä½™å‚æ•°é›†ä¸­æˆä¸€ä¸ªæ˜æ˜¾ç´§å‡‘çš„åˆ†å¸ƒã€‚å…·ä½“æ¥è¯´ï¼Œåˆå§‹çš„å¹¿æ³›èŒƒå›´ä¸»è¦å—åˆ°è¿™äº›æç«¯å¼‚å¸¸å€¼çš„å½±å“ï¼Œä»¤äººæƒŠè®¶çš„æ˜¯99%çš„å€¼é›†ä¸­åœ¨æ•´ä½“èŒƒå›´çš„ä»…20%å†…ã€‚è¿™ä¸€å‘ç°æˆä¸ºæˆ‘ä»¬æ–¹æ³•çš„å‚¬åŒ–å‰‚ï¼Œçµæ„Ÿæ¥æºäº(Kim
    etÂ al., [2023](#bib.bib21))ä¸­æå‡ºçš„ç¨ å¯†ä¸ç¨€ç–é‡åŒ–å™¨ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†æƒé‡åˆ†è§£ä¸ºä¸¤ä¸ªä¸åŒçš„çŸ©é˜µæ¥æœ‰æ•ˆæ”¹å–„å¼‚å¸¸å€¼é—®é¢˜ï¼šä¸€ä¸ªç¨ å¯†çŸ©é˜µå’Œä¸€ä¸ªç¨€ç–çŸ©é˜µã€‚æ­£å¼åœ°ï¼Œæ–¹æ³•å®šä¹‰å¦‚ä¸‹ï¼š
- en: '|  | $\displaystyle\bm{W}=\bm{D}+\bm{S}\enspace\mathrm{s.t.}\enspace\bm{D}$
    |  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{W}=\bm{D}+\bm{S}\enspace\mathrm{s.t.}\enspace\bm{D}$
    |  |'
- en: '|  | $\displaystyle\mathrm{and}\enspace\bm{S}$ |  | (3) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathrm{and}\enspace\bm{S}$ |  | (3) |'
- en: where $\bm{D}$ are the thresholds for identifying outliers, which can be determined
    by the percentage of the range. Itâ€™s important to highlight that the matrix decomposition
    process is numerically straightforward, ensuring a high level of computational
    efficiency with minimal repercussions on training overhead.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­$\bm{D}$æ˜¯ç”¨äºè¯†åˆ«å¼‚å¸¸å€¼çš„é˜ˆå€¼ï¼Œå¯ä»¥é€šè¿‡èŒƒå›´çš„ç™¾åˆ†æ¯”æ¥ç¡®å®šã€‚å€¼å¾—å¼ºè°ƒçš„æ˜¯ï¼ŒçŸ©é˜µåˆ†è§£è¿‡ç¨‹åœ¨æ•°å€¼ä¸Šæ˜¯ç®€å•çš„ï¼Œç¡®ä¿äº†é«˜æ°´å¹³çš„è®¡ç®—æ•ˆç‡ï¼ŒåŒæ—¶å¯¹è®­ç»ƒå¼€é”€çš„å½±å“æœ€å°ã€‚
- en: 'Subsequently, the dense matrix adheres to the simple uniform quantizer as described
    in Equation [1](#S3.E1 "In Uniform Quantizer for Gradients and Momentum â€£ 3.2
    Quantization â€£ 3 Methodology â€£ QFT: Quantized Full-parameter Tuning of LLMs with
    Affordable Resources"), while the sparse matrix retains its data in the floating-point
    format. Notably, given that the outliers constitute a relatively minor fraction,
    such as 1%, the sparse matrix can capitalize on memory-efficient storage techniques,
    like compressed sparse row (CSR) format, which can be instrumental in substantially
    mitigating memory overhead.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 'éšåï¼Œå¯†é›†çŸ©é˜µéµå¾ªå¦‚æ–¹ç¨‹[1](#S3.E1 "In Uniform Quantizer for Gradients and Momentum â€£ 3.2
    Quantization â€£ 3 Methodology â€£ QFT: Quantized Full-parameter Tuning of LLMs with
    Affordable Resources")ä¸­æ‰€æè¿°çš„ç®€å•å‡åŒ€é‡åŒ–å™¨ï¼Œè€Œç¨€ç–çŸ©é˜µåˆ™ä»¥æµ®ç‚¹æ ¼å¼ä¿ç•™æ•°æ®ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç”±äºå¼‚å¸¸å€¼æ„æˆäº†ç›¸å¯¹è¾ƒå°çš„æ¯”ä¾‹ï¼Œä¾‹å¦‚1%ï¼Œç¨€ç–çŸ©é˜µå¯ä»¥åˆ©ç”¨å†…å­˜é«˜æ•ˆå­˜å‚¨æŠ€æœ¯ï¼Œå¦‚å‹ç¼©ç¨€ç–è¡Œï¼ˆCSRï¼‰æ ¼å¼ï¼Œè¿™å¯¹äºå¤§å¹…å‡å°‘å†…å­˜å¼€é”€éå¸¸æœ‰å¸®åŠ©ã€‚'
- en: 3.3 Overall Framework
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 æ€»ä½“æ¡†æ¶
- en: In this section, we integrate the above efficient Lion optimizer and quantization
    methods and introduce a memory-efficient fine-tuning framework for LLMs. We provide
    a comprehensive description of each training phase, including forward propagation,
    backward propagation, and parameter update, with particular emphasis on the quantized
    gradient flow and the quantized optimizer step.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æ•´åˆäº†ä¸Šè¿°é«˜æ•ˆçš„Lionä¼˜åŒ–å™¨å’Œé‡åŒ–æ–¹æ³•ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§å†…å­˜é«˜æ•ˆçš„LLMå¾®è°ƒæ¡†æ¶ã€‚æˆ‘ä»¬æä¾›äº†æ¯ä¸ªè®­ç»ƒé˜¶æ®µçš„å…¨é¢æè¿°ï¼ŒåŒ…æ‹¬å‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°ï¼Œç‰¹åˆ«å¼ºè°ƒé‡åŒ–æ¢¯åº¦æµå’Œé‡åŒ–ä¼˜åŒ–å™¨æ­¥éª¤ã€‚
- en: Quantized Forward Propagation
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: é‡åŒ–å‰å‘ä¼ æ’­
- en: 'Within our framework, we initially represent weights as quantized integer values
    to optimize memory utilization. During the execution of forward propagation, we
    de-quantize these low-precision weights into the floating-point format on-the-fly,
    thereby enabling high-precision arithmetic operations. For more clarity, we visualize
    this critical process in Figure [1](#S3.F1 "Figure 1 â€£ 3.2 Quantization â€£ 3 Methodology
    â€£ QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources").'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨æˆ‘ä»¬çš„æ¡†æ¶ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå°†æƒé‡è¡¨ç¤ºä¸ºé‡åŒ–æ•´æ•°å€¼ï¼Œä»¥ä¼˜åŒ–å†…å­˜åˆ©ç”¨ã€‚åœ¨å‰å‘ä¼ æ’­æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬åŠ¨æ€åœ°å°†è¿™äº›ä½ç²¾åº¦æƒé‡è§£é‡åŒ–ä¸ºæµ®ç‚¹æ ¼å¼ï¼Œä»è€Œå®ç°é«˜ç²¾åº¦çš„ç®—æœ¯è¿ç®—ã€‚ä¸ºæ›´æ¸…æ™°åœ°è¯´æ˜è¿™ä¸€å…³é”®è¿‡ç¨‹ï¼Œæˆ‘ä»¬åœ¨å›¾[1](#S3.F1
    "Figure 1 â€£ 3.2 Quantization â€£ 3 Methodology â€£ QFT: Quantized Full-parameter Tuning
    of LLMs with Affordable Resources")ä¸­è¿›è¡Œäº†å¯è§†åŒ–ã€‚'
- en: Quantized Backward Propagation
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: é‡åŒ–åå‘ä¼ æ’­
- en: In the backward propagation phase, the final task loss is propagated forward
    from the last layer in a sequential manner, and throughout this process, the gradient
    of each parameter is computed. Itâ€™s worth noting that these gradients need to
    be kept in memory, as they serve as essential information for guiding subsequent
    updates to the parameters. However, in mainstream deep learning frameworks like
    PyTorch, only parameters in the floating-point format can possess the gradient
    property, while those in the integer format cannot. Consequently, we cannot compute
    and store the gradients using the automatic differentiation functionality (i.e.,
    AutoGrad) in such cases.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åå‘ä¼ æ’­é˜¶æ®µï¼Œæœ€ç»ˆä»»åŠ¡æŸå¤±ä»æœ€åä¸€å±‚é¡ºåºåœ°å‘å‰ä¼ æ’­ï¼Œåœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­ï¼Œè®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›æ¢¯åº¦éœ€è¦ä¿å­˜åœ¨å†…å­˜ä¸­ï¼Œå› ä¸ºå®ƒä»¬æ˜¯æŒ‡å¯¼åç»­å‚æ•°æ›´æ–°çš„é‡è¦ä¿¡æ¯ã€‚ç„¶è€Œï¼Œåœ¨ä¸»æµæ·±åº¦å­¦ä¹ æ¡†æ¶å¦‚PyTorchä¸­ï¼Œåªæœ‰æµ®ç‚¹æ ¼å¼çš„å‚æ•°å¯ä»¥å…·æœ‰æ¢¯åº¦å±æ€§ï¼Œè€Œæ•´æ•°æ ¼å¼çš„å‚æ•°åˆ™ä¸è¡Œã€‚å› æ­¤ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¸èƒ½ä½¿ç”¨è‡ªåŠ¨å¾®åˆ†åŠŸèƒ½ï¼ˆå³AutoGradï¼‰æ¥è®¡ç®—å’Œå­˜å‚¨æ¢¯åº¦ã€‚
- en: Algorithm 1 Gradient Flow of Quantized Weights
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ç®—æ³•1 é‡åŒ–æƒé‡çš„æ¢¯åº¦æµ
- en: $T_{l}$endÂ for
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: $T_{l}$end for
- en: Algorithm 2 Quantized Lion Optimizer
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ç®—æ³•2 é‡åŒ–Lionä¼˜åŒ–å™¨
- en: $\beta_{1}$ store as INT8endÂ for
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: $\beta_{1}$ ä»¥INT8æ ¼å¼å­˜å‚¨ end for
- en: 'To this end, we design the gradient flow of integer weights, as presented in
    Algorithm [1](#alg1 "Algorithm 1 â€£ Quantized Backward Propagation â€£ 3.3 Overall
    Framework â€£ 3 Methodology â€£ QFT: Quantized Full-parameter Tuning of LLMs with
    Affordable Resources"). As in forward propagation, we begin by de-quantizing the
    weights into the floating-point format. Subsequently, leveraging the gradient
    of the output, we apply the chain rule to compute the gradients of both the input
    and the weights. Beyond the computational aspect, preserving the gradients of
    the weights presents its own set of formidable challenges. To address this, we
    introduce a gradient retention scheme centered around the maintenance of a global
    stack. In this scheme, the gradient of each layer is sequentially pushed to the
    stack, following the backward flow of information during the backward propagation.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¸ºæ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†æ•´æ•°æƒé‡çš„æ¢¯åº¦æµï¼Œå¦‚ç®—æ³• [1](#alg1 "Algorithm 1 â€£ Quantized Backward Propagation
    â€£ 3.3 Overall Framework â€£ 3 Methodology â€£ QFT: Quantized Full-parameter Tuning
    of LLMs with Affordable Resources") æ‰€ç¤ºã€‚åœ¨å‰å‘ä¼ æ’­ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå°†æƒé‡å»é‡åŒ–ä¸ºæµ®ç‚¹æ ¼å¼ã€‚éšåï¼Œåˆ©ç”¨è¾“å‡ºçš„æ¢¯åº¦ï¼Œæˆ‘ä»¬åº”ç”¨é“¾å¼æ³•åˆ™è®¡ç®—è¾“å…¥å’Œæƒé‡çš„æ¢¯åº¦ã€‚é™¤äº†è®¡ç®—æ–¹é¢ï¼Œä¿æŒæƒé‡çš„æ¢¯åº¦ä¹Ÿé¢ä¸´ç€ä¸€ç³»åˆ—ä¸¥å³»çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä»¥ç»´æŠ¤å…¨å±€æ ˆä¸ºæ ¸å¿ƒçš„æ¢¯åº¦ä¿ç•™æ–¹æ¡ˆã€‚åœ¨è¯¥æ–¹æ¡ˆä¸­ï¼Œæ¯å±‚çš„æ¢¯åº¦æŒ‰é¡ºåºæ¨å…¥æ ˆä¸­ï¼Œè·Ÿéšåå‘ä¼ æ’­è¿‡ç¨‹ä¸­çš„ä¿¡æ¯æµåŠ¨ã€‚'
- en: Quantized Parameter Update
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: é‡åŒ–å‚æ•°æ›´æ–°
- en: 'Ultimately, the parameter update are executed in accordance with the Lion optimizer
    procedures, with the notable difference that the gradients and momentum are stored
    in the integer format. The quantized optimizer step is outlined in Algorithm [2](#alg2
    "Algorithm 2 â€£ Quantized Backward Propagation â€£ 3.3 Overall Framework â€£ 3 Methodology
    â€£ QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources"). Initially,
    we pop the elements from the global stack to access and retrieve the gradients.
    It is essential to emphasize the exceptional computational efficiency of this
    popping process, as its computational complexity consistently remains at O(1),
    independent of the stack length. This efficiency arises from a distinct pattern:
    in the backward propagation phase, the gradients are sequentially pushed into
    the stack, beginning from the last layer. Conversely, in the optimizer step, the
    gradients are popped in a sequential manner, commencing from the first layer.
    This strategic arrangement ensures that the gradient of the current layer always
    occupies the last position in the stack, fully capitalizing on the first-in-last-out
    property inherent to stack data structures.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 'æœ€ç»ˆï¼Œå‚æ•°æ›´æ–°æŒ‰ç…§ Lion ä¼˜åŒ–å™¨çš„ç¨‹åºæ‰§è¡Œï¼Œæ˜¾è‘—çš„ä¸åŒä¹‹å¤„åœ¨äºæ¢¯åº¦å’ŒåŠ¨é‡ä»¥æ•´æ•°æ ¼å¼å­˜å‚¨ã€‚é‡åŒ–ä¼˜åŒ–å™¨æ­¥éª¤åœ¨ç®—æ³• [2](#alg2 "Algorithm
    2 â€£ Quantized Backward Propagation â€£ 3.3 Overall Framework â€£ 3 Methodology â€£ QFT:
    Quantized Full-parameter Tuning of LLMs with Affordable Resources") ä¸­è¿›è¡Œäº†æ¦‚è¿°ã€‚æœ€åˆï¼Œæˆ‘ä»¬ä»å…¨å±€æ ˆä¸­å¼¹å‡ºå…ƒç´ ä»¥è®¿é—®å’Œæ£€ç´¢æ¢¯åº¦ã€‚å¿…é¡»å¼ºè°ƒçš„æ˜¯ï¼Œè¿™ä¸€å¼¹å‡ºè¿‡ç¨‹çš„è®¡ç®—æ•ˆç‡éå¸¸é«˜ï¼Œå…¶è®¡ç®—å¤æ‚åº¦å§‹ç»ˆä¿æŒåœ¨
    O(1) çº§åˆ«ï¼Œä¸å—æ ˆé•¿åº¦å½±å“ã€‚è¿™ç§æ•ˆç‡æºäºä¸€ä¸ªç‹¬ç‰¹çš„æ¨¡å¼ï¼šåœ¨åå‘ä¼ æ’­é˜¶æ®µï¼Œæ¢¯åº¦æŒ‰é¡ºåºæ¨å…¥æ ˆä¸­ï¼Œä»æœ€åä¸€å±‚å¼€å§‹ã€‚ç›¸åï¼Œåœ¨ä¼˜åŒ–å™¨æ­¥éª¤ä¸­ï¼Œæ¢¯åº¦æŒ‰é¡ºåºä»ç¬¬ä¸€å±‚å¼€å§‹å¼¹å‡ºã€‚è¿™ç§ç­–ç•¥å®‰æ’ç¡®ä¿å½“å‰å±‚çš„æ¢¯åº¦æ€»æ˜¯å æ®æ ˆä¸­çš„æœ€åä½ç½®ï¼Œå……åˆ†åˆ©ç”¨äº†æ ˆæ•°æ®ç»“æ„çš„å…ˆè¿›åå‡ºç‰¹æ€§ã€‚'
- en: 4 Experiments
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 å®éªŒ
- en: 4.1 Experimental Setup
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 å®éªŒè®¾ç½®
- en: Models and Benchmarks
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ¨¡å‹å’ŒåŸºå‡†
- en: We conduct adequate evaluation of the proposed QFT by fine-tuning the advanced
    pre-trained model, LLaMA-2Â (Touvron etÂ al., [2023b](#bib.bib42)), including the
    7b and 13b versions. The few-shot performance of fine-tuned models is comprehensively
    evaluated on a variety of standard benchmarks, including ARCÂ (Clark etÂ al., [2018](#bib.bib5)),
    HellaSwagÂ (Zellers etÂ al., [2019](#bib.bib44)), MMLUÂ (Hendrycks etÂ al., [2020](#bib.bib14)),
    and TruthfulQAÂ (Lin etÂ al., [2021](#bib.bib32)). All results are obtained using
    the Language Model Evaluation Harness toolÂ (Gao etÂ al., [2021](#bib.bib12)). In
    addition, we also use MT-BenchÂ (Zheng etÂ al., [2023](#bib.bib47)) with GPT-4 scores
    to evaluate the conversational abilities of the models.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€šè¿‡å¾®è°ƒå…ˆè¿›çš„é¢„è®­ç»ƒæ¨¡å‹ LLaMA-2 (Touvron et al., [2023b](#bib.bib42)) å¯¹æ‰€æå‡ºçš„ QFT è¿›è¡Œå……åˆ†è¯„ä¼°ï¼ŒåŒ…æ‹¬
    7b å’Œ 13b ç‰ˆæœ¬ã€‚å¾®è°ƒæ¨¡å‹çš„å°‘æ ·æœ¬æ€§èƒ½åœ¨å¤šä¸ªæ ‡å‡†åŸºå‡†ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼ŒåŒ…æ‹¬ ARC (Clark et al., [2018](#bib.bib5))ã€HellaSwag
    (Zellers et al., [2019](#bib.bib44))ã€MMLU (Hendrycks et al., [2020](#bib.bib14))
    å’Œ TruthfulQA (Lin et al., [2021](#bib.bib32))ã€‚æ‰€æœ‰ç»“æœå‡ä½¿ç”¨ Language Model Evaluation
    Harness å·¥å…· (Gao et al., [2021](#bib.bib12)) è·å¾—ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨ MT-Bench (Zheng et al.,
    [2023](#bib.bib47)) å’Œ GPT-4 åˆ†æ•°æ¥è¯„ä¼°æ¨¡å‹çš„å¯¹è¯èƒ½åŠ›ã€‚
- en: Dataset Preparation
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ•°æ®é›†å‡†å¤‡
- en: In our experiment, we utilized a dataset comprising 94.1K shareGPT entriesÂ (HuggingFace,
    [2023b](#bib.bib18); shareGPT, [2023](#bib.bib39)), which encompass user interactions
    with chatGPT. We adopted the data cleaning procedures from FastchatÂ (Chiang etÂ al.,
    [2023](#bib.bib4)), converting HTML to markdown, eliminating non-English conversations,
    and segmenting extended dialogues into sequences capped at a length of 2048.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨äº†ä¸€ä¸ªåŒ…å«94.1K shareGPT æ¡ç›®çš„æ•°æ®é›†ï¼ˆHuggingFace, [2023b](#bib.bib18); shareGPT,
    [2023](#bib.bib39)ï¼‰ï¼Œå…¶ä¸­æ¶µç›–äº†ç”¨æˆ·ä¸ chatGPT çš„äº’åŠ¨ã€‚æˆ‘ä»¬é‡‡ç”¨äº†æ¥è‡ª Fastchat çš„æ•°æ®æ¸…ç†ç¨‹åºï¼ˆChiang et al.,
    [2023](#bib.bib4)ï¼‰ï¼Œå°† HTML è½¬æ¢ä¸º markdownï¼Œå‰”é™¤éè‹±è¯­å¯¹è¯ï¼Œå¹¶å°†è¾ƒé•¿çš„å¯¹è¯æ‹†åˆ†ä¸ºæœ€é•¿ 2048 çš„åºåˆ—ã€‚
- en: Baseline Methods
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: åŸºçº¿æ–¹æ³•
- en: We evaluate QFT in terms of both training memory and performance. For training
    memory, QFT is compared to floating-point AdamÂ (Kingma & Ba, [2015](#bib.bib22)),
    LionÂ (Chen etÂ al., [2023](#bib.bib3)), as well as bitsandbytes with quantized
    optimizer statesÂ (Dettmers etÂ al., [2021](#bib.bib6)). For the performance of
    instruction tuning, we take VicunaÂ (Chiang etÂ al., [2023](#bib.bib4)), which performs
    full-parameter fine-tuning in the floating-point format, as the baseline method.
    For a fair comparison, we reproduce its results using the same dataset as QFT.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»è®­ç»ƒå†…å­˜å’Œæ€§èƒ½ä¸¤ä¸ªæ–¹é¢è¯„ä¼° QFTã€‚å¯¹äºè®­ç»ƒå†…å­˜ï¼ŒQFT ä¸æµ®ç‚¹ Adamï¼ˆKingma & Ba, [2015](#bib.bib22)ï¼‰ã€Lionï¼ˆChen
    et al., [2023](#bib.bib3)ï¼‰ä»¥åŠå…·æœ‰é‡åŒ–ä¼˜åŒ–å™¨çŠ¶æ€çš„ bitsandbytesï¼ˆDettmers et al., [2021](#bib.bib6)ï¼‰è¿›è¡Œæ¯”è¾ƒã€‚å¯¹äºæŒ‡ä»¤è°ƒä¼˜çš„æ€§èƒ½ï¼Œæˆ‘ä»¬ä»¥
    Vicunaï¼ˆChiang et al., [2023](#bib.bib4)ï¼‰ä½œä¸ºåŸºçº¿æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨æµ®ç‚¹æ ¼å¼ä¸­è¿›è¡Œå…¨å‚æ•°å¾®è°ƒã€‚ä¸ºäº†å…¬å¹³æ¯”è¾ƒï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸ QFT
    ç›¸åŒçš„æ•°æ®é›†é‡ç°å…¶ç»“æœã€‚
- en: Training Details
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è®­ç»ƒç»†èŠ‚
- en: 'During training, we apply channel-wise quantization for all quantizers of model
    states. The threshold $T$ in the dense-and-sparse quantizer is obtained from 1%
    of the distribution range (please see Appendix [A.1](#A1.SS1 "A.1 Discussion on
    Outlier Thresholds of Weight Quantizer â€£ Appendix A Appendix â€£ QFT: Quantized
    Full-parameter Tuning of LLMs with Affordable Resources") for details). The training
    parameters are set to align with Vicunaâ€™s settings: the global batch size is 128,
    the learning rate is 2e-5, and the total number of epochs is 3.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¯¹æ¨¡å‹çŠ¶æ€çš„æ‰€æœ‰é‡åŒ–å™¨åº”ç”¨é€šé“çº§é‡åŒ–ã€‚å¯†é›†ä¸ç¨€ç–é‡åŒ–å™¨ä¸­çš„é˜ˆå€¼ $T$ å–è‡ªåˆ†å¸ƒèŒƒå›´çš„ 1%ï¼ˆæœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§é™„å½• [A.1](#A1.SS1
    "A.1 Discussion on Outlier Thresholds of Weight Quantizer â€£ Appendix A Appendix
    â€£ QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources")ï¼‰ã€‚è®­ç»ƒå‚æ•°è®¾ç½®ä¸
    Vicuna çš„è®¾ç½®å¯¹é½ï¼šå…¨å±€æ‰¹æ¬¡å¤§å°ä¸º 128ï¼Œå­¦ä¹ ç‡ä¸º 2e-5ï¼Œæ€»å…± 3 ä¸ª epochã€‚'
- en: 'Table 1: Memory usage (in GB) when fine-tuning the LLaMA-2-7b model using different
    methods. We report the full spectrum of memory profiles, as well as the total
    allocated memory and peak allocated memory. For model states, the Lion optimizer
    in floating-point format provides a 25% memory reduction, and further, our QFT
    introduces quantization that reduces the memory to 21% of the Adam optimizer,
    allowing for fine-tuning within 30GB of RAM.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¡¨ 1: ä½¿ç”¨ä¸åŒæ–¹æ³•å¾®è°ƒ LLaMA-2-7b æ¨¡å‹æ—¶çš„å†…å­˜ä½¿ç”¨æƒ…å†µï¼ˆå•ä½ï¼šGBï¼‰ã€‚æˆ‘ä»¬æŠ¥å‘Šäº†å†…å­˜é…ç½®çš„å®Œæ•´èŒƒå›´ï¼Œä»¥åŠæ€»åˆ†é…å†…å­˜å’Œå³°å€¼åˆ†é…å†…å­˜ã€‚å¯¹äºæ¨¡å‹çŠ¶æ€ï¼Œæµ®ç‚¹æ ¼å¼çš„
    Lion ä¼˜åŒ–å™¨æä¾›äº† 25% çš„å†…å­˜å‡å°‘ï¼Œå¹¶ä¸”æˆ‘ä»¬çš„ QFT å¼•å…¥çš„é‡åŒ–å°†å†…å­˜å‡å°‘åˆ° Adam ä¼˜åŒ–å™¨çš„ 21%ï¼Œä½¿å¾—åœ¨ 30GB RAM å†…å®Œæˆå¾®è°ƒæˆä¸ºå¯èƒ½ã€‚'
- en: '| Method | Weights | Gradients | Optimizer States | Activation | Total | Peak
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| æ–¹æ³• | æƒé‡ | æ¢¯åº¦ | ä¼˜åŒ–å™¨çŠ¶æ€ | æ¿€æ´» | æ€»è®¡ | å³°å€¼ |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Weight Copies | Momentum | Variances |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| æƒé‡å‰¯æœ¬ | åŠ¨é‡ | æ–¹å·® |'
- en: '| --- | --- | --- |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Adam | 25.1 | 25.1 | - | 25.1 | 25.1 | 3.75 | 104 | 129 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Adam | 25.1 | 25.1 | - | 25.1 | 25.1 | 3.75 | 104 | 129 |'
- en: '| Adam-mixed | 12.6 | 12.6 | 25.1 | 25.1 | 25.1 | 3.75 | 104 | 123 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Adam-mixed | 12.6 | 12.6 | 25.1 | 25.1 | 25.1 | 3.75 | 104 | 123 |'
- en: '| bitsandbytes | 12.6 | 12.6 | 25.1 | 6.31 | 6.31 | 3.75 | 66.6 | 86.6 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| bitsandbytes | 12.6 | 12.6 | 25.1 | 6.31 | 6.31 | 3.75 | 66.6 | 86.6 |'
- en: '| Lion | 25.1 | 25.1 | - | 25.1 | - | 3.75 | 79.1 | 101 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Lion | 25.1 | 25.1 | - | 25.1 | - | 3.75 | 79.1 | 101 |'
- en: '| QFT | 7.42 | 7.06 | - | 7.06 | - | 3.75 | 25.3 | 28.9 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| QFT | 7.42 | 7.06 | - | 7.06 | - | 3.75 | 25.3 | 28.9 |'
- en: 4.2 Memory Profile
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 å†…å­˜é…ç½®
- en: 'We start by discussing the memory usage using different methods, and the results
    of fine-tuning the LLaMA-2-7b model are reported in Table [1](#S4.T1 "Table 1
    â€£ Training Details â€£ 4.1 Experimental Setup â€£ 4 Experiments â€£ QFT: Quantized Full-parameter
    Tuning of LLMs with Affordable Resources"). In the training that employs the Adam
    optimizer with standard settings, it becomes evident that the memory consumption
    becomes significantly substantial. Specifically, the model weights, gradients,
    momentum, and variances each occupy a considerable 25.1GB of RAM, which is 4 times
    the model parameters, resulting in a horrible resource burden. Remarkably, this
    memory issue persists when employing the Adam optimizer with mixed precision settings.
    Despite the fact that the numerical precision of both weights and gradients experiences
    a 50% reduction during the forward and backward computations, the necessity to
    uphold full-precision weight copies within the optimizer states remains paramount.
    This stringent requirement is essential to guarantee the stability of parameter
    updates, as discussed in detail in Appendix [A.2](#A1.SS2 "A.2 Discussion on Training
    Stability â€£ Appendix A Appendix â€£ QFT: Quantized Full-parameter Tuning of LLMs
    with Affordable Resources"), and thus the goal of conserving memory remains unattainable.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 'æˆ‘ä»¬é¦–å…ˆè®¨è®ºäº†ä½¿ç”¨ä¸åŒæ–¹æ³•çš„å†…å­˜ä½¿ç”¨æƒ…å†µï¼Œå¹¶åœ¨è¡¨æ ¼[1](#S4.T1 "Table 1 â€£ Training Details â€£ 4.1 Experimental
    Setup â€£ 4 Experiments â€£ QFT: Quantized Full-parameter Tuning of LLMs with Affordable
    Resources")ä¸­æŠ¥å‘Šäº†å¾®è°ƒLLaMA-2-7bæ¨¡å‹çš„ç»“æœã€‚åœ¨ä½¿ç”¨æ ‡å‡†è®¾ç½®çš„Adamä¼˜åŒ–å™¨è¿›è¡Œè®­ç»ƒæ—¶ï¼Œå¾ˆæ˜æ˜¾ï¼Œå†…å­˜æ¶ˆè€—å˜å¾—éå¸¸åºå¤§ã€‚å…·ä½“æ¥è¯´ï¼Œæ¨¡å‹çš„æƒé‡ã€æ¢¯åº¦ã€åŠ¨é‡å’Œæ–¹å·®å„è‡ªå ç”¨å¤§é‡çš„25.1GB
    RAMï¼Œè¿™ç›¸å½“äºæ¨¡å‹å‚æ•°çš„4å€ï¼Œå¯¼è‡´äº†å¯æ€•çš„èµ„æºè´Ÿæ‹…ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå½“ä½¿ç”¨æ··åˆç²¾åº¦è®¾ç½®çš„Adamä¼˜åŒ–å™¨æ—¶ï¼Œè¿™ä¸€å†…å­˜é—®é¢˜ä¾ç„¶å­˜åœ¨ã€‚å°½ç®¡åœ¨å‰å‘å’Œåå‘è®¡ç®—è¿‡ç¨‹ä¸­æƒé‡å’Œæ¢¯åº¦çš„æ•°å€¼ç²¾åº¦å‡å°‘äº†50%ï¼Œä½†å¿…é¡»åœ¨ä¼˜åŒ–å™¨çŠ¶æ€ä¸­ç»´æŒå…¨ç²¾åº¦çš„æƒé‡å‰¯æœ¬ä»ç„¶è‡³å…³é‡è¦ã€‚æ­£å¦‚åœ¨é™„å½•[A.2](#A1.SS2
    "A.2 Discussion on Training Stability â€£ Appendix A Appendix â€£ QFT: Quantized Full-parameter
    Tuning of LLMs with Affordable Resources")ä¸­è¯¦ç»†è®¨è®ºçš„é‚£æ ·ï¼Œè¿™ä¸€ä¸¥æ ¼è¦æ±‚å¯¹ç¡®ä¿å‚æ•°æ›´æ–°çš„ç¨³å®šæ€§æ˜¯å¿…è¦çš„ï¼Œå› æ­¤å†…å­˜èŠ‚çœçš„ç›®æ ‡ä»ç„¶éš¾ä»¥å®ç°ã€‚'
- en: The Lion optimizer simplifies the optimizer states by only keeping track of
    the momentum, resulting in a noteworthy reduction in memory usage, 25% less than
    that of the Adam optimizer. Hence, it takes up 25% less memory than the Adam optimizer.
    Notably, the model states still retain the floating-point format, and this redundant
    representation offers additional opportunities for optimization. To this end,
    bitsandbytes employs quantization methods to convert the momentum and variances
    into the integer format, resulting in an impressive memory savings of 37 GB. Nevertheless,
    the retention of floating-point weights and gradients remains a hurdle, preventing
    complete memory conservation and continuing to strain the training resources.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Lionä¼˜åŒ–å™¨é€šè¿‡ä»…è·Ÿè¸ªåŠ¨é‡æ¥ç®€åŒ–ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œä»è€Œæ˜¾è‘—å‡å°‘äº†å†…å­˜ä½¿ç”¨ï¼Œå‡å°‘äº†25%ç›¸æ¯”äºAdamä¼˜åŒ–å™¨ã€‚å› æ­¤ï¼Œå®ƒæ¯”Adamä¼˜åŒ–å™¨èŠ‚çœäº†25%çš„å†…å­˜ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ¨¡å‹çŠ¶æ€ä»ç„¶ä¿ç•™æµ®ç‚¹æ ¼å¼ï¼Œè¿™ç§å†—ä½™è¡¨ç¤ºæä¾›äº†é¢å¤–çš„ä¼˜åŒ–æœºä¼šã€‚ä¸ºæ­¤ï¼Œbitsandbytesé‡‡ç”¨é‡åŒ–æ–¹æ³•å°†åŠ¨é‡å’Œæ–¹å·®è½¬æ¢ä¸ºæ•´æ•°æ ¼å¼ï¼Œå®ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„37GBå†…å­˜èŠ‚çœã€‚ç„¶è€Œï¼Œæµ®ç‚¹æƒé‡å’Œæ¢¯åº¦çš„ä¿ç•™ä»ç„¶æ˜¯ä¸€ä¸ªéšœç¢ï¼Œé˜»ç¢äº†å®Œå…¨çš„å†…å­˜èŠ‚çœï¼Œå¹¶ç»§ç»­å¯¹è®­ç»ƒèµ„æºé€ æˆå‹åŠ›ã€‚
- en: Our QFT, built on top of the Lion optimizer, employs a comprehensive quantization
    scheme encompassing all model states, including weights, gradients, and optimizer
    momentum. These parameters can be efficiently stored in the low-precision integer
    format. This allows the GPU to allocate only 21.5GB of RAM to store these parameters,
    marking a remarkable reduction to a mere 21% in comparison to the memory requirements
    of the Adam optimizer. During the practical training process, when taking into
    account factors such as activation, as well as several caches and memory fragments,
    the peak allocated memory remains comfortably below 30GB, allowing us to fine-tune
    within budget-friendly computing resources.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„QFTï¼ŒåŸºäºLionä¼˜åŒ–å™¨ï¼Œé‡‡ç”¨äº†ä¸€ç§å…¨é¢çš„é‡åŒ–æ–¹æ¡ˆï¼Œæ¶µç›–äº†æ‰€æœ‰æ¨¡å‹çŠ¶æ€ï¼ŒåŒ…æ‹¬æƒé‡ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨åŠ¨é‡ã€‚è¿™äº›å‚æ•°å¯ä»¥æœ‰æ•ˆåœ°ä»¥ä½ç²¾åº¦æ•´æ•°æ ¼å¼å­˜å‚¨ã€‚è¿™ä½¿å¾—GPUåªéœ€åˆ†é…21.5GBçš„RAMæ¥å­˜å‚¨è¿™äº›å‚æ•°ï¼Œç›¸æ¯”äºAdamä¼˜åŒ–å™¨çš„å†…å­˜éœ€æ±‚ï¼Œå‡å°‘äº†æ˜¾è‘—çš„21%ã€‚åœ¨å®é™…è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè€ƒè™‘åˆ°æ¿€æ´»ç­‰å› ç´ ä»¥åŠå¤šä¸ªç¼“å­˜å’Œå†…å­˜ç¢ç‰‡ï¼Œå³°å€¼åˆ†é…çš„å†…å­˜ä»ç„¶è¿œä½äº30GBï¼Œè¿™ä½¿å¾—æˆ‘ä»¬å¯ä»¥åœ¨é¢„ç®—å‹å¥½çš„è®¡ç®—èµ„æºå†…è¿›è¡Œå¾®è°ƒã€‚
- en: 4.3 Performance Evaluation
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 æ€§èƒ½è¯„ä¼°
- en: 'Table 2: Few-shot performance of different models on various standard benchmarks.
    Here, the number of shots is aligned to Open LLM LeaderboardÂ (HuggingFace, [2023a](#bib.bib17)).
    We take the pre-trained LLaMA-2 model as the baseline and compare the instruction
    tuning results of our QFT and Vicuna. Our QFT, with less resource consumption,
    encouragingly provides substantial improvement over pre-trained models and rivals
    the outcomes of full-precision tuning.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 2ï¼šä¸åŒæ¨¡å‹åœ¨å„ç§æ ‡å‡†åŸºå‡†ä¸Šçš„å°‘-shot æ€§èƒ½ã€‚åœ¨è¿™é‡Œï¼Œshot æ•°é‡ä¸ Open LLM Leaderboard (HuggingFace, [2023a](#bib.bib17))
    å¯¹é½ã€‚æˆ‘ä»¬ä»¥é¢„è®­ç»ƒçš„ LLaMA-2 æ¨¡å‹ä¸ºåŸºå‡†ï¼Œå¹¶æ¯”è¾ƒäº†æˆ‘ä»¬ QFT å’Œ Vicuna çš„æŒ‡ä»¤è°ƒä¼˜ç»“æœã€‚æˆ‘ä»¬çš„ QFT åœ¨èµ„æºæ¶ˆè€—è¾ƒå°‘çš„æƒ…å†µä¸‹ï¼Œä»¤äººé¼“èˆåœ°æä¾›äº†æ¯”é¢„è®­ç»ƒæ¨¡å‹æ›´æ˜¾è‘—çš„æ”¹è¿›ï¼Œå¹¶ä¸å…¨ç²¾åº¦è°ƒä¼˜çš„ç»“æœç›¸åª²ç¾ã€‚
- en: '| Model |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| æ¨¡å‹ |'
- en: '&#124; ARC-c &#124;'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ARC-c &#124;'
- en: '&#124; (25-shot) &#124;'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (25-shot) &#124;'
- en: '|'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; HellaSwag &#124;'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; HellaSwag &#124;'
- en: '&#124; (10-shot) &#124;'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (10-shot) &#124;'
- en: '|'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; MMLU &#124;'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MMLU &#124;'
- en: '&#124; (5-shot) &#124;'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (5-shot) &#124;'
- en: '|'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; TruthfulQA-mc &#124;'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TruthfulQA-mc &#124;'
- en: '&#124; (0-shot) &#124;'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (0-shot) &#124;'
- en: '| Average |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| å¹³å‡ |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| LLaMA-2-7B | 53.1 | 78.6 | 46.9 | 38.8 | 54.4 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B | 53.1 | 78.6 | 46.9 | 38.8 | 54.4 |'
- en: '| Vicuna-7B* | 53.6 | 77.3 | 49.4 | 51.5 | 58.0 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-7B* | 53.6 | 77.3 | 49.4 | 51.5 | 58.0 |'
- en: '| LLaMA-2-7B-QFT | 52.9 | 76.7 | 48.8 | 51.1 | 57.4 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B-QFT | 52.9 | 76.7 | 48.8 | 51.1 | 57.4 |'
- en: '| LLaMA-2-13B | 59.4 | 82.1 | 55.8 | 37.4 | 58.7 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-13B | 59.4 | 82.1 | 55.8 | 37.4 | 58.7 |'
- en: '| Vicuna-13B* | 57.0 | 81.2 | 55.8 | 50.9 | 61.2 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-13B* | 57.0 | 81.2 | 55.8 | 50.9 | 61.2 |'
- en: '| LLaMA-2-13B-QFT | 56.2 | 81.0 | 55.9 | 48.6 | 60.4 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-13B-QFT | 56.2 | 81.0 | 55.9 | 48.6 | 60.4 |'
- en: 'In this section, we conduct a comprehensive evaluation of the instruction fine-tuning
    performance in both conventional and advanced manners, which are in turn compared
    and analyzed in detail below. In addition, we also provide a qualitative analysis
    of the modelâ€™s language generation capabilities in Appendix [A.3](#A1.SS3 "A.3
    Qualitative Analysis of Conversational Abilities â€£ Appendix A Appendix â€£ QFT:
    Quantized Full-parameter Tuning of LLMs with Affordable Resources").'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å¯¹æŒ‡ä»¤å¾®è°ƒæ€§èƒ½è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼ŒåŒ…æ‹¬å¸¸è§„æ–¹æ³•å’Œé«˜çº§æ–¹æ³•ï¼Œå¹¶åœ¨ä¸‹é¢è¿›è¡Œäº†è¯¦ç»†çš„æ¯”è¾ƒå’Œåˆ†æã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åœ¨é™„å½•[A.3](#A1.SS3 "A.3
    äº¤äº’èƒ½åŠ›çš„å®šæ€§åˆ†æ â€£ é™„å½• A é™„å½• â€£ QFTï¼šä½¿ç”¨å¯æ‰¿å—èµ„æºçš„LLMsçš„é‡åŒ–å…¨å‚æ•°å¾®è°ƒ")ä¸­æä¾›äº†æ¨¡å‹è¯­è¨€ç”Ÿæˆèƒ½åŠ›çš„å®šæ€§åˆ†æã€‚
- en: Few-Shot Evaluation
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å°‘-Shot è¯„ä¼°
- en: 'We perform few-shot performance evaluations across a range of well-established
    benchmarks to assess the effectiveness of QFT. The obtained results, pertaining
    to various model configurations, are comprehensively presented in Table [2](#S4.T2
    "Table 2 â€£ 4.3 Performance Evaluation â€£ 4 Experiments â€£ QFT: Quantized Full-parameter
    Tuning of LLMs with Affordable Resources"). To maintain consistency, we opt to
    employ the same evaluation metrics as those employed in Open LLM LeaderboardÂ (HuggingFace,
    [2023a](#bib.bib17)) and ensure alignment with key experimental settings, such
    as the number of shots. As we can see, when fine-tuning a LLaMA-2-7B model, it
    becomes evident that QFT introduces a remarkable enhancement in performance. Specifically,
    QFT substantially elevates the average performance score, catapulting it from
    an initial value of 54.4 to a significantly improved 57.4\. Impressively, this
    achievement positions QFT within a mere 0.6 points of the Vicuna model, which
    has undergone full-precision tuning. Regarding specific individual metrics, such
    as 5-shot MMLU, we observe an improvement in results from 46.9 to 48.8, highlighting
    the modelâ€™s enhanced problem-solving capability.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨ä¸€ç³»åˆ—æˆç†Ÿçš„åŸºå‡†ä¸Šè¿›è¡Œå°‘-shot æ€§èƒ½è¯„ä¼°ï¼Œä»¥è¯„ä¼° QFT çš„æœ‰æ•ˆæ€§ã€‚è·å¾—çš„ç»“æœæ¶‰åŠå„ç§æ¨¡å‹é…ç½®ï¼Œå…¨é¢å‘ˆç°åœ¨è¡¨[2](#S4.T2 "è¡¨ 2 â€£
    4.3 æ€§èƒ½è¯„ä¼° â€£ 4 å®éªŒ â€£ QFTï¼šä½¿ç”¨å¯æ‰¿å—èµ„æºçš„LLMsçš„é‡åŒ–å…¨å‚æ•°å¾®è°ƒ")ä¸­ã€‚ä¸ºäº†ä¿æŒä¸€è‡´æ€§ï¼Œæˆ‘ä»¬é€‰æ‹©ä½¿ç”¨ä¸ Open LLM Leaderboard
    (HuggingFace, [2023a](#bib.bib17)) ç›¸åŒçš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶ç¡®ä¿ä¸å…³é”®å®éªŒè®¾ç½®å¯¹é½ï¼Œä¾‹å¦‚ shot æ•°é‡ã€‚æ­£å¦‚æˆ‘ä»¬æ‰€è§ï¼Œåœ¨å¯¹ LLaMA-2-7B
    æ¨¡å‹è¿›è¡Œå¾®è°ƒæ—¶ï¼Œå¾ˆæ˜æ˜¾ QFT æ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼ŒQFT å¤§å¹…æå‡äº†å¹³å‡æ€§èƒ½å¾—åˆ†ï¼Œå°†å…¶ä»åˆå§‹å€¼ 54.4 æå‡è‡³æ˜¾è‘—æ”¹è¿›çš„ 57.4ã€‚ä»¤äººå°è±¡æ·±åˆ»çš„æ˜¯ï¼Œè¿™ä¸€æˆå°±ä½¿
    QFT åœ¨ä»…ç›¸å·® 0.6 åˆ†çš„æƒ…å†µä¸‹ï¼Œä¸ç»è¿‡å…¨ç²¾åº¦è°ƒä¼˜çš„ Vicuna æ¨¡å‹ç›¸ç«äº‰ã€‚å…³äºå…·ä½“çš„ä¸ªåˆ«æŒ‡æ ‡ï¼Œä¾‹å¦‚ 5-shot MMLUï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ç»“æœä» 46.9
    æå‡è‡³ 48.8ï¼Œçªæ˜¾äº†æ¨¡å‹è§£å†³é—®é¢˜èƒ½åŠ›çš„å¢å¼ºã€‚
- en: Furthermore, it is imperative to provide a clarification regarding the observed
    slight decline in the 10-shot HellaSwag results across both fine-tuning settings.
    This diminution can be attributed, in part, to the influence exerted by the fine-tuning
    dataset and, in part, to the inherent limitations of a single benchmark evaluation,
    which may introduce a certain degree of one-sidedness or even inaccuracies into
    the assessment processÂ (Liao etÂ al., [2021](#bib.bib30)). Consequently, it becomes
    increasingly evident that the central focus should shift to a careful comparison
    between the performance of Vicuna and QFT rather than dwelling extensively on
    the improvement of the pre-trained model itself, and it is indeed reassuring to
    note that QFT consistently demonstrates the ability to achieve results comparable
    to those achieved by the Vicuna model.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæœ‰å¿…è¦æ¾„æ¸…åœ¨ä¸¤ä¸ªå¾®è°ƒè®¾ç½®ä¸­è§‚å¯Ÿåˆ°çš„10-shot HellaSwagç»“æœçš„è½»å¾®ä¸‹é™ã€‚è¿™ç§å‡å°‘éƒ¨åˆ†å½’å› äºå¾®è°ƒæ•°æ®é›†çš„å½±å“ï¼Œéƒ¨åˆ†å½’å› äºå•ä¸€åŸºå‡†è¯„ä¼°çš„å›ºæœ‰å±€é™æ€§ï¼Œè¿™å¯èƒ½ä¼šå¯¹è¯„ä¼°è¿‡ç¨‹å¼•å…¥ä¸€å®šç¨‹åº¦çš„ç‰‡é¢æ€§æˆ–ç”šè‡³ä¸å‡†ç¡®æ€§ï¼ˆLiao
    et al., [2021](#bib.bib30)ï¼‰ã€‚å› æ­¤ï¼Œè¶Šæ¥è¶Šæ˜æ˜¾çš„æ˜¯ï¼Œæ ¸å¿ƒå…³æ³¨ç‚¹åº”è½¬å‘Vicunaä¸QFTæ€§èƒ½ä¹‹é—´çš„ä»”ç»†æ¯”è¾ƒï¼Œè€Œä¸æ˜¯è¿‡å¤šåœ°å…³æ³¨é¢„è®­ç»ƒæ¨¡å‹æœ¬èº«çš„æ”¹è¿›ã€‚å€¼å¾—æ¬£æ…°çš„æ˜¯ï¼ŒQFTå§‹ç»ˆèƒ½å¤Ÿå–å¾—ä¸Vicunaæ¨¡å‹ç›¸å½“çš„ç»“æœã€‚
- en: MT-Bench Score
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: MT-Bench åˆ†æ•°
- en: 'Table 3: MT-Bench scores using GPT-4 of different models. They can reflect
    the conversational abilities of these models. Our QFT significantly outperforms
    the pre-trained LLaMA-2 model, and achieves comparable results to the Vicuna model
    tuned in full precision.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨3ï¼šä¸åŒæ¨¡å‹çš„MT-Benchåˆ†æ•°ï¼Œä½¿ç”¨GPT-4è¯„ä¼°ã€‚è¿™äº›åˆ†æ•°å¯ä»¥åæ˜ è¿™äº›æ¨¡å‹çš„å¯¹è¯èƒ½åŠ›ã€‚æˆ‘ä»¬çš„QFTæ˜¾è‘—ä¼˜äºé¢„è®­ç»ƒçš„LLaMA-2æ¨¡å‹ï¼Œå¹¶å–å¾—äº†ä¸å®Œå…¨ç²¾åº¦è°ƒæ•´çš„Vicunaæ¨¡å‹ç›¸å½“çš„ç»“æœã€‚
- en: '| Model | MT-Bench Score (GPT-4) |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Model | MT-Bench Score (GPT-4) |'
- en: '| --- | --- |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| GPT-3.5 | 7.94 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 | 7.94 |'
- en: '| LLaMA-2-7B | 3.83 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B | 3.83 |'
- en: '| Vicuna-7B* | 6.08 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-7B* | 6.08 |'
- en: '| LLaMA-2-7B-QFT | 5.95 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B-QFT | 5.95 |'
- en: '| LLaMA-2-13B | 4.69 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-13B | 4.69 |'
- en: '| Vicuna-13B* | 6.46 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-13B* | 6.46 |'
- en: '| LLaMA-2-13B-QFT | 6.27 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-13B-QFT | 6.27 |'
- en: 'Besides the conventional benchmarks described above, there is a more advanced
    benchmark, MT-Bench, to evaluate the conversational abilities of LLMs. MT-bench
    consists of a series of challenging multi-round open-ended questions that match
    the characteristics and preferences of human conversations, and uses GPT-4 as
    a judge to automatically score the responses. The score results are reported in
    Table [3](#S4.T3 "Table 3 â€£ MT-Bench Score â€£ 4.3 Performance Evaluation â€£ 4 Experiments
    â€£ QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources"). As
    an illustrative example, we provide a detailed discussion of the 7B models. Initially,
    the LLaMA-2 model, in its pre-trained state, yields a rather modest score of 3.83,
    indicating a considerable limitation in its problem-solving ability. For the Vicuna
    model tuned in full precision, the score undergoes a substantial augmentation,
    surging to an impressive 6.08\. Notably, the QFT also mirrors this impressive
    improvement, boosting model performance to levels comparable to the Vicuna model.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 'é™¤äº†ä¸Šè¿°å¸¸è§„åŸºå‡†å¤–ï¼Œè¿˜æœ‰ä¸€ä¸ªæ›´å…ˆè¿›çš„åŸºå‡†â€”â€”MT-Benchï¼Œç”¨äºè¯„ä¼°LLMsçš„å¯¹è¯èƒ½åŠ›ã€‚MT-benchåŒ…å«ä¸€ç³»åˆ—å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šè½®å¼€æ”¾æ€§é—®é¢˜ï¼Œç¬¦åˆäººç±»å¯¹è¯çš„ç‰¹ç‚¹å’Œåå¥½ï¼Œå¹¶ä½¿ç”¨GPT-4ä½œä¸ºè£åˆ¤è‡ªåŠ¨è¯„åˆ†ã€‚åˆ†æ•°ç»“æœè§è¡¨æ ¼[3](#S4.T3
    "Table 3 â€£ MT-Bench Score â€£ 4.3 Performance Evaluation â€£ 4 Experiments â€£ QFT:
    Quantized Full-parameter Tuning of LLMs with Affordable Resources")ã€‚ä½œä¸ºä¸€ä¸ªè¯´æ˜æ€§ç¤ºä¾‹ï¼Œæˆ‘ä»¬è¯¦ç»†è®¨è®ºäº†7Bæ¨¡å‹ã€‚æœ€åˆï¼ŒLLaMA-2æ¨¡å‹åœ¨å…¶é¢„è®­ç»ƒçŠ¶æ€ä¸‹ï¼Œå¾—åˆ†ä¸º3.83ï¼Œæ˜¾ç¤ºå‡ºå…¶é—®é¢˜è§£å†³èƒ½åŠ›çš„ç›¸å½“å±€é™æ€§ã€‚å¯¹äºå®Œå…¨ç²¾åº¦è°ƒæ•´çš„Vicunaæ¨¡å‹ï¼Œå¾—åˆ†æ˜¾è‘—æé«˜ï¼Œè¾¾åˆ°ä»¤äººå°è±¡æ·±åˆ»çš„6.08ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒQFTä¹Ÿåæ˜ äº†è¿™ç§æ˜¾è‘—çš„æ”¹è¿›ï¼Œå°†æ¨¡å‹æ€§èƒ½æå‡åˆ°ä¸Vicunaæ¨¡å‹ç›¸å½“çš„æ°´å¹³ã€‚'
- en: 'To facilitate a more visual comparison, we provide radar charts that encompass
    eight capacity indicators, as illustrated in Figure [3](#S4.F3 "Figure 3 â€£ MT-Bench
    Score â€£ 4.3 Performance Evaluation â€£ 4 Experiments â€£ QFT: Quantized Full-parameter
    Tuning of LLMs with Affordable Resources"). These radar charts clearly shows that
    QFT provides a comprehensive and transformative improvement across all measured
    metrics compared to the baseline performance of the pre-trained LLaMA-2 model.
    In comparison to the Vicuna model tuned in full precision, QFT achieves comparable
    results and even outperforms it in certain aspects, e.g., in the 7B model setting,
    QFT exhibits superior performance in the Math metrics.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä¾¿äºæ›´ç›´è§‚çš„æ¯”è¾ƒï¼Œæˆ‘ä»¬æä¾›äº†æ¶µç›–å…«ä¸ªèƒ½åŠ›æŒ‡æ ‡çš„é›·è¾¾å›¾ï¼Œå¦‚å›¾ [3](#S4.F3 "å›¾ 3 â€£ MT-Bench è¯„åˆ† â€£ 4.3 æ€§èƒ½è¯„ä¼° â€£ 4
    å®éªŒ â€£ QFTï¼šå…·æœ‰ç»æµèµ„æºçš„ LLM é‡åŒ–å…¨å‚æ•°è°ƒæ•´") æ‰€ç¤ºã€‚è¿™äº›é›·è¾¾å›¾æ¸…æ™°åœ°è¡¨æ˜ï¼Œä¸é¢„è®­ç»ƒçš„ LLaMA-2 æ¨¡å‹çš„åŸºçº¿æ€§èƒ½ç›¸æ¯”ï¼ŒQFT åœ¨æ‰€æœ‰æµ‹é‡æŒ‡æ ‡ä¸Šéƒ½æä¾›äº†å…¨é¢è€Œæ˜¾è‘—çš„æ”¹è¿›ã€‚ä¸å®Œå…¨ç²¾åº¦è°ƒæ•´çš„
    Vicuna æ¨¡å‹ç›¸æ¯”ï¼ŒQFT è¾¾åˆ°äº†ç›¸ä¼¼çš„ç»“æœï¼Œç”šè‡³åœ¨æŸäº›æ–¹é¢è¶…è¿‡äº†å®ƒï¼Œä¾‹å¦‚åœ¨ 7B æ¨¡å‹è®¾ç½®ä¸­ï¼ŒQFT åœ¨æ•°å­¦æŒ‡æ ‡ä¸Šè¡¨ç°æ›´ä¼˜ã€‚
- en: '![Refer to caption](img/f439aa85455274e9085a8240d0b1578b.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/f439aa85455274e9085a8240d0b1578b.png)'
- en: (a) 7B models
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 7B æ¨¡å‹
- en: '![Refer to caption](img/028d42125d7f7ce16a00032c749ce6aa.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/028d42125d7f7ce16a00032c749ce6aa.png)'
- en: (b) 13B models
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 13B æ¨¡å‹
- en: 'Figure 3: Radar charts of each capability in MT-Bench of different models.
    Compared to the pre-trained LLaMA-2 model, our QFT yields across-the-board improvements
    in all metrics. Compared to the Vicuna model tuned in full precision, our QFT
    achieves similar results and even surpasses it in some abilities, such as the
    Math metrics in the 7B model setting.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3ï¼šä¸åŒæ¨¡å‹åœ¨ MT-Bench ä¸­çš„å„é¡¹èƒ½åŠ›é›·è¾¾å›¾ã€‚ä¸é¢„è®­ç»ƒçš„ LLaMA-2 æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ QFT åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šå‡å®ç°äº†å…¨é¢çš„æ”¹è¿›ã€‚ä¸å®Œå…¨ç²¾åº¦è°ƒæ•´çš„
    Vicuna æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ QFT å®ç°äº†ç±»ä¼¼çš„ç»“æœï¼Œç”šè‡³åœ¨æŸäº›èƒ½åŠ›ä¸Šè¶…è¶Šäº†å®ƒï¼Œä¾‹å¦‚åœ¨ 7B æ¨¡å‹è®¾ç½®ä¸­çš„æ•°å­¦æŒ‡æ ‡ã€‚
- en: 5 Conclusions and Broader Impacts
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 ç»“è®ºä¸æ›´å¹¿æ³›çš„å½±å“
- en: In this paper, we propose a Quantized Full-parameter Tuning (QFT) framework
    for LLMs, which leverages quantization techniques to comprehensively optimize
    training memory to enable fine-tuning on affordable resources. We employ the memory-efficient
    Lion optimizer, which provides significant advantages for robust quantized fine-tuning.
    Upon this, we develop customized quantizers to store all model states in the integer
    format, significantly reducing the memory usage. QFT incorporates these two innovations
    and designs a novel gradient flow scheme to accommodate them. We perform instruction
    tuning on the pre-trained LLaMA-2 models to verify the effectiveness of QFT, and
    the results demonstrate that QFT can reduce memory usage to 21% while achieving
    comparable performance to standard floating-point training.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äº LLM çš„é‡åŒ–å…¨å‚æ•°è°ƒæ•´ (QFT) æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨é‡åŒ–æŠ€æœ¯å…¨é¢ä¼˜åŒ–è®­ç»ƒå†…å­˜ï¼Œä»¥ä¾¿åœ¨ç»æµèµ„æºä¸‹è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬é‡‡ç”¨äº†å†…å­˜é«˜æ•ˆçš„
    Lion ä¼˜åŒ–å™¨ï¼Œè¯¥ä¼˜åŒ–å™¨ä¸ºç¨³å¥çš„é‡åŒ–å¾®è°ƒæä¾›äº†æ˜¾è‘—ä¼˜åŠ¿ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼€å‘äº†å®šåˆ¶çš„é‡åŒ–å™¨ï¼Œå°†æ‰€æœ‰æ¨¡å‹çŠ¶æ€ä»¥æ•´æ•°æ ¼å¼å­˜å‚¨ï¼Œä»è€Œæ˜¾è‘—å‡å°‘å†…å­˜ä½¿ç”¨ã€‚QFT èåˆäº†è¿™ä¸¤é¡¹åˆ›æ–°ï¼Œå¹¶è®¾è®¡äº†æ–°é¢–çš„æ¢¯åº¦æµæ–¹æ¡ˆä»¥é€‚åº”å®ƒä»¬ã€‚æˆ‘ä»¬å¯¹é¢„è®­ç»ƒçš„
    LLaMA-2 æ¨¡å‹è¿›è¡Œäº†æŒ‡ä»¤å¾®è°ƒï¼Œä»¥éªŒè¯ QFT çš„æœ‰æ•ˆæ€§ï¼Œç»“æœè¡¨æ˜ QFT èƒ½å°†å†…å­˜ä½¿ç”¨å‡å°‘åˆ° 21%ï¼ŒåŒæ—¶è¾¾åˆ°ä¸æ ‡å‡†æµ®ç‚¹è®­ç»ƒç›¸å½“çš„æ€§èƒ½ã€‚
- en: 'QFT can be easily integrated into mainstream LLM training tools and offers
    great compatibility with other memory optimization methods, demonstrating remarkable
    adaptability and utility in real-world applications. Additionally, it has the
    potential to produce broader impacts:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: QFT å¯ä»¥è½»æ¾é›†æˆåˆ°ä¸»æµ LLM è®­ç»ƒå·¥å…·ä¸­ï¼Œå¹¶ä¸å…¶ä»–å†…å­˜ä¼˜åŒ–æ–¹æ³•å…·æœ‰å¾ˆå¥½çš„å…¼å®¹æ€§ï¼Œå±•ç¤ºäº†åœ¨å®é™…åº”ç”¨ä¸­çš„æ˜¾è‘—é€‚åº”æ€§å’Œå®ç”¨æ€§ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å…·æœ‰äº§ç”Ÿæ›´å¹¿æ³›å½±å“çš„æ½œåŠ›ï¼š
- en: â€¢
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Quantized Training from Scratch: The parameters to be updated and optimizer
    configurations in the full-parameter tuning are consistent with the pre-training
    process, thus QFT can be migrated to be applied to training-from-scratch cases.'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»å¤´å¼€å§‹çš„é‡åŒ–è®­ç»ƒï¼šå…¨å‚æ•°è°ƒæ•´ä¸­çš„å‚æ•°æ›´æ–°å’Œä¼˜åŒ–å™¨é…ç½®ä¸é¢„è®­ç»ƒè¿‡ç¨‹ä¸€è‡´ï¼Œå› æ­¤ QFT å¯ä»¥è¿ç§»åº”ç”¨äºä»å¤´å¼€å§‹è®­ç»ƒçš„æ¡ˆä¾‹ã€‚
- en: â€¢
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Lower-Precision Optimizer Momentum: Recent research has explored the compression
    of optimizer states to 4-bitsÂ (Li etÂ al., [2023](#bib.bib25)). It holds promise
    to explore the combination of QFT with this approach for even more substantial
    memory reduction.'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä½ç²¾åº¦ä¼˜åŒ–å™¨åŠ¨é‡ï¼šè¿‘æœŸç ”ç©¶æ¢ç´¢äº†å°†ä¼˜åŒ–å™¨çŠ¶æ€å‹ç¼©åˆ° 4 ä½çš„æŠ€æœ¯ (Li et al., [2023](#bib.bib25))ã€‚æœ‰å‰æ™¯çš„æ¢ç´¢åŒ…æ‹¬å°† QFT
    ä¸è¿™ç§æ–¹æ³•ç»“åˆï¼Œä»¥å®ç°æ›´æ˜¾è‘—çš„å†…å­˜å‡å°‘ã€‚
- en: References
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: Brown etÂ al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, JaredÂ D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, etÂ al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877â€“1901, 2020.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown ç­‰ï¼ˆ2020ï¼‰Tom Brownã€Benjamin Mannã€Nick Ryderã€Melanie Subbiahã€Jared D Kaplanã€Prafulla
    Dhariwalã€Arvind Neelakantanã€Pranav Shyamã€Girish Sastryã€Amanda Askell ç­‰ã€‚è¯­è¨€æ¨¡å‹æ˜¯å°‘æ ·æœ¬å­¦ä¹ è€…ã€‚*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•*ï¼Œ33:1877â€“1901ï¼Œ2020å¹´ã€‚
- en: Chen etÂ al. (2016) Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.
    Training deep nets with sublinear memory cost. *arXiv preprint arXiv:1604.06174*,
    2016.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen ç­‰ï¼ˆ2016ï¼‰Tianqi Chenã€Bing Xuã€Chiyuan Zhang å’Œ Carlos Guestrinã€‚ä»¥äºšçº¿æ€§å†…å­˜æˆæœ¬è®­ç»ƒæ·±åº¦ç½‘ç»œã€‚*arXiv
    é¢„å°æœ¬ arXiv:1604.06174*ï¼Œ2016å¹´ã€‚
- en: Chen etÂ al. (2023) XÂ Chen, CÂ Liang, DÂ Huang, EÂ Real, KÂ Wang, YÂ Liu, HÂ Pham,
    XÂ Dong, TÂ Luong, CJÂ Hsieh, etÂ al. Symbolic discovery of optimization algorithms.
    arxiv 2023. *arXiv preprint arXiv:2302.06675*, 2023.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen ç­‰ï¼ˆ2023ï¼‰X Chenã€C Liangã€D Huangã€E Realã€K Wangã€Y Liuã€H Phamã€X Dongã€T Luongã€CJ
    Hsieh ç­‰ã€‚ä¼˜åŒ–ç®—æ³•çš„ç¬¦å·å‘ç°ã€‚*arXiv é¢„å°æœ¬ arXiv:2302.06675*ï¼Œ2023å¹´ã€‚
- en: 'Chiang etÂ al. (2023) Wei-Lin Chiang, Zhuohan Li, ZiÂ Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, JosephÂ E. Gonzalez,
    Ion Stoica, and EricÂ P. Xing. Vicuna: An open-source chatbot impressing gpt-4
    with 90%* chatgpt quality, March 2023. URL [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/).'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chiang ç­‰ï¼ˆ2023ï¼‰Wei-Lin Chiangã€Zhuohan Liã€Zi Linã€Ying Shengã€Zhanghao Wuã€Hao Zhangã€Lianmin
    Zhengã€Siyuan Zhuangã€Yonghao Zhuangã€Joseph E. Gonzalezã€Ion Stoica å’Œ Eric P. Xingã€‚Vicunaï¼šä¸€ä¸ªå¼€æºèŠå¤©æœºå™¨äººï¼Œä»¥
    90%* ChatGPT è´¨é‡ä»¤ GPT-4 å°è±¡æ·±åˆ»ï¼Œ2023å¹´3æœˆã€‚ç½‘å€ [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/)ã€‚
- en: Clark etÂ al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question
    answering? try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*,
    2018.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark ç­‰ï¼ˆ2018ï¼‰Peter Clarkã€Isaac Cowheyã€Oren Etzioniã€Tushar Khotã€Ashish Sabharwalã€Carissa
    Schoenick å’Œ Oyvind Tafjordã€‚è®¤ä¸ºä½ å·²ç»è§£å†³äº†é—®ç­”é—®é¢˜ï¼Ÿè¯•è¯• arcï¼ŒAI2 æ¨ç†æŒ‘æˆ˜ã€‚*arXiv é¢„å°æœ¬ arXiv:1803.05457*ï¼Œ2018å¹´ã€‚
- en: Dettmers etÂ al. (2021) Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer.
    8-bit optimizers via block-wise quantization. In *International Conference on
    Learning Representations*, 2021.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers ç­‰ï¼ˆ2021ï¼‰Tim Dettmersã€Mike Lewisã€Sam Shleifer å’Œ Luke Zettlemoyerã€‚8-bit
    ä¼˜åŒ–å™¨é€šè¿‡å—çº§é‡åŒ–ã€‚åœ¨ *å›½é™…å­¦ä¹ è¡¨ç¤ºå¤§ä¼š*ï¼Œ2021å¹´ã€‚
- en: 'Dettmers etÂ al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms. *arXiv preprint arXiv:2305.14314*,
    2023.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers ç­‰ï¼ˆ2023ï¼‰Tim Dettmersã€Artidoro Pagnoniã€Ari Holtzman å’Œ Luke Zettlemoyerã€‚Qloraï¼šé«˜æ•ˆå¾®è°ƒé‡åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚*arXiv
    é¢„å°æœ¬ arXiv:2305.14314*ï¼Œ2023å¹´ã€‚
- en: 'Ding etÂ al. (2022) Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang,
    Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, etÂ al. Delta tuning:
    A comprehensive study of parameter efficient methods for pre-trained language
    models. *arXiv preprint arXiv:2203.06904*, 2022.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding ç­‰ï¼ˆ2022ï¼‰Ning Dingã€Yujia Qinã€Guang Yangã€Fuchao Weiã€Zonghan Yangã€Yusheng Suã€Shengding
    Huã€Yulin Chenã€Chi-Min Chanã€Weize Chen ç­‰ã€‚Delta è°ƒä¼˜ï¼šé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å‚æ•°é«˜æ•ˆæ–¹æ³•çš„å…¨é¢ç ”ç©¶ã€‚*arXiv é¢„å°æœ¬
    arXiv:2203.06904*ï¼Œ2022å¹´ã€‚
- en: 'Dong etÂ al. (2019) Zhen Dong, Zhewei Yao, Amir Gholami, MichaelÂ W Mahoney,
    and Kurt Keutzer. Hawq: Hessian aware quantization of neural networks with mixed-precision.
    In *Proceedings of the IEEE/CVF International Conference on Computer Vision*,
    pp.Â  293â€“302, 2019.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong ç­‰ï¼ˆ2019ï¼‰Zhen Dongã€Zhewei Yaoã€Amir Gholamiã€Michael W Mahoney å’Œ Kurt Keutzerã€‚Hawqï¼šå…³æ³¨æµ·æ£®çŸ©é˜µçš„æ··åˆç²¾åº¦ç¥ç»ç½‘ç»œé‡åŒ–ã€‚åœ¨
    *IEEE/CVF å›½é™…è®¡ç®—æœºè§†è§‰å¤§ä¼šè®ºæ–‡é›†*ï¼Œç¬¬ 293â€“302 é¡µï¼Œ2019å¹´ã€‚
- en: 'Dong etÂ al. (2020) Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, MichaelÂ W
    Mahoney, and Kurt Keutzer. Hawq-v2: Hessian aware trace-weighted quantization
    of neural networks. *Advances in neural information processing systems*, 33:18518â€“18529,
    2020.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong ç­‰ï¼ˆ2020ï¼‰Zhen Dongã€Zhewei Yaoã€Daiyaan Arfeenã€Amir Gholamiã€Michael W Mahoney
    å’Œ Kurt Keutzerã€‚Hawq-v2ï¼šå…³æ³¨æµ·æ£®çŸ©é˜µçš„ç¥ç»ç½‘ç»œè¿¹æƒé‡åŒ–ã€‚*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•*ï¼Œ33:18518â€“18529ï¼Œ2020å¹´ã€‚
- en: 'Frantar etÂ al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*, 2022.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar ç­‰ï¼ˆ2022ï¼‰Elias Frantarã€Saleh Ashkboosã€Torsten Hoefler å’Œ Dan Alistarhã€‚Gptqï¼šç”Ÿæˆé¢„è®­ç»ƒå˜æ¢å™¨çš„å‡†ç¡®åè®­ç»ƒé‡åŒ–ã€‚*arXiv
    é¢„å°æœ¬ arXiv:2210.17323*ï¼Œ2022å¹´ã€‚
- en: Gao etÂ al. (2021) Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and
    Andy Zou. A framework for few-shot language model evaluation, September 2021.
    URL [https://doi.org/10.5281/zenodo.5371628](https://doi.org/10.5281/zenodo.5371628).
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gaoç­‰äººï¼ˆ2021ï¼‰Leo Gaoã€Jonathan Towã€Stella Bidermanã€Sid Blackã€Anthony DiPofiã€Charles
    Fosterã€Laurence Goldingã€Jeffrey Hsuã€Kyle McDonellã€Niklas Muennighoffã€Jason Phangã€Laria
    Reynoldsã€Eric Tangã€Anish Thiteã€Ben Wangã€Kevin Wangå’ŒAndy Zouã€‚å°‘æ ·æœ¬è¯­è¨€æ¨¡å‹è¯„ä¼°æ¡†æ¶ï¼Œ2021å¹´9æœˆã€‚ç½‘å€[https://doi.org/10.5281/zenodo.5371628](https://doi.org/10.5281/zenodo.5371628)ã€‚
- en: Gholami etÂ al. (2022) Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, MichaelÂ W
    Mahoney, and Kurt Keutzer. A survey of quantization methods for efficient neural
    network inference. In *Low-Power Computer Vision*, pp.Â  291â€“326\. Chapman and
    Hall/CRC, 2022.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gholamiç­‰äººï¼ˆ2022ï¼‰Amir Gholamiã€Sehoon Kimã€Zhen Dongã€Zhewei Yaoã€Michael W Mahoneyå’ŒKurt
    Keutzerã€‚é«˜æ•ˆç¥ç»ç½‘ç»œæ¨ç†çš„é‡åŒ–æ–¹æ³•ç»¼è¿°ã€‚åœ¨*ã€Šä½åŠŸè€—è®¡ç®—æœºè§†è§‰ã€‹*ä¸­ï¼Œpp. 291â€“326ã€‚Chapman and Hall/CRCï¼Œ2022å¹´ã€‚
- en: Hendrycks etÂ al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language
    understanding. *arXiv preprint arXiv:2009.03300*, 2020.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycksç­‰äººï¼ˆ2020ï¼‰Dan Hendrycksã€Collin Burnsã€Steven Basartã€Andy Zouã€Mantas Mazeikaã€Dawn
    Songå’ŒJacob Steinhardtã€‚æµ‹é‡å¤§è§„æ¨¡å¤šä»»åŠ¡è¯­è¨€ç†è§£ã€‚*arXivé¢„å°æœ¬arXiv:2009.03300*ï¼Œ2020å¹´ã€‚
- en: 'Hu etÂ al. (2021) EdwardÂ J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, LuÂ Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models. *arXiv preprint arXiv:2106.09685*, 2021.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huç­‰äººï¼ˆ2021ï¼‰Edward J Huã€Yelong Shenã€Phillip Wallisã€Zeyuan Allen-Zhuã€Yuanzhi Liã€Shean
    Wangã€Lu Wangå’ŒWeizhu Chenã€‚Lora: å¤§å‹è¯­è¨€æ¨¡å‹çš„ä½ç§©é€‚åº”ã€‚*arXivé¢„å°æœ¬arXiv:2106.09685*ï¼Œ2021å¹´ã€‚'
- en: 'Huang etÂ al. (2020) Chien-Chin Huang, GuÂ Jin, and Jinyang Li. Swapadvisor:
    Pushing deep learning beyond the gpu memory limit via smart swapping. In *Proceedings
    of the Twenty-Fifth International Conference on Architectural Support for Programming
    Languages and Operating Systems*, pp.Â 1341â€“1355, 2020.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huangç­‰äººï¼ˆ2020ï¼‰Chien-Chin Huangã€Gu Jinå’ŒJinyang Liã€‚Swapadvisor: é€šè¿‡æ™ºèƒ½äº¤æ¢å°†æ·±åº¦å­¦ä¹ æ¨å‘è¶…è¶ŠGPUå†…å­˜é™åˆ¶çš„æé™ã€‚åœ¨*ã€Šç¬¬äºŒåäº”å±Šå›½é™…ç¨‹åºè¯­è¨€å’Œæ“ä½œç³»ç»Ÿä½“ç³»ç»“æ„æ”¯æŒä¼šè®®è®ºæ–‡é›†ã€‹*ä¸­ï¼Œpp.
    1341â€“1355ï¼Œ2020å¹´ã€‚'
- en: HuggingFace (2023a) HuggingFace. Open llm leaderboard, 2023a. URL [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HuggingFaceï¼ˆ2023aï¼‰HuggingFaceã€‚å¼€æ”¾LLMæ’è¡Œæ¦œï¼Œ2023aã€‚ç½‘å€[https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)ã€‚
- en: HuggingFace (2023b) HuggingFace. Sharegpt data, 2023b. URL [https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered?doi=true](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered?doi=true).
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HuggingFaceï¼ˆ2023bï¼‰HuggingFaceã€‚ShareGPTæ•°æ®ï¼Œ2023bã€‚ç½‘å€[https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered?doi=true](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered?doi=true)ã€‚
- en: Jacob etÂ al. (2018) Benoit Jacob, Skirmantas Kligys, BoÂ Chen, Menglong Zhu,
    Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization
    and training of neural networks for efficient integer-arithmetic-only inference.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition*,
    pp.Â  2704â€“2713, 2018.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jacobç­‰äººï¼ˆ2018ï¼‰Benoit Jacobã€Skirmantas Kligysã€Bo Chenã€Menglong Zhuã€Matthew Tangã€Andrew
    Howardã€Hartwig Adamå’ŒDmitry Kalenichenkoã€‚ç¥ç»ç½‘ç»œçš„é‡åŒ–å’Œè®­ç»ƒï¼Œä»¥å®ç°é«˜æ•ˆçš„æ•´æ•°ç®—æœ¯æ¨ç†ã€‚åœ¨*ã€ŠIEEEè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†ã€‹*ä¸­ï¼Œpp.
    2704â€“2713ï¼Œ2018å¹´ã€‚
- en: 'Jain etÂ al. (2020) Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami,
    Pieter Abbeel, Joseph Gonzalez, Kurt Keutzer, and Ion Stoica. Checkmate: Breaking
    the memory wall with optimal tensor rematerialization. *Proceedings of Machine
    Learning and Systems*, 2:497â€“511, 2020.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jainç­‰äººï¼ˆ2020ï¼‰Paras Jainã€Ajay Jainã€Aniruddha Nrusimhaã€Amir Gholamiã€Pieter Abbeelã€Joseph
    Gonzalezã€Kurt Keutzerå’ŒIon Stoicaã€‚Checkmate: é€šè¿‡æœ€ä½³å¼ é‡é‡æ–°è®¡ç®—æ‰“ç ´å†…å­˜å£å’ã€‚*ã€Šæœºå™¨å­¦ä¹ ä¸ç³»ç»Ÿä¼šè®®è®ºæ–‡é›†ã€‹*ï¼Œ2:497â€“511ï¼Œ2020å¹´ã€‚'
- en: 'Kim etÂ al. (2023) Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu
    Li, Sheng Shen, MichaelÂ W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse
    quantization. *arXiv preprint arXiv:2306.07629*, 2023.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kimç­‰äººï¼ˆ2023ï¼‰Sehoon Kimã€Coleman Hooperã€Amir Gholamiã€Zhen Dongã€Xiuyu Liã€Sheng
    Shenã€Michael W Mahoneyå’ŒKurt Keutzerã€‚Squeezellm: å¯†é›†å’Œç¨€ç–é‡åŒ–ã€‚*arXivé¢„å°æœ¬arXiv:2306.07629*ï¼Œ2023å¹´ã€‚'
- en: 'Kingma & Ba (2015) DiederikÂ P Kingma and Jimmy Ba. Adam: A method for stochastic
    optimization. In *International Conference on Learning Representations*, 2015.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kingma & Baï¼ˆ2015ï¼‰Diederik P Kingmaå’ŒJimmy Baã€‚Adam: ä¸€ç§éšæœºä¼˜åŒ–æ–¹æ³•ã€‚åœ¨*ã€Šå›½é™…å­¦ä¹ è¡¨ç¤ºä¼šè®®ã€‹*ï¼Œ2015å¹´ã€‚'
- en: Kirisame etÂ al. (2020) Marisa Kirisame, Steven Lyubomirsky, Altan Haan, Jennifer
    Brennan, Mike He, Jared Roesch, Tianqi Chen, and Zachary Tatlock. Dynamic tensor
    rematerialization. *arXiv preprint arXiv:2006.09616*, 2020.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kirisame et al. (2020) Marisa Kirisame, Steven Lyubomirsky, Altan Haan, Jennifer
    Brennan, Mike He, Jared Roesch, Tianqi Chen, å’Œ Zachary Tatlock. åŠ¨æ€å¼ é‡é‡ææ–™åŒ–ã€‚*arXiv
    é¢„å°æœ¬ arXiv:2006.09616*ï¼Œ2020å¹´ã€‚
- en: Kumar etÂ al. (2019) Ravi Kumar, Manish Purohit, Zoya Svitkina, Erik Vee, and
    Joshua Wang. Efficient rematerialization for deep networks. *Advances in Neural
    Information Processing Systems*, 32, 2019.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar et al. (2019) Ravi Kumar, Manish Purohit, Zoya Svitkina, Erik Vee, å’Œ Joshua
    Wang. æ·±åº¦ç½‘ç»œçš„é«˜æ•ˆé‡ææ–™åŒ–ã€‚*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•*ï¼Œ32ï¼Œ2019å¹´ã€‚
- en: Li etÂ al. (2023) Bingrui Li, Jianfei Chen, and Jun Zhu. Memory efficient optimizers
    with 4-bit states. *arXiv preprint arXiv:2309.01507*, 2023.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023) Bingrui Li, Jianfei Chen, å’Œ Jun Zhu. ä½¿ç”¨4ä½çŠ¶æ€çš„å†…å­˜é«˜æ•ˆä¼˜åŒ–å™¨ã€‚*arXiv
    é¢„å°æœ¬ arXiv:2309.01507*ï¼Œ2023å¹´ã€‚
- en: 'Li & Liang (2021) XiangÂ Lisa Li and Percy Liang. Prefix-tuning: Optimizing
    continuous prompts for generation. *arXiv preprint arXiv:2101.00190*, 2021.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li & Liang (2021) Xiang Lisa Li å’Œ Percy Liang. å‰ç¼€è°ƒä¼˜ï¼šä¼˜åŒ–è¿ç»­æç¤ºç”¨äºç”Ÿæˆã€‚*arXiv é¢„å°æœ¬ arXiv:2101.00190*ï¼Œ2021å¹´ã€‚
- en: 'Li & Gu (2022) Zhikai Li and Qingyi Gu. I-vit: integer-only quantization for
    efficient vision transformer inference. *arXiv preprint arXiv:2207.01405*, 2022.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li & Gu (2022) Zhikai Li å’Œ Qingyi Gu. I-vit: æ•´æ•°é‡åŒ–ä»…ç”¨äºé«˜æ•ˆçš„è§†è§‰å˜æ¢å™¨æ¨æ–­ã€‚*arXiv é¢„å°æœ¬ arXiv:2207.01405*ï¼Œ2022å¹´ã€‚'
- en: Li etÂ al. (2022a) Zhikai Li, Liping Ma, Mengjuan Chen, Junrui Xiao, and Qingyi
    Gu. Patch similarity aware data-free quantization for vision transformers. In
    *European Conference on Computer Vision*, pp.Â  154â€“170. Springer, 2022a.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2022a) Zhikai Li, Liping Ma, Mengjuan Chen, Junrui Xiao, å’Œ Qingyi
    Gu. åŸºäºæ•°æ®çš„è§†è§‰å˜æ¢å™¨çš„è¡¥ä¸ç›¸ä¼¼æ€§æ„ŸçŸ¥é‡åŒ–ã€‚åœ¨ *æ¬§æ´²è®¡ç®—æœºè§†è§‰ä¼šè®®*ï¼Œç¬¬154â€“170é¡µã€‚Springerï¼Œ2022å¹´ã€‚
- en: 'Li etÂ al. (2022b) Zhikai Li, Junrui Xiao, Lianwei Yang, and Qingyi Gu. Repq-vit:
    Scale reparameterization for post-training quantization of vision transformers.
    *arXiv preprint arXiv:2212.08254*, 2022b.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2022b) Zhikai Li, Junrui Xiao, Lianwei Yang, å’Œ Qingyi Gu. Repq-vit:
    ç”¨äºè§†è§‰å˜æ¢å™¨çš„è®­ç»ƒåé‡åŒ–çš„å°ºåº¦é‡å‚æ•°åŒ–ã€‚*arXiv é¢„å°æœ¬ arXiv:2212.08254*ï¼Œ2022å¹´ã€‚'
- en: Liao etÂ al. (2021) Thomas Liao, Rohan Taori, InioluwaÂ Deborah Raji, and Ludwig
    Schmidt. Are we learning yet? a meta review of evaluation failures across machine
    learning. In *Thirty-fifth Conference on Neural Information Processing Systems
    Datasets and Benchmarks Track (Round 2)*, 2021.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liao et al. (2021) Thomas Liao, Rohan Taori, Inioluwa Deborah Raji, å’Œ Ludwig
    Schmidt. æˆ‘ä»¬å·²ç»åœ¨å­¦ä¹ äº†å—ï¼Ÿæœºå™¨å­¦ä¹ ä¸­çš„è¯„ä¼°å¤±è´¥çš„å…ƒåˆ†æã€‚åœ¨ *ç¬¬35å±Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿä¼šè®®æ•°æ®é›†ä¸åŸºå‡†è·Ÿè¸ªï¼ˆç¬¬äºŒè½®ï¼‰*ï¼Œ2021å¹´ã€‚
- en: 'Lin etÂ al. (2023) JiÂ Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. Awq: Activation-aware weight quantization for llm compression and
    acceleration. *arXiv preprint arXiv:2306.00978*, 2023.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    å’Œ Song Han. Awq: é‡åŒ–æ„ŸçŸ¥æƒé‡çš„æ¿€æ´»æ„ŸçŸ¥ç”¨äºLLMå‹ç¼©å’ŒåŠ é€Ÿã€‚*arXiv é¢„å°æœ¬ arXiv:2306.00978*ï¼Œ2023å¹´ã€‚'
- en: 'Lin etÂ al. (2021) Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa:
    Measuring how models mimic human falsehoods. *arXiv preprint arXiv:2109.07958*,
    2021.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. (2021) Stephanie Lin, Jacob Hilton, å’Œ Owain Evans. Truthfulqa: æµ‹é‡æ¨¡å‹å¦‚ä½•æ¨¡ä»¿äººç±»çš„è™šå‡ä¿¡æ¯ã€‚*arXiv
    é¢„å°æœ¬ arXiv:2109.07958*ï¼Œ2021å¹´ã€‚'
- en: 'Liu etÂ al. (2023) Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra.
    Llm-qat: Data-free quantization aware training for large language models. *arXiv
    preprint arXiv:2305.17888*, 2023.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2023) Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, å’Œ Vikas Chandra.
    Llm-qat: æ— æ•°æ®é‡åŒ–æ„ŸçŸ¥è®­ç»ƒç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ã€‚*arXiv é¢„å°æœ¬ arXiv:2305.17888*ï¼Œ2023å¹´ã€‚'
- en: Loshchilov & Hutter (2017) Ilya Loshchilov and Frank Hutter. Decoupled weight
    decay regularization. *arXiv preprint arXiv:1711.05101*, 2017.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loshchilov & Hutter (2017) Ilya Loshchilov å’Œ Frank Hutter. è§£è€¦æƒé‡è¡°å‡æ­£åˆ™åŒ–ã€‚*arXiv
    é¢„å°æœ¬ arXiv:1711.05101*ï¼Œ2017å¹´ã€‚
- en: Lv etÂ al. (2023) Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo,
    and Xipeng Qiu. Full parameter fine-tuning for large language models with limited
    resources. *arXiv preprint arXiv:2306.09782*, 2023.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lv et al. (2023) Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo,
    å’Œ Xipeng Qiu. åœ¨æœ‰é™èµ„æºä¸‹å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå®Œæ•´å‚æ•°å¾®è°ƒã€‚*arXiv é¢„å°æœ¬ arXiv:2306.09782*ï¼Œ2023å¹´ã€‚
- en: Micikevicius etÂ al. (2017) Paulius Micikevicius, Sharan Narang, Jonah Alben,
    Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii
    Kuchaiev, Ganesh Venkatesh, etÂ al. Mixed precision training. *arXiv preprint arXiv:1710.03740*,
    2017.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Micikevicius et al. (2017) Paulius Micikevicius, Sharan Narang, Jonah Alben,
    Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii
    Kuchaiev, Ganesh Venkateshï¼Œç­‰ã€‚æ··åˆç²¾åº¦è®­ç»ƒã€‚*arXiv é¢„å°æœ¬ arXiv:1710.03740*ï¼Œ2017å¹´ã€‚
- en: 'Peng etÂ al. (2020) Xuan Peng, Xuanhua Shi, Hulin Dai, Hai Jin, Weiliang Ma,
    Qian Xiong, Fan Yang, and Xuehai Qian. Capuchin: Tensor-based gpu memory management
    for deep learning. In *Proceedings of the Twenty-Fifth International Conference
    on Architectural Support for Programming Languages and Operating Systems*, pp.Â 891â€“905,
    2020.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng ç­‰ï¼ˆ2020ï¼‰Xuan Pengã€Xuanhua Shiã€Hulin Daiã€Hai Jinã€Weiliang Maã€Qian Xiongã€Fan
    Yang å’Œ Xuehai Qianã€‚ã€ŠCapuchinï¼šåŸºäºå¼ é‡çš„ GPU å†…å­˜ç®¡ç†ç”¨äºæ·±åº¦å­¦ä¹ ã€‹ã€‚å‘è¡¨äº*ç¬¬äºŒåäº”å±Šç¼–ç¨‹è¯­è¨€å’Œæ“ä½œç³»ç»Ÿæ¶æ„æ”¯æŒå›½é™…ä¼šè®®è®ºæ–‡é›†*ï¼Œç¬¬
    891â€“905 é¡µï¼Œ2020ã€‚
- en: 'Rajbhandari etÂ al. (2020) Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
    and Yuxiong He. Zero: Memory optimizations toward training trillion parameter
    models. In *SC20: International Conference for High Performance Computing, Networking,
    Storage and Analysis*, pp.Â  1â€“16\. IEEE, 2020.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rajbhandari ç­‰ï¼ˆ2020ï¼‰Samyam Rajbhandariã€Jeff Rasleyã€Olatunji Ruwase å’Œ Yuxiong
    Heã€‚ã€ŠZeroï¼šé’ˆå¯¹è®­ç»ƒä¸‡äº¿å‚æ•°æ¨¡å‹çš„å†…å­˜ä¼˜åŒ–ã€‹ã€‚å‘è¡¨äº*SC20ï¼šå›½é™…é«˜æ€§èƒ½è®¡ç®—ã€ç½‘ç»œã€å­˜å‚¨ä¸åˆ†æä¼šè®®*ï¼Œç¬¬ 1â€“16 é¡µã€‚IEEEï¼Œ2020ã€‚
- en: shareGPT (2023) shareGPT. Sharegpt, 2023. URL [https://sharegpt.com/](https://sharegpt.com/).
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: shareGPTï¼ˆ2023ï¼‰shareGPTã€‚ã€ŠSharegptã€‹ï¼Œ2023ã€‚ç½‘å€ [https://sharegpt.com/](https://sharegpt.com/)ã€‚
- en: 'Shazeer & Stern (2018) Noam Shazeer and Mitchell Stern. Adafactor: Adaptive
    learning rates with sublinear memory cost. In *International Conference on Machine
    Learning*, pp.Â 4596â€“4604\. PMLR, 2018.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shazeer & Sternï¼ˆ2018ï¼‰Noam Shazeer å’Œ Mitchell Sternã€‚ã€ŠAdafactorï¼šå…·æœ‰äºšçº¿æ€§å†…å­˜æˆæœ¬çš„è‡ªé€‚åº”å­¦ä¹ ç‡ã€‹ã€‚å‘è¡¨äº*å›½é™…æœºå™¨å­¦ä¹ ä¼šè®®*ï¼Œç¬¬
    4596â€“4604 é¡µã€‚PMLRï¼Œ2018ã€‚
- en: 'Touvron etÂ al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal,
    Eric Hambro, Faisal Azhar, etÂ al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron ç­‰ï¼ˆ2023aï¼‰Hugo Touvronã€Thibaut Lavrilã€Gautier Izacardã€Xavier Martinetã€Marie-Anne
    Lachauxã€TimothÃ©e Lacroixã€Baptiste RoziÃ¨reã€Naman Goyalã€Eric Hambroã€Faisal Azhar
    ç­‰ã€‚ã€ŠLlamaï¼šå¼€æ”¾ä¸”é«˜æ•ˆçš„åŸºç¡€è¯­è¨€æ¨¡å‹ã€‹ã€‚*arXiv é¢„å°æœ¬ arXiv:2302.13971*ï¼Œ2023aã€‚
- en: 'Touvron etÂ al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, etÂ al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron ç­‰ï¼ˆ2023bï¼‰Hugo Touvronã€Louis Martinã€Kevin Stoneã€Peter Albertã€Amjad Almahairiã€Yasmine
    Babaeiã€Nikolay Bashlykovã€Soumya Batraã€Prajjwal Bhargavaã€Shruti Bhosale ç­‰ã€‚ã€ŠLlama
    2ï¼šå¼€æ”¾åŸºç¡€å’Œå¾®è°ƒèŠå¤©æ¨¡å‹ã€‹ã€‚*arXiv é¢„å°æœ¬ arXiv:2307.09288*ï¼Œ2023bã€‚
- en: 'Wang etÂ al. (2018) Linnan Wang, Jinmian Ye, Yiyang Zhao, Wei Wu, Ang Li, ShuaiwenÂ Leon
    Song, Zenglin Xu, and Tim Kraska. Superneurons: Dynamic gpu memory management
    for training deep neural networks. In *Proceedings of the 23rd ACM SIGPLAN symposium
    on principles and practice of parallel programming*, pp.Â  41â€“53, 2018.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang ç­‰ï¼ˆ2018ï¼‰Linnan Wangã€Jinmian Yeã€Yiyang Zhaoã€Wei Wuã€Ang Liã€Shuaiwen Leon Songã€Zenglin
    Xu å’Œ Tim Kraskaã€‚ã€ŠSuperneuronsï¼šç”¨äºè®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œçš„åŠ¨æ€ GPU å†…å­˜ç®¡ç†ã€‹ã€‚å‘è¡¨äº*ç¬¬ 23 å±Š ACM SIGPLAN å¹¶è¡Œç¼–ç¨‹åŸç†ä¸å®è·µç ”è®¨ä¼šè®ºæ–‡é›†*ï¼Œç¬¬
    41â€“53 é¡µï¼Œ2018ã€‚
- en: 'Zellers etÂ al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. Hellaswag: Can a machine really finish your sentence? *arXiv preprint
    arXiv:1905.07830*, 2019.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zellers ç­‰ï¼ˆ2019ï¼‰Rowan Zellersã€Ari Holtzmanã€Yonatan Biskã€Ali Farhadi å’Œ Yejin Choiã€‚ã€ŠHellaswagï¼šæœºå™¨çœŸçš„èƒ½å®Œæˆä½ çš„å¥å­å—ï¼Ÿã€‹*arXiv
    é¢„å°æœ¬ arXiv:1905.07830*ï¼Œ2019ã€‚
- en: 'Zhang etÂ al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, XiÂ Victoria Lin,
    etÂ al. Opt: Open pre-trained transformer language models. *arXiv preprint arXiv:2205.01068*,
    2022.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang ç­‰ï¼ˆ2022ï¼‰Susan Zhangã€Stephen Rollerã€Naman Goyalã€Mikel Artetxeã€Moya Chenã€Shuohui
    Chenã€Christopher Dewanã€Mona Diabã€Xian Liã€Xi Victoria Lin ç­‰ã€‚ã€ŠOptï¼šå¼€æ”¾çš„é¢„è®­ç»ƒ Transformer
    è¯­è¨€æ¨¡å‹ã€‹ã€‚*arXiv é¢„å°æœ¬ arXiv:2205.01068*ï¼Œ2022ã€‚
- en: Zhao etÂ al. (2023) WayneÂ Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
    Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, etÂ al.
    A survey of large language models. *arXiv preprint arXiv:2303.18223*, 2023.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao ç­‰ï¼ˆ2023ï¼‰Wayne Xin Zhaoã€Kun Zhouã€Junyi Liã€Tianyi Tangã€Xiaolei Wangã€Yupeng
    Houã€Yingqian Minã€Beichen Zhangã€Junjie Zhangã€Zican Dong ç­‰ã€‚ã€Šå¤§å‹è¯­è¨€æ¨¡å‹è°ƒæŸ¥ã€‹ã€‚*arXiv é¢„å°æœ¬
    arXiv:2303.18223*ï¼Œ2023ã€‚
- en: Zheng etÂ al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, ZiÂ Lin, Zhuohan Li, Dacheng Li, Eric Xing, etÂ al.
    Judging llm-as-a-judge with mt-bench and chatbot arena. *arXiv preprint arXiv:2306.05685*,
    2023.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng ç­‰ï¼ˆ2023ï¼‰Lianmin Zhengã€Wei-Lin Chiangã€Ying Shengã€Siyuan Zhuangã€Zhanghao
    Wuã€Yonghao Zhuangã€Zi Linã€Zhuohan Liã€Dacheng Liã€Eric Xing ç­‰ã€‚ã€Šä½¿ç”¨ mt-bench å’Œ chatbot
    arena è¯„åˆ¤ llm-as-a-judgeã€‹ã€‚*arXiv é¢„å°æœ¬ arXiv:2306.05685*ï¼Œ2023ã€‚
- en: Appendix A Appendix
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é™„å½• A é™„å½•
- en: A.1 Discussion on Outlier Thresholds of Weight Quantizer
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 æƒé‡é‡åŒ–å™¨çš„å¼‚å¸¸å€¼é˜ˆå€¼è®¨è®º
- en: 'In this section, we discuss the selection and updating strategies for outlier
    thresholds in dense-and-sparse quantizers. We first report the memory and accuracy
    of dense-and-sparse quantizers using different percentage thresholds, and the
    results are shown in Table [4](#A1.T4 "Table 4 â€£ A.1 Discussion on Outlier Thresholds
    of Weight Quantizer â€£ Appendix A Appendix â€£ QFT: Quantized Full-parameter Tuning
    of LLMs with Affordable Resources"). The accuracy, i.e., the degree of distributional
    approximation of the quantizers, is evaluated by $L_{2}$, where the quantized
    weights are from the final down projection layer.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†åœ¨ç¨ å¯†å’Œç¨€ç–é‡åŒ–å™¨ä¸­ç¦»ç¾¤å€¼é˜ˆå€¼çš„é€‰æ‹©å’Œæ›´æ–°ç­–ç•¥ã€‚æˆ‘ä»¬é¦–å…ˆæŠ¥å‘Šäº†ä½¿ç”¨ä¸åŒç™¾åˆ†æ¯”é˜ˆå€¼çš„ç¨ å¯†å’Œç¨€ç–é‡åŒ–å™¨çš„å†…å­˜å’Œå‡†ç¡®æ€§ï¼Œç»“æœè§è¡¨ [4](#A1.T4
    "è¡¨4 â€£ A.1 ç¦»ç¾¤å€¼é˜ˆå€¼çš„è®¨è®º â€£ é™„å½•A é™„å½• â€£ QFTï¼šä½¿ç”¨è´Ÿæ‹…å¾—èµ·çš„èµ„æºå¯¹LLMè¿›è¡Œé‡åŒ–å…¨å‚æ•°è°ƒä¼˜")ã€‚å‡†ç¡®æ€§ï¼Œå³é‡åŒ–å™¨çš„åˆ†å¸ƒé€¼è¿‘åº¦ï¼Œé€šè¿‡ $L_{2}$
    è¯„ä¼°ï¼Œå…¶ä¸­é‡åŒ–çš„æƒé‡æ¥è‡ªæœ€ç»ˆçš„ä¸‹æŠ•å½±å±‚ã€‚
- en: 'Table 4: Comparison of memory (in GB) and accuracy of dense-and-sparse quantizers
    using different percentage thresholds for weights. Here, accuracy is measured
    by $L_{2}$.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨4ï¼šä½¿ç”¨ä¸åŒç™¾åˆ†æ¯”é˜ˆå€¼çš„ç¨ å¯†å’Œç¨€ç–é‡åŒ–å™¨çš„å†…å­˜ï¼ˆä»¥GBä¸ºå•ä½ï¼‰å’Œå‡†ç¡®æ€§æ¯”è¾ƒã€‚æ­¤å¤„çš„å‡†ç¡®æ€§ç”± $L_{2}$ æµ‹é‡ã€‚
- en: '| Percentile | 0 | 0.45% | 1.0% | 3.0% | 5.0% |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| ç™¾åˆ†ä½æ•° | 0 | 0.45% | 1.0% | 3.0% | 5.0% |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Memory | 7.06 | 7.23 | 7.42 | 8.23 | 9.16 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| å†…å­˜ | 7.06 | 7.23 | 7.42 | 8.23 | 9.16 |'
- en: '| $L_{2}$ Distance | 436 | 0.846 | 0.619 | 0.566 | 0.479 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| $L_{2}$ è·ç¦» | 436 | 0.846 | 0.619 | 0.566 | 0.479 |'
- en: The benefits of employing matrix decomposition in dense-and-sparse quantizers
    are readily evident from the results. When the percentile is set to 0, the quantizer
    no longer filters out the outliers and degenerates into a standard uniform quantizer,
    resulting in intolerable quantization errors. A noteworthy value suggested in
    SqueezeLLMÂ (Kim etÂ al., [2023](#bib.bib21)) is 0.45%. However, itâ€™s essential
    to acknowledge that this tight bound is primarily designed for inference scenarios.
    To accommodate potential fluctuations during training, some level of relaxation
    in the thresholds is necessary. To this end, we assess the relaxed constraints
    at percentiles of 1.0%, 3.0%, and 5.0%. The findings reveal that, although the
    3% and 5% percentiles yield a slight boost in accuracy, they also incur higher
    memory usage. Consequently, we advocate for the 1% percentile as a more favorable
    choice for achieving a balance between memory conservation and accuracy.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¨ å¯†å’Œç¨€ç–é‡åŒ–å™¨ä¸­ä½¿ç”¨çŸ©é˜µåˆ†è§£çš„å¥½å¤„ä»ç»“æœä¸­æ˜¾è€Œæ˜“è§ã€‚å½“ç™¾åˆ†ä½æ•°è®¾ç½®ä¸º0æ—¶ï¼Œé‡åŒ–å™¨ä¸å†æ»¤é™¤ç¦»ç¾¤å€¼ï¼Œé€€åŒ–ä¸ºæ ‡å‡†å‡åŒ€é‡åŒ–å™¨ï¼Œå¯¼è‡´ä¸å¯æ¥å—çš„é‡åŒ–è¯¯å·®ã€‚SqueezeLLMÂ (Kim
    etÂ al., [2023](#bib.bib21)) ä¸­å»ºè®®çš„ä¸€ä¸ªå€¼å¾—æ³¨æ„çš„å€¼æ˜¯0.45%ã€‚ç„¶è€Œï¼Œå¿…é¡»æ‰¿è®¤è¿™ä¸ªç´§æŸç¼šä¸»è¦æ˜¯ä¸ºæ¨ç†åœºæ™¯è®¾è®¡çš„ã€‚ä¸ºäº†é€‚åº”è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ½œåœ¨æ³¢åŠ¨ï¼Œæœ‰å¿…è¦åœ¨é˜ˆå€¼ä¸Šåšä¸€äº›æ”¾æ¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è¯„ä¼°äº†1.0%ã€3.0%å’Œ5.0%ç™¾åˆ†ä½æ•°ä¸‹çš„æ”¾æ¾çº¦æŸã€‚ç»“æœè¡¨æ˜ï¼Œå°½ç®¡3%å’Œ5%ç™¾åˆ†ä½æ•°ç•¥å¾®æé«˜äº†å‡†ç¡®æ€§ï¼Œä½†ä¹Ÿå¸¦æ¥äº†æ›´é«˜çš„å†…å­˜ä½¿ç”¨ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å»ºè®®é€‰æ‹©1%ç™¾åˆ†ä½æ•°ä½œä¸ºåœ¨å†…å­˜èŠ‚çœå’Œå‡†ç¡®æ€§ä¹‹é—´å–å¾—å¹³è¡¡çš„æ›´ç†æƒ³é€‰æ‹©ã€‚
- en: We also explore the update frequency of the thresholds. In order to maintain
    a high training throughput, we adopt a strategy of lazy updates, meaning that
    the thresholds set in the first iteration are reused in subsequent iterations.
    This approach is viable due to the smooth numerical updates of the weights during
    the fine-tuning process across neighboring iterations. Additionally, to our surprise,
    we discover that the initial thresholds remain effective for an entire epoch.
    As a result, we only update the thresholds once at the beginning of each epoch.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜æ¢è®¨äº†é˜ˆå€¼çš„æ›´æ–°é¢‘ç‡ã€‚ä¸ºäº†ä¿æŒé«˜è®­ç»ƒååé‡ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å»¶è¿Ÿæ›´æ–°ç­–ç•¥ï¼Œå³åœ¨ç¬¬ä¸€æ¬¡è¿­ä»£ä¸­è®¾ç½®çš„é˜ˆå€¼åœ¨åç»­è¿­ä»£ä¸­é‡å¤ä½¿ç”¨ã€‚è¿™ç§æ–¹æ³•æ˜¯å¯è¡Œçš„ï¼Œå› ä¸ºåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œæƒé‡çš„æ•°å€¼æ›´æ–°åœ¨ç›¸é‚»è¿­ä»£ä¸­æ˜¯å¹³æ»‘çš„ã€‚æ­¤å¤–ï¼Œä»¤æˆ‘ä»¬æƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°åˆå§‹é˜ˆå€¼åœ¨æ•´ä¸ªè®­ç»ƒå‘¨æœŸå†…ä»ç„¶æœ‰æ•ˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä»…åœ¨æ¯ä¸ªè®­ç»ƒå‘¨æœŸå¼€å§‹æ—¶æ›´æ–°ä¸€æ¬¡é˜ˆå€¼ã€‚
- en: A.2 Discussion on Training Stability
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 è®­ç»ƒç¨³å®šæ€§çš„è®¨è®º
- en: In this section, we delve into the essentiality of maintaining full-precision
    weight copies in mixed-precision training and provide a comprehensive illustration
    of the benefits of QFT through a comparative analysis. In mixed-precision training,
    both forward and backward propagation involve weights and gradients in the FP16
    format. However, during parameter updates within the optimizer, the weights are
    involved in calculations using the FP32 format. This approach is employed to circumvent
    potential data overflow issues that may arise in the FP16 formatÂ (Micikevicius
    etÂ al., [2017](#bib.bib36)), given its limited representation range. Consequently,
    maintaining full-precision weight copies becomes indispensable to ensure the stability
    of the training process.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥æ¢è®¨äº†åœ¨æ··åˆç²¾åº¦è®­ç»ƒä¸­ä¿æŒå…¨ç²¾åº¦æƒé‡å‰¯æœ¬çš„é‡è¦æ€§ï¼Œå¹¶é€šè¿‡å¯¹æ¯”åˆ†æå…¨é¢è¯´æ˜äº†QFTçš„å¥½å¤„ã€‚åœ¨æ··åˆç²¾åº¦è®­ç»ƒä¸­ï¼Œå‰å‘å’Œåå‘ä¼ æ’­éƒ½æ¶‰åŠFP16æ ¼å¼çš„æƒé‡å’Œæ¢¯åº¦ã€‚ç„¶è€Œï¼Œåœ¨ä¼˜åŒ–å™¨å†…è¿›è¡Œå‚æ•°æ›´æ–°æ—¶ï¼Œæƒé‡ä¼šä½¿ç”¨FP32æ ¼å¼è¿›è¡Œè®¡ç®—ã€‚è¿™ç§æ–¹æ³•çš„é‡‡ç”¨æ˜¯ä¸ºäº†è§„é¿FP16æ ¼å¼ä¸­å¯èƒ½å‡ºç°çš„æ•°æ®æº¢å‡ºé—®é¢˜ï¼ˆMicikeviciusç­‰ï¼Œ[2017](#bib.bib36)ï¼‰ï¼Œå› ä¸ºå…¶è¡¨ç¤ºèŒƒå›´æœ‰é™ã€‚å› æ­¤ï¼Œä¿æŒå…¨ç²¾åº¦æƒé‡å‰¯æœ¬æˆä¸ºç¡®ä¿è®­ç»ƒè¿‡ç¨‹ç¨³å®šæ€§çš„å¿…è¦æ¡ä»¶ã€‚
- en: 'Conversely, QFT effectively addresses the aforementioned issue without necessitating
    extra storage for weight copies. Specifically, unlike the rudimentary numerical
    truncation encountered in the FP16 format, the quantization procedure in QFT maps
    FP32 format values as a cohesive unit to a set of evenly distributed integer values,
    as illustrated in Equation [1](#S3.E1 "In Uniform Quantizer for Gradients and
    Momentum â€£ 3.2 Quantization â€£ 3 Methodology â€£ QFT: Quantized Full-parameter Tuning
    of LLMs with Affordable Resources"). Consequently, when performing the calculations,
    these integer values are dynamically de-quantized into the FP32 format, without
    any potential risk of data overflow.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç›¸åï¼ŒQFTæœ‰æ•ˆåœ°è§£å†³äº†ä¸Šè¿°é—®é¢˜ï¼Œæ— éœ€é¢å¤–å­˜å‚¨æƒé‡å‰¯æœ¬ã€‚å…·ä½“è€Œè¨€ï¼Œä¸FP16æ ¼å¼ä¸­é‡åˆ°çš„åŸºæœ¬æ•°å€¼æˆªæ–­ä¸åŒï¼ŒQFTä¸­çš„é‡åŒ–è¿‡ç¨‹å°†FP32æ ¼å¼çš„å€¼ä½œä¸ºä¸€ä¸ªæ•´ä½“æ˜ å°„åˆ°ä¸€ç»„å‡åŒ€åˆ†å¸ƒçš„æ•´æ•°å€¼ï¼Œå¦‚æ–¹ç¨‹å¼[1](#S3.E1
    "In Uniform Quantizer for Gradients and Momentum â€£ 3.2 Quantization â€£ 3 Methodology
    â€£ QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources")æ‰€ç¤ºã€‚å› æ­¤ï¼Œåœ¨è¿›è¡Œè®¡ç®—æ—¶ï¼Œè¿™äº›æ•´æ•°å€¼ä¼šåŠ¨æ€åœ°è§£é‡åŒ–å›FP32æ ¼å¼ï¼Œè€Œæ²¡æœ‰æ•°æ®æº¢å‡ºçš„æ½œåœ¨é£é™©ã€‚'
- en: A.3 Qualitative Analysis of Conversational Abilities
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 å¯¹è¯èƒ½åŠ›çš„å®šæ€§åˆ†æ
- en: In the experimental section, we demonstrate the effectiveness of QFT through
    quantitative evaluations of LLMsâ€™ performance on a variety of benchmarks. Despite
    the large number of benchmarks, the results of the machine evaluations are not
    considered to be comprehensive, and in some cases, their reliability may be subject
    to scrutiny, e.g., the â€shortcutâ€ issue. Consequently, in the following, we intuitively
    show and compare examples generated by LLMs, aiming to further verify the conversational
    abilities through qualitative analysis.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®éªŒéƒ¨åˆ†ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹å„ç§åŸºå‡†æµ‹è¯•ä¸­LLMsæ€§èƒ½çš„å®šé‡è¯„ä¼°å±•ç¤ºäº†QFTçš„æœ‰æ•ˆæ€§ã€‚å°½ç®¡åŸºå‡†æµ‹è¯•æ•°é‡ä¼—å¤šï¼Œä½†æœºå™¨è¯„ä¼°ç»“æœå¹¶ä¸è¢«è®¤ä¸ºæ˜¯å…¨é¢çš„ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå…¶å¯é æ€§å¯èƒ½ä¼šå—åˆ°è´¨ç–‘ï¼Œä¾‹å¦‚â€œæ·å¾„â€é—®é¢˜ã€‚å› æ­¤ï¼Œåœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ï¼Œæˆ‘ä»¬ç›´è§‚åœ°å±•ç¤ºå’Œæ¯”è¾ƒç”±LLMsç”Ÿæˆçš„ç¤ºä¾‹ï¼Œæ—¨åœ¨é€šè¿‡å®šæ€§åˆ†æè¿›ä¸€æ­¥éªŒè¯å¯¹è¯èƒ½åŠ›ã€‚
- en: '[Writing Ability]'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[å†™ä½œèƒ½åŠ›]'
- en: '*Question*: Write a captivating movie review for a recently released science
    fiction film, discussing its plot, characters, and special effects.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '*é—®é¢˜*ï¼šæ’°å†™ä¸€ç¯‡å¼•äººå…¥èƒœçš„ç”µå½±è¯„è®ºï¼Œè®¨è®ºæœ€è¿‘ä¸Šæ˜ çš„ä¸€éƒ¨ç§‘å¹»ç”µå½±çš„æƒ…èŠ‚ã€è§’è‰²å’Œç‰¹æ•ˆã€‚'
- en: 'Vicuna-7B*: â€The Last Voyageâ€ is a thrilling science fiction film that follows
    a group of astronauts on a mission to explore a distant planet. When they arrive,
    they discover that the planet is inhabited by a mysterious alien race. As they
    try to communicate with the aliens and gather data, they realize that their mission
    may be more dangerous than they ever imagined. With stunning special effects and
    a gripping plot, â€The Last Voyageâ€ is a must-see for science fiction fans.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Vicuna-7B*ï¼šâ€œæœ€åçš„èˆªè¡Œâ€æ˜¯ä¸€éƒ¨åˆºæ¿€çš„ç§‘å¹»ç”µå½±ï¼Œè®²è¿°äº†ä¸€ç»„å®‡èˆªå‘˜æ¢ç´¢é¥è¿œæ˜Ÿçƒçš„ä»»åŠ¡ã€‚å½“ä»–ä»¬æŠµè¾¾æ—¶ï¼Œå‘ç°è¿™ä¸ªæ˜Ÿçƒä¸Šå±…ä½ç€ä¸€ä¸ªç¥ç§˜çš„å¤–æ˜Ÿç§æ—ã€‚éšç€ä»–ä»¬è¯•å›¾ä¸å¤–æ˜Ÿäººæ²Ÿé€šå¹¶æ”¶é›†æ•°æ®ï¼Œä»–ä»¬æ„è¯†åˆ°ä»–ä»¬çš„ä»»åŠ¡å¯èƒ½æ¯”ä»–ä»¬æƒ³è±¡çš„æ›´å±é™©ã€‚å‡­å€ŸæƒŠäººçš„ç‰¹æ•ˆå’Œå¼•äººå…¥èƒœçš„æƒ…èŠ‚ï¼Œâ€œæœ€åçš„èˆªè¡Œâ€æ˜¯ç§‘å¹»è¿·å¿…çœ‹çš„å½±ç‰‡ã€‚
- en: 'LLaMA-2-7B-QFT: â€Intergalactic Intrigueâ€ is a thrilling science fiction film
    that takes viewers on a journey through the stars. The film follows a group of
    space explorers as they embark on a mission to uncover the secrets of an ancient
    alien civilization. The filmâ€™s special effects are top-notch, with stunning visuals
    that transport viewers to a world of wonder and discovery. The characters are
    well-developed and likable, with each member of the crew bringing their own unique
    skills and personalities to the table.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLaMA-2-7B-QFT: ã€Šæ˜Ÿé™…é˜´è°‹ã€‹æ˜¯ä¸€éƒ¨æ¿€åŠ¨äººå¿ƒçš„ç§‘å¹»ç‰‡ï¼Œå¸¦é¢†è§‚ä¼—è¸ä¸Šæ˜Ÿé™…ä¹‹æ—…ã€‚å½±ç‰‡è®²è¿°äº†ä¸€ç¾¤å¤ªç©ºæ¢é™©è€…çš„æ•…äº‹ï¼Œä»–ä»¬è¸ä¸Šä½¿å‘½ä»¥æ­å¼€å¤è€å¤–æ˜Ÿæ–‡æ˜çš„ç§˜å¯†ã€‚å½±ç‰‡çš„ç‰¹æ•ˆä¸€æµï¼Œä»¤äººæƒŠå¹çš„è§†è§‰æ•ˆæœå°†è§‚ä¼—å¸¦å…¥ä¸€ä¸ªå……æ»¡å¥‡è¿¹å’Œå‘ç°çš„ä¸–ç•Œã€‚è§’è‰²å‘å±•è‰¯å¥½ä¸”è®¨äººå–œæ¬¢ï¼Œæ¯ä¸ªèˆ¹å‘˜éƒ½å¸¦æ¥äº†è‡ªå·±ç‹¬ç‰¹çš„æŠ€èƒ½å’Œä¸ªæ€§ã€‚'
- en: '[Roleplay Ability]'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[è§’è‰²æ‰®æ¼”èƒ½åŠ›]'
- en: '*Question*: As a superhero, how would you explain your origin story to a curious
    child?'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '*é—®é¢˜*: ä½œä¸ºä¸€ä¸ªè¶…çº§è‹±é›„ï¼Œä½ ä¼šå¦‚ä½•å‘å¥½å¥‡çš„å­©å­è§£é‡Šä½ çš„èµ·æºæ•…äº‹ï¼Ÿ'
- en: 'Vicuna-7B*: I was just like any other kid, until one day I was exposed to a
    strange chemical that gave me incredible powers. I learned to control the elements
    and use them to protect the innocent. I became known as the Elemental Guardian,
    and Iâ€™ve been fighting crime and saving the day ever since.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 'Vicuna-7B*: æˆ‘æ›¾ç»å’Œå…¶ä»–å­©å­ä¸€æ ·ï¼Œç›´åˆ°æœ‰ä¸€å¤©æˆ‘æ¥è§¦åˆ°ä¸€ç§å¥‡æ€ªçš„åŒ–å­¦ç‰©è´¨ï¼Œè¿™è®©æˆ‘è·å¾—äº†ä»¤äººéš¾ä»¥ç½®ä¿¡çš„åŠ›é‡ã€‚æˆ‘å­¦ä¼šäº†æ§åˆ¶å…ƒç´ ï¼Œå¹¶åˆ©ç”¨è¿™äº›åŠ›é‡ä¿æŠ¤æ— è¾œã€‚æˆ‘è¢«ç§°ä¸ºå…ƒç´ å®ˆæŠ¤è€…ï¼Œä»é‚£æ—¶èµ·æˆ‘å°±å¼€å§‹æ‰“å‡»çŠ¯ç½ªï¼Œæ‹¯æ•‘ä¸–ç•Œã€‚'
- en: 'LLaMA-2-7B-QFT: I was just like any other kid, playing in the park and having
    fun. But one day, I was hit by a strange ray that gave me superpowers. I could
    fly, turn invisible, and even control the weather! I decided to use my powers
    to help people and fight crime. Thatâ€™s how I became the superhero you know and
    love!'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLaMA-2-7B-QFT: æˆ‘æ›¾ç»å’Œå…¶ä»–å­©å­ä¸€æ ·ï¼Œåœ¨å…¬å›­é‡Œç©è€å¹¶äº«å—ä¹è¶£ã€‚ä½†æœ‰ä¸€å¤©ï¼Œæˆ‘è¢«ä¸€æŸå¥‡æ€ªçš„å…‰çº¿å‡»ä¸­ï¼Œè¿™ç»™äº†æˆ‘è¶…èƒ½åŠ›ã€‚æˆ‘å¯ä»¥é£è¡Œã€éšèº«ï¼Œç”šè‡³æ§åˆ¶å¤©æ°”ï¼æˆ‘å†³å®šåˆ©ç”¨æˆ‘çš„åŠ›é‡å¸®åŠ©äººä»¬å’Œæ‰“å‡»çŠ¯ç½ªã€‚è¿™å°±æ˜¯æˆ‘æˆä¸ºä½ ä»¬æ‰€çŸ¥å¹¶å–œçˆ±çš„è¶…çº§è‹±é›„çš„åŸå› ï¼'
- en: '[Common-sense Ability]'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[å¸¸è¯†èƒ½åŠ›]'
- en: '*Question*: In a world where automation is becoming increasingly prevalent,
    is it more important to prioritize job creation or technological progress?'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '*é—®é¢˜*: åœ¨è‡ªåŠ¨åŒ–è¶Šæ¥è¶Šæ™®åŠçš„ä¸–ç•Œé‡Œï¼Œæ˜¯ä¼˜å…ˆè€ƒè™‘åˆ›é€ å°±ä¸šæœºä¼šè¿˜æ˜¯æŠ€æœ¯è¿›æ­¥æ›´é‡è¦ï¼Ÿ'
- en: 'Vicuna-7B*: Job creation is essential for a thriving economy, as it provides
    individuals with employment opportunities and a source of income. While technological
    progress is important for driving innovation and improving efficiency, it is crucial
    to balance technological advancements with the creation of new jobs. This can
    be achieved by investing in education and training programs that help workers
    develop the skills necessary to adapt to new technologies and by promoting policies
    that support small businesses and entrepreneurship. By prioritizing both job creation
    and technological progress, we can ensure a more stable and prosperous future
    for all.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 'Vicuna-7B*: åˆ›é€ å°±ä¸šå¯¹äºä¸€ä¸ªç¹è£çš„ç»æµè‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒä¸ºä¸ªäººæä¾›äº†å°±ä¸šæœºä¼šå’Œæ”¶å…¥æ¥æºã€‚è™½ç„¶æŠ€æœ¯è¿›æ­¥å¯¹äºæ¨åŠ¨åˆ›æ–°å’Œæé«˜æ•ˆç‡å¾ˆé‡è¦ï¼Œä½†å¹³è¡¡æŠ€æœ¯è¿›æ­¥ä¸æ–°å·¥ä½œçš„åˆ›é€ åŒæ ·å…³é”®ã€‚è¿™å¯ä»¥é€šè¿‡æŠ•èµ„æ•™è‚²å’ŒåŸ¹è®­é¡¹ç›®æ¥å®ç°ï¼Œè¿™äº›é¡¹ç›®å¸®åŠ©å·¥äººå‘å±•é€‚åº”æ–°æŠ€æœ¯çš„æŠ€èƒ½ï¼ŒåŒæ—¶ä¿ƒè¿›æ”¯æŒå°ä¼ä¸šå’Œåˆ›ä¸šçš„æ”¿ç­–ã€‚é€šè¿‡ä¼˜å…ˆè€ƒè™‘å°±ä¸šåˆ›é€ å’ŒæŠ€æœ¯è¿›æ­¥ï¼Œæˆ‘ä»¬å¯ä»¥ç¡®ä¿ä¸€ä¸ªæ›´ç¨³å®šå’Œç¹è£çš„æœªæ¥ã€‚'
- en: 'LLaMA-2-7B-QFT: In the short term, job creation is a critical concern for policymakers
    and businesses. The unemployment rate remains high, and many people are struggling
    to find work. However, in the long term, technological progress is essential for
    driving economic growth and improving the standard of living for everyone. Automation
    has the potential to increase productivity and efficiency, which can lead to higher
    wages and better working conditions for employees. However, it can also lead to
    job displacement, particularly in industries where automation is most prevalent.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLaMA-2-7B-QFT: åœ¨çŸ­æœŸå†…ï¼Œåˆ›é€ å°±ä¸šæ˜¯æ”¿ç­–åˆ¶å®šè€…å’Œä¼ä¸šé¢ä¸´çš„å…³é”®é—®é¢˜ã€‚å¤±ä¸šç‡ä»ç„¶å¾ˆé«˜ï¼Œè®¸å¤šäººæ­£åœ¨åŠªåŠ›å¯»æ‰¾å·¥ä½œã€‚ç„¶è€Œï¼Œä»é•¿è¿œæ¥çœ‹ï¼ŒæŠ€æœ¯è¿›æ­¥å¯¹äºæ¨åŠ¨ç»æµå¢é•¿å’Œæé«˜æ¯ä¸ªäººçš„ç”Ÿæ´»æ°´å¹³è‡³å…³é‡è¦ã€‚è‡ªåŠ¨åŒ–æœ‰å¯èƒ½æé«˜ç”Ÿäº§åŠ›å’Œæ•ˆç‡ï¼Œè¿™å¯ä»¥å¸¦æ¥æ›´é«˜çš„å·¥èµ„å’Œæ›´å¥½çš„å·¥ä½œæ¡ä»¶ã€‚ç„¶è€Œï¼Œå®ƒä¹Ÿå¯èƒ½å¯¼è‡´å·¥ä½œå²—ä½çš„æµå¤±ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªåŠ¨åŒ–æœ€ä¸ºæ™®åŠçš„è¡Œä¸šä¸­ã€‚'
- en: '[Fermi Ability]'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[è´¹ç±³èƒ½åŠ›]'
- en: '*Question*: How many snowflakes fall during a typical winter? Try to explain
    your answer. Your explanation should take the reader through your reasoning step-by-step.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '*é—®é¢˜*: åœ¨ä¸€ä¸ªå…¸å‹çš„å†¬å­£é‡Œä¼šæœ‰å¤šå°‘é›ªèŠ±è½ä¸‹ï¼Ÿå°è¯•è§£é‡Šä½ çš„ç­”æ¡ˆã€‚ä½ çš„è§£é‡Šåº”è¯¥ä¸€æ­¥ä¸€æ­¥åœ°å¼•å¯¼è¯»è€…ç†è§£ä½ çš„æ¨ç†è¿‡ç¨‹ã€‚'
- en: 'Vicuna-7B*: It is difficult to determine the average number of snowflakes that
    fall during a typical winter, as it can vary greatly depending on location, climate,
    and other factors. However, it is estimated that on average, a single snowflake
    can have up to 100,000 ice crystals per flake. Over the course of a typical winter,
    this could result in billions of snowflakes falling from the sky.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 'Vicuna-7B*: å¾ˆéš¾ç¡®å®šå…¸å‹å†¬å­£æœŸé—´å¹³å‡é™é›ªé‡ï¼Œå› ä¸ºå®ƒä¼šå› åœ°ç‚¹ã€æ°”å€™å’Œå…¶ä»–å› ç´ è€Œæœ‰å¾ˆå¤§å·®å¼‚ã€‚ç„¶è€Œï¼Œæ®ä¼°è®¡ï¼Œå•ä¸ªé›ªèŠ±ä¸Šå¯èƒ½æœ‰å¤šè¾¾100,000ä¸ªå†°æ™¶ã€‚æ•´ä¸ªå…¸å‹å†¬å­£ï¼Œè¿™å¯èƒ½å¯¼è‡´æ•°åäº¿ä¸ªé›ªèŠ±ä»å¤©ç©ºä¸­é™è½ã€‚'
- en: 'LLaMA-2-7B-QFT: To answer this question, we need to consider the average snowfall
    for a typical winter in a given location. In the United States, for example, snowfall
    tends to be heaviest in the northern states, where the average snowfall for a
    typical winter is around 60 inches. In the southern states, snowfall is much less
    common and the average snowfall for a typical winter is around 10 inches.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLaMA-2-7B-QFT: ä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦è€ƒè™‘ç»™å®šåœ°ç‚¹å…¸å‹å†¬å­£çš„å¹³å‡é™é›ªé‡ã€‚ä¾‹å¦‚ï¼Œåœ¨ç¾å›½ï¼Œé™é›ªé€šå¸¸åœ¨åŒ—æ–¹å·æœ€ä¸ºä¸¥é‡ï¼Œå…¸å‹å†¬å­£çš„å¹³å‡é™é›ªé‡çº¦ä¸º60è‹±å¯¸ã€‚åœ¨å—æ–¹å·ï¼Œé™é›ªåˆ™å°‘å¾—å¤šï¼Œå…¸å‹å†¬å­£çš„å¹³å‡é™é›ªé‡çº¦ä¸º10è‹±å¯¸ã€‚'
- en: '[Counterfactual Ability]'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '[åäº‹å®èƒ½åŠ›]'
- en: '*Question*: What if the Internet had been invented during the Renaissance period?'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '*é—®é¢˜*: å¦‚æœäº’è”ç½‘åœ¨æ–‡è‰ºå¤å…´æ—¶æœŸå·²ç»å‘æ˜ï¼Œä¼šæ€æ ·ï¼Ÿ'
- en: 'Vicuna-7B*: The Renaissance would have been a much different time period if
    the internet had been invented. The flow of information would have been much faster
    and more efficient, leading to a greater exchange of ideas and innovations. The
    world would have been connected in ways never before seen, allowing for the rapid
    spread of new ideas and technologies.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 'Vicuna-7B*: å¦‚æœäº’è”ç½‘åœ¨æ–‡è‰ºå¤å…´æ—¶æœŸå·²ç»å‘æ˜ï¼Œé‚£ä¸ªæ—¶æœŸå°†ä¼šå¤§ä¸ç›¸åŒã€‚ä¿¡æ¯æµåŠ¨å°†ä¼šæ›´åŠ è¿…é€Ÿå’Œé«˜æ•ˆï¼Œå¯¼è‡´æ€æƒ³å’Œåˆ›æ–°çš„æ›´å¤§äº¤æµã€‚ä¸–ç•Œå°†ä»¥ä»æœªè§è¿‡çš„æ–¹å¼ç›¸äº’è¿æ¥ï¼Œä½¿æ–°æ€æƒ³å’ŒæŠ€æœ¯çš„ä¼ æ’­å˜å¾—æ›´å¿«ã€‚'
- en: 'LLaMA-2-7B-QFT: The Renaissance was characterized by a renewed interest in
    classical learning and the arts, and the Internet would have provided a way for
    people to access and share this knowledge. It would have also facilitated the
    exchange of ideas and the development of new technologies, leading to even greater
    advancements in the fields of science, medicine, and engineering.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLaMA-2-7B-QFT: æ–‡è‰ºå¤å…´æ—¶æœŸçš„ç‰¹ç‚¹æ˜¯å¯¹å¤å…¸å­¦ä¹ å’Œè‰ºæœ¯çš„é‡æ–°å…´è¶£ï¼Œè€Œäº’è”ç½‘å°†æä¾›ä¸€ç§è®©äººä»¬è®¿é—®å’Œåˆ†äº«è¿™äº›çŸ¥è¯†çš„æ–¹å¼ã€‚å®ƒè¿˜å°†ä¿ƒè¿›æ€æƒ³äº¤æµå’Œæ–°æŠ€æœ¯çš„å‘å±•ï¼Œä»è€Œåœ¨ç§‘å­¦ã€åŒ»å­¦å’Œå·¥ç¨‹é¢†åŸŸå¸¦æ¥æ›´å¤§çš„è¿›æ­¥ã€‚'
- en: '[Generic Ability]'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '[é€šç”¨èƒ½åŠ›]'
- en: '*Question*: Can you explain the basics of quantum computing?'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '*é—®é¢˜*: ä½ èƒ½è§£é‡Šä¸€ä¸‹é‡å­è®¡ç®—çš„åŸºæœ¬åŸç†å—ï¼Ÿ'
- en: 'Vicuna-7B*: Quantum computing is a type of computing that uses quantum bits
    (qubits) to perform calculations, unlike traditional computers that use binary.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 'Vicuna-7B*: é‡å­è®¡ç®—æ˜¯ä¸€ç§ä½¿ç”¨é‡å­ä½ï¼ˆqubitsï¼‰è¿›è¡Œè®¡ç®—çš„è®¡ç®—ç±»å‹ï¼Œä¸ä½¿ç”¨äºŒè¿›åˆ¶çš„ä¼ ç»Ÿè®¡ç®—æœºä¸åŒã€‚'
- en: 'LLaMA-2-7B-QFT: Quantum computing is a type of computing that uses quantum-mechanical
    phenomena, such as superposition and entanglement, to perform operations on data.
    These phenomena allow quantum computers to perform certain calculations much faster
    than classical computers.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLaMA-2-7B-QFT: é‡å­è®¡ç®—æ˜¯ä¸€ç§åˆ©ç”¨é‡å­åŠ›å­¦ç°è±¡ï¼Œå¦‚å åŠ å’Œçº ç¼ ï¼Œæ¥å¯¹æ•°æ®è¿›è¡Œæ“ä½œçš„è®¡ç®—ç±»å‹ã€‚è¿™äº›ç°è±¡ä½¿é‡å­è®¡ç®—æœºèƒ½å¤Ÿæ¯”ç»å…¸è®¡ç®—æœºæ›´å¿«åœ°æ‰§è¡ŒæŸäº›è®¡ç®—ã€‚'
- en: '[Knowledge Ability]'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '[çŸ¥è¯†èƒ½åŠ›]'
- en: '*Question*: Explain the process of natural selection and how it contributes
    to the evolution and adaptation of species.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '*é—®é¢˜*: è§£é‡Šè‡ªç„¶é€‰æ‹©çš„è¿‡ç¨‹ä»¥åŠå®ƒå¦‚ä½•ä¿ƒè¿›ç‰©ç§çš„è¿›åŒ–å’Œé€‚åº”ã€‚'
- en: 'Vicuna-7B*: Natural selection is the process by which species adapt to their
    environment through the survival and reproduction of individuals with favorable
    traits. Over time, these favorable traits become more common in the population,
    leading to the evolution of new species.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 'Vicuna-7B*: è‡ªç„¶é€‰æ‹©æ˜¯ç‰©ç§é€šè¿‡å…·æœ‰æœ‰åˆ©ç‰¹å¾çš„ä¸ªä½“çš„ç”Ÿå­˜å’Œç¹æ®–æ¥é€‚åº”ç¯å¢ƒçš„è¿‡ç¨‹ã€‚éšç€æ—¶é—´çš„æ¨ç§»ï¼Œè¿™äº›æœ‰åˆ©ç‰¹å¾åœ¨ç§ç¾¤ä¸­å˜å¾—è¶Šæ¥è¶Šæ™®éï¼Œä»è€Œå¯¼è‡´æ–°ç‰©ç§çš„è¿›åŒ–ã€‚'
- en: 'LLaMA-2-7B-QFT: Natural selection is a process where species that are better
    adapted to their environment tend to survive and reproduce more than those that
    are less adapted. This leads to the survival of the fittest and the gradual evolution
    of species over time.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLaMA-2-7B-QFT: è‡ªç„¶é€‰æ‹©æ˜¯ä¸€ä¸ªè¿‡ç¨‹ï¼Œå…¶ä¸­æ›´å¥½åœ°é€‚åº”ç¯å¢ƒçš„ç‰©ç§å¾€å¾€æ¯”é€‚åº”æ€§å·®çš„ç‰©ç§æ›´èƒ½ç”Ÿå­˜å’Œç¹æ®–ã€‚è¿™å¯¼è‡´äº†é€‚è€…ç”Ÿå­˜å’Œç‰©ç§çš„é€æ¸è¿›åŒ–ã€‚'
