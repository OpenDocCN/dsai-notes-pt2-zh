- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»åˆ«ï¼šæœªåˆ†ç±»
- en: 'date: 2024-09-08 18:49:47'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¥æœŸï¼š2024-09-08 18:49:47
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Accurate Block Quantization in LLMs with Outliers
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM ä¸­å¸¦æœ‰ç¦»ç¾¤å€¼çš„å‡†ç¡®å—é‡åŒ–
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2403.20137](https://ar5iv.labs.arxiv.org/html/2403.20137)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2403.20137](https://ar5iv.labs.arxiv.org/html/2403.20137)
- en: Nikita Trukhanov d-Matrix Santa Clara, CA, USA
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Nikita Trukhanov d-Matrix åœ£å…‹æ‹‰æ‹‰ï¼ŒåŠ å·ï¼Œç¾å›½
- en: ntrukhanov@d-matrix.ai â€ƒâ€ƒ Ilya Soloveychik d-Matrix Santa Clara, CA, USA
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ntrukhanov@d-matrix.ai â€ƒâ€ƒ Ilya Soloveychik d-Matrix åœ£å…‹æ‹‰æ‹‰ï¼ŒåŠ å·ï¼Œç¾å›½
- en: ilyas@d-matrix.ai
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ilyas@d-matrix.ai
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: The demand for inference on extremely large scale LLMs has seen enormous growth
    in the recent months. It made evident the colossal shortage of dedicated hardware
    capable of efficient and fast processing of the involved compute and memory movement.
    The problem is aggravated by the exploding raise in the lengths of the sequences
    being processed, since those require efficient on-chip storage of the KV-cache
    of size proportional to the sequence length. To make the required compute feasible
    and fit the involved data into available memory, numerous quantization techniques
    have been proposed that allow accurate quantization for both weights and activations.
    One of the main recent breakthroughs in this direction was introduction of the
    family of Block Floating Point (BFP) formats characterized by a block of mantissas
    with a shared scale factor. These enable memory- power-, and compute- efficient
    hardware support of the tensor operations and provide extremely good quantization
    accuracy. The main issues preventing widespread application of block formats is
    caused by the presence of outliers in weights and activations since those affect
    the accuracy of the other values in the same block. In this paper, we focus on
    the most critical problem of limited KV-cache storage. We propose a novel approach
    enabling usage of low precision BFP formats without compromising the resulting
    model accuracy. We exploit the common channel-wise patterns exhibited by the outliers
    to rearrange them in such a way, that their quantization quality is significantly
    improved. The methodology yields 2x savings in the memory footprint without significant
    degradation of the modelâ€™s accuracy. Importantly, the rearrangement of channels
    happens at the compile time and thus has no impact on the inference latency.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘å‡ ä¸ªæœˆï¼Œå¯¹æå¤§è§„æ¨¡ LLM çš„æ¨æ–­éœ€æ±‚æ€¥å‰§å¢é•¿ã€‚è¿™æ˜¾è‘—æš´éœ²å‡ºä¸“ç”¨ç¡¬ä»¶åœ¨å¤„ç†ç›¸å…³è®¡ç®—å’Œå†…å­˜ç§»åŠ¨æ–¹é¢çš„å·¨å¤§çŸ­ç¼ºã€‚é—®é¢˜ç”±äºå¤„ç†åºåˆ—é•¿åº¦çš„æ¿€å¢è€ŒåŠ å‰§ï¼Œå› ä¸ºè¿™äº›åºåˆ—éœ€è¦æŒ‰åºåˆ—é•¿åº¦æ¯”ä¾‹å­˜å‚¨
    KV-cache çš„é«˜æ•ˆç‰‡ä¸Šå­˜å‚¨ã€‚ä¸ºäº†ä½¿æ‰€éœ€çš„è®¡ç®—æˆä¸ºå¯èƒ½å¹¶å°†ç›¸å…³æ•°æ®é€‚é…åˆ°å¯ç”¨å†…å­˜ä¸­ï¼Œå·²ç»æå‡ºäº†è®¸å¤šé‡åŒ–æŠ€æœ¯ï¼Œè¿™äº›æŠ€æœ¯å…è®¸å¯¹æƒé‡å’Œæ¿€æ´»è¿›è¡Œå‡†ç¡®é‡åŒ–ã€‚æœ€è¿‘åœ¨è¿™æ–¹é¢çš„ä¸»è¦çªç ´ä¹‹ä¸€æ˜¯å¼•å…¥äº†ä¸€ç³»åˆ—å—æµ®ç‚¹ï¼ˆBFPï¼‰æ ¼å¼ï¼Œè¿™äº›æ ¼å¼ä»¥å…·æœ‰å…±äº«å°ºåº¦å› å­çš„å°¾æ•°å—ä¸ºç‰¹å¾ã€‚è¿™äº›æ ¼å¼ä½¿å¾—ç¡¬ä»¶æ”¯æŒå¼ é‡æ“ä½œçš„å†…å­˜ã€åŠŸè€—å’Œè®¡ç®—æ•ˆç‡å¾—åˆ°äº†æå¤§çš„æå‡ï¼Œå¹¶æä¾›äº†æå¥½çš„é‡åŒ–ç²¾åº¦ã€‚é˜»ç¢å—æ ¼å¼å¹¿æ³›åº”ç”¨çš„ä¸»è¦é—®é¢˜æ˜¯æƒé‡å’Œæ¿€æ´»ä¸­çš„ç¦»ç¾¤å€¼ï¼Œå› ä¸ºå®ƒä»¬ä¼šå½±å“åŒä¸€å—ä¸­å…¶ä»–å€¼çš„ç²¾åº¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å…³æ³¨äºæœ‰é™çš„
    KV-cache å­˜å‚¨çš„æœ€å…³é”®é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œä½¿å¾—åœ¨ä¸å½±å“æ¨¡å‹ç²¾åº¦çš„æƒ…å†µä¸‹ä½¿ç”¨ä½ç²¾åº¦ BFP æ ¼å¼ã€‚æˆ‘ä»¬åˆ©ç”¨ç¦»ç¾¤å€¼è¡¨ç°å‡ºçš„å¸¸è§é€šé“çº§æ¨¡å¼ï¼Œé€šè¿‡é‡æ–°æ’åˆ—è¿™äº›ç¦»ç¾¤å€¼ï¼Œæ˜¾è‘—æé«˜å®ƒä»¬çš„é‡åŒ–è´¨é‡ã€‚è¿™ä¸€æ–¹æ³•åœ¨ä¸æ˜¾è‘—é™ä½æ¨¡å‹ç²¾åº¦çš„æƒ…å†µä¸‹å®ç°äº†å†…å­˜å ç”¨çš„
    2 å€èŠ‚çœã€‚é‡è¦çš„æ˜¯ï¼Œé€šé“çš„é‡æ–°æ’åˆ—å‘ç”Ÿåœ¨ç¼–è¯‘æ—¶ï¼Œå› æ­¤å¯¹æ¨æ–­å»¶è¿Ÿæ²¡æœ‰å½±å“ã€‚
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å…³é”®è¯ï¼š
- en: LLM inference; block formats, outliers, cache.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: LLM æ¨æ–­ï¼›å—æ ¼å¼ï¼Œç¦»ç¾¤å€¼ï¼Œç¼“å­˜ã€‚
- en: I Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I å¼•è¨€
- en: Pretrained Large Language Models (LLMs) have become enormously popular in the
    recent years [[1](#bib.bibx1), [2](#bib.bibx2), [3](#bib.bibx3), [4](#bib.bibx4),
    [5](#bib.bibx5)]. Such popularity has mostly been gained due to the extremely
    high quality of the text generated by the state-of-the-art models. However, such
    improvements often come at the cost of increased model sizes which makes training
    of these large models and using them for inference highly challenging in terms
    of storage capacity, memory transfer, and compute. The architecture of the modern
    LLMs is typically based on the decoder part of a transformer [[6](#bib.bibx6)].
    While the LLM training process can fully exploit parallelization across the input
    tokens, the inference must be performed sequentially. The generation process produces
    one token on every pass over the network given the prompt and all previously generated
    tokens. The core building block of the transformer architecture â€“ the attention
    mechanism â€“ requires computation of the so called keys ${\bm{K}}$ matrices become
    prohibitively resource greedy. To avoid those redundant operations, one could
    exploit the fact that the keys and values of the already appended tokens never
    change and can therefore be cached on chip.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¿‘å¹´æ¥å˜å¾—æä¸ºæµè¡Œ [[1](#bib.bibx1), [2](#bib.bibx2), [3](#bib.bibx3),
    [4](#bib.bibx4), [5](#bib.bibx5)]ã€‚è¿™ç§æµè¡Œä¸»è¦æ˜¯ç”±äºæœ€å…ˆè¿›æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬è´¨é‡æé«˜ã€‚ç„¶è€Œï¼Œè¿™ç§æ”¹è¿›é€šå¸¸ä»¥å¢åŠ æ¨¡å‹è§„æ¨¡ä¸ºä»£ä»·ï¼Œè¿™ä½¿å¾—è¿™äº›å¤§å‹æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†åœ¨å­˜å‚¨å®¹é‡ã€å†…å­˜ä¼ è¾“å’Œè®¡ç®—æ–¹é¢å˜å¾—æå…·æŒ‘æˆ˜æ€§ã€‚ç°ä»£LLMsçš„æ¶æ„é€šå¸¸åŸºäºtransformerçš„è§£ç å™¨éƒ¨åˆ†
    [[6](#bib.bibx6)]ã€‚è™½ç„¶LLMçš„è®­ç»ƒè¿‡ç¨‹å¯ä»¥å……åˆ†åˆ©ç”¨è¾“å…¥æ ‡è®°çš„å¹¶è¡ŒåŒ–ï¼Œä½†æ¨ç†å¿…é¡»æŒ‰é¡ºåºè¿›è¡Œã€‚ç”Ÿæˆè¿‡ç¨‹åœ¨æ¯æ¬¡ç»è¿‡ç½‘ç»œæ—¶ç”Ÿæˆä¸€ä¸ªæ ‡è®°ï¼Œç»™å®šæç¤ºå’Œæ‰€æœ‰å…ˆå‰ç”Ÿæˆçš„æ ‡è®°ã€‚transformeræ¶æ„çš„æ ¸å¿ƒæ„å»ºå—â€”â€”æ³¨æ„åŠ›æœºåˆ¶â€”â€”éœ€è¦è®¡ç®—æ‰€è°“çš„é”®
    ${\bm{K}}$ çŸ©é˜µï¼Œè¿™ä½¿å¾—è®¡ç®—å˜å¾—æå…¶èµ„æºå¯†é›†ã€‚ä¸ºäº†é¿å…è¿™äº›å†—ä½™æ“ä½œï¼Œå¯ä»¥åˆ©ç”¨å·²é™„åŠ æ ‡è®°çš„é”®å’Œå€¼æ°¸è¿œä¸å˜çš„äº‹å®ï¼Œå› æ­¤å¯ä»¥åœ¨èŠ¯ç‰‡ä¸Šè¿›è¡Œç¼“å­˜ã€‚
- en: Caching ${\bm{K}}$ matrices is extremely helpful if the on-chip storage allows
    it. However, the ever growing demand for generation of longer sequence dwarfs
    any amount of on-chip storage [[7](#bib.bibx7), [8](#bib.bibx8)]. Hence, every
    possible technique must be exploited to reduce the memory footprint of the cached
    tensors. The most promising approach consists in efficient quantization of keys
    and values. To this end such algorithms as GPTQ [[9](#bib.bibx9)], SmoothQuant
    [[10](#bib.bibx10)], and many others have been proposed. For example, the GPTQ
    technique prescribes successive quantization of the weight columns in such a way
    that the rounding of every next column carefully takes into account the accumulated
    error of the previously quantized columns. The error is calculated on a small
    representative batch of data. In contrast, SmoothQuant is targeted to better quantization
    of activations. The authors notice that in the activations they were observing,
    a few channels had consistently higher values on various tokens. They introduced
    per-channel scaling factors to carry the dynamic range of activations over into
    weights. This way they transferred part of quantization burden from harder-to-quantize
    activations to easier-to-quantize weights. In all quantization approaches, the
    goal is always to enable a low-bit, e.g. 4 bits per element, storage for the tensors,
    with a common scaling vector, and, in some cases, bias vectors.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼“å­˜ ${\bm{K}}$ çŸ©é˜µåœ¨èŠ¯ç‰‡å­˜å‚¨å…è®¸çš„æƒ…å†µä¸‹æä¸ºæœ‰ç”¨ã€‚ç„¶è€Œï¼Œå¯¹æ›´é•¿åºåˆ—ç”Ÿæˆçš„ä¸æ–­å¢é•¿çš„éœ€æ±‚ä½¿å¾—ä»»ä½•èŠ¯ç‰‡ä¸Šçš„å­˜å‚¨é‡éƒ½æ˜¾å¾—å¾®ä¸è¶³é“ [[7](#bib.bibx7),
    [8](#bib.bibx8)]ã€‚å› æ­¤ï¼Œå¿…é¡»å……åˆ†åˆ©ç”¨æ‰€æœ‰å¯èƒ½çš„æŠ€æœ¯æ¥å‡å°‘ç¼“å­˜å¼ é‡çš„å†…å­˜å ç”¨ã€‚æœ€æœ‰å‰é€”çš„æ–¹æ³•æ˜¯å¯¹é”®å’Œå€¼è¿›è¡Œé«˜æ•ˆçš„é‡åŒ–ã€‚ä¸ºæ­¤ï¼Œå·²ç»æå‡ºäº†å¦‚GPTQ
    [[9](#bib.bibx9)]ã€SmoothQuant [[10](#bib.bibx10)]ç­‰ç®—æ³•ã€‚ä¾‹å¦‚ï¼ŒGPTQæŠ€æœ¯è§„å®šäº†æƒé‡åˆ—çš„è¿ç»­é‡åŒ–ï¼Œä½¿å¾—æ¯ä¸ªä¸‹ä¸€åˆ—çš„å››èˆäº”å…¥éƒ½ä»”ç»†è€ƒè™‘äº†ä¹‹å‰é‡åŒ–åˆ—çš„ç´¯è®¡è¯¯å·®ã€‚è¯¯å·®æ˜¯åœ¨ä¸€ä¸ªå°çš„ä»£è¡¨æ€§æ•°æ®æ‰¹æ¬¡ä¸Šè®¡ç®—çš„ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒSmoothQuantæ—¨åœ¨æ›´å¥½åœ°é‡åŒ–æ¿€æ´»ã€‚ä½œè€…æ³¨æ„åˆ°ï¼Œåœ¨ä»–ä»¬è§‚å¯Ÿçš„æ¿€æ´»ä¸­ï¼Œå‡ ä¸ªé€šé“åœ¨å„ç§æ ‡è®°ä¸Š
    consistentlyå…·æœ‰æ›´é«˜çš„å€¼ã€‚ä»–ä»¬å¼•å…¥äº†æ¯é€šé“çš„ç¼©æ”¾å› å­ï¼Œå°†æ¿€æ´»çš„åŠ¨æ€èŒƒå›´ä¼ é€’åˆ°æƒé‡ä¸­ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œä»–ä»¬å°†éƒ¨åˆ†é‡åŒ–è´Ÿæ‹…ä»æ›´éš¾é‡åŒ–çš„æ¿€æ´»è½¬ç§»åˆ°æ›´å®¹æ˜“é‡åŒ–çš„æƒé‡ä¸Šã€‚åœ¨æ‰€æœ‰é‡åŒ–æ–¹æ³•ä¸­ï¼Œç›®æ ‡å§‹ç»ˆæ˜¯å®ç°ä½æ¯”ç‰¹å­˜å‚¨ï¼Œä¾‹å¦‚æ¯å…ƒç´ 4æ¯”ç‰¹ï¼Œä½¿ç”¨å…±åŒçš„ç¼©æ”¾å‘é‡ï¼Œå¹¶åœ¨æŸäº›æƒ…å†µä¸‹ä½¿ç”¨åç½®å‘é‡ã€‚
- en: Further refinements of the algorithmic and software solutions have only limited
    impact on the overall efficiency if not supported by hardware. Most of the modern
    LLM models are designed and run on Graphics Processing Unit (GPUs) which exploit
    floating-point arithmetic [[11](#bib.bibx11), [12](#bib.bibx12)]. As mentioned
    earlier, the computational load required by modern transformers has reached such
    enormous volumes that traditional GPUs cannot fully meet the growing demand, pushing
    both accelerators and high performance GPUs towards narrow arithmetic. As a consequence,
    unmatched research efforts have been applied by the engineering community to replace
    narrow floating-point with even denser fixed-point representations [[13](#bib.bibx13),
    [14](#bib.bibx14), [15](#bib.bibx15), [16](#bib.bibx16)]. Despite the excellent
    gains in both speed and computational density achieved by fixed-point arithmetic,
    training using it or even half-precision floating-point arithmetic has not provided
    clear evidence in its favor due to the limited dynamic range inherent in such
    formats [[17](#bib.bibx17)].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ²¡æœ‰ç¡¬ä»¶æ”¯æŒï¼Œç®—æ³•å’Œè½¯ä»¶è§£å†³æ–¹æ¡ˆçš„è¿›ä¸€æ­¥æ”¹è¿›å¯¹æ•´ä½“æ•ˆç‡çš„å½±å“æœ‰é™ã€‚å¤§å¤šæ•°ç°ä»£LLMæ¨¡å‹è®¾è®¡å¹¶è¿è¡Œåœ¨åˆ©ç”¨æµ®ç‚¹è¿ç®—çš„å›¾å½¢å¤„ç†å•å…ƒï¼ˆGPUsï¼‰ä¸Š[[11](#bib.bibx11),
    [12](#bib.bibx12)]ã€‚å¦‚å‰æ‰€è¿°ï¼Œç°ä»£å˜å‹å™¨æ‰€éœ€çš„è®¡ç®—è´Ÿè½½å·²ç»è¾¾åˆ°å¦‚æ­¤å·¨å¤§çš„ä½“ç§¯ï¼Œä»¥è‡³äºä¼ ç»Ÿçš„GPUæ— æ³•å®Œå…¨æ»¡è¶³ä¸æ–­å¢é•¿çš„éœ€æ±‚ï¼Œè¿™æ¨åŠ¨äº†åŠ é€Ÿå™¨å’Œé«˜æ€§èƒ½GPUå‘ç‹­çª„çš„ç®—æœ¯æ–¹å‘å‘å±•ã€‚å› æ­¤ï¼Œå·¥ç¨‹ç•Œä»˜å‡ºäº†å·¨å¤§ç ”ç©¶åŠªåŠ›ï¼Œè¯•å›¾ç”¨æ›´å¯†é›†çš„å®šç‚¹è¡¨ç¤ºæ¥å–ä»£ç‹­çª„çš„æµ®ç‚¹è¡¨ç¤º[[13](#bib.bibx13),
    [14](#bib.bibx14), [15](#bib.bibx15), [16](#bib.bibx16)]ã€‚å°½ç®¡å®šç‚¹è¿ç®—åœ¨é€Ÿåº¦å’Œè®¡ç®—å¯†åº¦ä¸Šéƒ½å–å¾—äº†ä¼˜ç§€çš„æå‡ï¼Œä½†ç”±äºè¿™ç§æ ¼å¼å›ºæœ‰çš„æœ‰é™åŠ¨æ€èŒƒå›´ï¼Œä½¿ç”¨å®ƒæˆ–åŠç²¾åº¦æµ®ç‚¹è¿ç®—è¿›è¡Œè®­ç»ƒå¹¶æ²¡æœ‰æä¾›æ˜ç¡®çš„è¯æ®[[17](#bib.bibx17)]ã€‚
- en: Block Floating Point (BFP) numerical formats have received renewed interest
    recently for LLM inference applications due to their combination of wide dynamic
    range, numerical accuracy, and efficient hardware implementation of inner products
    using simple integer arithmetic [[18](#bib.bibx18), [19](#bib.bibx19), [20](#bib.bibx20),
    [21](#bib.bibx21)]. BFP formats are characterized by a block of mantissas with
    a shared scale factor. The simplest implementation has the scale factor as a power
    of two, the so-called exponent, in which case the inner product between two blocks
    involves multiplying the integer mantissas and adding the two block exponents.
    The industry has thus far mainly exploited BFP12 (with $4$ elements [[19](#bib.bibx19),
    [20](#bib.bibx20), [18](#bib.bibx18), [21](#bib.bibx21)]. Alternative formats,
    using low-bit floating point elements, with a wider range common exponent, are
    also considered [[22](#bib.bibx22)].
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå…¶å¹¿æ³›çš„åŠ¨æ€èŒƒå›´ã€æ•°å€¼ç²¾åº¦å’Œä½¿ç”¨ç®€å•æ•´æ•°è¿ç®—é«˜æ•ˆå®ç°å†…ç§¯ï¼ŒBlock Floating Pointï¼ˆBFPï¼‰æ•°å€¼æ ¼å¼æœ€è¿‘åœ¨LLMæ¨ç†åº”ç”¨ä¸­å—åˆ°äº†é‡æ–°å…³æ³¨[[18](#bib.bibx18),
    [19](#bib.bibx19), [20](#bib.bibx20), [21](#bib.bibx21)]ã€‚BFPæ ¼å¼çš„ç‰¹ç‚¹æ˜¯å…·æœ‰å…±äº«å°ºåº¦å› å­çš„å°¾æ•°å—ã€‚æœ€ç®€å•çš„å®ç°æ–¹å¼æ˜¯å°†å°ºåº¦å› å­è®¾ä¸ºäºŒçš„å¹‚ï¼Œå³æ‰€è°“çš„æŒ‡æ•°ï¼Œè¿™ç§æƒ…å†µä¸‹ï¼Œä¸¤ä¸ªå—ä¹‹é—´çš„å†…ç§¯æ¶‰åŠåˆ°ä¹˜æ³•æ•´æ•°å°¾æ•°å’ŒåŠ æ³•ä¸¤ä¸ªå—çš„æŒ‡æ•°ã€‚å› æ­¤ï¼Œä¸šå†…è¿„ä»Šä¸ºæ­¢ä¸»è¦åˆ©ç”¨BFP12ï¼ˆå…·æœ‰$4$ä¸ªå…ƒç´ [[19](#bib.bibx19),
    [20](#bib.bibx20), [18](#bib.bibx18), [21](#bib.bibx21)]ï¼‰ã€‚è¿˜è€ƒè™‘äº†ä½¿ç”¨ä½æ¯”ç‰¹æµ®ç‚¹å…ƒç´ çš„æ›¿ä»£æ ¼å¼ï¼Œè¿™äº›æ ¼å¼å…·æœ‰æ›´å®½èŒƒå›´çš„å…±åŒæŒ‡æ•°[[22](#bib.bibx22)]ã€‚
- en: One of the main numerical issues faced by the ML engineers dealing with LLMs
    both from theoretical and practical perspectives is the sporadic emergence of
    so-called outliers in weights and activations of the modern large-scale transformers
    [[10](#bib.bibx10), [7](#bib.bibx7)]. Existence of outliers becomes especially
    challenging when it comes to block formats, since presence of even a single element
    with an extremely large magnitude in a block can completely ruin the quantization
    accuracy of all the other elements in that same block.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ç†è®ºå’Œå®è·µçš„è§’åº¦æ¥çœ‹ï¼Œå¤„ç†LLMçš„MLå·¥ç¨‹å¸ˆé¢ä¸´çš„ä¸»è¦æ•°å€¼é—®é¢˜ä¹‹ä¸€æ˜¯ç°ä»£å¤§è§„æ¨¡å˜å‹å™¨æƒé‡å’Œæ¿€æ´»ä¸­æ‰€è°“çš„å¼‚å¸¸å€¼çš„å¶å‘å‡ºç°[[10](#bib.bibx10),
    [7](#bib.bibx7)]ã€‚å½“æ¶‰åŠåˆ°å—æ ¼å¼æ—¶ï¼Œå¼‚å¸¸å€¼çš„å­˜åœ¨å˜å¾—å°¤ä¸ºå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå—ä¸­å³ä½¿åªæœ‰ä¸€ä¸ªå…·æœ‰æå¤§å¹…åº¦çš„å…ƒç´ ä¹Ÿä¼šå®Œå…¨ç ´åè¯¥å—ä¸­æ‰€æœ‰å…¶ä»–å…ƒç´ çš„é‡åŒ–ç²¾åº¦ã€‚
- en: Below, we address this problem. We demonstrate how the advantages of the BFP
    quantization can be maintained when weights or activations contain numerous outliers.
    The key observation behind our approach consists in the fact that the inner product
    is invariant to synchronized reshuffling of the tensors being multiplied. For
    instance, if we focus on the ${\bm{q}}{\bm{K}}^{\top}$ happens at the compile
    time. It requires no calibration data and has no effect on the inference latency.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢ï¼Œæˆ‘ä»¬è§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬å±•ç¤ºäº†å½“æƒé‡æˆ–æ¿€æ´»åŒ…å«å¤§é‡å¼‚å¸¸å€¼æ—¶ï¼ŒBFPé‡åŒ–çš„ä¼˜åŠ¿å¦‚ä½•å¾—ä»¥ä¿æŒã€‚æˆ‘ä»¬æ–¹æ³•èƒŒåçš„å…³é”®è§‚å¯Ÿæ˜¯å†…ç§¯å¯¹è¢«ä¹˜å¼ é‡çš„åŒæ­¥é‡æ–°æ’åˆ—æ˜¯ä¸å˜çš„ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬å…³æ³¨${\bm{q}}{\bm{K}}^{\top}$å‘ç”Ÿåœ¨ç¼–è¯‘æ—¶ã€‚è¿™ä¸éœ€è¦æ ¡å‡†æ•°æ®ï¼Œå¯¹æ¨ç†å»¶è¿Ÿæ²¡æœ‰å½±å“ã€‚
- en: The rest of the paper is organized as follows. In section [II](#S2 "II Inference
    in LLMs â€£ Accurate Block Quantization in LLMs with Outliers") we describe the
    setup in more detail and define the block formats. Section [III](#S3 "III K-sort
    Algorithm â€£ Accurate Block Quantization in LLMs with Outliers") features our novel
    ${\bm{K}}$ cache containing outliers. Supporting empirical data is provided in
    Section [IV](#S4 "IV Experiments â€£ Accurate Block Quantization in LLMs with Outliers").
    We summarize our findings in Section [V](#S5 "V Conclusion â€£ Accurate Block Quantization
    in LLMs with Outliers").
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡çš„å…¶ä½™éƒ¨åˆ†ç»„ç»‡å¦‚ä¸‹ã€‚åœ¨[II](#S2 "II Inference in LLMs â€£ Accurate Block Quantization in
    LLMs with Outliers")èŠ‚ä¸­ï¼Œæˆ‘ä»¬è¯¦ç»†æè¿°äº†è®¾ç½®å¹¶å®šä¹‰äº†å—æ ¼å¼ã€‚[III](#S3 "III K-sort Algorithm â€£ Accurate
    Block Quantization in LLMs with Outliers")èŠ‚ä»‹ç»äº†æˆ‘ä»¬æ–°é¢–çš„åŒ…å«å¼‚å¸¸å€¼çš„${\bm{K}}$ç¼“å­˜ã€‚æ”¯æŒçš„å®è¯æ•°æ®åœ¨[IV](#S4
    "IV Experiments â€£ Accurate Block Quantization in LLMs with Outliers")èŠ‚ä¸­æä¾›ã€‚æˆ‘ä»¬åœ¨[V](#S5
    "V Conclusion â€£ Accurate Block Quantization in LLMs with Outliers")èŠ‚ä¸­æ€»ç»“äº†æˆ‘ä»¬çš„å‘ç°ã€‚
- en: II Inference in LLMs
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II LLMsä¸­çš„æ¨ç†
- en: In this paper, we focus on the problem of inference in LLMs. The sizes of the
    up-to-date models have become so large and the amount of compute involved became
    so enormous that efficient processing requires dedicated hardware and specialized
    algorithms.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºLLMsä¸­çš„æ¨ç†é—®é¢˜ã€‚å½“å‰æ¨¡å‹çš„è§„æ¨¡å·²ç»å˜å¾—å¦‚æ­¤åºå¤§ï¼Œæ¶‰åŠçš„è®¡ç®—é‡ä¹Ÿå˜å¾—å¦‚æ­¤å·¨å¤§ï¼Œä»¥è‡³äºé«˜æ•ˆå¤„ç†éœ€è¦ä¸“ç”¨ç¡¬ä»¶å’Œä¸“ä¸šç®—æ³•ã€‚
- en: II-A KV-cache
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A KVç¼“å­˜
- en: Inference on modern transformers essentially means sequential generation of
    tokens one by one given the initial prompt. After every pass through the modelâ€™s
    stack of decoders, the newly generated token is appended to the growing sequence
    and the process repeats with the updated context. The very nature of the attention
    mechanism requires calculation of the keys and values for the entire sequence
    generated up until current iteration. This leads to a lot of duplicated compute
    since every head inside every decoder block will repeatedly calculate the entire
    ${\bm{K}}$ contains numerous outliers.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç°ä»£å˜æ¢å™¨ä¸Šçš„æ¨ç†å®è´¨ä¸Šæ„å‘³ç€æ ¹æ®åˆå§‹æç¤ºé€ä¸ªç”Ÿæˆä»¤ç‰Œã€‚åœ¨æ¯æ¬¡é€šè¿‡æ¨¡å‹çš„è§£ç å™¨å †æ ˆåï¼Œæ–°ç”Ÿæˆçš„ä»¤ç‰Œä¼šé™„åŠ åˆ°å¢é•¿çš„åºåˆ—ä¸­ï¼Œè¿‡ç¨‹ä¼šåœ¨æ›´æ–°çš„ä¸Šä¸‹æ–‡ä¸­é‡å¤ã€‚æ³¨æ„æœºåˆ¶çš„æœ¬è´¨è¦æ±‚è®¡ç®—æ•´ä¸ªç”Ÿæˆåºåˆ—çš„é”®å’Œå€¼ï¼Œç›´åˆ°å½“å‰è¿­ä»£ã€‚è¿™å¯¼è‡´äº†å¤§é‡é‡å¤è®¡ç®—ï¼Œå› ä¸ºæ¯ä¸ªè§£ç å™¨å—ä¸­çš„æ¯ä¸ªå¤´å°†åå¤è®¡ç®—æ•´ä¸ª${\bm{K}}$ï¼Œå…¶ä¸­åŒ…å«å¤§é‡å¼‚å¸¸å€¼ã€‚
- en: II-B Block Floating Point Formats
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B å—æµ®ç‚¹æ ¼å¼
- en: The unprecedented and ever growing amount of compute and storage required by
    the modern LLMs has lead to the development of numerous new data formats and novel
    directions and techniques involving quantization of weights and activations. New
    data formats are announced every few months both by the computer science community
    training the models [[23](#bib.bibx23), [24](#bib.bibx24)] and by the manufacturers
    of hardware [[18](#bib.bibx18), [22](#bib.bibx22), [25](#bib.bibx25), [21](#bib.bibx21)].
    Different techniques are proposed separately for storage and for compute [[7](#bib.bibx7),
    [24](#bib.bibx24), [23](#bib.bibx23)].
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ç°ä»£LLMsæ‰€éœ€çš„å‰æ‰€æœªæœ‰ä¸”ä¸æ–­å¢é•¿çš„è®¡ç®—å’Œå­˜å‚¨é‡å¯¼è‡´äº†è®¸å¤šæ–°æ•°æ®æ ¼å¼ä»¥åŠæ¶‰åŠæƒé‡å’Œæ¿€æ´»é‡åŒ–çš„æ–°æ–¹å‘å’ŒæŠ€æœ¯çš„å‘å±•ã€‚è®¡ç®—æœºç§‘å­¦ç¤¾åŒºåœ¨è®­ç»ƒæ¨¡å‹æ—¶[[23](#bib.bibx23),
    [24](#bib.bibx24)]å’Œç¡¬ä»¶åˆ¶é€ å•†[[18](#bib.bibx18), [22](#bib.bibx22), [25](#bib.bibx25),
    [21](#bib.bibx21)]æ¯å‡ ä¸ªæœˆå°±ä¼šå®£å¸ƒæ–°çš„æ•°æ®æ ¼å¼ã€‚ä¸åŒçš„æŠ€æœ¯åˆ†åˆ«ç”¨äºå­˜å‚¨å’Œè®¡ç®—[[7](#bib.bibx7), [24](#bib.bibx24),
    [23](#bib.bibx23)]ã€‚
- en: In this work, we focus on an extremely promising Block Floating Point family
    of formats that has become very popular in the recent months [[18](#bib.bibx18),
    [19](#bib.bibx19), [20](#bib.bibx20), [21](#bib.bibx21)]. The idea is based on
    the observation that quite often the elements of involved tensors have comparable
    amplitudes and thus can share the same or close exponent value when written in
    floating-point notation. As a consequence, we can store entire blocks of elements
    using shared exponent and individual integer mantissas. Numerous companies design
    there hardware specifically to support this family of formats [[18](#bib.bibx18),
    [19](#bib.bibx19), [20](#bib.bibx20)]. The main advantage enjoyed by the chips
    designed to support BFP formats consists in very significant reduction of required
    storage and effectively integer matrix multiplication, see [[19](#bib.bibx19),
    [20](#bib.bibx20)] for more details. This further leads to a huge reduction in
    consumed power and energy.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å…³æ³¨çš„æ˜¯ä¸€ç§éå¸¸æœ‰å‰æ™¯çš„å—æµ®ç‚¹æ ¼å¼ï¼Œå®ƒåœ¨æœ€è¿‘å‡ ä¸ªæœˆå˜å¾—éå¸¸æµè¡Œ [[18](#bib.bibx18), [19](#bib.bibx19),
    [20](#bib.bibx20), [21](#bib.bibx21)]ã€‚è¿™ä¸ªæƒ³æ³•åŸºäºä¸€ä¸ªè§‚å¯Ÿï¼šæ¶‰åŠçš„å¼ é‡çš„å…ƒç´ å¾€å¾€å…·æœ‰å¯æ¯”çš„å¹…åº¦ï¼Œå› æ­¤åœ¨æµ®ç‚¹è¡¨ç¤ºä¸­å¯ä»¥å…±äº«ç›¸åŒæˆ–æ¥è¿‘çš„æŒ‡æ•°å€¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å…±äº«çš„æŒ‡æ•°å’Œå„ä¸ªæ•´æ•°å°¾æ•°æ¥å­˜å‚¨æ•´ä¸ªå—çš„å…ƒç´ ã€‚è®¸å¤šå…¬å¸ä¸“é—¨è®¾è®¡ç¡¬ä»¶ä»¥æ”¯æŒè¿™ç§æ ¼å¼
    [[18](#bib.bibx18), [19](#bib.bibx19), [20](#bib.bibx20)]ã€‚æ”¯æŒ BFP æ ¼å¼çš„èŠ¯ç‰‡çš„ä¸»è¦ä¼˜ç‚¹åœ¨äºæ˜¾è‘—å‡å°‘æ‰€éœ€å­˜å‚¨é‡å¹¶æœ‰æ•ˆè¿›è¡Œæ•´æ•°çŸ©é˜µä¹˜æ³•ï¼Œæ›´å¤šç»†èŠ‚è§
    [[19](#bib.bibx19), [20](#bib.bibx20)]ã€‚è¿™è¿›ä¸€æ­¥å¯¼è‡´äº†æ¶ˆè€—çš„åŠŸç‡å’Œèƒ½é‡çš„å·¨å¤§å‡å°‘ã€‚
- en: More specifically, a Block Floating Point format is characterized by the block
    size $n\in\mathbb{N}$, and their values are computed as
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å…·ä½“åœ°è¯´ï¼Œå—æµ®ç‚¹æ ¼å¼çš„ç‰¹å¾æ˜¯å—å¤§å° $n\in\mathbb{N}$ï¼Œå®ƒä»¬çš„å€¼è®¡ç®—ä¸º
- en: '|  | $\{2^{e}\cdot M_{1},\dots,2^{e}\cdot M_{n}\},$ |  | (1) |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|  | $\{2^{e}\cdot M_{1},\dots,2^{e}\cdot M_{n}\},$ |  | (1) |'
- en: where $e$-bit integer.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $e$ ä½æ•´æ•°ã€‚
- en: Blocks formats are extremely efficient for matrix operations, since dot product
    using this family of formats effective turns into integer matrix multiplication
    and simple addition of the corresponding block exponents [[20](#bib.bibx20), [18](#bib.bibx18),
    [21](#bib.bibx21)]. The typical values of $p$.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å—æ ¼å¼å¯¹äºçŸ©é˜µæ“ä½œéå¸¸é«˜æ•ˆï¼Œå› ä¸ºä½¿ç”¨è¿™ç±»æ ¼å¼çš„ç‚¹ç§¯å®é™…ä¸Šè½¬åŒ–ä¸ºæ•´æ•°çŸ©é˜µä¹˜æ³•å’Œç›¸åº”å—æŒ‡æ•°çš„ç®€å•åŠ æ³• [[20](#bib.bibx20), [18](#bib.bibx18),
    [21](#bib.bibx21)]ã€‚$p$ çš„å…¸å‹å€¼ã€‚
- en: II-C Sorting Channels of ${\bm{W}}_{\bm{k}}$
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C ${\bm{W}}_{\bm{k}}$ çš„æ’åºé€šé“
- en: To efficiently store matrix ${\bm{K}}$ faster, we propose to quantize the former
    into a low-precision block format. The definition of the BFP format, says that
    the quantization range of a block is determined by its largest (in absolute value)
    element. If some blocks contain outliers, their overall quantization accuracy
    will be poor because the smallest elements might be rounded to zero. Next we show
    how to resolve this problem.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´é«˜æ•ˆåœ°å­˜å‚¨çŸ©é˜µ ${\bm{K}}$ï¼Œæˆ‘ä»¬å»ºè®®å°†å…¶é‡åŒ–ä¸ºä½ç²¾åº¦å—æ ¼å¼ã€‚BFP æ ¼å¼çš„å®šä¹‰è¡¨ç¤ºï¼Œä¸€ä¸ªå—çš„é‡åŒ–èŒƒå›´ç”±å…¶æœ€å¤§ï¼ˆç»å¯¹å€¼ï¼‰å…ƒç´ å†³å®šã€‚å¦‚æœæŸäº›å—åŒ…å«å¼‚å¸¸å€¼ï¼Œå®ƒä»¬çš„æ€»ä½“é‡åŒ–ç²¾åº¦ä¼šå¾ˆå·®ï¼Œå› ä¸ºæœ€å°çš„å…ƒç´ å¯èƒ½è¢«èˆå…¥ä¸ºé›¶ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬å±•ç¤ºå¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜ã€‚
- en: 'The natural approach would be to sort the elements of the tensor by their absolute
    values before quantization. In that case, each block will only contain elements
    of comparable magnitudes: there will be blocks with larger elements and blocks
    with smaller elements, but we will avoid the undesirable scenario of having numerous
    blocks containing mixtures of elements of wide dynamic range. However, we must
    note that sorting tensors on the fly would be prohibitively expensive. Also, if
    we need to keep the sorting order to restore the original one for every token
    for every attention layer, it would outweigh any memory savings. Therefore, the
    brute-force sorting of elements will not work and we need a finer approach.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªç„¶çš„åšæ³•æ˜¯åœ¨é‡åŒ–ä¹‹å‰æŒ‰ç»å¯¹å€¼å¯¹å¼ é‡çš„å…ƒç´ è¿›è¡Œæ’åºã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¯ä¸ªå—åªä¼šåŒ…å«å…·æœ‰å¯æ¯”å¤§å°çš„å…ƒç´ ï¼šä¼šæœ‰åŒ…å«è¾ƒå¤§å…ƒç´ çš„å—å’ŒåŒ…å«è¾ƒå°å…ƒç´ çš„å—ï¼Œä½†æˆ‘ä»¬ä¼šé¿å…æœ‰è®¸å¤šåŒ…å«å¹¿æ³›åŠ¨æ€èŒƒå›´å…ƒç´ æ··åˆçš„å—ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¿…é¡»æ³¨æ„åˆ°ï¼ŒåŠ¨æ€æ’åºå¼ é‡å°†æ˜¯æå…¶æ˜‚è´µçš„ã€‚æ­¤å¤–ï¼Œå¦‚æœæˆ‘ä»¬éœ€è¦ä¿æŒæ’åºé¡ºåºä»¥ä¾¿åœ¨æ¯ä¸ªæ³¨æ„åŠ›å±‚æ¢å¤åŸå§‹é¡ºåºï¼Œè¿™å°†è¶…è¿‡ä»»ä½•å†…å­˜èŠ‚çœã€‚å› æ­¤ï¼Œå…ƒç´ çš„è›®åŠ›æ’åºå°†ä¸èµ·ä½œç”¨ï¼Œæˆ‘ä»¬éœ€è¦æ›´ç»†è‡´çš„æ–¹æ³•ã€‚
- en: '![Refer to caption](img/8818003d88b76896579d4cef2b6f2c06.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/8818003d88b76896579d4cef2b6f2c06.png)'
- en: 'Figure 1: Left: original ${\bm{W}}_{\bm{k}}$ since the entries of the former
    ending up in same blocks are closer in their absolute values.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 1ï¼šå·¦ä¾§ï¼šåŸå§‹ ${\bm{W}}_{\bm{k}}$ï¼Œå› ä¸ºå‰è€…çš„æ¡ç›®æœ€ç»ˆè½åœ¨ç›¸åŒçš„å—ä¸­ï¼Œå®ƒä»¬çš„ç»å¯¹å€¼æ›´æ¥è¿‘ã€‚
- en: As noted in [[7](#bib.bibx7)], the keys tend to exhibit certain outlier patterns.
    Namely, the outliers often concentrate in particular channels, which are quite
    consistent both across tokens in input sequences, and across different input sequences.
    Such behavior is usually caused by higher norms of the corresponding rows of ${\bm{W}}_{\bm{k}}$,
    then due to the linearity of inner product we can say that
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ [[7](#bib.bibx7)] æ‰€è¿°ï¼Œé”®å¾€å¾€è¡¨ç°å‡ºæŸäº›å¼‚å¸¸å€¼æ¨¡å¼ã€‚å³ï¼Œå¼‚å¸¸å€¼é€šå¸¸é›†ä¸­åœ¨ç‰¹å®šé€šé“ï¼Œè¿™åœ¨è¾“å…¥åºåˆ—çš„æ ‡è®°ä¹‹é—´ä»¥åŠä¸åŒçš„è¾“å…¥åºåˆ—ä¹‹é—´éå¸¸ä¸€è‡´ã€‚è¿™ç§è¡Œä¸ºé€šå¸¸æ˜¯ç”±äº
    ${\bm{W}}_{\bm{k}}$ çš„ç›¸åº”è¡Œçš„èŒƒæ•°è¾ƒé«˜ï¼Œç„¶åç”±äºå†…ç§¯çš„çº¿æ€§æ€§ï¼Œæˆ‘ä»¬å¯ä»¥è¯´
- en: '|  | ${\bm{W}}_{q}^{\top}\cdot{\bm{W}}_{\bm{k}}=[\pi({\bm{W}}_{\bm{q}})]^{\top}\cdot\pi({\bm{W}}_{\bm{k}}).$
    |  | (2) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\bm{W}}_{q}^{\top}\cdot{\bm{W}}_{\bm{k}}=[\pi({\bm{W}}_{\bm{q}})]^{\top}\cdot\pi({\bm{W}}_{\bm{k}}).$
    |  | (2) |'
- en: As a consequence, we have
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $1$2 |  | (3) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: Now that we have applied permutation $\pi$. The idea is illustrated by Figure
    [1](#S2.F1 "Figure 1 â€£ II-C Sorting Channels of ğ–_ğ¤ â€£ II Inference in LLMs â€£ Accurate
    Block Quantization in LLMs with Outliers"). The colors of the heat-map reflect
    the absolute values of the elements, from lower (green) to larger (red). It is
    important to note that we do not store the queries in the cache and they can therefore
    be cast to a higher precision format. To enable application of this technique
    to any transformer, we need to show how it works when rotary embeddings are applied
    to keys and queries.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»åº”ç”¨äº†ç½®æ¢ $\pi$ã€‚è¿™ä¸ªæ€æƒ³ç”±å›¾ [1](#S2.F1 "å›¾ 1 â€£ II-C ğ–_ğ¤ çš„æ’åº â€£ II åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ¨ç† â€£ å¸¦å¼‚å¸¸å€¼çš„LLMsçš„ç²¾ç¡®å—é‡åŒ–")
    è¯´æ˜ã€‚çƒ­å›¾çš„é¢œè‰²åæ˜ äº†å…ƒç´ çš„ç»å¯¹å€¼ï¼Œä»è¾ƒä½ï¼ˆç»¿è‰²ï¼‰åˆ°è¾ƒé«˜ï¼ˆçº¢è‰²ï¼‰ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬ä¸ä¼šåœ¨ç¼“å­˜ä¸­å­˜å‚¨æŸ¥è¯¢ï¼Œå› æ­¤å®ƒä»¬å¯ä»¥è¢«è½¬æ¢ä¸ºæ›´é«˜ç²¾åº¦çš„æ ¼å¼ã€‚ä¸ºäº†ä½¿è¿™ç§æŠ€æœ¯èƒ½å¤Ÿåº”ç”¨äºä»»ä½•å˜å‹å™¨ï¼Œæˆ‘ä»¬éœ€è¦å±•ç¤ºåœ¨åº”ç”¨æ—‹è½¬åµŒå…¥äºé”®å’Œå€¼æ—¶ï¼Œå®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚
- en: II-D Rotary Embeddings
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-D æ—‹è½¬åµŒå…¥
- en: Many modern LLMs use rotary positional embeddings (RoPE) [[26](#bib.bibx26)]
    to encode information about the order of tokens in the input sequence. Rotary
    embeddings are linear transformations applied to keys and queries defined as
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ä½¿ç”¨æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰[[26](#bib.bibx26)]æ¥ç¼–ç è¾“å…¥åºåˆ—ä¸­æ ‡è®°çš„é¡ºåºä¿¡æ¯ã€‚æ—‹è½¬åµŒå…¥æ˜¯åº”ç”¨äºé”®å’Œå€¼çš„çº¿æ€§å˜æ¢ï¼Œå®šä¹‰ä¸º
- en: '|  | $${\bm{R}}_{\Theta,m}^{d_{h}}=\begin{pmatrix}\cos{m\theta_{1}}&amp;-\sin{m\theta_{1}}&amp;\cdots&amp;0\\
    \sin{m\theta_{1}}&amp;\cos{m\theta_{1}}&amp;\cdots&amp;0\\'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $${\bm{R}}_{\Theta,m}^{d_{h}}=\begin{pmatrix}\cos{m\theta_{1}}&amp;-\sin{m\theta_{1}}&amp;\cdots&amp;0\\
    \sin{m\theta_{1}}&amp;\cos{m\theta_{1}}&amp;\cdots&amp;0\\'
- en: \vdots&amp;\ddots&amp;\ddots&amp;\vdots\\
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots&amp;\ddots&amp;\ddots&amp;\vdots\\
- en: 0&amp;\cdots&amp;\cos{m\theta_{d_{h}/2}}&amp;-\sin{m\theta_{d_{h}/2}}\\
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 0&amp;\cdots&amp;\cos{m\theta_{d_{h}/2}}&amp;-\sin{m\theta_{d_{h}/2}}\\
- en: 0&amp;\cdots&amp;\sin{m\theta_{d_{h}/2}}&amp;\cos{m\theta_{d_{h}/2}}\end{pmatrix},$$
    |  |
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 0&amp;\cdots&amp;\sin{m\theta_{d_{h}/2}}&amp;\cos{m\theta_{d_{h}/2}}\end{pmatrix},$$
    |  |
- en: where $m$,
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $m$ï¼Œ
- en: '|  | $${\bm{R}}^{d_{h}}_{\Theta,m}{\bm{x}}=\begin{pmatrix}x_{1}\\ x_{2}\\'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $${\bm{R}}^{d_{h}}_{\Theta,m}{\bm{x}}=\begin{pmatrix}x_{1}\\ x_{2}\\'
- en: x_{3}\\
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: x_{3}\\
- en: x_{4}\\
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: x_{4}\\
- en: \vdots\\
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots\\
- en: x_{d_{h}-1}\\
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: x_{d_{h}-1}\\
- en: x_{d_{h}}\end{pmatrix}\otimes\begin{pmatrix}\cos{m\theta_{1}}\\
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: x_{d_{h}}\end{pmatrix}\otimes\begin{pmatrix}\cos{m\theta_{1}}\\
- en: \cos{m\theta_{1}}\\
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: \cos{m\theta_{1}}\\
- en: \cos{m\theta_{2}}\\
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: \cos{m\theta_{2}}\\
- en: \cos{m\theta_{2}}\\
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: \cos{m\theta_{2}}\\
- en: \vdots\\
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots\\
- en: \cos{m\theta_{d_{h}/2}}\\
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: \cos{m\theta_{d_{h}/2}}\\
- en: \cos{m\theta_{d_{h}/2}}\end{pmatrix}\\
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: \cos{m\theta_{d_{h}/2}}\end{pmatrix}\\
- en: +\begin{pmatrix}-x_{2}\\
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: +\begin{pmatrix}-x_{2}\\
- en: x_{1}\\
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: x_{1}\\
- en: -x_{4}\\
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: -x_{4}\\
- en: x_{3}\\
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: x_{3}\\
- en: \vdots\\
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots\\
- en: -x_{d_{h}}\\
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: -x_{d_{h}}\\
- en: x_{d_{h}-1}\end{pmatrix}\otimes\begin{pmatrix}\sin{m\theta_{1}}\\
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: x_{d_{h}-1}\end{pmatrix}\otimes\begin{pmatrix}\sin{m\theta_{1}}\\
- en: \sin{m\theta_{1}}\\
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: \sin{m\theta_{1}}\\
- en: \sin{m\theta_{2}}\\
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: \sin{m\theta_{2}}\\
- en: \sin{m\theta_{2}}\\
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: \sin{m\theta_{2}}\\
- en: \vdots\\
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots\\
- en: \sin{m\theta_{d_{h}/2}}\\
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: \sin{m\theta_{d_{h}/2}}\\
- en: \sin{m\theta_{d_{h}/2}}\end{pmatrix},$$ |  | (4) |
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: \sin{m\theta_{d_{h}/2}}\end{pmatrix},$$ |  | (4) |
- en: where $\otimes$ is the element-wise product. Next we provide a general version
    of our sorting algorithm that works well when rotary embeddings are used.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\otimes$ æ˜¯é€å…ƒç´ ä¹˜ç§¯ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªé€‚ç”¨äºæ—‹è½¬åµŒå…¥çš„é€šç”¨æ’åºç®—æ³•ç‰ˆæœ¬ã€‚
- en: III K-sort Algorithm
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III K-æ’åºç®—æ³•
- en: The main idea of our ${\bm{K}}$ is known at the compile time, all the necessary
    permutations of the frequencies and signs needed for correct application of RoPE
    can be done then as well - this does not delay the inference.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ ${\bm{K}}$ çš„ä¸»è¦æ€æƒ³åœ¨ç¼–è¯‘æ—¶å·²çŸ¥ï¼Œå› æ­¤æ‰€æœ‰å¿…è¦çš„é¢‘ç‡å’Œç¬¦å·çš„ç½®æ¢å¯ä»¥åœ¨æ­¤æ—¶å®Œæˆâ€”â€”è¿™ä¸ä¼šå»¶è¿Ÿæ¨ç†è¿‡ç¨‹ã€‚
- en: Algorithm 1 ${\bm{K}}$-sort algorithm for a head
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ç®—æ³• 1 ${\bm{K}}$-æ’åºç®—æ³•
- en: '1: ${\bm{N}}_{i}\leftarrow||{\bm{W}}_{\bm{k}}[i,:]||,\;\forall i\in\{1,\dots,d_{h}\}$'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '1: ${\bm{N}}_{i}\leftarrow||{\bm{W}}_{\bm{k}}[i,:]||,\;\forall i\in\{1,\dots,d_{h}\}$'
- en: In practice, we propose to use ${\bm{K}}$-bits per element mantissa without
    any significant effect on the performance.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ ${\bm{K}}$-ä½æ¯å…ƒç´ å°¾æ•°ï¼Œè€Œä¸ä¼šå¯¹æ€§èƒ½äº§ç”Ÿé‡å¤§å½±å“ã€‚
- en: While present work concentrates on the keys ${\bm{K}}$ stored in the cache without
    any run-time overhead. For the lack of space, we postpone the details for further
    publications.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: å½“å‰å·¥ä½œé›†ä¸­äºç¼“å­˜ä¸­å­˜å‚¨çš„ ${\bm{K}}$ é”®ï¼Œæ²¡æœ‰ä»»ä½•è¿è¡Œæ—¶å¼€é”€ã€‚ç”±äºç¯‡å¹…é™åˆ¶ï¼Œæˆ‘ä»¬å°†ç»†èŠ‚æ¨è¿Ÿåˆ°åç»­å‡ºç‰ˆç‰©ä¸­ã€‚
- en: IV Experiments
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV å®éªŒ
- en: Numerous recent publications have reported the issues of outliers in K-cache
    and their significant impact on the accuracy and storage requirements [[10](#bib.bibx10),
    [7](#bib.bibx7), [27](#bib.bibx27)]. For the lack space, in this short contribution
    we focus on one of such popular LLMs, Llama2-7B-hf model [[2](#bib.bibx2)]. As
    shown in [[7](#bib.bibx7)], this network and its many relatives and variations
    exhibit the K-outliers phenomenon very clearly. In this section, we demonstrate
    the advantages of our ${\bm{K}}$-sort. This implies that the gain on larger models
    will be even more remarkable.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šè¿‘æœŸå‡ºç‰ˆç‰©æŠ¥å‘Šäº† K-cache ä¸­ç¦»ç¾¤å€¼çš„é—®é¢˜åŠå…¶å¯¹å‡†ç¡®æ€§å’Œå­˜å‚¨éœ€æ±‚çš„æ˜¾è‘—å½±å“ [[10](#bib.bibx10), [7](#bib.bibx7),
    [27](#bib.bibx27)]ã€‚ç”±äºç¯‡å¹…æœ‰é™ï¼Œæˆ‘ä»¬åœ¨è¿™ç¯‡ç®€çŸ­çš„è´¡çŒ®ä¸­ä¸“æ³¨äºä¸€ç§æµè¡Œçš„ LLMï¼Œå³ Llama2-7B-hf æ¨¡å‹ [[2](#bib.bibx2)]ã€‚å¦‚
    [[7](#bib.bibx7)] æ‰€ç¤ºï¼Œè¿™ä¸ªç½‘ç»œåŠå…¶è®¸å¤šäº²å±å’Œå˜ä½“éå¸¸æ˜æ˜¾åœ°å±•ç¤ºäº† K-outliers ç°è±¡ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„ ${\bm{K}}$-æ’åºçš„ä¼˜åŠ¿ã€‚è¿™æ„å‘³ç€åœ¨æ›´å¤§çš„æ¨¡å‹ä¸Šçš„æ”¶ç›Šå°†æ›´åŠ æ˜¾è‘—ã€‚
- en: The experiments were carried out using the default Hugging Face checkpoint without
    extra fine-tuning. The baseline perplexity of the model with FP16 weights on wikitext-2
    [[28](#bib.bibx28)] dataset is $9.4881$-s are not stored in the cache so their
    compression is not required. For fair comparison, the rest of the operations were
    performed exactly as in the baseline model - in FP16 format.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å®éªŒä½¿ç”¨äº†é»˜è®¤çš„ Hugging Face æ£€æŸ¥ç‚¹ï¼Œæ²¡æœ‰é¢å¤–çš„å¾®è°ƒã€‚å…·æœ‰ FP16 æƒé‡çš„æ¨¡å‹åœ¨ wikitext-2 [[28](#bib.bibx28)]
    æ•°æ®é›†ä¸Šçš„åŸºçº¿å›°æƒ‘åº¦ä¸º $9.4881$ï¼Œä¸åœ¨ç¼“å­˜ä¸­å­˜å‚¨ï¼Œå› æ­¤ä¸éœ€è¦å‹ç¼©ã€‚ä¸ºäº†å…¬å¹³æ¯”è¾ƒï¼Œå…¶ä½™æ“ä½œä¸åŸºçº¿æ¨¡å‹å®Œå…¨ä¸€è‡´ - ä½¿ç”¨ FP16 æ ¼å¼ã€‚
- en: Table [I](#S4.T1 "TABLE I â€£ IV Experiments â€£ Accurate Block Quantization in
    LLMs with Outliers") demonstrates the obtained results. As a sanity check, we
    see that for the block size of $128$.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ [I](#S4.T1 "è¡¨ I â€£ IV å®éªŒ â€£ LLM ä¸­çš„å‡†ç¡®å—é‡åŒ–ä¸ç¦»ç¾¤å€¼") å±•ç¤ºäº†è·å¾—çš„ç»“æœã€‚ä½œä¸ºä¸€ä¸ªç†æ™ºæ£€æŸ¥ï¼Œæˆ‘ä»¬çœ‹åˆ°å¯¹äºå—å¤§å°ä¸º $128$ã€‚
- en: 'TABLE I: LLama2-7B perplexity on wikitext-2'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¡¨ I: LLama2-7B åœ¨ wikitext-2 ä¸Šçš„å›°æƒ‘åº¦'
- en: '| format | algorithm |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| æ ¼å¼ | ç®—æ³• |'
- en: '| --- | --- |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Q | K | original | ${\bm{K}}$-sorted |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Q | K | åŸå§‹ | ${\bm{K}}$-æ’åº |'
- en: '| FP16 | FP16 | 9.4881 | 9.4881 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | FP16 | 9.4881 | 9.4881 |'
- en: '| BFP16_128 | BFP12_128 | 10.0861 | 10.0861 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| BFP16_128 | BFP12_128 | 10.0861 | 10.0861 |'
- en: '| BFP16_64 | BFP12_64 | 9.9999 | 9.6061 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| BFP16_64 | BFP12_64 | 9.9999 | 9.6061 |'
- en: '| BFP16_32 | BFP12_32 | 9.8300 | 9.5196 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| BFP16_32 | BFP12_32 | 9.8300 | 9.5196 |'
- en: V Conclusion
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V ç»“è®º
- en: In this paper, we demonstrate that simple reshuffling of the static weights
    in popular LLMs can make their quantization quality much better. Specifically,
    we advocate for the use of Block Floating Point formats and show that BFP12 format
    with $4$-cache and therefore allows generation of much longer sequences on the
    same hardware.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æµè¡Œ LLM ä¸­é™æ€æƒé‡çš„ç®€å•é‡æ–°æ’åˆ—å¯ä»¥æ˜¾è‘—æé«˜å…¶é‡åŒ–è´¨é‡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸»å¼ ä½¿ç”¨å—æµ®ç‚¹æ ¼å¼ï¼Œå¹¶å±•ç¤ºäº† BFP12 æ ¼å¼ä¸ $4$-ç¼“å­˜çš„ç»“åˆï¼Œå¯ä»¥åœ¨ç›¸åŒç¡¬ä»¶ä¸Šç”Ÿæˆæ›´é•¿çš„åºåˆ—ã€‚
- en: References
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] Susan Zhang et al. â€œOPT: open pre-trained transformer language modelsâ€
    In *arXiv preprint arXiv:2205.01068*, 2022'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Susan Zhang ç­‰. â€œOPT: å¼€æ”¾é¢„è®­ç»ƒå˜æ¢å™¨è¯­è¨€æ¨¡å‹â€ è§ *arXiv é¢„å°æœ¬ arXiv:2205.01068*ï¼Œ2022'
- en: '[2] Hugo Touvron et al. â€œLlama 2: open Foundation and Fine-Tuned Chat Modelsâ€
    In *arXiv preprint arXiv:2307.09288*, 2023'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Hugo Touvron ç­‰. â€œLlama 2: å¼€æ”¾çš„åŸºç¡€å’Œå¾®è°ƒèŠå¤©æ¨¡å‹â€ è§ *arXiv é¢„å°æœ¬ arXiv:2307.09288*ï¼Œ2023'
- en: '[3] OpenAI â€œGPT-4 Technical Reportâ€ In *arXiv preprint arXiv:2303.08774*, 2024'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] OpenAI â€œGPT-4 æŠ€æœ¯æŠ¥å‘Šâ€ è§ *arXiv é¢„å°æœ¬ arXiv:2303.08774*ï¼Œ2024'
- en: '[4] Albert Q. Jiang et al. â€œMixtral of Expertsâ€ In *arXiv preprint arXiv:2401.04088*,
    2024'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Albert Q. Jiang ç­‰. â€œä¸“å®¶çš„æ··åˆâ€ è§ *arXiv é¢„å°æœ¬ arXiv:2401.04088*ï¼Œ2024'
- en: '[5] Gemma Team et al. â€œGemma: open Models Based on Gemini Research and Technologyâ€
    In *arXiv preprint arXiv:2403.08295*, 2024'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Gemma Team ç­‰. â€œGemma: åŸºäº Gemini ç ”ç©¶å’ŒæŠ€æœ¯çš„å¼€æ”¾æ¨¡å‹â€ è§ *arXiv é¢„å°æœ¬ arXiv:2403.08295*ï¼Œ2024'
- en: '[6] A. Vaswani et al. â€œAttention is all you needâ€ In *Advances in Neural Information
    Processing Systems* 30, 2017'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] A. Vaswani ç­‰. â€œæ³¨æ„åŠ›å³ä½ æ‰€éœ€â€ è§ *ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•* 30, 2017'
- en: '[7] Coleman Hooper et al. â€œKVQuant: towards 10 Million Context Length LLM Inference
    with KV Cache Quantizationâ€ In *arXiv preprint arXiv:2401.18079*, 2024'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Coleman Hooper ç­‰. â€œKVQuant: æœç€ 1000 ä¸‡ä¸Šä¸‹æ–‡é•¿åº¦ LLM æ¨ç†ä¸ KV ç¼“å­˜é‡åŒ–â€ è§ *arXiv é¢„å°æœ¬
    arXiv:2401.18079*ï¼Œ2024'
- en: '[8] Yiran Ding et al. â€œLongRoPE: Extending LLM Context Window Beyond 2 Million
    Tokensâ€ In *arXiv preprint arXiv:2402.13753*, 2024'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Yiran Ding ç­‰ â€œLongRoPE: æ‰©å±• LLM ä¸Šä¸‹æ–‡çª—å£è‡³ 200 ä¸‡ä¸ªæ ‡è®°â€ è§äº *arXiv é¢„å°æœ¬ arXiv:2402.13753*ï¼Œ2024
    å¹´'
- en: '[9] Elias Frantar, Saleh Ashkboos, Torsten Hoefler and Dan Alistarh â€œGPTQ:
    accurate Post-Training Quantization for Generative Pre-trained Transformersâ€ In
    *arXiv preprint arXiv:2210.17323*, 2023'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Elias Frantar, Saleh Ashkboos, Torsten Hoefler å’Œ Dan Alistarh â€œGPTQ: ç²¾ç¡®çš„åè®­ç»ƒé‡åŒ–ç”Ÿæˆé¢„è®­ç»ƒå˜æ¢å™¨â€
    è§äº *arXiv é¢„å°æœ¬ arXiv:2210.17323*ï¼Œ2023 å¹´'
- en: '[10] Guangxuan Xiao et al. â€œSmoothQuant: accurate and Efficient Post-Training
    Quantization for Large Language Modelsâ€ In *arXiv preprint arXiv:2211.10438*,
    2023'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Guangxuan Xiao ç­‰ â€œSmoothQuant: å¤§å‹è¯­è¨€æ¨¡å‹çš„å‡†ç¡®å’Œé«˜æ•ˆçš„åè®­ç»ƒé‡åŒ–â€ è§äº *arXiv é¢„å°æœ¬ arXiv:2211.10438*ï¼Œ2023
    å¹´'
- en: '[11] Y.. Wang, G.-Y. Wei and D. Brooks â€œBenchmarking TPU, GPU, and CPU platforms
    for deep learningâ€ In *arXiv preprint arXiv:1907.10701*, 2019'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Y.. Wang, G.-Y. Wei å’Œ D. Brooks â€œTPUã€GPU å’Œ CPU å¹³å°çš„æ·±åº¦å­¦ä¹ åŸºå‡†æµ‹è¯•â€ è§äº *arXiv
    é¢„å°æœ¬ arXiv:1907.10701*ï¼Œ2019 å¹´'
- en: '[12] A. Srinivas et al. â€œBottleneck transformers for visual recognitionâ€ In
    *IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021, pp. 16519â€“16529'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] A. Srinivas ç­‰ â€œç”¨äºè§†è§‰è¯†åˆ«çš„ç“¶é¢ˆå˜æ¢å™¨â€ è§äº *IEEE/CVF è®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®*ï¼Œ2021 å¹´ï¼Œç¬¬ 16519â€“16529
    é¡µ'
- en: '[13] A.. Zadeh, I. Edo, O.. Awad and A. Moshovos â€œGOBO: quantizing attention-based
    NLP models for low latency and energy efficient inferenceâ€ In *IEEE/ACM International
    Symposium on Microarchitecture*, 2020, pp. 811â€“824 IEEE'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] A.. Zadeh, I. Edo, O.. Awad å’Œ A. Moshovos â€œGOBO: é‡åŒ–åŸºäºæ³¨æ„åŠ›çš„ NLP æ¨¡å‹ä»¥å®ç°ä½å»¶è¿Ÿå’ŒèŠ‚èƒ½æ¨ç†â€
    è§äº *IEEE/ACM å¾®æ¶æ„å›½é™…ç ”è®¨ä¼š*ï¼Œ2020 å¹´ï¼Œç¬¬ 811â€“824 é¡µ IEEE'
- en: '[14] O. Zafrir, G. Boudoukh, P. Izsak and M. Wasserblat â€œQ8BERT: Quantized
    8bit BERTâ€ In *Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS
    Edition*, 2019, pp. 36â€“39 IEEE'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] O. Zafrir, G. Boudoukh, P. Izsak å’Œ M. Wasserblat â€œQ8BERT: é‡åŒ–çš„ 8 ä½ BERTâ€
    è§äº *èŠ‚èƒ½æœºå™¨å­¦ä¹ ä¸è®¤çŸ¥è®¡ç®—ç ”è®¨ä¼š- NeurIPS ç‰ˆ*ï¼Œ2019 å¹´ï¼Œç¬¬ 36â€“39 é¡µ IEEE'
- en: '[15] S. Shen et al. â€œQ-BERT: hessian based ultra low precision quantization
    of BERTâ€ In *Proceedings of the AAAI Conference on Artificial Intelligence* 34.05,
    2020, pp. 8815â€“8821'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] S. Shen ç­‰ â€œQ-BERT: åŸºäº Hessian çš„è¶…ä½ç²¾åº¦ BERT é‡åŒ–â€ è§äº *AAAI äººå·¥æ™ºèƒ½ä¼šè®®è®ºæ–‡é›†* 34.05ï¼Œ2020
    å¹´ï¼Œç¬¬ 8815â€“8821 é¡µ'
- en: '[16] W. Zhang et al. â€œTernaryBERT: distillation-aware ultra-low bit BERTâ€ In
    *arXiv preprint arXiv:2009.12812*, 2020'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] W. Zhang ç­‰ â€œTernaryBERT: æ³¨é‡è’¸é¦çš„è¶…ä½ä½ BERTâ€ è§äº *arXiv é¢„å°æœ¬ arXiv:2009.12812*ï¼Œ2020
    å¹´'
- en: '[17] P. Micikevicius et al. â€œMixed precision trainingâ€ In *arXiv preprint arXiv:1710.03740*,
    2017'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] P. Micikevicius ç­‰ â€œæ··åˆç²¾åº¦è®­ç»ƒâ€ è§äº *arXiv é¢„å°æœ¬ arXiv:1710.03740*ï¼Œ2017 å¹´'
- en: '[18] Bita Darvish Rouhani et al. â€œPushing the limits of narrow precision inferencing
    at cloud scale with microsoft floating pointâ€ In *Advances in neural information
    processing systems* 33, 2020, pp. 10271â€“10281'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Bita Darvish Rouhani ç­‰ â€œåˆ©ç”¨ Microsoft æµ®ç‚¹åœ¨äº‘è§„æ¨¡ä¸Šæ¨åŠ¨ç‹­çª„ç²¾åº¦æ¨ç†çš„æé™â€ è§äº *ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•*
    33ï¼Œ2020 å¹´ï¼Œç¬¬ 10271â€“10281 é¡µ'
- en: '[19] I. Lyubomirsky and X. Wang â€œBlock Floating Point (BFP) for Efficient Deep
    Neural Net Inferenceâ€ In *IEEE P3109 Working Group, June 6*, 2022'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] I. Lyubomirsky å’Œ X. Wang â€œå—æµ®ç‚¹ï¼ˆBFPï¼‰ç”¨äºé«˜æ•ˆæ·±åº¦ç¥ç»ç½‘ç»œæ¨ç†â€ è§äº *IEEE P3109 å·¥ä½œç»„ï¼Œ6 æœˆ
    6 æ—¥*ï¼Œ2022 å¹´'
- en: '[20] I. Soloveychik, I. Lyubomirsky, X. Wang and S. Bhoja â€œBlock Format Error
    Bounds and Optimal Block Size Selectionâ€ In *arXiv preprint arXiv:2210.05470*,
    2022'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] I. Soloveychik, I. Lyubomirsky, X. Wang å’Œ S. Bhoja â€œå—æ ¼å¼è¯¯å·®ç•Œé™å’Œæœ€ä¼˜å—å¤§å°é€‰æ‹©â€ è§äº
    *arXiv é¢„å°æœ¬ arXiv:2210.05470*ï¼Œ2022 å¹´'
- en: '[21] Microsoft â€œMX Pytorch Emulation Libraryâ€ In *https://github.com/microsoft/microxcaling*,
    2023'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Microsoft â€œMX Pytorch æ¨¡æ‹Ÿåº“â€ è§äº *https://github.com/microsoft/microxcaling*ï¼Œ2023
    å¹´'
- en: '[22] Bita Darvish Rouhani et al. â€œMicroscaling Data Formats for Deep Learningâ€
    In *arXiv preprint arXiv:2310.10537*, 2023'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Bita Darvish Rouhani ç­‰ â€œæ·±åº¦å­¦ä¹ çš„å¾®ç¼©æ•°æ®æ ¼å¼â€ è§äº *arXiv é¢„å°æœ¬ arXiv:2310.10537*ï¼Œ2023
    å¹´'
- en: '[23] Shuming Ma et al. â€œThe Era of 1-bit LLMs: all large language lodels are
    in 1.58 bitsâ€ In *arXiv preprint arXiv:2402.17764*, 2024'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Shuming Ma ç­‰ â€œ1 ä½ LLM æ—¶ä»£ï¼šæ‰€æœ‰å¤§å‹è¯­è¨€æ¨¡å‹éƒ½åœ¨ 1.58 ä½â€ è§äº *arXiv é¢„å°æœ¬ arXiv:2402.17764*ï¼Œ2024
    å¹´'
- en: '[24] Houwen Peng et al. â€œFP8-lm: Training FP8 large language modelsâ€ In *arXiv
    preprint arXiv:2310.18313*, 2023'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Houwen Peng ç­‰ â€œFP8-lm: è®­ç»ƒ FP8 å¤§å‹è¯­è¨€æ¨¡å‹â€ è§äº *arXiv é¢„å°æœ¬ arXiv:2310.18313*ï¼Œ2023
    å¹´'
- en: '[25] Paulius Micikevicius et al. â€œFP8 formats for deep learningâ€ In *arXiv
    preprint arXiv:2209.05433*, 2022'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Paulius Micikevicius ç­‰ â€œç”¨äºæ·±åº¦å­¦ä¹ çš„ FP8 æ ¼å¼â€ è§äº *arXiv é¢„å°æœ¬ arXiv:2209.05433*ï¼Œ2022
    å¹´'
- en: '[26] Jianlin Su et al. â€œRoFormer: enhanced transformer with rotary position
    embeddingâ€ In *Neurocomputing* 568 Elsevier, 2024, pp. 127063'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Jianlin Su ç­‰ â€œRoFormer: å¢å¼ºçš„å¸¦æ—‹è½¬ä½ç½®åµŒå…¥çš„å˜æ¢å™¨â€ è§äº *ç¥ç»è®¡ç®—* 568 Elsevierï¼Œ2024 å¹´ï¼Œç¬¬
    127063 é¡µ'
- en: '[27] Tim Dettmers, Mike Lewis, Younes Belkada and Luke Zettlemoyer â€œLLM.int8():
    8-bit Matrix Multiplication for Transformers at Scaleâ€ In *arXiv preprint arXiv:2208.07339*,
    2022'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Tim Dettmersã€Mike Lewisã€Younes Belkada å’Œ Luke Zettlemoyer â€œLLM.int8():
    8-bit Matrix Multiplication for Transformers at Scaleâ€ è§äº *arXiv é¢„å°æœ¬ arXiv:2208.07339*ï¼Œ2022
    å¹´'
- en: '[28] Stephen Merity, Caiming Xiong, James Bradbury and Richard Socher â€œPointer
    sentinel mixture modelsâ€ In *arXiv preprint arXiv:1609.07843*, 2016'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Stephen Merityã€Caiming Xiongã€James Bradbury å’Œ Richard Socher â€œPointer
    sentinel mixture modelsâ€ è§äº *arXiv é¢„å°æœ¬ arXiv:1609.07843*ï¼Œ2016 å¹´'
