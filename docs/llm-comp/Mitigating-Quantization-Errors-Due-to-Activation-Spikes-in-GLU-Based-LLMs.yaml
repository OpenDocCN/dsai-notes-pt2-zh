- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»åˆ«ï¼šæœªåˆ†ç±»
- en: 'date: 2024-09-08 18:49:29'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¥æœŸï¼š2024-09-08 18:49:29
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¼“è§£åŸºäºGLUçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ¿€æ´»å³°å€¼å¯¼è‡´çš„é‡åŒ–è¯¯å·®
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2405.14428](https://ar5iv.labs.arxiv.org/html/2405.14428)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2405.14428](https://ar5iv.labs.arxiv.org/html/2405.14428)
- en: Jaewoo Yang, Hayun Kim, Younghoon Kim
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨å®°å®‡ï¼Œé‡‘æµ·äº‘ï¼Œé‡‘æ°¸å‹‹
- en: Department of Applied Artificial Intelligence, Hanyang University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ±‰é˜³å¤§å­¦åº”ç”¨äººå·¥æ™ºèƒ½ç³»
- en: '{onnoo, lin5478, nongaussian}@hanyang.ac.kr Corresponding Author.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{onnoo, lin5478, nongaussian}@hanyang.ac.kr é€šè®¯ä½œè€…ã€‚'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: 'Modern large language models (LLMs) have established state-of-the-art performance
    through architectural improvements, but still require significant computational
    cost for inference. In an effort to reduce the inference cost, post-training quantization
    (PTQ) has become a popular approach, quantizing weights and activations to lower
    precision, such as INT8. In this paper, we reveal the challenges of activation
    quantization in GLU variants [[40](#bib.bib40)], which are widely used in feed-forward
    network (FFN) of modern LLMs, such as LLaMA family. The problem is that severe
    local quantization errors, caused by excessive magnitudes of activation in GLU
    variants, significantly degrade the performance of the quantized LLM. We denote
    these activations as activation spikes. Our further observations provide a systematic
    pattern of activation spikes: 1) The activation spikes occur in the FFN of specific
    layers, particularly in the early and late layers, 2) The activation spikes are
    dedicated to a couple of tokens, rather than being shared across a sequence. Based
    on our observations, we propose two empirical methods, Quantization-free Module
    (QFeM) and Quantization-free Prefix (QFeP), to isolate the activation spikes during
    quantization. Our extensive experiments validate the effectiveness of the proposed
    methods for the activation quantization, especially with coarse-grained scheme,
    of latest LLMs with GLU variants, including LLaMA-2/3, Mistral, Mixtral, SOLAR,
    and Gemma. In particular, our methods enhance the current alleviation techniques
    (e.g., SmoothQuant) that fail to control the activation spikes.Â¹Â¹1Code is available
    at [https://github.com/onnoo/activation-spikes](https://github.com/onnoo/activation-spikes).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ç°ä»£çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡æ¶æ„æ”¹è¿›å»ºç«‹äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä½†ä»ç„¶éœ€è¦å¤§é‡çš„è®¡ç®—æˆæœ¬è¿›è¡Œæ¨ç†ã€‚ä¸ºäº†å‡å°‘æ¨ç†æˆæœ¬ï¼Œåè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰å·²æˆä¸ºä¸€ç§æµè¡Œçš„æ–¹æ³•ï¼Œå°†æƒé‡å’Œæ¿€æ´»é‡åŒ–ä¸ºæ›´ä½çš„ç²¾åº¦ï¼Œä¾‹å¦‚INT8ã€‚æœ¬æ–‡æ­ç¤ºäº†åœ¨å¹¿æ³›ç”¨äºç°ä»£LLMsçš„å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ä¸­çš„GLUå˜ä½“[[40](#bib.bib40)]ä¸­ï¼Œæ¿€æ´»é‡åŒ–é¢ä¸´çš„æŒ‘æˆ˜ã€‚é—®é¢˜åœ¨äºï¼Œç”±äºGLUå˜ä½“ä¸­æ¿€æ´»å€¼çš„è¿‡å¤§å¹…åº¦å¯¼è‡´çš„ä¸¥é‡å±€éƒ¨é‡åŒ–è¯¯å·®ï¼Œæ˜¾è‘—é™ä½äº†é‡åŒ–LLMçš„æ€§èƒ½ã€‚æˆ‘ä»¬å°†è¿™äº›æ¿€æ´»å€¼ç§°ä¸ºæ¿€æ´»å³°å€¼ã€‚æˆ‘ä»¬çš„è¿›ä¸€æ­¥è§‚å¯Ÿæä¾›äº†æ¿€æ´»å³°å€¼çš„ç³»ç»Ÿæ¨¡å¼ï¼š1ï¼‰æ¿€æ´»å³°å€¼å‘ç”Ÿåœ¨ç‰¹å®šå±‚çš„FFNä¸­ï¼Œç‰¹åˆ«æ˜¯åœ¨æ—©æœŸå’Œæ™šæœŸå±‚ï¼›2ï¼‰æ¿€æ´»å³°å€¼é›†ä¸­åœ¨å‡ ä¸ªç‰¹å®šçš„æ ‡è®°ä¸Šï¼Œè€Œä¸æ˜¯åœ¨æ•´ä¸ªåºåˆ—ä¸­å…±äº«ã€‚åŸºäºæˆ‘ä»¬çš„è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§ç»éªŒæ–¹æ³•ï¼šæ— é‡åŒ–æ¨¡å—ï¼ˆQFeMï¼‰å’Œæ— é‡åŒ–å‰ç¼€ï¼ˆQFePï¼‰ï¼Œä»¥åœ¨é‡åŒ–è¿‡ç¨‹ä¸­éš”ç¦»æ¿€æ´»å³°å€¼ã€‚æˆ‘ä»¬å¹¿æ³›çš„å®éªŒéªŒè¯äº†æ‰€ææ–¹æ³•åœ¨æ¿€æ´»é‡åŒ–ä¸­çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ€æ–°çš„GLUå˜ä½“LLMsï¼ˆåŒ…æ‹¬LLaMA-2/3ã€Mistralã€Mixtralã€SOLARå’ŒGemmaï¼‰çš„ç²—ç²’åº¦æ–¹æ¡ˆä¸­ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¢å¼ºäº†å½“å‰çš„ç¼“è§£æŠ€æœ¯ï¼ˆä¾‹å¦‚SmoothQuantï¼‰ï¼Œè¿™ç±»æŠ€æœ¯æœªèƒ½æ§åˆ¶æ¿€æ´»å³°å€¼ã€‚Â¹Â¹1ä»£ç å¯åœ¨
    [https://github.com/onnoo/activation-spikes](https://github.com/onnoo/activation-spikes)
    æ‰¾åˆ°ã€‚
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 ä»‹ç»
- en: Large language models (LLMs) have become a key paradigm in natural language
    processing, accelerating the release of variations within the community [[58](#bib.bib58),
    [49](#bib.bib49)]. Furthermore, latest LLMs establish state-of-the-art performance
    by training with increased scale, as well as by adopting architectural improvements
    such as GLU [[40](#bib.bib40)], RoPE [[41](#bib.bib41)], GQA [[2](#bib.bib2)],
    and MoE [[21](#bib.bib21)]. Especially, GLU (Gated Linear Unit) variants (e.g.,
    SwiGLU, GeGLU) has been adopted in the most of modern LLM architectures (e.g.,
    LLaMA family [[46](#bib.bib46)]), due to training efficiency [[40](#bib.bib40),
    [31](#bib.bib31)]. Although LLMs broaden foundational capabilities in natural
    language tasks and potential for various applications, billions of parameters
    in the large models impose considerable computational costs on end users in practice.
    To reduce GPU memory requirements and accelerate inference speed, post-training
    quantization (PTQ) offers an affordable solution by quantizing weights and activations
    into a lower precision (e.g., INT8) without a need for expensive retraining steps
    [[19](#bib.bib19), [17](#bib.bib17), [30](#bib.bib30)]. However, recent studies
    have revealed that large magnitude values at certain coordinates exist in the
    activations of LLMs, which are often called outliers, posing a key challenge in
    activation quantization [[12](#bib.bib12), [51](#bib.bib51), [1](#bib.bib1), [50](#bib.bib50)].
    Another line of works attempts to explain the role of outlier values in the attention
    mechanism [[9](#bib.bib9), [42](#bib.bib42)]. Nevertheless, current research on
    the impact of evolving LLM architectures on the outliers remains insufficient.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²æˆä¸ºè‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å…³é”®èŒƒå¼ï¼ŒåŠ é€Ÿäº†ç¤¾åŒºå†…å„ç§å˜ä½“çš„å‘å¸ƒ[[58](#bib.bib58), [49](#bib.bib49)]ã€‚æ­¤å¤–ï¼Œæœ€æ–°çš„LLMsé€šè¿‡å¢åŠ è®­ç»ƒè§„æ¨¡ä»¥åŠé‡‡ç”¨è¯¸å¦‚GLU
    [[40](#bib.bib40)]ã€RoPE [[41](#bib.bib41)]ã€GQA [[2](#bib.bib2)]å’ŒMoE [[21](#bib.bib21)]ç­‰æ¶æ„æ”¹è¿›ï¼Œå»ºç«‹äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯ï¼Œç”±äºè®­ç»ƒæ•ˆç‡[[40](#bib.bib40),
    [31](#bib.bib31)]ï¼ŒGLUï¼ˆé—¨æ§çº¿æ€§å•å…ƒï¼‰å˜ä½“ï¼ˆä¾‹å¦‚ï¼ŒSwiGLUã€GeGLUï¼‰å·²è¢«å¤§å¤šæ•°ç°ä»£LLMæ¶æ„ï¼ˆä¾‹å¦‚ï¼ŒLLaMAå®¶æ—[[46](#bib.bib46)]ï¼‰é‡‡ç”¨ã€‚å°½ç®¡LLMsæ‹“å®½äº†è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­çš„åŸºç¡€èƒ½åŠ›å¹¶å…·æœ‰å„ç§åº”ç”¨æ½œåŠ›ï¼Œä½†å¤§å‹æ¨¡å‹ä¸­çš„æ•°åäº¿ä¸ªå‚æ•°åœ¨å®è·µä¸­å¯¹æœ€ç»ˆç”¨æˆ·æ–½åŠ äº†ç›¸å½“å¤§çš„è®¡ç®—æˆæœ¬ã€‚ä¸ºäº†å‡å°‘GPUå†…å­˜éœ€æ±‚å¹¶åŠ å¿«æ¨ç†é€Ÿåº¦ï¼Œè®­ç»ƒåé‡åŒ–ï¼ˆPTQï¼‰é€šè¿‡å°†æƒé‡å’Œæ¿€æ´»é‡åŒ–ä¸ºè¾ƒä½ç²¾åº¦ï¼ˆä¾‹å¦‚ï¼ŒINT8ï¼‰æä¾›äº†ä¸€ç§ç»æµå®æƒ çš„è§£å†³æ–¹æ¡ˆï¼Œè€Œæ— éœ€æ˜‚è´µçš„é‡æ–°è®­ç»ƒæ­¥éª¤[[19](#bib.bib19),
    [17](#bib.bib17), [30](#bib.bib30)]ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼ŒLLMsçš„æ¿€æ´»å€¼åœ¨æŸäº›åæ ‡å¤„å­˜åœ¨å¤§å¹…åº¦çš„å€¼ï¼Œè¿™äº›å€¼é€šå¸¸è¢«ç§°ä¸ºç¦»ç¾¤å€¼ï¼Œå¯¹æ¿€æ´»é‡åŒ–æ„æˆäº†ä¸»è¦æŒ‘æˆ˜[[12](#bib.bib12),
    [51](#bib.bib51), [1](#bib.bib1), [50](#bib.bib50)]ã€‚å¦ä¸€ç³»åˆ—å·¥ä½œå°è¯•è§£é‡Šç¦»ç¾¤å€¼åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„ä½œç”¨[[9](#bib.bib9),
    [42](#bib.bib42)]ã€‚å°½ç®¡å¦‚æ­¤ï¼Œç›®å‰å¯¹æ¼”å˜ä¸­çš„LLMæ¶æ„å¯¹ç¦»ç¾¤å€¼å½±å“çš„ç ”ç©¶ä»ç„¶ä¸å¤Ÿå……åˆ†ã€‚
- en: 'In this paper, we present our discovery that the GLU architecture in the feed-forward
    network (FFN) generates excessively large activation values, which are responsible
    for significant local quantization errors. Specifically, we observe that these
    problematic activation values occur in specific linear layers and are dedicated
    to a couple of tokens, which will be discussed in SectionÂ [3](#S3 "3 Activation
    Spikes: Excessive Magnitude of GLU Activations â€£ Mitigating Quantization Errors
    Due to Activation Spikes in GLU-Based LLMs"). To distinguish the excessive GLU
    activations from the outliers, we refer to them as activation spikes. In light
    of our observations, we propose two empirical methods to mitigate the impact of
    activation spikes on quantization: Quantization-free Module (QFeM) and Quantization-free
    Prefix (QFeP). QFeM aims to partially exclude quantization for linear layers (or
    modules) where large quantization errors occur, instead of quantizing the entire
    linear modules in the LLM. By scoring the extent of scale disparity, QFeM selects
    linear modules to exclude. On the other hand, QFeP identifies the prefix that
    triggers activation spikes and preserves its context as a key-value (KV) cache,
    thereby preventing the recurrence of activation spikes in subsequent tokens. It
    is noteworthy that both QFeM and QFeP rely on calibration results to capture activation
    spikes in advance, without any modifications to the target LLM. This indicates
    that our methods can be integrated into any existing quantization methods.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åœ¨å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ä¸­GLUæ¶æ„ç”Ÿæˆè¿‡å¤§æ¿€æ´»å€¼çš„å‘ç°ï¼Œè¿™äº›è¿‡å¤§æ¿€æ´»å€¼å¯¼è‡´äº†æ˜¾è‘—çš„å±€éƒ¨é‡åŒ–è¯¯å·®ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è¿™äº›é—®é¢˜æ¿€æ´»å€¼å‘ç”Ÿåœ¨ç‰¹å®šçš„çº¿æ€§å±‚ï¼Œå¹¶ä¸”ä¸“é—¨ä¸å‡ ä¸ªä»¤ç‰Œç›¸å…³ï¼Œè¯¦ç»†è®¨è®ºå°†åœ¨ç¬¬[3](#S3
    "3 Activation Spikes: Excessive Magnitude of GLU Activations â€£ Mitigating Quantization
    Errors Due to Activation Spikes in GLU-Based LLMs")èŠ‚ä¸­è¿›è¡Œã€‚ä¸ºäº†åŒºåˆ†è¿‡é‡GLUæ¿€æ´»å€¼ä¸å¼‚å¸¸å€¼ï¼Œæˆ‘ä»¬å°†å…¶ç§°ä¸ºæ¿€æ´»å³°å€¼ã€‚æ ¹æ®æˆ‘ä»¬çš„è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§ç»éªŒæ–¹æ³•æ¥å‡è½»æ¿€æ´»å³°å€¼å¯¹é‡åŒ–çš„å½±å“ï¼šé‡åŒ–å…æ¨¡å—ï¼ˆQFeMï¼‰å’Œé‡åŒ–å…å‰ç¼€ï¼ˆQFePï¼‰ã€‚QFeMæ—¨åœ¨éƒ¨åˆ†æ’é™¤å‘ç”Ÿå¤§é‡åŒ–è¯¯å·®çš„çº¿æ€§å±‚ï¼ˆæˆ–æ¨¡å—ï¼‰çš„é‡åŒ–ï¼Œè€Œä¸æ˜¯å¯¹LLMä¸­çš„æ•´ä¸ªçº¿æ€§æ¨¡å—è¿›è¡Œé‡åŒ–ã€‚é€šè¿‡è¯„åˆ†å°ºåº¦å·®å¼‚çš„ç¨‹åº¦ï¼ŒQFeMé€‰æ‹©çº¿æ€§æ¨¡å—è¿›è¡Œæ’é™¤ã€‚å¦ä¸€æ–¹é¢ï¼ŒQFePè¯†åˆ«è§¦å‘æ¿€æ´»å³°å€¼çš„å‰ç¼€ï¼Œå¹¶å°†å…¶ä¸Šä¸‹æ–‡ä¿ç•™ä¸ºé”®å€¼ï¼ˆKVï¼‰ç¼“å­˜ï¼Œä»è€Œé˜²æ­¢åœ¨åç»­ä»¤ç‰Œä¸­æ¿€æ´»å³°å€¼çš„å†ç°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒQFeMå’ŒQFePéƒ½ä¾èµ–äºæ ¡å‡†ç»“æœæå‰æ•æ‰æ¿€æ´»å³°å€¼ï¼Œè€Œæ— éœ€å¯¹ç›®æ ‡LLMè¿›è¡Œä»»ä½•ä¿®æ”¹ã€‚è¿™è¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥é›†æˆåˆ°ä»»ä½•ç°æœ‰çš„é‡åŒ–æ–¹æ³•ä¸­ã€‚'
- en: In our comprehensive experiments, we demonstrate that recently released LLMs
    incorporating GLU variants struggle with activation spikes when applying activation
    quantization. Consequently, the proposed methods, QFeM and QFeP, substantially
    enhance the performance of the primitive quantization method, the round-to-nearest
    (RTN) method. Furthermore, we observe that current outlier alleviation methods
    [[51](#bib.bib51), [50](#bib.bib50)] are exposed to the activation spikes and
    benefit from our proposed methods. Compared to the strong baseline of fine-grained
    activation quantization [[55](#bib.bib55)], our methods show competitive performance,
    achieving reduced latency and memory footprint.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„å…¨é¢å®éªŒä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æœ€è¿‘å‘å¸ƒçš„åŒ…å«GLUå˜ä½“çš„LLMåœ¨åº”ç”¨æ¿€æ´»é‡åŒ–æ—¶å­˜åœ¨æ¿€æ´»å³°å€¼é—®é¢˜ã€‚å› æ­¤ï¼Œæå‡ºçš„æ–¹æ³•QFeMå’ŒQFePæ˜¾è‘—æé«˜äº†åŸå§‹é‡åŒ–æ–¹æ³•ï¼Œå³æœ€æ¥è¿‘ï¼ˆRTNï¼‰æ–¹æ³•çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å½“å‰çš„å¼‚å¸¸å€¼ç¼“è§£æ–¹æ³•[[51](#bib.bib51),
    [50](#bib.bib50)]æš´éœ²äºæ¿€æ´»å³°å€¼ï¼Œå¹¶ä»æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ä¸­å—ç›Šã€‚ä¸ç»†ç²’åº¦æ¿€æ´»é‡åŒ–çš„å¼ºåŸºå‡†[[55](#bib.bib55)]ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°å‡ºå…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œå®ç°äº†å»¶è¿Ÿå’Œå†…å­˜å ç”¨çš„å‡å°‘ã€‚
- en: 'In summary, the contributions of our work are as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“æ¥è¯´ï¼Œæˆ‘ä»¬å·¥ä½œçš„è´¡çŒ®å¦‚ä¸‹ï¼š
- en: â€¢
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: We find that the GLU architecture in modern LLMs systematically generates excessive
    activation values, which are responsible for significant performance degradation
    in activation quantization.
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å‘ç°ç°ä»£LLMä¸­çš„GLUæ¶æ„ç³»ç»Ÿæ€§åœ°ç”Ÿæˆè¿‡å¤šçš„æ¿€æ´»å€¼ï¼Œè¿™äº›æ¿€æ´»å€¼å¯¼è‡´äº†æ¿€æ´»é‡åŒ–æ€§èƒ½çš„æ˜¾è‘—ä¸‹é™ã€‚
- en: â€¢
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Based on our observations, we propose two empirical methods, QFeM and QFeP,
    which effectively exclude the activation spikes during quantization, with negligible
    computational overhead and compatibility with any existing quantization techniques.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ ¹æ®æˆ‘ä»¬çš„è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§ç»éªŒæ–¹æ³•ï¼ŒQFeM å’Œ QFePï¼Œè¿™äº›æ–¹æ³•æœ‰æ•ˆåœ°æ’é™¤äº†é‡åŒ–è¿‡ç¨‹ä¸­çš„æ¿€æ´»å³°å€¼ï¼Œè®¡ç®—å¼€é”€å¾®ä¹å…¶å¾®ï¼Œå¹¶ä¸”ä¸ç°æœ‰çš„é‡åŒ–æŠ€æœ¯å…¼å®¹ã€‚
- en: â€¢
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Our extensive experimental results validate the detrimental impact of the activation
    spikes on activation quantization, while our proposed methods consistently enhance
    the quantization performance.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„å¤§é‡å®éªŒç»“æœéªŒè¯äº†æ¿€æ´»å³°å€¼å¯¹æ¿€æ´»é‡åŒ–çš„æœ‰å®³å½±å“ï¼Œè€Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åˆ™ä¸€è‡´åœ°æé«˜äº†é‡åŒ–æ€§èƒ½ã€‚
- en: 2 Related Works
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 ç›¸å…³å·¥ä½œ
- en: Outlier Values in LLMs.
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLMä¸­çš„å¼‚å¸¸å€¼ã€‚
- en: Previously, outlier values have been observed in the transformer-based language
    models such as BERT [[14](#bib.bib14)] and early GPT [[36](#bib.bib36)] models
    through numerous studies [[24](#bib.bib24), [8](#bib.bib8), [27](#bib.bib27),
    [45](#bib.bib45), [35](#bib.bib35)]. Since the advent of LLMs [[10](#bib.bib10),
    [57](#bib.bib57)] rooted in the GPT, recent studies by [[12](#bib.bib12), [51](#bib.bib51),
    [1](#bib.bib1)] have tackled the existence of outlier values in LLMs. According
    to them, these outliers exhibit a large magnitude of values at the shared dimensions
    of hidden states across tokens. More recently, [[9](#bib.bib9), [42](#bib.bib42)]
    explain that the outliers attribute to the vertical pattern in the attention mechanism
    [[52](#bib.bib52), [25](#bib.bib25)], which influences the performance of LLMs.
    In particular, [[42](#bib.bib42)] claims a different type of outlier existing
    in the hidden states of specific tokens. However, prior studies merely focus on
    the superficial hidden states between the decoder layers. Our work provides a
    module-level investigation where quantization is applied practically, focusing
    on different LLM architectures.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹‹å‰ï¼Œé€šè¿‡å¤§é‡ç ”ç©¶å‘ç°äº†å˜å‹å™¨åŸºç¡€è¯­è¨€æ¨¡å‹ï¼ˆå¦‚BERT [[14](#bib.bib14)] å’Œæ—©æœŸ GPT [[36](#bib.bib36)] æ¨¡å‹ï¼‰ä¸­çš„å¼‚å¸¸å€¼
    [[24](#bib.bib24), [8](#bib.bib8), [27](#bib.bib27), [45](#bib.bib45), [35](#bib.bib35)]ã€‚è‡ªä»æ ¹æ¤äº
    GPT çš„ LLMs [[10](#bib.bib10), [57](#bib.bib57)] å‡ºç°ä»¥æ¥ï¼Œ[[12](#bib.bib12), [51](#bib.bib51),
    [1](#bib.bib1)] çš„è¿‘æœŸç ”ç©¶å·²æ¢è®¨äº† LLMs ä¸­å¼‚å¸¸å€¼çš„å­˜åœ¨ã€‚æ®ä»–ä»¬æ‰€è¿°ï¼Œè¿™äº›å¼‚å¸¸å€¼åœ¨è·¨ token çš„éšè—çŠ¶æ€å…±äº«ç»´åº¦ä¸Šè¡¨ç°å‡ºè¾ƒå¤§çš„å€¼å¹…åº¦ã€‚æœ€è¿‘ï¼Œ[[9](#bib.bib9),
    [42](#bib.bib42)] è§£é‡Šäº†è¿™äº›å¼‚å¸¸å€¼å½’å› äºæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„å‚ç›´æ¨¡å¼ [[52](#bib.bib52), [25](#bib.bib25)]ï¼Œè¿™ç§æ¨¡å¼å½±å“äº†
    LLMs çš„æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯ï¼Œ[[42](#bib.bib42)] å£°ç§°åœ¨ç‰¹å®š token çš„éšè—çŠ¶æ€ä¸­å­˜åœ¨ä¸åŒç±»å‹çš„å¼‚å¸¸å€¼ã€‚ç„¶è€Œï¼Œä»¥å¾€çš„ç ”ç©¶ä»…å…³æ³¨è§£ç å™¨å±‚ä¹‹é—´çš„è¡¨é¢éšè—çŠ¶æ€ã€‚æˆ‘ä»¬çš„å·¥ä½œæä¾›äº†ä¸€ä¸ªæ¨¡å—çº§åˆ«çš„è°ƒæŸ¥ï¼Œåœ¨å®é™…åº”ç”¨ä¸­ä½¿ç”¨äº†é‡åŒ–ï¼Œé‡ç‚¹å…³æ³¨ä¸åŒçš„
    LLM æ¶æ„ã€‚
- en: Post-training Quantization for LLMs.
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLMs çš„åè®­ç»ƒé‡åŒ–ã€‚
- en: Post-training quantization (PTQ) refers to the quantization of a neural network
    model to low precision, such as INT8, without additional parameter updates [[19](#bib.bib19),
    [17](#bib.bib17)]. Especially for LLMs, this approach cost-effectively achieves
    inference with low memory usage and faster inference latency by quantizing the
    weights and activations used in matrix multiplication (e.g., linear layer). However,
    because of the challenges in activation quantization of LLMs, many recent works
    are mainly focused on the weight-only quantization [[15](#bib.bib15), [23](#bib.bib23),
    [26](#bib.bib26), [11](#bib.bib11), [54](#bib.bib54), [13](#bib.bib13), [39](#bib.bib39)].
    Otherwise, the activation quantization faces inherent outliers, which hinder accurate
    quantization by reducing representation resolution. To address this challenge,
    [[12](#bib.bib12)] proposes a mixed-precision quantization method where the outlier
    dimensions are computed in high precision. [[51](#bib.bib51), [50](#bib.bib50)]
    approach migration of scale from activation to weights to alleviate the scale
    of outlier activations. Along this line of research, we propose to enhance the
    activation quantization based on our observations.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰æŒ‡å°†ç¥ç»ç½‘ç»œæ¨¡å‹é‡åŒ–ä¸ºä½ç²¾åº¦ï¼Œå¦‚ INT8ï¼Œè€Œæ— éœ€é¢å¤–çš„å‚æ•°æ›´æ–° [[19](#bib.bib19), [17](#bib.bib17)]ã€‚ç‰¹åˆ«æ˜¯å¯¹äº
    LLMsï¼Œè¿™ç§æ–¹æ³•é€šè¿‡é‡åŒ–åœ¨çŸ©é˜µä¹˜æ³•ï¼ˆä¾‹å¦‚çº¿æ€§å±‚ï¼‰ä¸­ä½¿ç”¨çš„æƒé‡å’Œæ¿€æ´»ï¼Œæˆæœ¬æ•ˆç›Šé«˜åœ°å®ç°äº†ä½å†…å­˜ä½¿ç”¨å’Œæ›´å¿«çš„æ¨ç†å»¶è¿Ÿã€‚ç„¶è€Œï¼Œç”±äº LLMs çš„æ¿€æ´»é‡åŒ–é¢ä¸´çš„æŒ‘æˆ˜ï¼Œè®¸å¤šè¿‘æœŸçš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨ä»…é‡åŒ–æƒé‡
    [[15](#bib.bib15), [23](#bib.bib23), [26](#bib.bib26), [11](#bib.bib11), [54](#bib.bib54),
    [13](#bib.bib13), [39](#bib.bib39)]ã€‚å¦åˆ™ï¼Œæ¿€æ´»é‡åŒ–é¢ä¸´å›ºæœ‰çš„å¼‚å¸¸å€¼ï¼Œè¿™ä¼šé€šè¿‡é™ä½è¡¨ç¤ºåˆ†è¾¨ç‡æ¥é˜»ç¢å‡†ç¡®çš„é‡åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œ[[12](#bib.bib12)]
    æå‡ºäº†æ··åˆç²¾åº¦é‡åŒ–æ–¹æ³•ï¼Œå…¶ä¸­å¼‚å¸¸ç»´åº¦ä»¥é«˜ç²¾åº¦è®¡ç®—ã€‚[[51](#bib.bib51), [50](#bib.bib50)] é‡‡å–äº†å°†ç¼©æ”¾ä»æ¿€æ´»è¿ç§»åˆ°æƒé‡çš„æ–¹æ³•ï¼Œä»¥å‡è½»å¼‚å¸¸æ¿€æ´»çš„ç¼©æ”¾ã€‚åœ¨è¿™æ–¹é¢çš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å»ºè®®åŸºäºæˆ‘ä»¬çš„è§‚å¯Ÿæ¥å¢å¼ºæ¿€æ´»é‡åŒ–ã€‚
- en: '3 Activation Spikes: Excessive Magnitude of GLU Activations'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 æ¿€æ´»å°–å³°ï¼šGLU æ¿€æ´»çš„è¿‡åº¦å¹…åº¦
- en: 'For clarity, "hidden states" refer to the output tensor of a transformer layer
    (or block), while "input activations" or "activations" denote the input tensor
    of a linear layer (or module) in the remain of this paper. Recent work [[42](#bib.bib42)]
    has investigated a novel type of outlier existing in the hidden states across
    modern LLMs. Although these outliers of hidden states play a crucial role in the
    attention mechanism [[42](#bib.bib42), [9](#bib.bib9), [52](#bib.bib52)], their
    relationship with input activations for quantization has not been fully explored.
    Importantly, because recent LLMs adopt Pre-LN [[53](#bib.bib53), [4](#bib.bib4)],
    which normalizes hidden states before self-attention and feed-forward network
    (FFN) blocks, the scale of hidden states does not reflect the scale of input activations
    within the transformer block. Therefore, we focus on the input activations fed
    into each linear module within the transformer block to connect to activation
    quantization. Specifically, we examine the four linear (projection) layers: query
    (parallel to key and value), out, up (parallel to gate), and down modules. For
    detailed illustration of Pre-LN transformer, please see AppendixÂ [D.1](#A4.SS1
    "D.1 Transformer Architecture. â€£ Appendix D Miscellaneous â€£ Mitigating Quantization
    Errors Due to Activation Spikes in GLU-Based LLMs").'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ¸…æ¥šèµ·è§ï¼Œâ€œéšè—çŠ¶æ€â€æŒ‡çš„æ˜¯å˜æ¢å™¨å±‚ï¼ˆæˆ–æ¨¡å—ï¼‰çš„è¾“å‡ºå¼ é‡ï¼Œè€Œâ€œè¾“å…¥æ¿€æ´»â€æˆ–â€œæ¿€æ´»â€åœ¨æœ¬æ–‡å…¶ä½™éƒ¨åˆ†æŒ‡çš„æ˜¯çº¿æ€§å±‚ï¼ˆæˆ–æ¨¡å—ï¼‰çš„è¾“å…¥å¼ é‡ã€‚è¿‘æœŸçš„å·¥ä½œ [[42](#bib.bib42)]
    ç ”ç©¶äº†ç°ä»£LLMsä¸­å­˜åœ¨çš„ä¸€ç§æ–°å‹å¼‚å¸¸å€¼ã€‚å°½ç®¡è¿™äº›éšè—çŠ¶æ€çš„å¼‚å¸¸å€¼åœ¨æ³¨æ„æœºåˆ¶ä¸­å‘æŒ¥äº†é‡è¦ä½œç”¨ [[42](#bib.bib42), [9](#bib.bib9),
    [52](#bib.bib52)]ï¼Œä½†å®ƒä»¬ä¸è¾“å…¥æ¿€æ´»çš„é‡åŒ–å…³ç³»å°šæœªå®Œå…¨æ¢è®¨ã€‚é‡è¦çš„æ˜¯ï¼Œç”±äºè¿‘æœŸLLMsé‡‡ç”¨äº†Pre-LN [[53](#bib.bib53),
    [4](#bib.bib4)]ï¼Œè¯¥æ–¹æ³•åœ¨è‡ªæ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰å—ä¹‹å‰å¯¹éšè—çŠ¶æ€è¿›è¡Œå½’ä¸€åŒ–ï¼Œå› æ­¤éšè—çŠ¶æ€çš„å°ºåº¦æ— æ³•åæ˜ å˜æ¢å™¨å—å†…è¾“å…¥æ¿€æ´»çš„å°ºåº¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å…³æ³¨è¾“å…¥æ¿€æ´»ï¼Œè¿æ¥åˆ°å˜æ¢å™¨å—å†…çš„æ¯ä¸ªçº¿æ€§æ¨¡å—çš„æ¿€æ´»é‡åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ£€æŸ¥äº†å››ä¸ªçº¿æ€§ï¼ˆæŠ•å½±ï¼‰å±‚ï¼šæŸ¥è¯¢ï¼ˆä¸é”®å’Œå€¼å¹¶è¡Œï¼‰ã€è¾“å‡ºã€å‘ä¸Šï¼ˆä¸é—¨å¹¶è¡Œï¼‰å’Œå‘ä¸‹æ¨¡å—ã€‚æœ‰å…³Pre-LNå˜æ¢å™¨çš„è¯¦ç»†è¯´æ˜ï¼Œè¯·å‚è§é™„å½•Â [D.1](#A4.SS1
    "D.1 å˜æ¢å™¨æ¶æ„ â€£ é™„å½•D æ‚é¡¹ â€£ å‡å°‘å› GLUåŸºç¡€LLMsä¸­çš„æ¿€æ´»å³°å€¼å¯¼è‡´çš„é‡åŒ–è¯¯å·®")ã€‚
- en: '![Refer to caption](img/5b194f12a0e814a168c7fa3ccfcd2320.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/5b194f12a0e814a168c7fa3ccfcd2320.png)'
- en: 'Figure 1: Calibration results on GLU-implemented and non GLU-implemented LLMs.
    We present the maximum magnitudes of input activations for each linear modules
    and layer-wise hidden states. For more results on different LLMs, see AppendixÂ [A.2](#A1.SS2
    "A.2 Other Calibration Results on GLU-implementation â€£ Appendix A Additional Calibration
    Results â€£ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based
    LLMs"), [A.3](#A1.SS3 "A.3 Other Calibration Results on Non GLU-implementation
    â€£ Appendix A Additional Calibration Results â€£ Mitigating Quantization Errors Due
    to Activation Spikes in GLU-Based LLMs").'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1ï¼šGLUå®ç°å’ŒéGLUå®ç°LLMsçš„æ ¡å‡†ç»“æœã€‚æˆ‘ä»¬å±•ç¤ºäº†æ¯ä¸ªçº¿æ€§æ¨¡å—çš„è¾“å…¥æ¿€æ´»çš„æœ€å¤§å¹…åº¦ä»¥åŠæŒ‰å±‚æ¬¡åˆ’åˆ†çš„éšè—çŠ¶æ€ã€‚æœ‰å…³ä¸åŒLLMsçš„æ›´å¤šç»“æœï¼Œè¯·å‚è§é™„å½•Â [A.2](#A1.SS2
    "A.2 GLUå®ç°çš„å…¶ä»–æ ¡å‡†ç»“æœ â€£ é™„å½•A é¢å¤–çš„æ ¡å‡†ç»“æœ â€£ å‡å°‘å› GLUåŸºç¡€LLMsä¸­çš„æ¿€æ´»å³°å€¼å¯¼è‡´çš„é‡åŒ–è¯¯å·®")ï¼Œ[A.3](#A1.SS3 "A.3
    éGLUå®ç°çš„å…¶ä»–æ ¡å‡†ç»“æœ â€£ é™„å½•A é¢å¤–çš„æ ¡å‡†ç»“æœ â€£ å‡å°‘å› GLUåŸºç¡€LLMsä¸­çš„æ¿€æ´»å³°å€¼å¯¼è‡´çš„é‡åŒ–è¯¯å·®")ã€‚
- en: 3.1 Existence of Activation Spikes in GLU Variants
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 GLUå˜ä½“ä¸­æ¿€æ´»å³°å€¼çš„å­˜åœ¨
- en: To analyze the input activations, we employ a calibration method, which is used
    to estimate the quantization factors such as scale and zero-point. For the calibration
    data, we use 512 samples randomly collected from the C4 [[37](#bib.bib37)] training
    dataset. Afterwards, we feed each sample into the LLM and monitor each hidden
    state and input activation through the decoder layers. To estimate the scale factor,
    we use absolute maximum value. The tested LLMs are listed in AppendixÂ [A.1](#A1.SS1
    "A.1 Detailed Specification of LLMs â€£ Appendix A Additional Calibration Results
    â€£ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs").
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åˆ†æè¾“å…¥æ¿€æ´»ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§æ ¡å‡†æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç”¨äºä¼°è®¡é‡åŒ–å› å­ï¼Œå¦‚å°ºåº¦å’Œé›¶ç‚¹ã€‚å¯¹äºæ ¡å‡†æ•°æ®ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä»C4 [[37](#bib.bib37)] è®­ç»ƒæ•°æ®é›†ä¸­éšæœºæ”¶é›†çš„512ä¸ªæ ·æœ¬ã€‚éšåï¼Œæˆ‘ä»¬å°†æ¯ä¸ªæ ·æœ¬è¾“å…¥åˆ°LLMä¸­ï¼Œå¹¶é€šè¿‡è§£ç å™¨å±‚ç›‘æ§æ¯ä¸ªéšè—çŠ¶æ€å’Œè¾“å…¥æ¿€æ´»ã€‚ä¸ºäº†ä¼°è®¡å°ºåº¦å› å­ï¼Œæˆ‘ä»¬ä½¿ç”¨ç»å¯¹æœ€å¤§å€¼ã€‚æµ‹è¯•è¿‡çš„LLMsåˆ—åœ¨é™„å½•Â [A.1](#A1.SS1
    "A.1 è¯¦ç»†çš„LLMsè§„èŒƒ â€£ é™„å½•A é¢å¤–çš„æ ¡å‡†ç»“æœ â€£ å‡å°‘å› GLUåŸºç¡€LLMsä¸­çš„æ¿€æ´»å³°å€¼å¯¼è‡´çš„é‡åŒ–è¯¯å·®")ä¸­ã€‚
- en: GLU-implemented LLMs exhibit activation spikes at specific layers.
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å®ç°äº†GLUçš„LLMsåœ¨ç‰¹å®šå±‚æ¬¡ä¸Šè¡¨ç°å‡ºæ¿€æ´»å³°å€¼ã€‚
- en: 'In FigureÂ [1a](#S3.F1 "Figure 1 â€£ 3 Activation Spikes: Excessive Magnitude
    of GLU Activations â€£ Mitigating Quantization Errors Due to Activation Spikes in
    GLU-Based LLMs"), we display the calibrated scale factors for the LLMs that implement
    GLU variants (e.g., SwiGLU, GeGLU). Across models, we observe a shared pattern
    of scale from the results. Within the early and late layers, the down modules
    in the FFN show noticeable magnitudes of input activations. Note that these input
    activations are derived from the Hadamard Product within GLU. Thus, the GLU variants
    generate activation spikes at the specific layers. Interestingly, we notice a
    high correlation between the emergence of activation spikes and intermediate hidden
    states of large scale. This indicates that the FFN contributes to amplifying the
    hidden states via the addition operation in the residual connection [[18](#bib.bib18)].
    Once the magnitude of the hidden states is exploded, it persists through layers
    until encounter the activation spikes at late layers.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å›¾Â [1a](#S3.F1 "å›¾ 1 â€£ 3 æ¿€æ´»å°–å³°ï¼šGLU æ¿€æ´»çš„è¿‡åº¦å¹…åº¦ â€£ ç¼“è§£ GLU åŸºäº LLM çš„æ¿€æ´»å°–å³°å¯¼è‡´çš„é‡åŒ–è¯¯å·®") ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å®æ–½
    GLU å˜ä½“ï¼ˆä¾‹å¦‚ SwiGLUã€GeGLUï¼‰çš„ LLM çš„æ ¡å‡†å°ºåº¦å› å­ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå„æ¨¡å‹ä¹‹é—´æœ‰å…±äº«çš„è§„æ¨¡æ¨¡å¼ã€‚åœ¨æ—©æœŸå’Œæ™šæœŸå±‚ä¸­ï¼ŒFFN ä¸­çš„ä¸‹æ¸¸æ¨¡å—æ˜¾ç¤ºå‡ºæ˜æ˜¾çš„è¾“å…¥æ¿€æ´»å¹…åº¦ã€‚æ³¨æ„ï¼Œè¿™äº›è¾“å…¥æ¿€æ´»æ¥æºäº
    GLU ä¸­çš„ Hadamard ä¹˜ç§¯ã€‚å› æ­¤ï¼ŒGLU å˜ä½“åœ¨ç‰¹å®šå±‚ä¸­äº§ç”Ÿæ¿€æ´»å°–å³°ã€‚æœ‰è¶£çš„æ˜¯ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°æ¿€æ´»å°–å³°çš„å‡ºç°ä¸å¤§è§„æ¨¡çš„ä¸­é—´éšè—çŠ¶æ€ä¹‹é—´å­˜åœ¨é«˜åº¦ç›¸å…³æ€§ã€‚è¿™è¡¨æ˜
    FFN é€šè¿‡æ®‹å·®è¿æ¥ä¸­çš„åŠ æ³•æ“ä½œæœ‰åŠ©äºæ”¾å¤§éšè—çŠ¶æ€ [[18](#bib.bib18)]ã€‚ä¸€æ—¦éšè—çŠ¶æ€çš„å¹…åº¦çˆ†ç‚¸ï¼Œå®ƒä¼šåœ¨å±‚é—´æŒç»­ï¼Œç›´åˆ°é‡åˆ°æ™šæœŸå±‚çš„æ¿€æ´»å°–å³°ã€‚
- en: Non GLU-implemented LLMs show modest scale distribution.
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: é GLU å®ç°çš„ LLM æ˜¾ç¤ºå‡ºé€‚åº¦çš„è§„æ¨¡åˆ†å¸ƒã€‚
- en: 'FigureÂ [1b](#S3.F1 "Figure 1 â€£ 3 Activation Spikes: Excessive Magnitude of
    GLU Activations â€£ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based
    LLMs") illustrates the calibration results for LLMs with the original feed-forward
    implementation in Transformer [[48](#bib.bib48)]. We observe that the LLMs continue
    to generate the large-scale hidden states, regardless of the GLU implementation.
    This corresponds to the observations in [[42](#bib.bib42)]. More importantly,
    our module-level results elaborate that the scale of hidden states is not transferable
    to the input activations of inner linear modules. Instead, we reveal that GLU
    variants are associated with the hidden states and generate activation spikes.
    This clarifies the quantization challenge of the GLU-implemented LLMs concentrated
    in the early and late layers. Because excessive scales of activation spikes have
    the potential to hinder the accurate quantization, we conduct an in-depth analysis
    to better understand these activation spikes in the following sections.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾Â [1b](#S3.F1 "å›¾ 1 â€£ 3 æ¿€æ´»å°–å³°ï¼šGLU æ¿€æ´»çš„è¿‡åº¦å¹…åº¦ â€£ ç¼“è§£ GLU åŸºäº LLM çš„æ¿€æ´»å°–å³°å¯¼è‡´çš„é‡åŒ–è¯¯å·®") å±•ç¤ºäº†ä½¿ç”¨
    Transformer [[48](#bib.bib48)] ä¸­åŸå§‹å‰é¦ˆå®ç°çš„ LLM çš„æ ¡å‡†ç»“æœã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œæ— è®º GLU å®ç°å¦‚ä½•ï¼ŒLLM ç»§ç»­ç”Ÿæˆå¤§è§„æ¨¡çš„éšè—çŠ¶æ€ã€‚è¿™ä¸
    [[42](#bib.bib42)] ä¸­çš„è§‚å¯Ÿç»“æœç›¸ç¬¦ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å—çº§ç»“æœé˜æ˜äº†éšè—çŠ¶æ€çš„è§„æ¨¡æ— æ³•è½¬ç§»åˆ°å†…éƒ¨çº¿æ€§æ¨¡å—çš„è¾“å…¥æ¿€æ´»ä¸Šã€‚ç›¸åï¼Œæˆ‘ä»¬æ­ç¤ºäº†
    GLU å˜ä½“ä¸éšè—çŠ¶æ€ç›¸å…³ï¼Œå¹¶äº§ç”Ÿæ¿€æ´»å°–å³°ã€‚è¿™æ¾„æ¸…äº† GLU å®ç°çš„ LLM ä¸­é‡åŒ–æŒ‘æˆ˜ä¸»è¦é›†ä¸­åœ¨æ—©æœŸå’Œæ™šæœŸå±‚ã€‚ç”±äºæ¿€æ´»å°–å³°çš„è¿‡åº¦è§„æ¨¡å¯èƒ½ä¼šå¦¨ç¢å‡†ç¡®é‡åŒ–ï¼Œæˆ‘ä»¬å°†åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†è¿›è¡Œæ·±å…¥åˆ†æï¼Œä»¥æ›´å¥½åœ°ç†è§£è¿™äº›æ¿€æ´»å°–å³°ã€‚
- en: 3.2 Token-level Scale Analysis within Activation Spikes
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 æ¿€æ´»å°–å³°ä¸­çš„ä»¤ç‰Œçº§åˆ«è§„æ¨¡åˆ†æ
- en: 'In the previous section, we observed the excessive scale of the input activations
    derived from GLU activation. When quantizing the input activations, the variance
    of input activation scales for each token affects the quantization performance
    [[55](#bib.bib55)]. To delve into the disparity between token-wise scales in the
    activation spikes, we unroll them through the sequence of tokens. FigureÂ [2](#S3.F2
    "Figure 2 â€£ 3.2 Token-level Scale Analysis within Activation Spikes â€£ 3 Activation
    Spikes: Excessive Magnitude of GLU Activations â€£ Mitigating Quantization Errors
    Due to Activation Spikes in GLU-Based LLMs") illustrates the individual input
    activation scales where the activation spike appears. Given a token sequence,
    the large magnitudes of input activations are observed in a couple of tokens,
    such as the BOS token, newline (\n), and apostrophe (''). These specific tokens
    coincide with the observations of [[42](#bib.bib42)], which suggests that such
    tokens exhibit massive values in the hidden states. Thus, the activation spike
    is associated with the process of assigning a special role to these tokens in
    later transformer layers. However, the excessive scale of specific token hinders
    the estimation of scale factor for the other tokens, such as in per-tensor quantization.
    Additionally, the largest scale is dedicated to the first instance of the specified
    token, while the following usage exhibits a modest scale. This phenomenon makes
    the quantization more complicated, as the activation spikes dynamically occur
    depending on the current input sequence.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æ¥è‡ªGLUæ¿€æ´»çš„è¾“å…¥æ¿€æ´»çš„è§„æ¨¡è¿‡å¤§ã€‚å½“é‡åŒ–è¾“å…¥æ¿€æ´»æ—¶ï¼Œæ¯ä¸ªæ ‡è®°çš„è¾“å…¥æ¿€æ´»è§„æ¨¡çš„æ–¹å·®ä¼šå½±å“é‡åŒ–æ€§èƒ½ [[55](#bib.bib55)]ã€‚ä¸ºäº†æ·±å…¥äº†è§£æ¿€æ´»å³°å€¼ä¸­æ ‡è®°é—´è§„æ¨¡çš„å·®å¼‚ï¼Œæˆ‘ä»¬é€šè¿‡æ ‡è®°åºåˆ—å±•å¼€å®ƒä»¬ã€‚å›¾
    [2](#S3.F2 "å›¾ 2 â€£ 3.2 æ¿€æ´»å³°å€¼ä¸­çš„æ ‡è®°çº§åˆ«è§„æ¨¡åˆ†æ â€£ 3 æ¿€æ´»å³°å€¼ï¼šGLU æ¿€æ´»çš„è¿‡å¤§å¹…åº¦ â€£ å‡è½»ç”±äº GLU åŸºäº LLM çš„æ¿€æ´»å³°å€¼å¯¼è‡´çš„é‡åŒ–è¯¯å·®")
    è¯´æ˜äº†æ¿€æ´»å³°å€¼å‡ºç°çš„ä¸ªåˆ«è¾“å…¥æ¿€æ´»è§„æ¨¡ã€‚ç»™å®šä¸€ä¸ªæ ‡è®°åºåˆ—ï¼Œä¼šè§‚å¯Ÿåˆ°æŸäº›æ ‡è®°çš„è¾“å…¥æ¿€æ´»å¹…åº¦è¾ƒå¤§ï¼Œä¾‹å¦‚ BOS æ ‡è®°ã€æ–°è¡Œï¼ˆ\nï¼‰å’Œæ’‡å·ï¼ˆ'ï¼‰ã€‚è¿™äº›ç‰¹å®šæ ‡è®°ä¸ [[42](#bib.bib42)]
    çš„è§‚å¯Ÿä¸€è‡´ï¼Œè¡¨æ˜è¿™äº›æ ‡è®°åœ¨éšè—çŠ¶æ€ä¸­è¡¨ç°å‡ºå·¨å¤§çš„å€¼ã€‚å› æ­¤ï¼Œæ¿€æ´»å³°å€¼ä¸åç»­å˜æ¢å™¨å±‚ä¸­ä¸ºè¿™äº›æ ‡è®°åˆ†é…ç‰¹æ®Šè§’è‰²çš„è¿‡ç¨‹ç›¸å…³ã€‚ç„¶è€Œï¼Œç‰¹å®šæ ‡è®°çš„è¿‡å¤§è§„æ¨¡ä¼šé˜»ç¢å¯¹å…¶ä»–æ ‡è®°è§„æ¨¡å› å­çš„ä¼°è®¡ï¼Œä¾‹å¦‚åœ¨æ¯å¼ é‡é‡åŒ–ä¸­ã€‚æ­¤å¤–ï¼Œæœ€å¤§çš„è§„æ¨¡ä¸“ç”¨äºæŒ‡å®šæ ‡è®°çš„ç¬¬ä¸€æ¬¡å®ä¾‹ï¼Œè€Œéšåçš„ä½¿ç”¨è¡¨ç°å‡ºé€‚åº¦çš„è§„æ¨¡ã€‚è¿™ç§ç°è±¡ä½¿å¾—é‡åŒ–å˜å¾—æ›´åŠ å¤æ‚ï¼Œå› ä¸ºæ¿€æ´»å³°å€¼ä¼šæ ¹æ®å½“å‰è¾“å…¥åºåˆ—åŠ¨æ€å‘ç”Ÿã€‚
- en: '![Refer to caption](img/a814317a5c3e527c90a69e3e916569ac.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒæ ‡é¢˜](img/a814317a5c3e527c90a69e3e916569ac.png)'
- en: 'Figure 2: Token-wise scales in a specific layer with an activation spike. When
    quantizing the input activations using a per-tensor scale, the scale of the activation
    spike dominates the scales of the other tokens. For more examples, see AppendixÂ [D.2](#A4.SS2
    "D.2 Additional Results for Token-level Scale Analysis â€£ Appendix D Miscellaneous
    â€£ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs").'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2ï¼šæ¿€æ´»å³°å€¼ä¸­æŸä¸€ç‰¹å®šå±‚çš„æ ‡è®°çº§åˆ«è§„æ¨¡ã€‚ä½¿ç”¨æ¯å¼ é‡å°ºåº¦è¿›è¡Œè¾“å…¥æ¿€æ´»é‡åŒ–æ—¶ï¼Œæ¿€æ´»å³°å€¼çš„è§„æ¨¡ä¸»å¯¼äº†å…¶ä»–æ ‡è®°çš„è§„æ¨¡ã€‚æ›´å¤šç¤ºä¾‹ï¼Œè¯·å‚è§é™„å½• [D.2](#A4.SS2
    "D.2 æ ‡è®°çº§åˆ«è§„æ¨¡åˆ†æçš„é™„åŠ ç»“æœ â€£ é™„å½• D æ‚é¡¹ â€£ å‡è½»ç”±äº GLU åŸºäº LLM çš„æ¿€æ´»å³°å€¼å¯¼è‡´çš„é‡åŒ–è¯¯å·®")ã€‚
- en: 'Table 1: Perplexity and MSE of partial activation quantization of LLMs'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 1ï¼šéƒ¨åˆ†æ¿€æ´»é‡åŒ–çš„å›°æƒ‘åº¦å’Œå‡æ–¹è¯¯å·®
- en: '| Model | Perplexity($\downarrow$) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| æ¨¡å‹ | å›°æƒ‘åº¦($\downarrow$) |'
- en: '| --- | --- | --- |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| FP16 | Top 4 | Middle 4 | Bottom 4 | Top 4 | Middle 4 | Bottom 4 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | å‰ 4 | ä¸­é—´ 4 | å 4 | å‰ 4 | ä¸­é—´ 4 | å 4 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| LLaMA-2-7B | 7.37 | 11.77 | 7.38 | 7.40 | 1908.80 | 1.03 | 12.90 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B | 7.37 | 11.77 | 7.38 | 7.40 | 1908.80 | 1.03 | 12.90 |'
- en: '| LLaMA-2-13B | 6.84 | 15.09 | 6.84 | 6.84 | 4762.11 | 0.91 | 10.38 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-13B | 6.84 | 15.09 | 6.84 | 6.84 | 4762.11 | 0.91 | 10.38 |'
- en: '| Mistral-7B | 8.35 | 69.45 | 8.35 | 8.36 | 218.60 | 0.02 | 0.18 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B | 8.35 | 69.45 | 8.35 | 8.36 | 218.60 | 0.02 | 0.18 |'
- en: '| Gemma-7B | 10.85 | 85.83 | 10.94 | 10.87 | 213.93 | 1.60 | 1.07 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-7B | 10.85 | 85.83 | 10.94 | 10.87 | 213.93 | 1.60 | 1.07 |'
- en: 3.3 Effect of Quantization on Activation Spikes
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 é‡åŒ–å¯¹æ¿€æ´»å³°å€¼çš„å½±å“
- en: 'We explore the impact of local quantization errors caused by activation spikes
    on LLM outputs. To identify the layers where activation spikes occur, we utilize
    a ratio between the maximum and median values of the token-wise input activation
    scales, instead of using the maximum scale value alone. The max-median ratio for
    linear layer $m$. This max-median ratio captures the extent to which maximum scale
    dominate the other token scales. For comparison, we choose the activation quantization
    targets as the top-4, middle-4, and bottom-4 modules, based on the max-median
    ratio in descending order. Then, we evaluate the perplexity and mean-squared error
    (MSE) using the calibration dataset. Here, the MSE is calculated for the last
    hidden states between the original (FP16) and partially quantized LLM. As shown
    in TableÂ [1](#S3.T1 "Table 1 â€£ 3.2 Token-level Scale Analysis within Activation
    Spikes â€£ 3 Activation Spikes: Excessive Magnitude of GLU Activations â€£ Mitigating
    Quantization Errors Due to Activation Spikes in GLU-Based LLMs"), quantization
    on the top-4 rated modules solely degrades the LLM performance by significant
    margins, while the other cases exhibit negligible performance changes. We consider
    these quantization-sensitive input activations (inter alia activation spikes)
    to be the quantization bottleneck, which, in this paper, refers to the quantization
    error caused by outliers.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 'æˆ‘ä»¬æ¢è®¨äº†ç”±æ¿€æ´»è„‰å†²å¼•èµ·çš„å±€éƒ¨é‡åŒ–è¯¯å·®å¯¹ LLM è¾“å‡ºçš„å½±å“ã€‚ä¸ºäº†è¯†åˆ«æ¿€æ´»è„‰å†²å‘ç”Ÿçš„å±‚ï¼Œæˆ‘ä»¬åˆ©ç”¨ä»¤ç‰Œçº§è¾“å…¥æ¿€æ´»å°ºåº¦çš„æœ€å¤§å€¼å’Œä¸­ä½å€¼ä¹‹é—´çš„æ¯”ç‡ï¼Œè€Œä¸æ˜¯å•ç‹¬ä½¿ç”¨æœ€å¤§å°ºåº¦å€¼ã€‚çº¿æ€§å±‚çš„
    max-median æ¯”ç‡ $m$ã€‚è¿™ä¸ª max-median æ¯”ç‡æ•æ‰äº†æœ€å¤§å°ºåº¦åœ¨å…¶ä»–ä»¤ç‰Œå°ºåº¦ä¸­çš„ä¸»å¯¼ç¨‹åº¦ã€‚ä¸ºäº†æ¯”è¾ƒï¼Œæˆ‘ä»¬é€‰æ‹©äº†åŸºäº max-median
    æ¯”ç‡çš„å‰å››ã€ä¸­å››å’Œåå››æ¨¡å—ä½œä¸ºæ¿€æ´»é‡åŒ–ç›®æ ‡ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨æ ¡å‡†æ•°æ®é›†è¯„ä¼°å›°æƒ‘åº¦å’Œå‡æ–¹è¯¯å·® (MSE)ã€‚è¿™é‡Œï¼ŒMSE æ˜¯è®¡ç®—åŸå§‹ (FP16) å’Œéƒ¨åˆ†é‡åŒ– LLM
    ä¹‹é—´çš„æœ€åéšè—çŠ¶æ€ã€‚æ­£å¦‚è¡¨ [1](#S3.T1 "Table 1 â€£ 3.2 Token-level Scale Analysis within Activation
    Spikes â€£ 3 Activation Spikes: Excessive Magnitude of GLU Activations â€£ Mitigating
    Quantization Errors Due to Activation Spikes in GLU-Based LLMs") æ‰€ç¤ºï¼Œå¯¹å‰å››åæ¨¡å—çš„é‡åŒ–å•ç‹¬æ˜¾è‘—é™ä½äº†
    LLM æ€§èƒ½ï¼Œè€Œå…¶ä»–æƒ…å†µè¡¨ç°å‡ºå¾®ä¸è¶³é“çš„æ€§èƒ½å˜åŒ–ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™äº›å¯¹é‡åŒ–æ•æ„Ÿçš„è¾“å…¥æ¿€æ´»ï¼ˆåŒ…æ‹¬æ¿€æ´»è„‰å†²ï¼‰æ˜¯é‡åŒ–ç“¶é¢ˆï¼Œåœ¨æœ¬æ–‡ä¸­æŒ‡çš„æ˜¯ç”±ç¦»ç¾¤å€¼å¼•èµ·çš„é‡åŒ–è¯¯å·®ã€‚'
- en: 'Furthermore, the activation spikes are conditioned on the specific context
    of the input sequence as discussed in SectionÂ [3.2](#S3.SS2 "3.2 Token-level Scale
    Analysis within Activation Spikes â€£ 3 Activation Spikes: Excessive Magnitude of
    GLU Activations â€£ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based
    LLMs"). Altogether, such dynamic bottlenecks must be handled with caution to enhance
    the quantization performance of LLMs.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ­¤å¤–ï¼Œæ¿€æ´»è„‰å†²ä¾èµ–äºè¾“å…¥åºåˆ—çš„ç‰¹å®šä¸Šä¸‹æ–‡ï¼Œå¦‚ç¬¬ [3.2](#S3.SS2 "3.2 Token-level Scale Analysis within
    Activation Spikes â€£ 3 Activation Spikes: Excessive Magnitude of GLU Activations
    â€£ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs")
    èŠ‚æ‰€è®¨è®ºçš„ã€‚æ€»ä¹‹ï¼Œè¿™ç§åŠ¨æ€ç“¶é¢ˆå¿…é¡»è°¨æ…å¤„ç†ï¼Œä»¥æå‡ LLM çš„é‡åŒ–æ€§èƒ½ã€‚'
- en: 4 Mitigating Quantization Quality Degradation Based on the Observation
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 åŸºäºè§‚å¯Ÿçš„é‡åŒ–è´¨é‡é™è§£ç¼“è§£
- en: To address the quantization bottleneck, our approach is based on the deterministic
    occurrence patterns of activation spikes. First, we utilize the observation that
    bottlenecks occur at a few specific layers. This implies that naive full quantization
    of LLMs is affected by these bottlenecks. Second, we exploit the phenomenon that
    the activation spike is derived from the first occurrence of specific tokens.
    Thus, the planned occurrence prevents recurrence in the subsequent and possibly
    future tokens. In the following sections, we propose two methods inspired the
    above insights.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è§£å†³é‡åŒ–ç“¶é¢ˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•åŸºäºæ¿€æ´»è„‰å†²çš„ç¡®å®šæ€§å‘ç”Ÿæ¨¡å¼ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ©ç”¨ç“¶é¢ˆå‘ç”Ÿåœ¨å‡ ä¸ªç‰¹å®šå±‚çš„è§‚å¯Ÿç»“æœã€‚è¿™æ„å‘³ç€ LLM çš„ç®€å•å…¨é¢é‡åŒ–å—åˆ°è¿™äº›ç“¶é¢ˆçš„å½±å“ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åˆ©ç”¨æ¿€æ´»è„‰å†²æºäºç‰¹å®šä»¤ç‰Œé¦–æ¬¡å‡ºç°çš„ç°è±¡ã€‚å› æ­¤ï¼Œè®¡åˆ’ä¸­çš„å‡ºç°ä¼šé˜»æ­¢åœ¨åç»­åŠå¯èƒ½çš„æœªæ¥ä»¤ç‰Œä¸­é‡å¤å‡ºç°ã€‚åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ–¹æ³•ï¼Œçµæ„Ÿæ¥è‡ªä¸Šè¿°è§è§£ã€‚
- en: '![Refer to caption](img/1e9188ddec89ff6ad8acdba3d6ea5a9a.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/1e9188ddec89ff6ad8acdba3d6ea5a9a.png)'
- en: 'Figure 3: Overview of QFeM and QFeP. (Left): QFeM excludes the modules whose
    $r^{(m)}$ from quantization. (Right): QFeP computes in advance the prefix of activation
    spikes and utilizes solely their KV cache during the quantization phase, effectively
    preventing further activation spikes in subsequent sequences.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 3: QFeM å’Œ QFeP çš„æ¦‚è§ˆã€‚ï¼ˆå·¦ï¼‰ï¼šQFeM æ’é™¤äº† $r^{(m)}$ é‡åŒ–æ¨¡å—ã€‚ï¼ˆå³ï¼‰ï¼šQFeP é¢„å…ˆè®¡ç®—æ¿€æ´»è„‰å†²çš„å‰ç¼€ï¼Œå¹¶åœ¨é‡åŒ–é˜¶æ®µä»…åˆ©ç”¨å®ƒä»¬çš„
    KV ç¼“å­˜ï¼Œæœ‰æ•ˆé˜²æ­¢åœ¨åç»­åºåˆ—ä¸­è¿›ä¸€æ­¥çš„æ¿€æ´»è„‰å†²ã€‚'
- en: 4.1 Quantization-free Module (QFeM)
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 æ— é‡åŒ–æ¨¡å— (QFeM)
- en: In the full quantization of LLM, all linear layers within the LLM are quantized.
    Among these linear layers, we propose omitting the quantization of input activations
    for linear layers where significant quantization errors are caused by activation
    spikes. To be noted, increasing the number of unquantized modules exhibits a trade-off
    between the inference latency and the model performance. Thus, determining which
    module should be quantized (or left unquantized) is crucial to retain the efficacy
    of quantization. Here, we use the max-median ratio $r^{(m)}$. For clarity, we
    treat sibling linear layers, such as query-key-value, as a single linear layer.
    To control the impact of activation quantization only, we leave the weight parameters
    in unquantized linear layers as INT8 and dequantize them into FP16 during matrix
    multiplication with the incoming activations, operating as weight-only quantization.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨LLMçš„å…¨é¢é‡åŒ–ä¸­ï¼ŒLLMä¸­çš„æ‰€æœ‰çº¿æ€§å±‚éƒ½è¢«é‡åŒ–ã€‚åœ¨è¿™äº›çº¿æ€§å±‚ä¸­ï¼Œæˆ‘ä»¬å»ºè®®çœç•¥å¯¹è¾“å…¥æ¿€æ´»çš„é‡åŒ–ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¿€æ´»å³°å€¼å¯¼è‡´æ˜¾è‘—é‡åŒ–è¯¯å·®çš„çº¿æ€§å±‚ä¸­ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¢åŠ æœªé‡åŒ–æ¨¡å—çš„æ•°é‡ä¼šåœ¨æ¨ç†å»¶è¿Ÿå’Œæ¨¡å‹æ€§èƒ½ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚å› æ­¤ï¼Œç¡®å®šå“ªä¸ªæ¨¡å—åº”è¯¥è¢«é‡åŒ–ï¼ˆæˆ–ä¿æŒæœªé‡åŒ–ï¼‰å¯¹äºä¿æŒé‡åŒ–çš„æœ‰æ•ˆæ€§è‡³å…³é‡è¦ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨æœ€å¤§ä¸­ä½æ•°æ¯”ç‡$r^{(m)}$ã€‚ä¸ºäº†æ¸…æ™°èµ·è§ï¼Œæˆ‘ä»¬å°†åŒèƒçº¿æ€§å±‚ï¼ˆå¦‚æŸ¥è¯¢-é”®-å€¼ï¼‰è§†ä¸ºä¸€ä¸ªå•ä¸€çš„çº¿æ€§å±‚ã€‚ä¸ºäº†ä»…æ§åˆ¶æ¿€æ´»é‡åŒ–çš„å½±å“ï¼Œæˆ‘ä»¬å°†æœªé‡åŒ–çº¿æ€§å±‚ä¸­çš„æƒé‡å‚æ•°ä¿ç•™ä¸ºINT8ï¼Œå¹¶åœ¨ä¸ä¼ å…¥æ¿€æ´»è¿›è¡ŒçŸ©é˜µä¹˜æ³•æ—¶å°†å…¶è§£é‡åŒ–ä¸ºFP16ï¼Œè¿™æ ·çš„æ“ä½œè¢«ç§°ä¸ºä»…æƒé‡é‡åŒ–ã€‚
- en: Optimizing the threshold $\alpha$.
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–é˜ˆå€¼$\alpha$ã€‚
- en: '![Refer to caption](img/40bf1f893d9f6dc532f173d7c8d2582e.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒå›¾ç¤º](img/40bf1f893d9f6dc532f173d7c8d2582e.png)'
- en: 'Figure 4: Trade-off between perplexity (stands for performance) and $|M_{unq}|$
    for LLaMA-2-13B model.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4ï¼šLLaMA-2-13Bæ¨¡å‹çš„å›°æƒ‘åº¦ï¼ˆä»£è¡¨æ€§èƒ½ï¼‰ä¸$|M_{unq}|$ä¹‹é—´çš„æƒè¡¡ã€‚
- en: 'To calculate the activation scale ratio for each linear layer, we first gather
    token-wise input activation scales from the calibration examples discussed in
    SectionÂ [3.1](#S3.SS1 "3.1 Existence of Activation Spikes in GLU Variants â€£ 3
    Activation Spikes: Excessive Magnitude of GLU Activations â€£ Mitigating Quantization
    Errors Due to Activation Spikes in GLU-Based LLMs"). Exceptionally, for FFN experts
    in the mixture of experts (MoE) architectures like the Mixtral model [[21](#bib.bib21)],
    calibration is performed separately. After determining these ratios, we use binary
    search to set the threshold value $\alpha$ and its impact on performance is depicted
    in FigureÂ [4](#S4.F4 "Figure 4 â€£ Optimizing the threshold ğ›¼. â€£ 4.1 Quantization-free
    Module (QFeM) â€£ 4 Mitigating Quantization Quality Degradation Based on the Observation
    â€£ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs"),
    demonstrating how full quantization can degrade performance. Rather than fully
    quantizing, we identify an optimal threshold by finding the intersection of two
    performance curves; in FigureÂ [4](#S4.F4 "Figure 4 â€£ Optimizing the threshold
    ğ›¼. â€£ 4.1 Quantization-free Module (QFeM) â€£ 4 Mitigating Quantization Quality Degradation
    Based on the Observation â€£ Mitigating Quantization Errors Due to Activation Spikes
    in GLU-Based LLMs"), this threshold is approximately 16\. Details on the QFeM
    implementation are provided in TableÂ [2](#S4.T2 "Table 2 â€£ Implementation Details.
    â€£ 4.2 Quantization-free Prefix (QFeP) â€£ 4 Mitigating Quantization Quality Degradation
    Based on the Observation â€£ Mitigating Quantization Errors Due to Activation Spikes
    in GLU-Based LLMs").'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¸ºäº†è®¡ç®—æ¯ä¸ªçº¿æ€§å±‚çš„æ¿€æ´»ç¼©æ”¾æ¯”ç‡ï¼Œæˆ‘ä»¬é¦–å…ˆä»ç¬¬[3.1èŠ‚](#S3.SS1 "3.1 Existence of Activation Spikes in
    GLU Variants â€£ 3 Activation Spikes: Excessive Magnitude of GLU Activations â€£ Mitigating
    Quantization Errors Due to Activation Spikes in GLU-Based LLMs")è®¨è®ºçš„æ ¡å‡†ç¤ºä¾‹ä¸­æ”¶é›†é€ä¸ªä»¤ç‰Œçš„è¾“å…¥æ¿€æ´»ç¼©æ”¾ã€‚ç‰¹åˆ«åœ°ï¼Œå¯¹äºæ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„ä¸­çš„FFNä¸“å®¶ï¼ˆå¦‚Mixtralæ¨¡å‹[[21](#bib.bib21)]ï¼‰ï¼Œæ ¡å‡†æ˜¯å•ç‹¬è¿›è¡Œçš„ã€‚åœ¨ç¡®å®šè¿™äº›æ¯”ç‡åï¼Œæˆ‘ä»¬ä½¿ç”¨äºŒåˆ†æœç´¢æ¥è®¾ç½®é˜ˆå€¼$\alpha$ï¼Œå…¶å¯¹æ€§èƒ½çš„å½±å“å¦‚å›¾[4](#S4.F4
    "Figure 4 â€£ Optimizing the threshold ğ›¼. â€£ 4.1 Quantization-free Module (QFeM)
    â€£ 4 Mitigating Quantization Quality Degradation Based on the Observation â€£ Mitigating
    Quantization Errors Due to Activation Spikes in GLU-Based LLMs")æ‰€ç¤ºï¼Œå±•ç¤ºäº†å…¨é¢é‡åŒ–å¦‚ä½•é™ä½æ€§èƒ½ã€‚æˆ‘ä»¬é€šè¿‡æ‰¾åˆ°ä¸¤ä¸ªæ€§èƒ½æ›²çº¿çš„äº¤ç‚¹æ¥è¯†åˆ«æœ€ä½³é˜ˆå€¼ï¼›åœ¨å›¾[4](#S4.F4
    "Figure 4 â€£ Optimizing the threshold ğ›¼. â€£ 4.1 Quantization-free Module (QFeM)
    â€£ 4 Mitigating Quantization Quality Degradation Based on the Observation â€£ Mitigating
    Quantization Errors Due to Activation Spikes in GLU-Based LLMs")ä¸­ï¼Œè¿™ä¸ªé˜ˆå€¼å¤§çº¦ä¸º16ã€‚QFeMå®æ–½çš„è¯¦ç»†ä¿¡æ¯è§è¡¨[2](#S4.T2
    "Table 2 â€£ Implementation Details. â€£ 4.2 Quantization-free Prefix (QFeP) â€£ 4 Mitigating
    Quantization Quality Degradation Based on the Observation â€£ Mitigating Quantization
    Errors Due to Activation Spikes in GLU-Based LLMs")ã€‚'
- en: 4.2 Quantization-free Prefix (QFeP)
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 æ— é‡åŒ–å‰ç¼€ï¼ˆQFePï¼‰
- en: 'Orthogonal to the QFeM, we propose Quantization-free Prefix (QFeP) that mitigates
    the quantization errors by precomputing the prefix (or short prompt) corresponding
    to activation spikes. This method is based on the observations presented in SectionÂ [3.2](#S3.SS2
    "3.2 Token-level Scale Analysis within Activation Spikes â€£ 3 Activation Spikes:
    Excessive Magnitude of GLU Activations â€£ Mitigating Quantization Errors Due to
    Activation Spikes in GLU-Based LLMs"), which indicate that significant quantization
    errors result from the overestimated scale factor of the first instance within
    the restricted token set. Inspired by this occurrence pattern of activation spikes,
    we aim to construct a prefix which stabilizes the quantization scale factor of
    the tokens that come after the prefix. In other words, once the prefix is fixed
    at the beginning, the activation spikes consistently occur within the prefix.
    Afterward, we employ key-value (KV) caching mechanism to process the activation
    spikes in advance. In practice, KV cache is utilized to optimize the decoding
    speed of causal language models by storing precomputed key and value states of
    the previous tokens [[34](#bib.bib34), [32](#bib.bib32)]. This approach provides
    a bypass of the quantization including activation spikes, while preserving the
    context of prefix through the KV cache. The KV cache for the prefix is precomputed
    once through the offline inference of LLM without quantization. Then, this KV
    cache is exploited in the quantization phases, such as calibration or dynamic
    quantization, even for quantized inference. The process of QFeP is illustrated
    in FigureÂ [3](#S4.F3 "Figure 3 â€£ 4 Mitigating Quantization Quality Degradation
    Based on the Observation â€£ Mitigating Quantization Errors Due to Activation Spikes
    in GLU-Based LLMs").'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¸ QFeM æ­£äº¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ— é‡åŒ–å‰ç¼€ (QFeP)ï¼Œé€šè¿‡é¢„è®¡ç®—ä¸æ¿€æ´»å³°å€¼å¯¹åº”çš„å‰ç¼€ï¼ˆæˆ–ç®€çŸ­æç¤ºï¼‰æ¥å‡è½»é‡åŒ–è¯¯å·®ã€‚è¯¥æ–¹æ³•åŸºäºç¬¬ [3.2](#S3.SS2
    "3.2 Token-level Scale Analysis within Activation Spikes â€£ 3 Activation Spikes:
    Excessive Magnitude of GLU Activations â€£ Mitigating Quantization Errors Due to
    Activation Spikes in GLU-Based LLMs") èŠ‚ä¸­æå‡ºçš„è§‚å¯Ÿç»“æœï¼Œè¿™äº›è§‚å¯Ÿè¡¨æ˜ï¼Œæ˜¾è‘—çš„é‡åŒ–è¯¯å·®æºäºåœ¨é™åˆ¶çš„æ ‡è®°é›†å†…é¦–æ¬¡å®ä¾‹çš„è¿‡é«˜ä¼°è®¡å°ºåº¦å› å­ã€‚å—æ¿€æ´»å³°å€¼å‘ç”Ÿæ¨¡å¼çš„å¯å‘ï¼Œæˆ‘ä»¬æ—¨åœ¨æ„é€ ä¸€ä¸ªå‰ç¼€ï¼Œè¯¥å‰ç¼€ç¨³å®šåœ¨å‰ç¼€ä¹‹åçš„æ ‡è®°çš„é‡åŒ–å°ºåº¦å› å­ã€‚æ¢å¥è¯è¯´ï¼Œä¸€æ—¦å‰ç¼€åœ¨å¼€å§‹æ—¶å›ºå®šï¼Œæ¿€æ´»å³°å€¼å°±ä¼šå§‹ç»ˆå‡ºç°åœ¨å‰ç¼€å†…ã€‚ä¹‹åï¼Œæˆ‘ä»¬ä½¿ç”¨é”®å€¼
    (KV) ç¼“å­˜æœºåˆ¶æå‰å¤„ç†æ¿€æ´»å³°å€¼ã€‚å®é™…ä¸Šï¼ŒKV ç¼“å­˜ç”¨äºé€šè¿‡å­˜å‚¨å…ˆå‰æ ‡è®°çš„é¢„è®¡ç®—é”®å’Œå€¼çŠ¶æ€æ¥ä¼˜åŒ–å› æœè¯­è¨€æ¨¡å‹çš„è§£ç é€Ÿåº¦ [[34](#bib.bib34),
    [32](#bib.bib32)]ã€‚è¿™ç§æ–¹æ³•æä¾›äº†ç»•è¿‡é‡åŒ–ï¼ˆåŒ…æ‹¬æ¿€æ´»å³°å€¼ï¼‰çš„é€”å¾„ï¼ŒåŒæ—¶é€šè¿‡ KV ç¼“å­˜ä¿ç•™å‰ç¼€çš„ä¸Šä¸‹æ–‡ã€‚å‰ç¼€çš„ KV ç¼“å­˜åœ¨ç¦»çº¿æ¨ç† LLM
    çš„é‡åŒ–ä¹‹å‰ä¸€æ¬¡æ€§é¢„è®¡ç®—ã€‚ç„¶åï¼Œåœ¨é‡åŒ–é˜¶æ®µï¼ˆå¦‚æ ¡å‡†æˆ–åŠ¨æ€é‡åŒ–ï¼‰ä¸­åˆ©ç”¨è¯¥ KV ç¼“å­˜ï¼Œå³ä½¿åœ¨é‡åŒ–æ¨ç†ä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚QFeP çš„è¿‡ç¨‹åœ¨å›¾ [3](#S4.F3 "Figure
    3 â€£ 4 Mitigating Quantization Quality Degradation Based on the Observation â€£ Mitigating
    Quantization Errors Due to Activation Spikes in GLU-Based LLMs") ä¸­è¯´æ˜ã€‚'
- en: Prefix Search.
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å‰ç¼€æœç´¢ã€‚
- en: To form a prefix of explicit activation spike, we first identify candidate token
    that represent the activation spike at the linear layer with the highest max-median
    ratio $r^{(m)}$. Note that the latter sequence in the template can be replaced
    with sequences from dataset instead of repetition.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å½¢æˆæ˜ç¡®æ¿€æ´»å³°å€¼çš„å‰ç¼€ï¼Œæˆ‘ä»¬é¦–å…ˆè¯†åˆ«è¡¨ç¤ºçº¿æ€§å±‚ä¸­æœ€å¤§ä¸­ä½æ•°æ¯”ç‡ $r^{(m)}$ çš„æ¿€æ´»å³°å€¼çš„å€™é€‰æ ‡è®°ã€‚è¯·æ³¨æ„ï¼Œæ¨¡æ¿ä¸­çš„åç»­åºåˆ—å¯ä»¥ç”¨æ•°æ®é›†ä¸­åºåˆ—æ›¿æ¢ï¼Œè€Œä¸æ˜¯é‡å¤ã€‚
- en: Implementation Details.
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å®ç°ç»†èŠ‚ã€‚
- en: 'Table 2: Specifications for QFeM and QFeP used in experiments. $|M|$ represents
    the number of unquantized layers for QFeM.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 2ï¼šå®éªŒä¸­ä½¿ç”¨çš„ QFeM å’Œ QFeP çš„è§„æ ¼ã€‚$|M|$ è¡¨ç¤º QFeM çš„æœªé‡åŒ–å±‚æ•°ã€‚
- en: '| Model | Prefix | $\boldsymbol{\alpha}$ |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| æ¨¡å‹ | å‰ç¼€ | $\boldsymbol{\alpha}$ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| LLaMA-2-7B | [BOS] all . | 6.68 | 17 / 128 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B | [BOS] all . | 6.68 | 17 / 128 |'
- en: '| LLaMA-2-13B | [BOS] then , | 12.91 | 6 / 160 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-13B | [BOS] then , | 12.91 | 6 / 160 |'
- en: '| LLaMA-2-70B | [BOS] I â€™ | 9.16 | 25 / 320 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-70B | [BOS] I â€™ | 9.16 | 25 / 320 |'
- en: '| Mistral-7B | [BOS] how \n | 49.00 | 3 / 128 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B | [BOS] how \n | 49.00 | 3 / 128 |'
- en: '| Mixtral-8x7B | [BOS] ). \n | 4.03 | 191 / 608 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral-8x7B | [BOS] ). \n | 4.03 | 191 / 608 |'
- en: '| SOLAR-10.7B | [BOS] a 1 | 6.48 | 11 / 192 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| SOLAR-10.7B | [BOS] a 1 | 6.48 | 11 / 192 |'
- en: '| Gemma-7B | [BOS] . PiÃ¹ | 10.65 | 5 / 112 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-7B | [BOS] . PiÃ¹ | 10.65 | 5 / 112 |'
- en: '| LLaMA-3-8B | [BOS] - nd | 6.64 | 6 / 128 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-3-8B | [BOS] - nd | 6.64 | 6 / 128 |'
- en: '| LLaMA-3-70B | [BOS] and , | 78.37 | 3 / 320 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-3-70B | [BOS] and , | 78.37 | 3 / 320 |'
- en: 'During the prefix search phase, we exploit the calibration dataset used in
    SectionÂ [3.1](#S3.SS1 "3.1 Existence of Activation Spikes in GLU Variants â€£ 3
    Activation Spikes: Excessive Magnitude of GLU Activations â€£ Mitigating Quantization
    Errors Due to Activation Spikes in GLU-Based LLMs"). For the candidate tokens,
    we consider the tokens with the top three largest input activation magnitudes.
    Then, we search for the middle context token among top 200 most frequent tokens
    in the calibration dataset, which is the subset of the vocabulary $V$. Finally,
    with the search result, we prepare the KV cache for the target model in FP16 precision.
    Exceptionally, for the Mixtral [[21](#bib.bib21)] model, we use the scale of output
    hidden states instead of input activations, as the tokens are divided sparsely
    in a mixture of experts architecture. TableÂ [2](#S4.T2 "Table 2 â€£ Implementation
    Details. â€£ 4.2 Quantization-free Prefix (QFeP) â€£ 4 Mitigating Quantization Quality
    Degradation Based on the Observation â€£ Mitigating Quantization Errors Due to Activation
    Spikes in GLU-Based LLMs") presents the searched prefix.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‰ç¼€æœç´¢é˜¶æ®µï¼Œæˆ‘ä»¬åˆ©ç”¨äº†ç¬¬[3.1èŠ‚](#S3.SS1 "3.1 GLU å˜ä½“ä¸­çš„æ¿€æ´»å³°å€¼çš„å­˜åœ¨ â€£ 3 æ¿€æ´»å³°å€¼ï¼šGLU æ¿€æ´»çš„è¿‡åº¦å¹…åº¦ â€£ å‡è½»å› æ¿€æ´»å³°å€¼å¯¼è‡´çš„
    GLU åŸºäº LLM çš„é‡åŒ–è¯¯å·®")ä¸­ä½¿ç”¨çš„æ ¡å‡†æ•°æ®é›†ã€‚å¯¹äºå€™é€‰æ ‡è®°ï¼Œæˆ‘ä»¬è€ƒè™‘è¾“å…¥æ¿€æ´»å¹…åº¦å‰ä¸‰å¤§çš„æ ‡è®°ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨æ ¡å‡†æ•°æ®é›†ä¸­å‰ 200 ä¸ªæœ€é¢‘ç¹çš„æ ‡è®°ä¸­æœç´¢ä¸­é—´ä¸Šä¸‹æ–‡æ ‡è®°ï¼Œè¿™äº›æ ‡è®°æ˜¯è¯æ±‡è¡¨
    $V$ çš„å­é›†ã€‚æœ€åï¼Œæ ¹æ®æœç´¢ç»“æœï¼Œæˆ‘ä»¬ä¸ºç›®æ ‡æ¨¡å‹å‡†å¤‡ FP16 ç²¾åº¦çš„ KV ç¼“å­˜ã€‚ä¾‹å¤–çš„æ˜¯ï¼Œå¯¹äº Mixtral [[21](#bib.bib21)]
    æ¨¡å‹ï¼Œæˆ‘ä»¬ä½¿ç”¨è¾“å‡ºéšè—çŠ¶æ€çš„è§„æ¨¡ï¼Œè€Œä¸æ˜¯è¾“å…¥æ¿€æ´»ï¼Œå› ä¸ºè¿™äº›æ ‡è®°åœ¨ä¸“å®¶æ··åˆæ¶æ„ä¸­è¢«ç¨€ç–åˆ’åˆ†ã€‚è¡¨[2](#S4.T2 "è¡¨ 2 â€£ å®æ–½ç»†èŠ‚ â€£ 4.2 æ— é‡åŒ–å‰ç¼€
    (QFeP) â€£ 4 åŸºäºè§‚å¯Ÿçš„é‡åŒ–è´¨é‡é™çº§å‡è½» â€£ å‡è½»å› æ¿€æ´»å³°å€¼å¯¼è‡´çš„ GLU åŸºäº LLM çš„é‡åŒ–è¯¯å·®")å±•ç¤ºäº†æœç´¢çš„å‰ç¼€ã€‚
- en: 5 Experiments
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 å®éªŒ
- en: 5.1 Experimental Setup
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 å®éªŒè®¾ç½®
- en: Models.
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ¨¡å‹ã€‚
- en: 'Our proposed methods, QFeM and QFeP, aim to mitigate the quantization bottleneck,
    which is discussed in SectionÂ [3.3](#S3.SS3 "3.3 Effect of Quantization on Activation
    Spikes â€£ 3 Activation Spikes: Excessive Magnitude of GLU Activations â€£ Mitigating
    Quantization Errors Due to Activation Spikes in GLU-Based LLMs"), caused by the
    activation spikes, especially in the GLU variants. To validate the efficiency
    proposed methods, we tested publicly released LLMs that were implemented with
    GLU, according to their paper and source code. We recognize recent LLMs, including
    LLAMA-2-{7B, 13B, 70B} [[47](#bib.bib47)], LLaMA-3-{7B, 70B}, Mistral-7B [[20](#bib.bib20)],
    Mixtral-8x7B [[21](#bib.bib21)], SOLAR-10.7B [[22](#bib.bib22)], and Gemma-7B
    [[43](#bib.bib43)], utilize the GLU architecture. The LLMs with original FFN are
    not covered, as they suffer from the existing outliers rather than activation
    spikes. All models are sourced from the huggingface-hubÂ²Â²2https://huggingface.co/models
    repository.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æå‡ºçš„æ–¹æ³• QFeM å’Œ QFeP æ—¨åœ¨å‡è½»ç”±æ¿€æ´»å³°å€¼å¼•èµ·çš„é‡åŒ–ç“¶é¢ˆï¼Œè¿™åœ¨ç¬¬[3.3èŠ‚](#S3.SS3 "3.3 é‡åŒ–å¯¹æ¿€æ´»å³°å€¼çš„å½±å“ â€£ 3 æ¿€æ´»å³°å€¼ï¼šGLU
    æ¿€æ´»çš„è¿‡åº¦å¹…åº¦ â€£ å‡è½»å› æ¿€æ´»å³°å€¼å¯¼è‡´çš„ GLU åŸºäº LLM çš„é‡åŒ–è¯¯å·®")ä¸­è¿›è¡Œäº†è®¨è®ºï¼Œç‰¹åˆ«æ˜¯åœ¨ GLU å˜ä½“ä¸­ã€‚ä¸ºäº†éªŒè¯æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬æµ‹è¯•äº†å…¬å¼€å‘å¸ƒçš„ã€åŸºäº
    GLU å®ç°çš„ LLMsï¼Œè¿™äº›æ¨¡å‹ä¾æ®å…¶è®ºæ–‡å’Œæºä»£ç å®ç°ã€‚æˆ‘ä»¬è¯†åˆ«å‡ºè¿‘æœŸçš„ LLMsï¼ŒåŒ…æ‹¬ LLAMA-2-{7B, 13B, 70B} [[47](#bib.bib47)]ã€LLaMA-3-{7B,
    70B}ã€Mistral-7B [[20](#bib.bib20)]ã€Mixtral-8x7B [[21](#bib.bib21)]ã€SOLAR-10.7B
    [[22](#bib.bib22)] å’Œ Gemma-7B [[43](#bib.bib43)]ï¼Œéƒ½é‡‡ç”¨äº† GLU æ¶æ„ã€‚åŸå§‹çš„ FFN LLMs æœªè¢«æ¶µç›–ï¼Œå› ä¸ºå®ƒä»¬é¢ä¸´çš„æ˜¯ç°æœ‰çš„å¼‚å¸¸å€¼ï¼Œè€Œéæ¿€æ´»å³°å€¼ã€‚æ‰€æœ‰æ¨¡å‹å‡æ¥æºäº
    huggingface-hubÂ²Â²2https://huggingface.co/models ä»“åº“ã€‚
- en: Quantization.
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: é‡åŒ–ã€‚
- en: In the experiments, we quantize both the input activations and the weights of
    linear layers for INT8 matrix multiplication operations. Note that in TableÂ [2](#S4.T2
    "Table 2 â€£ Implementation Details. â€£ 4.2 Quantization-free Prefix (QFeP) â€£ 4 Mitigating
    Quantization Quality Degradation Based on the Observation â€£ Mitigating Quantization
    Errors Due to Activation Spikes in GLU-Based LLMs"), $|M|$ denotes the total number
    of linear modules targeted for quantization. In these linear layers, we opt for
    dynamic per-tensor quantization as the quantization scheme of input activations,
    and per-channel quantization for weights, respectively. Regarding both input activations
    and weights, we symmetrically quantize the range using the absolute maximum value
    as the scale estimation function. For comparison, we use FP16 and per-token activation
    quantization [[55](#bib.bib55)] as baselines. We refer the reader to AppendixÂ [B](#A2
    "Appendix B BMM Quantization â€£ Mitigating Quantization Errors Due to Activation
    Spikes in GLU-Based LLMs") for Batch Matrix-Multiplication (BMM) quantization,
    which involves quantizing tensors in the self-attention.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬å¯¹ INT8 çŸ©é˜µä¹˜æ³•æ“ä½œçš„çº¿æ€§å±‚çš„è¾“å…¥æ¿€æ´»å’Œæƒé‡è¿›è¡Œé‡åŒ–ã€‚è¯·æ³¨æ„ï¼Œåœ¨è¡¨æ ¼Â [2](#S4.T2 "Table 2 â€£ Implementation
    Details. â€£ 4.2 Quantization-free Prefix (QFeP) â€£ 4 Mitigating Quantization Quality
    Degradation Based on the Observation â€£ Mitigating Quantization Errors Due to Activation
    Spikes in GLU-Based LLMs") ä¸­ï¼Œ$|M|$ è¡¨ç¤ºç›®æ ‡é‡åŒ–çš„çº¿æ€§æ¨¡å—çš„æ€»æ•°ã€‚åœ¨è¿™äº›çº¿æ€§å±‚ä¸­ï¼Œæˆ‘ä»¬é€‰æ‹©åŠ¨æ€æ¯å¼ é‡é‡åŒ–ä½œä¸ºè¾“å…¥æ¿€æ´»çš„é‡åŒ–æ–¹æ¡ˆï¼Œè€Œå¯¹æƒé‡ä½¿ç”¨æ¯é€šé“é‡åŒ–ã€‚å¯¹äºè¾“å…¥æ¿€æ´»å’Œæƒé‡ï¼Œæˆ‘ä»¬ä½¿ç”¨ç»å¯¹æœ€å¤§å€¼ä½œä¸ºå°ºåº¦ä¼°è®¡å‡½æ•°å¯¹èŒƒå›´è¿›è¡Œå¯¹ç§°é‡åŒ–ã€‚ä¸ºäº†è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬ä½¿ç”¨
    FP16 å’Œæ¯ä»¤ç‰Œæ¿€æ´»é‡åŒ– [[55](#bib.bib55)] ä½œä¸ºåŸºå‡†ã€‚æœ‰å…³ Batch Matrix-Multiplication (BMM) é‡åŒ–çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬å‚è€ƒé™„å½•
    [B](#A2 "Appendix B BMM Quantization â€£ Mitigating Quantization Errors Due to Activation
    Spikes in GLU-Based LLMs")ï¼Œè¿™æ¶‰åŠå¯¹è‡ªæ³¨æ„åŠ›ä¸­çš„å¼ é‡è¿›è¡Œé‡åŒ–ã€‚
- en: Evaluations.
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è¯„ä¼°ã€‚
- en: 'We evaluate the quantized LLMs with two metrics: zero-shot evaluation accuracy
    and perplexity. For zero-shot evaluation, we use the four datasets: PIQA [[7](#bib.bib7)],
    LAMBADA [[33](#bib.bib33)], HellaSwag [[56](#bib.bib56)], and WinoGrande [[38](#bib.bib38)].
    We utilize the lm-evaluation-harness library [[16](#bib.bib16)] to evaluate zero-shot
    tasks. To measure perplexity, we use the WikiText-2 [[28](#bib.bib28)] dataset.
    In all cases, we use the [BOS] token as the starting token for each input sequence
    by default.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªæŒ‡æ ‡æ¥è¯„ä¼°é‡åŒ–çš„ LLMsï¼šé›¶-shot è¯„ä¼°å‡†ç¡®ç‡å’Œå›°æƒ‘åº¦ã€‚å¯¹äºé›¶-shot è¯„ä¼°ï¼Œæˆ‘ä»¬ä½¿ç”¨å››ä¸ªæ•°æ®é›†ï¼šPIQA [[7](#bib.bib7)]ã€LAMBADA
    [[33](#bib.bib33)]ã€HellaSwag [[56](#bib.bib56)] å’Œ WinoGrande [[38](#bib.bib38)]ã€‚æˆ‘ä»¬åˆ©ç”¨
    lm-evaluation-harness åº“ [[16](#bib.bib16)] æ¥è¯„ä¼°é›¶-shot ä»»åŠ¡ã€‚ä¸ºäº†æµ‹é‡å›°æƒ‘åº¦ï¼Œæˆ‘ä»¬ä½¿ç”¨ WikiText-2
    [[28](#bib.bib28)] æ•°æ®é›†ã€‚åœ¨æ‰€æœ‰æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬é»˜è®¤ä½¿ç”¨ [BOS] ä»¤ç‰Œä½œä¸ºæ¯ä¸ªè¾“å…¥åºåˆ—çš„èµ·å§‹ä»¤ç‰Œã€‚
- en: 5.2 Main Results
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 ä¸»è¦ç»“æœ
- en: 'Table 3: Perplexity and zero-shot evaluation for the quantization on LLaMA-2
    models. FP16 denotes the original model precision, and W8A8 denotes the model
    quantized to INT8 for both weights and activations.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 3ï¼šå¯¹ LLaMA-2 æ¨¡å‹è¿›è¡Œé‡åŒ–çš„å›°æƒ‘åº¦å’Œé›¶-shot è¯„ä¼°ã€‚FP16 è¡¨ç¤ºåŸå§‹æ¨¡å‹ç²¾åº¦ï¼Œè€Œ W8A8 è¡¨ç¤ºå°†æ¨¡å‹é‡åŒ–ä¸º INT8 çš„æƒé‡å’Œæ¿€æ´»ã€‚
- en: '| Method |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| æ–¹æ³• |'
- en: '&#124; WikiText-2 &#124;'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; WikiText-2 &#124;'
- en: '&#124; (ppl$\downarrow$) &#124;'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (ppl$\downarrow$) &#124;'
- en: '|'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PIQA &#124;'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PIQA &#124;'
- en: '&#124; (acc$\uparrow$) &#124;'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (acc$\uparrow$) &#124;'
- en: '|'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LAMBADA &#124;'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LAMBADA &#124;'
- en: '&#124; (acc$\uparrow$) &#124;'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (acc$\uparrow$) &#124;'
- en: '|'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; HellaSwag &#124;'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; HellaSwag &#124;'
- en: '&#124; (acc$\uparrow$) &#124;'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (acc$\uparrow$) &#124;'
- en: '|'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; WinoGrande &#124;'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; WinoGrande &#124;'
- en: '&#124; (acc$\uparrow$) &#124;'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (acc$\uparrow$) &#124;'
- en: '|'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Avg &#124;'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; å¹³å‡ &#124;'
- en: '&#124; (acc$\uparrow$) &#124;'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (acc$\uparrow$) &#124;'
- en: '|'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| LLaMA-2-7B |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B |'
- en: '| FP16 | 5.268 | 78.18% | 73.67% | 57.13% | 69.46% | 69.61% |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 5.268 | 78.18% | 73.67% | 57.13% | 69.46% | 69.61% |'
- en: '| W8A8 | 8.634 | 72.80% | 62.27% | 49.57% | 63.69% | 62.08% |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| W8A8 | 8.634 | 72.80% | 62.27% | 49.57% | 63.69% | 62.08% |'
- en: '| Â Â +QFeM | 5.758[-2.876] | 78.02% | 73.86% | 56.32% | 68.35% | 69.14%[+7.06]
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Â Â +QFeM | 5.758[-2.876] | 78.02% | 73.86% | 56.32% | 68.35% | 69.14%[+7.06]
    |'
- en: '| Â Â +QFeP | 5.758[-2.876] | 76.44% | 73.57% | 55.55% | 69.22% | 68.69%[+6.61]
    |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Â Â +QFeP | 5.758[-2.876] | 76.44% | 73.57% | 55.55% | 69.22% | 68.69%[+6.61]
    |'
- en: '| Â Â +QFeM+QFeP | 5.573[-3.061] | 77.86% | 74.58% | 56.05% | 69.38% | 69.47%[+7.39]
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| Â Â +QFeM+QFeP | 5.573[-3.061] | 77.86% | 74.58% | 56.05% | 69.38% | 69.47%[+7.39]
    |'
- en: '| LLaMA-2-13B |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-13B |'
- en: '| FP16 | 4.789 | 79.49% | 76.54% | 60.20% | 72.38% | 72.15% |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 4.789 | 79.49% | 76.54% | 60.20% | 72.38% | 72.15% |'
- en: '| W8A8 | 34.089 | 70.13% | 49.66% | 42.65% | 58.72% | 55.29% |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| W8A8 | 34.089 | 70.13% | 49.66% | 42.65% | 58.72% | 55.29% |'
- en: '| Â Â +QFeM | 5.241[-28.848] | 77.58% | 75.68% | 59.13% | 72.61% | 71.25%[+15.96]
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| Â Â +QFeM | 5.241[-28.848] | 77.58% | 75.68% | 59.13% | 72.61% | 71.25%[+15.96]
    |'
- en: '| Â Â +QFeP | 6.000[-28.089] | 77.53% | 73.94% | 57.23% | 70.96% | 69.91%[+14.62]
    |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| Â Â +QFeP | 6.000[-28.089] | 77.53% | 73.94% | 57.23% | 70.96% | 69.91%[+14.62]
    |'
- en: '| Â Â +QFeM+QFeP | 5.126[-28.963] | 78.51% | 75.86% | 59.44% | 72.61% | 71.61%[+16.32]
    |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Â Â +QFeM+QFeP | 5.126[-28.963] | 78.51% | 75.86% | 59.44% | 72.61% | 71.61%[+16.32]
    |'
- en: '| LLaMA-2-70B |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-70B |'
- en: '| FP16 | 3.218 | 81.45% | 79.45% | 65.29% | 80.43% | 76.65% |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 3.218 | 81.45% | 79.45% | 65.29% | 80.43% | 76.65% |'
- en: '| W8A8 | 8.055 | 74.05% | 70.27% | 55.21% | 67.96% | 66.87% |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| W8A8 | 8.055 | 74.05% | 70.27% | 55.21% | 67.96% | 66.87% |'
- en: '| Â Â +QFeM | 3.830[-4.225] | 81.23% | 77.66% | 64.15% | 78.14% | 75.30%[+8.43]
    |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| Â Â +QFeM | 3.830[-4.225] | 81.23% | 77.66% | 64.15% | 78.14% | 75.30%[+8.43]
    |'
- en: '| Â Â +QFeP | 6.007[-2.048] | 77.64% | 73.26% | 63.40% | 76.16% | 72.62%[+5.75]
    |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Â Â +QFeP | 6.007[-2.048] | 77.64% | 73.26% | 63.40% | 76.16% | 72.62%[+5.75]
    |'
- en: '| Â Â +QFeM+QFeP | 3.708[-4.347] | 81.23% | 77.82% | 64.65% | 77.11% | 75.20%[+8.33]
    | ![Refer to caption](img/0592725db7336eb0f205080da311fd87.png)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '| Â Â +QFeM+QFeP | 3.708[-4.347] | 81.23% | 77.82% | 64.65% | 77.11% | 75.20%[+8.33]
    | ![å‚è§è¯´æ˜](img/0592725db7336eb0f205080da311fd87.png)'
- en: 'Figure 5: The average accuracy of zero-shot evaluation on other GLU-implemented
    LLMs. Most models recover significantly compared to W8A8, with performance close
    to FP16.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5ï¼šå…¶ä»–GLUå®ç°çš„LLMåœ¨é›¶-shotè¯„ä¼°ä¸­çš„å¹³å‡å‡†ç¡®ç‡ã€‚ä¸W8A8ç›¸æ¯”ï¼Œå¤§å¤šæ•°æ¨¡å‹çš„æ¢å¤æƒ…å†µæ˜¾è‘—ï¼Œæ€§èƒ½æ¥è¿‘FP16ã€‚
- en: LLaMA-2 Models.
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLaMA-2æ¨¡å‹ã€‚
- en: We report the evaluation results of quantization on LLaMA-2 models in TableÂ [3](#S5.T3
    "Table 3 â€£ 5.2 Main Results â€£ 5 Experiments â€£ Mitigating Quantization Errors Due
    to Activation Spikes in GLU-Based LLMs"). Compared to FP16 precision, quantizing
    both weights and activations (W8A8) degrades the overall performance. The results
    demonstrate that our proposed methods resolve the activation spikes and, surprisingly,
    restore the performance of the W8A8 close to that of FP16. For example, the LLaMA-2
    7B model achieves less than a 1% performance drop from FP16. It is worth noting
    that the proposed QFeM and QFeP improve at comparable levels. This indicates that
    the activation spikes present a direct cause of the significant decrease in quantization
    performance. Because the proposed methods are orthogonal, the performance slightly
    increases when incorporating both QFeM and QFeP compared to applying them individually.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨è¡¨æ ¼[3](#S5.T3 "Table 3 â€£ 5.2 Main Results â€£ 5 Experiments â€£ Mitigating Quantization
    Errors Due to Activation Spikes in GLU-Based LLMs")ä¸­æŠ¥å‘Šäº†å¯¹LLaMA-2æ¨¡å‹é‡åŒ–çš„è¯„ä¼°ç»“æœã€‚ä¸FP16ç²¾åº¦ç›¸æ¯”ï¼Œé‡åŒ–æƒé‡å’Œæ¿€æ´»ï¼ˆW8A8ï¼‰ä¼šé™ä½æ•´ä½“æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•è§£å†³äº†æ¿€æ´»å°–å³°é—®é¢˜ï¼Œä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œä½¿W8A8çš„æ€§èƒ½æ¢å¤åˆ°æ¥è¿‘FP16ã€‚ä¾‹å¦‚ï¼ŒLLaMA-2
    7Bæ¨¡å‹çš„æ€§èƒ½ä¸‹é™ä¸åˆ°1%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæå‡ºçš„QFeMå’ŒQFePåœ¨ç›¸ä¼¼æ°´å¹³ä¸Šæœ‰æ‰€æ”¹è¿›ã€‚è¿™è¡¨æ˜ï¼Œæ¿€æ´»å°–å³°æ˜¯é‡åŒ–æ€§èƒ½æ˜¾è‘—ä¸‹é™çš„ç›´æ¥åŸå› ã€‚ç”±äºæ‰€ææ–¹æ³•æ˜¯æ­£äº¤çš„ï¼Œç»“åˆQFeMå’ŒQFePæ—¶æ€§èƒ½ç•¥æœ‰æé«˜ï¼Œç›¸è¾ƒäºå•ç‹¬åº”ç”¨å®ƒä»¬ã€‚
- en: Other GLU-implemented LLMs.
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å…¶ä»–GLUå®ç°çš„LLMã€‚
- en: For other LLMs that incorporate GLU, we investigated the effectiveness of our
    methods in mitigating the quantization bottleneck. As can be seen in FigureÂ [5](#S5.F5
    "Figure 5 â€£ 5.2 Main Results â€£ 5 Experiments â€£ Mitigating Quantization Errors
    Due to Activation Spikes in GLU-Based LLMs"), our methods consistently remedy
    the performance drop caused by activation spikes. Noticeably, the Mixtral model
    demonstrates robustness towards the performance degradation. This indicates that
    the mixture of experts architecture, which divides the MLP experts by tokens,
    helps to alleviate the impact of the activation spikes. Meanwhile, addressing
    the activation spikes is not a sufficient complement for the Gemma model compared
    to other models. We attribute this to the choice of activation function among
    GLU variants; specifically, Gemma uses GeGLU, while other models employ SwiGLU.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå…¶ä»–é‡‡ç”¨GLUçš„LLMï¼Œæˆ‘ä»¬ç ”ç©¶äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç¼“è§£é‡åŒ–ç“¶é¢ˆæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚å¦‚å›¾[5](#S5.F5 "Figure 5 â€£ 5.2 Main Results
    â€£ 5 Experiments â€£ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based
    LLMs")æ‰€ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•æŒç»­ä¿®å¤äº†æ¿€æ´»å°–å³°å¼•èµ·çš„æ€§èƒ½ä¸‹é™ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒMixtralæ¨¡å‹åœ¨æ€§èƒ½é€€åŒ–æ–¹é¢è¡¨ç°å‡ºç¨³å¥æ€§ã€‚è¿™è¡¨æ˜ï¼Œå°†MLPä¸“å®¶æŒ‰ä»¤ç‰Œåˆ’åˆ†çš„ä¸“å®¶æ··åˆæ¶æ„æœ‰åŠ©äºå‡è½»æ¿€æ´»å°–å³°çš„å½±å“ã€‚åŒæ—¶ï¼Œä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”ï¼Œè§£å†³æ¿€æ´»å°–å³°å¯¹äºGemmaæ¨¡å‹å¹¶ä¸æ˜¯ä¸€ä¸ªè¶³å¤Ÿçš„è¡¥å……ã€‚æˆ‘ä»¬å°†æ­¤å½’å› äºGLUå˜ä½“ä¸­çš„æ¿€æ´»å‡½æ•°é€‰æ‹©ï¼›å…·ä½“è€Œè¨€ï¼ŒGemmaä½¿ç”¨GeGLUï¼Œè€Œå…¶ä»–æ¨¡å‹ä½¿ç”¨SwiGLUã€‚
- en: 5.3 Combining Outlier Alleviation Methods
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 ç»“åˆå¼‚å¸¸å€¼ç¼“è§£æ–¹æ³•
- en: 'Table 4: Evaluation of outlier alleviation methods with QFeM and QFeP. We report
    perplexity on WikiText-2 and averaged accuracy of four zero-shot tasks. The same
    quantization scheme for used on both SQ and OSP. Per-tensor weight quantization
    results are provided in AppendixÂ [C.1](#A3.SS1 "C.1 Additional Results for Combining
    Outlier Alleviation Methods â€£ Appendix C Supplementary Experiment Results â€£ Mitigating
    Quantization Errors Due to Activation Spikes in GLU-Based LLMs").'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 4ï¼šä¸ QFeM å’Œ QFeP ä¸€èµ·è¯„ä¼°ç¦»ç¾¤å€¼ç¼“è§£æ–¹æ³•ã€‚æˆ‘ä»¬æŠ¥å‘Šäº† WikiText-2 ä¸Šçš„å›°æƒ‘åº¦ä»¥åŠå››ä¸ªé›¶æ ·æœ¬ä»»åŠ¡çš„å¹³å‡å‡†ç¡®ç‡ã€‚SQ å’Œ OSP
    ä½¿ç”¨äº†ç›¸åŒçš„é‡åŒ–æ–¹æ¡ˆã€‚æ¯ä¸ªå¼ é‡æƒé‡é‡åŒ–ç»“æœè§é™„å½•[C.1](#A3.SS1 "C.1 Additional Results for Combining Outlier
    Alleviation Methods â€£ Appendix C Supplementary Experiment Results â€£ Mitigating
    Quantization Errors Due to Activation Spikes in GLU-Based LLMs")ã€‚
- en: '| Method | LLaMA-2-7B | LLaMA-2-13B | LLaMA-2-70B |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| æ–¹æ³• | LLaMA-2-7B | LLaMA-2-13B | LLaMA-2-70B |'
- en: '| --- | --- | --- | --- |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| ppl($\downarrow$) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| ppl($\downarrow$) |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| SQ [[51](#bib.bib51)] | 9.907 | 61.08% | 34.869 | 59.45% | 8.800 | 70.25%
    |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| SQ [[51](#bib.bib51)] | 9.907 | 61.08% | 34.869 | 59.45% | 8.800 | 70.25%
    |'
- en: '| +QFeM | 5.534 | 69.65% | 5.118 | 71.23% | 3.599 | 75.93% |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| +QFeM | 5.534 | 69.65% | 5.118 | 71.23% | 3.599 | 75.93% |'
- en: '| +QFeP | 5.715 | 68.66% | 6.551 | 69.33% | 5.228 | 74.07% |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| +QFeP | 5.715 | 68.66% | 6.551 | 69.33% | 5.228 | 74.07% |'
- en: '| OSP [[50](#bib.bib50)] | 38.490 | 59.90% | 5.148 | 71.29% | 3.827 | 75.52%
    |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| OSP [[50](#bib.bib50)] | 38.490 | 59.90% | 5.148 | 71.29% | 3.827 | 75.52%
    |'
- en: '| +QFeM | 5.493 | 69.37% | 5.099 | 71.37% | 3.559 | 75.92% |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| +QFeM | 5.493 | 69.37% | 5.099 | 71.37% | 3.559 | 75.92% |'
- en: '| +QFeP | 5.642 | 68.95% | 5.144 | 71.05% | 3.752 | 75.36% |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| +QFeP | 5.642 | 68.95% | 5.144 | 71.05% | 3.752 | 75.36% |'
- en: While our method focuses on the activation spikes, the inherent outlier values
    in the input activations remain. Here, we combine the prior outlier alleviation
    methods, such as SmoothQuant (SQ) [[51](#bib.bib51)] and OutlierSuppressionPlus
    (OSP) [[50](#bib.bib50)], to further improve the quantization error. In practice,
    our methods are utilized during the scale calibration phase of alleviation methods
    to mitigate the impact of activation spikes on scale migration between activations
    and weights. TableÂ [4](#S5.T4 "Table 4 â€£ 5.3 Combining Outlier Alleviation Methods
    â€£ 5 Experiments â€£ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based
    LLMs") demonstrates the evaluation results of applying the outlier alleviation
    methods solely and combining them with our methods. We find that there are cases
    where the alleviation method fails to recover the performance when quantizing
    the activations with per-tensor scheme.Â³Â³3In their papers, the activations of
    LLaMA models are quantized using only a per-token scheme. This indicates that
    alleviating the outlier scales, including the activation spikes, is challenging.
    With the QFeM, the activation spikes are excluded, and the accurate alleviation
    is enabled. In addition, the QFeP also benefits from the SQ method, as seen in
    the case of LLaMA-2 70B. Exceptionally, the OSP successfully addresses the activation
    spikes in the 13B and 70B cases.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æˆ‘ä»¬çš„æ–¹æ³•ä¸“æ³¨äºæ¿€æ´»å³°å€¼ï¼Œä½†è¾“å…¥æ¿€æ´»ä¸­çš„å›ºæœ‰ç¦»ç¾¤å€¼ä»ç„¶å­˜åœ¨ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç»“åˆäº†å…ˆå‰çš„ç¦»ç¾¤å€¼ç¼“è§£æ–¹æ³•ï¼Œå¦‚ SmoothQuant (SQ) [[51](#bib.bib51)]
    å’Œ OutlierSuppressionPlus (OSP) [[50](#bib.bib50)]ï¼Œä»¥è¿›ä¸€æ­¥æ”¹å–„é‡åŒ–è¯¯å·®ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç¼“è§£æ–¹æ³•çš„å°ºåº¦æ ¡å‡†é˜¶æ®µä½¿ç”¨ï¼Œä»¥å‡å°‘æ¿€æ´»å³°å€¼å¯¹æ¿€æ´»ä¸æƒé‡ä¹‹é—´å°ºåº¦è¿ç§»çš„å½±å“ã€‚è¡¨[4](#S5.T4
    "Table 4 â€£ 5.3 Combining Outlier Alleviation Methods â€£ 5 Experiments â€£ Mitigating
    Quantization Errors Due to Activation Spikes in GLU-Based LLMs")å±•ç¤ºäº†å•ç‹¬åº”ç”¨ç¦»ç¾¤å€¼ç¼“è§£æ–¹æ³•åŠå…¶ä¸æˆ‘ä»¬æ–¹æ³•ç»“åˆçš„è¯„ä¼°ç»“æœã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨ä½¿ç”¨æ¯ä¸ªå¼ é‡æ–¹æ¡ˆå¯¹æ¿€æ´»è¿›è¡Œé‡åŒ–æ—¶ï¼Œç¼“è§£æ–¹æ³•åœ¨æŸäº›æƒ…å†µä¸‹æœªèƒ½æ¢å¤æ€§èƒ½ã€‚åœ¨ä»–ä»¬çš„è®ºæ–‡ä¸­ï¼ŒLLaMA
    æ¨¡å‹çš„æ¿€æ´»ä»…ä½¿ç”¨æ¯ä¸ªä»¤ç‰Œæ–¹æ¡ˆè¿›è¡Œé‡åŒ–ã€‚è¿™è¡¨æ˜ï¼Œç¼“è§£ç¦»ç¾¤å€¼å°ºåº¦ï¼ˆåŒ…æ‹¬æ¿€æ´»å³°å€¼ï¼‰æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚ä½¿ç”¨ QFeMï¼Œå¯ä»¥æ’é™¤æ¿€æ´»å³°å€¼ï¼Œä»è€Œå®ç°å‡†ç¡®çš„ç¼“è§£ã€‚æ­¤å¤–ï¼ŒQFeP
    ä¹Ÿå—ç›Šäº SQ æ–¹æ³•ï¼Œå¦‚ LLaMA-2 70B çš„æƒ…å†µæ‰€ç¤ºã€‚ä¾‹å¤–çš„æ˜¯ï¼ŒOSP æˆåŠŸè§£å†³äº† 13B å’Œ 70B æƒ…å†µä¸‹çš„æ¿€æ´»å³°å€¼é—®é¢˜ã€‚
- en: 5.4 Ablation Study
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 æ¶ˆèç ”ç©¶
- en: '![Refer to caption](img/fc8930c86e0b47ace7a2ad3b479acfb9.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒæ ‡é¢˜](img/fc8930c86e0b47ace7a2ad3b479acfb9.png)'
- en: 'Figure 6: Prefix ablation. Y-axis represents averaged accuracy of four zero-shot
    tasks.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 6ï¼šå‰ç¼€æ¶ˆèã€‚Y è½´è¡¨ç¤ºå››ä¸ªé›¶æ ·æœ¬ä»»åŠ¡çš„å¹³å‡å‡†ç¡®ç‡ã€‚
- en: For the QFeP, we designed a length-three prefix for the KV cache, including
    the BOS token, context token, and extra token for activation spike. Because the
    KV cache consumes the capacity of the pretrained sequence position, it raises
    a question about the length of the prefix. Therefore, we conduct ablation study
    for different prefixes for the KV cache. For the prefixes, we prepare random,
    BOS only, and both QFeP without and with the context token. We illustrate the
    results of ablation study in FigureÂ [6](#S5.F6 "Figure 6 â€£ 5.4 Ablation Study
    â€£ 5 Experiments â€£ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based
    LLMs"). In all cases, the random prefix showcases the lowest performance. While
    the KV cache with the BOS token demonstrates inconsistent performance, our QFeP
    consistently shows significant improvement. Importantly, the results imply that
    the sufficient prefix for the models exhibits differences. However, we emphasize
    that our KV design for QFeP shows improvements by large margins across all models.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº QFePï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé•¿åº¦ä¸ºä¸‰çš„å‰ç¼€ç”¨äº KV ç¼“å­˜ï¼ŒåŒ…æ‹¬ BOS æ ‡è®°ã€ä¸Šä¸‹æ–‡æ ‡è®°å’Œç”¨äºæ¿€æ´»å³°å€¼çš„é¢å¤–æ ‡è®°ã€‚ç”±äº KV ç¼“å­˜æ¶ˆè€—äº†é¢„è®­ç»ƒåºåˆ—ä½ç½®çš„å®¹é‡ï¼Œè¿™å°±å¯¹å‰ç¼€çš„é•¿åº¦æå‡ºäº†é—®é¢˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯¹
    KV ç¼“å­˜çš„ä¸åŒå‰ç¼€è¿›è¡Œäº†æ¶ˆèç ”ç©¶ã€‚å¯¹äºå‰ç¼€ï¼Œæˆ‘ä»¬å‡†å¤‡äº†éšæœºå‰ç¼€ã€ä»… BOS å‰ç¼€ä»¥åŠæœ‰å’Œæ²¡æœ‰ä¸Šä¸‹æ–‡æ ‡è®°çš„ QFeP å‰ç¼€ã€‚æˆ‘ä»¬åœ¨å›¾Â [6](#S5.F6
    "å›¾ 6 â€£ 5.4 æ¶ˆèç ”ç©¶ â€£ 5 å®éªŒ â€£ å‡è½»åŸºäº GLU çš„ LLM ä¸­çš„æ¿€æ´»å³°å€¼é‡åŒ–è¯¯å·®")ä¸­å±•ç¤ºäº†æ¶ˆèç ”ç©¶çš„ç»“æœã€‚åœ¨æ‰€æœ‰æƒ…å†µä¸‹ï¼Œéšæœºå‰ç¼€å±•ç¤ºäº†æœ€ä½çš„æ€§èƒ½ã€‚è™½ç„¶å¸¦æœ‰
    BOS æ ‡è®°çš„ KV ç¼“å­˜è¡¨ç°ä¸ç¨³å®šï¼Œä½†æˆ‘ä»¬çš„ QFeP ä¸€è‡´åœ°æ˜¾ç¤ºäº†æ˜¾è‘—çš„æ”¹è¿›ã€‚é‡è¦çš„æ˜¯ï¼Œè¿™äº›ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹æ‰€éœ€çš„å‰ç¼€åœ¨å±•ç¤ºä¸Šå­˜åœ¨å·®å¼‚ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¼ºè°ƒï¼Œæˆ‘ä»¬ä¸º
    QFeP è®¾è®¡çš„ KV ç¼“å­˜åœ¨æ‰€æœ‰æ¨¡å‹ä¸­éƒ½å¤§å¹…åº¦æå‡äº†æ€§èƒ½ã€‚
- en: 5.5 Computational Cost Analysis
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 è®¡ç®—æˆæœ¬åˆ†æ
- en: 'The proposed methods require additional resources to evict the activation spikes.
    Therefore, we analyze the computational costs of the methods and compare them
    in various schemes. For comparison, we evaluate different activation quantization
    schemes: dynamic per-token, dynamic per-tensor, and static per-tensor, denoted
    as AQ1, AQ2, and AQ3, respectively. This distinction establishes strong baselines
    and demonstrates the potential of the methods. To calibrate the static scales,
    we estimate the absolute maximum value using the calibration dataset, which is
    used in SectionÂ [3.1](#S3.SS1 "3.1 Existence of Activation Spikes in GLU Variants
    â€£ 3 Activation Spikes: Excessive Magnitude of GLU Activations â€£ Mitigating Quantization
    Errors Due to Activation Spikes in GLU-Based LLMs").'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æå‡ºçš„æ–¹æ³•éœ€è¦é¢å¤–èµ„æºæ¥é©±é€æ¿€æ´»å³°å€¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åˆ†æäº†è¿™äº›æ–¹æ³•çš„è®¡ç®—æˆæœ¬ï¼Œå¹¶åœ¨å„ç§æ–¹æ¡ˆä¸­è¿›è¡Œäº†æ¯”è¾ƒã€‚ä¸ºäº†æ¯”è¾ƒï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¸åŒçš„æ¿€æ´»é‡åŒ–æ–¹æ¡ˆï¼šåŠ¨æ€æ¯æ ‡è®°ã€åŠ¨æ€æ¯å¼ é‡å’Œé™æ€æ¯å¼ é‡ï¼Œåˆ†åˆ«è¡¨ç¤ºä¸º
    AQ1ã€AQ2 å’Œ AQ3ã€‚è¿™ç§åŒºåˆ†å»ºç«‹äº†å¼ºæœ‰åŠ›çš„åŸºå‡†ï¼Œå¹¶å±•ç¤ºäº†æ–¹æ³•çš„æ½œåŠ›ã€‚ä¸ºäº†æ ¡å‡†é™æ€æ¯”ä¾‹ï¼Œæˆ‘ä»¬ä½¿ç”¨æ ¡å‡†æ•°æ®é›†ä¼°è®¡ç»å¯¹æœ€å¤§å€¼ï¼Œè¿™åœ¨ç¬¬Â [3.1](#S3.SS1
    "3.1 GLU å˜ä½“ä¸­çš„æ¿€æ´»å³°å€¼å­˜åœ¨ â€£ 3 æ¿€æ´»å³°å€¼ï¼šGLU æ¿€æ´»çš„è¿‡åº¦å¹…åº¦ â€£ å‡è½»åŸºäº GLU çš„ LLM ä¸­çš„æ¿€æ´»å³°å€¼é‡åŒ–è¯¯å·®")èŠ‚ä¸­ä½¿ç”¨ã€‚
- en: Inference Latency.
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ¨ç†å»¶è¿Ÿã€‚
- en: For each setting, we present the accuracy of the zero-shot tasks and inference
    latency of the fixed token sequence, as shown in FigureÂ [7](#S5.F7 "Figure 7 â€£
    Table 5 â€£ Inference Latency. â€£ 5.5 Computational Cost Analysis â€£ 5 Experiments
    â€£ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs").
    While the fine-grained scheme (AQ1) shows a negligible accuracy drop, the counterparts
    (AQ2, AQ3) degrade with the quantization bottleneck. However, by applying our
    methods, the coarse-grained schemes achieve a competitive performance gain. For
    example, the combination of AQ2 and QFeM demonstrates the performance close to
    the AQ1 but with faster latency. The results signify that addressing the quantization
    bottleneck is important to accelerate the inference latency with coarser granularity.
    Specifically, the naive static quantization (AQ3), the fastest scheme, exhibits
    a significant decline. We hope that our work contributes to the future works,
    which address the remaining challenges in static quantization.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªè®¾ç½®ï¼Œæˆ‘ä»¬å±•ç¤ºäº†é›¶æ ·æœ¬ä»»åŠ¡çš„å‡†ç¡®æ€§å’Œå›ºå®šæ ‡è®°åºåˆ—çš„æ¨ç†å»¶è¿Ÿï¼Œå¦‚å›¾Â [7](#S5.F7 "å›¾ 7 â€£ è¡¨ 5 â€£ æ¨ç†å»¶è¿Ÿ â€£ 5.5 è®¡ç®—æˆæœ¬åˆ†æ
    â€£ 5 å®éªŒ â€£ å‡è½»åŸºäº GLU çš„ LLM ä¸­çš„æ¿€æ´»å³°å€¼é‡åŒ–è¯¯å·®")æ‰€ç¤ºã€‚å°½ç®¡ç»†ç²’åº¦æ–¹æ¡ˆ (AQ1) æ˜¾ç¤ºäº†å¾®ä¸è¶³é“çš„å‡†ç¡®æ€§ä¸‹é™ï¼Œä½†å…¶ä½™æ–¹æ¡ˆ (AQ2ã€AQ3)
    åœ¨é‡åŒ–ç“¶é¢ˆä¸‹è¡¨ç°ä¸‹é™ã€‚ç„¶è€Œï¼Œé€šè¿‡åº”ç”¨æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç²—ç²’åº¦æ–¹æ¡ˆå®ç°äº†ç«äº‰æ€§çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼ŒAQ2 å’Œ QFeM çš„ç»„åˆå±•ç¤ºäº†æ¥è¿‘ AQ1 çš„æ€§èƒ½ï¼Œä½†å…·æœ‰æ›´å¿«çš„å»¶è¿Ÿã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè§£å†³é‡åŒ–ç“¶é¢ˆå¯¹åŠ é€Ÿæ¨ç†å»¶è¿Ÿå…·æœ‰é‡è¦æ„ä¹‰ï¼Œå°¤å…¶æ˜¯åœ¨è¾ƒç²—ç²’åº¦ä¸‹ã€‚å…·ä½“æ¥è¯´ï¼Œæœ€ç®€å•çš„é™æ€é‡åŒ–
    (AQ3)ï¼Œä½œä¸ºæœ€å¿«çš„æ–¹æ¡ˆï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—ä¸‹é™ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„å·¥ä½œå¯¹æœªæ¥çš„å·¥ä½œæœ‰æ‰€è´¡çŒ®ï¼Œè§£å†³é™æ€é‡åŒ–ä¸­çš„å‰©ä½™æŒ‘æˆ˜ã€‚
- en: '![Refer to caption](img/8f6af0fb421b36a6b972d00ee2945100.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒæ ‡é¢˜](img/8f6af0fb421b36a6b972d00ee2945100.png)'
- en: 'Figure 7: Accuracy-latency comparison of different activation quantization
    schemes: dynamic per-token (AQ1), dynamic per-tensor (AQ2), and static per-tensor
    (AQ3).'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾7ï¼šä¸åŒæ¿€æ´»é‡åŒ–æ–¹æ¡ˆçš„å‡†ç¡®ç‡-å»¶è¿Ÿæ¯”è¾ƒï¼šåŠ¨æ€æ¯ä»¤ç‰Œï¼ˆAQ1ï¼‰ã€åŠ¨æ€æ¯å¼ é‡ï¼ˆAQ2ï¼‰å’Œé™æ€æ¯å¼ é‡ï¼ˆAQ3ï¼‰ã€‚
- en: 'Table 5: Memory footprint.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨5ï¼šå†…å­˜å ç”¨ã€‚
- en: '| Method | SeqLen |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| æ–¹æ³• | SeqLen |'
- en: '| 1K | 2K |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 1K | 2K |'
- en: '| LLaMA-2-7B |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B |'
- en: '| AQ1 | 8185MiB | 9516MiB |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| AQ1 | 8185MiB | 9516MiB |'
- en: '| AQ2 | 8148MiB | 9474MiB |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| AQ2 | 8148MiB | 9474MiB |'
- en: '| +QFeP | 8149MiB | 9478MiB |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| +QFeP | 8149MiB | 9478MiB |'
- en: '| +QFeM | 8148MiB | 9474MiB |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| +QFeM | 8148MiB | 9474MiB |'
- en: '| LLaMA-2-70B |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-70B |'
- en: '| AQ1 | 67756MiB | 69037MiB |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| AQ1 | 67756MiB | 69037MiB |'
- en: '| AQ2 | 67648MiB | 68820MiB |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| AQ2 | 67648MiB | 68820MiB |'
- en: '| +QFeP | 67651MiB | 68822MiB |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| +QFeP | 67651MiB | 68822MiB |'
- en: '| +QFeM | 67838MiB | 68819MiB |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| +QFeM | 67838MiB | 68819MiB |'
- en: Memory Footprint.
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å†…å­˜å ç”¨ã€‚
- en: In TableÂ [5](#S5.T5 "Table 5 â€£ Inference Latency. â€£ 5.5 Computational Cost Analysis
    â€£ 5 Experiments â€£ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based
    LLMs"), we record the maximum memory footprint of our methods. For QFeP, the additional
    memory is consistently required for the preserved KV cache. However, this memory
    overhead is much smaller than that used in the fine-grained quantization (AQ1),
    as QFeM utilizes only three tokens for the cache. Contrary to QFeP, QFeM shows
    inconsistent memory utilization. For example, the 7B model with QFeM exhibits
    memory usage similar to AQ2, while the 70B model with QFeM incur additional consumption
    for a sequence length of 1K. This is attributed to the use of W8A16 for the unquantization
    modules in QFeM. To tailor the memory usage or inference speed, an alternative
    strategy can be utilized for QFeM, such as applying fine-grained activation quantization
    to the unquantization modules instead of using W8A16.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¡¨Â [5](#S5.T5 "è¡¨ 5 â€£ æ¨ç†å»¶è¿Ÿã€‚ â€£ 5.5 è®¡ç®—æˆæœ¬åˆ†æ â€£ 5 å®éªŒ â€£ ç¼“è§£å› GLUåŸºäºLLMä¸­çš„æ¿€æ´»å³°å€¼å¼•èµ·çš„é‡åŒ–è¯¯å·®")
    ä¸­ï¼Œæˆ‘ä»¬è®°å½•äº†æˆ‘ä»¬æ–¹æ³•çš„æœ€å¤§å†…å­˜å ç”¨ã€‚å¯¹äºQFePï¼Œé¢å¤–çš„å†…å­˜å§‹ç»ˆç”¨äºä¿ç•™çš„KVç¼“å­˜ã€‚ç„¶è€Œï¼Œè¿™ä¸€å†…å­˜å¼€é”€è¿œå°äºç»†ç²’åº¦é‡åŒ–ï¼ˆAQ1ï¼‰æ‰€éœ€çš„ï¼Œå› ä¸ºQFeMä»…ä½¿ç”¨ä¸‰ä¸ªä»¤ç‰Œè¿›è¡Œç¼“å­˜ã€‚ä¸QFePç›¸åï¼ŒQFeMè¡¨ç°å‡ºä¸ä¸€è‡´çš„å†…å­˜åˆ©ç”¨æƒ…å†µã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨QFeMçš„7Bæ¨¡å‹è¡¨ç°å‡ºç±»ä¼¼äºAQ2çš„å†…å­˜ä½¿ç”¨æƒ…å†µï¼Œè€Œä½¿ç”¨QFeMçš„70Bæ¨¡å‹åœ¨åºåˆ—é•¿åº¦ä¸º1Kæ—¶ä¼šå¢åŠ é¢å¤–çš„æ¶ˆè€—ã€‚è¿™æ˜¯ç”±äºQFeMä¸­æœªé‡åŒ–æ¨¡å—ä½¿ç”¨äº†W8A16ã€‚ä¸ºäº†è°ƒæ•´å†…å­˜ä½¿ç”¨æˆ–æ¨ç†é€Ÿåº¦ï¼Œå¯ä»¥å¯¹QFeMä½¿ç”¨æ›¿ä»£ç­–ç•¥ï¼Œä¾‹å¦‚å¯¹æœªé‡åŒ–æ¨¡å—åº”ç”¨ç»†ç²’åº¦æ¿€æ´»é‡åŒ–ï¼Œè€Œä¸æ˜¯ä½¿ç”¨W8A16ã€‚
- en: 6 Conclusion
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 ç»“è®º
- en: We explore the quantization challenge of GLU activations for modern LLMs. We
    find that the GLU variants generates excessive activation scales, which cause
    significant quantization bottlenecks at the specific layers. Based on the systematic
    generation pattern of the activation spikes, we propose methods that address the
    spikes in a layer-wise (QFeM) and token-wise manner (QFeP). In the experiments,
    we confirm that the proposed methods effectively resolve the quantization bottlenecks
    and result in a large performance gain. We expect that our work sheds light on
    the potential challenges in future studies regarding quantization and facilitates
    the development of efficient LLM systems.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ¢è®¨äº†ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­GLUæ¿€æ´»çš„é‡åŒ–æŒ‘æˆ˜ã€‚æˆ‘ä»¬å‘ç°GLUå˜ä½“ç”Ÿæˆè¿‡å¤šçš„æ¿€æ´»å°ºåº¦ï¼Œè¿™åœ¨ç‰¹å®šå±‚ä¸­é€ æˆäº†æ˜¾è‘—çš„é‡åŒ–ç“¶é¢ˆã€‚åŸºäºæ¿€æ´»å³°å€¼çš„ç³»ç»Ÿç”Ÿæˆæ¨¡å¼ï¼Œæˆ‘ä»¬æå‡ºäº†åœ¨å±‚çº§ï¼ˆQFeMï¼‰å’Œä»¤ç‰Œçº§åˆ«ï¼ˆQFePï¼‰å¤„ç†è¿™äº›å³°å€¼çš„æ–¹æ³•ã€‚åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬ç¡®è®¤æ‰€æå‡ºçš„æ–¹æ³•æœ‰æ•ˆè§£å†³äº†é‡åŒ–ç“¶é¢ˆï¼Œå¹¶å¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬æœŸæœ›æˆ‘ä»¬çš„å·¥ä½œèƒ½å¯¹æœªæ¥å…³äºé‡åŒ–çš„ç ”ç©¶ä¸­çš„æ½œåœ¨æŒ‘æˆ˜æä¾›å¯ç¤ºï¼Œå¹¶ä¿ƒè¿›é«˜æ•ˆLLMç³»ç»Ÿçš„å‘å±•ã€‚
- en: References
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] Arash Ahmadian, Saurabh Dash, Hongyu Chen, Bharat Venkitesh, ZhenÂ Stephen
    Gou, Phil Blunsom, Ahmet ÃœstÃ¼n, and Sara Hooker. Intriguing properties of quantization
    at scale. Advances in Neural Information Processing Systems, 36:34278â€“34294, 2023.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Arash Ahmadian, Saurabh Dash, Hongyu Chen, Bharat Venkitesh, Zhen Stephen
    Gou, Phil Blunsom, Ahmet ÃœstÃ¼n å’Œ Sara Hookerã€‚é‡åŒ–è§„æ¨¡çš„æœ‰è¶£ç‰¹æ€§ã€‚ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ï¼Œ36:34278â€“34294ï¼Œ2023å¹´ã€‚'
- en: '[2] Joshua Ainslie, James Lee-Thorp, Michiel deÂ Jong, Yury Zemlyanskiy, Federico
    LebrÃ³n, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models
    from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico
    LebrÃ³n å’Œ Sumit Sanghaiã€‚Gqa: ä»å¤šå¤´æ£€æŸ¥ç‚¹è®­ç»ƒé€šç”¨å¤šæŸ¥è¯¢å˜æ¢å™¨æ¨¡å‹ã€‚arXiv é¢„å°æœ¬ arXiv:2305.13245ï¼Œ2023å¹´ã€‚'
- en: '[3] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli,
    Ruxandra Cojocaru, MÃ©rouane Debbah, Ã‰tienne Goffinet, Daniel Hesslow, Julien Launay,
    Quentin Malartic, etÂ al. The falcon series of open language models. arXiv preprint
    arXiv:2311.16867, 2023.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli,
    Ruxandra Cojocaru, MÃ©rouane Debbah, Ã‰tienne Goffinet, Daniel Hesslow, Julien Launay,
    Quentin Malartic ç­‰ã€‚å¼€æ”¾è¯­è¨€æ¨¡å‹çš„çŒé¹°ç³»åˆ—ã€‚arXiv é¢„å°æœ¬ arXiv:2311.16867ï¼Œ2023å¹´ã€‚'
- en: '[4] Alexei Baevski and Michael Auli. Adaptive input representations for neural
    language modeling. In International Conference on Learning Representations, 2018.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Alexei Baevski å’Œ Michael Auli. ç¥ç»è¯­è¨€å»ºæ¨¡çš„è‡ªé€‚åº”è¾“å…¥è¡¨ç¤ºã€‚åœ¨å›½é™…å­¦ä¹ è¡¨å¾ä¼šè®®ä¸Šï¼Œ2018å¹´ã€‚'
- en: '[5] Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi,
    Reshinth Adithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, etÂ al.
    Stable lm 2 1.6 b technical report. arXiv preprint arXiv:2402.17834, 2024.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Marco Bellagenteã€Jonathan Towã€Dakota Mahanã€Duy Phungã€Maksym Zhuravinskyiã€Reshinth
    Adithyanã€James Baicoianuã€Ben Brooksã€Nathan Cooperã€Ashish Datta ç­‰äººã€‚ç¨³å®šçš„ LM 2 1.6
    b æŠ€æœ¯æŠ¥å‘Šã€‚arXiv é¢„å°æœ¬ arXiv:2402.17834ï¼Œ2024å¹´ã€‚'
- en: '[6] Stella Biderman, Hailey Schoelkopf, QuentinÂ Gregory Anthony, Herbie Bradley,
    Kyle Oâ€™Brien, Eric Hallahan, MohammadÂ Aflah Khan, Shivanshu Purohit, USVSNÂ Sai
    Prashanth, Edward Raff, etÂ al. Pythia: A suite for analyzing large language models
    across training and scaling. In International Conference on Machine Learning,
    pages 2397â€“2430\. PMLR, 2023.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Stella Bidermanã€Hailey Schoelkopfã€Quentin Gregory Anthonyã€Herbie Bradleyã€Kyle
    Oâ€™Brienã€Eric Hallahanã€Mohammad Aflah Khanã€Shivanshu Purohitã€USVSN Sai Prashanthã€Edward
    Raff ç­‰äººã€‚Pythia: ä¸€ä¸ªç”¨äºåˆ†æå¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒå’Œæ‰©å±•çš„å¥—ä»¶ã€‚åœ¨å›½é™…æœºå™¨å­¦ä¹ å¤§ä¼šä¸Šï¼Œç¬¬2397â€“2430é¡µã€‚PMLRï¼Œ2023å¹´ã€‚'
- en: '[7] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, etÂ al. Piqa: Reasoning
    about physical commonsense in natural language. In Proceedings of the AAAI conference
    on artificial intelligence, volumeÂ 34, pages 7432â€“7439, 2020.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Yonatan Biskã€Rowan Zellersã€Jianfeng Gaoã€Yejin Choi ç­‰äººã€‚Piqa: ä»¥è‡ªç„¶è¯­è¨€æ¨ç†å…³äºç‰©ç†å¸¸è¯†ã€‚åœ¨
    AAAI äººå·¥æ™ºèƒ½ä¼šè®®è®ºæ–‡é›†ä¸­ï¼Œç¬¬34å·ï¼Œç¬¬7432â€“7439é¡µï¼Œ2020å¹´ã€‚'
- en: '[8] Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding
    and overcoming the challenges of efficient transformer quantization. In Marie-Francine
    Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings
    of the 2021 Conference on Empirical Methods in Natural Language Processing, pages
    7947â€“7969, Online and Punta Cana, Dominican Republic, November 2021\. Association
    for Computational Linguistics.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Yelysei Bondarenko, Markus Nagel å’Œ Tijmen Blankevoort. ç†è§£å’Œå…‹æœé«˜æ•ˆå˜å‹å™¨é‡åŒ–çš„æŒ‘æˆ˜ã€‚åœ¨
    Marie-Francine Moensã€Xuanjing Huangã€Lucia Specia å’Œ Scott Wen-tau Yih ä¸»ç¼–çš„ã€Š2021å¹´è‡ªç„¶è¯­è¨€å¤„ç†ç»éªŒæ–¹æ³•ä¼šè®®è®ºæ–‡é›†ã€‹ä¸­ï¼Œç¬¬7947â€“7969é¡µï¼Œåœ¨çº¿åŠå¤šç±³å°¼åŠ å…±å’Œå›½è“¬å¡”å¡çº³ï¼Œ2021å¹´11æœˆã€‚è®¡ç®—è¯­è¨€å­¦åä¼šã€‚'
- en: '[9] Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Quantizable transformers:
    Removing outliers by helping attention heads do nothing. Advances in Neural Information
    Processing Systems, 36, 2024.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Yelysei Bondarenkoã€Markus Nagel å’Œ Tijmen Blankevoort. å¯é‡åŒ–çš„å˜å‹å™¨: é€šè¿‡å¸®åŠ©æ³¨æ„åŠ›å¤´æ— æ‰€ä½œä¸ºæ¥å»é™¤å¼‚å¸¸å€¼ã€‚ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ï¼Œ36ï¼Œ2024å¹´ã€‚'
- en: '[10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, JaredÂ D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    etÂ al. Language models are few-shot learners. Advances in neural information processing
    systems, 33:1877â€“1901, 2020.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Tom Brownã€Benjamin Mannã€Nick Ryderã€Melanie Subbiahã€Jared D Kaplanã€Prafulla
    Dhariwalã€Arvind Neelakantanã€Pranav Shyamã€Girish Sastryã€Amanda Askell ç­‰äººã€‚è¯­è¨€æ¨¡å‹æ˜¯å°‘æ ·æœ¬å­¦ä¹ è€…ã€‚ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ï¼Œ33:1877â€“1901ï¼Œ2020å¹´ã€‚'
- en: '[11] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and ChristopherÂ M DeÂ Sa. Quip:
    2-bit quantization of large language models with guarantees. Advances in Neural
    Information Processing Systems, 36, 2024.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Jerry Cheeã€Yaohui Caiã€Volodymyr Kuleshov å’Œ Christopher M De Sa. Quip:
    å¸¦ä¿è¯çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„2ä½é‡åŒ–ã€‚ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ï¼Œ36ï¼Œ2024å¹´ã€‚'
- en: '[12] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3\.
    int8 (): 8-bit matrix multiplication for transformers at scale. Advances in Neural
    Information Processing Systems, 35:30318â€“30332, 2022.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Tim Dettmersã€Mike Lewisã€Younes Belkada å’Œ Luke Zettlemoyer. Gpt3\. int8
    (): å¤§è§„æ¨¡å˜å‹å™¨çš„8ä½çŸ©é˜µä¹˜æ³•ã€‚ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ï¼Œ35:30318â€“30332ï¼Œ2022å¹´ã€‚'
- en: '[13] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev,
    Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh.
    Spqr: A sparse-quantized representation for near-lossless llm weight compression.
    arXiv preprint arXiv:2306.03078, 2023.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Tim Dettmersã€Ruslan Svirschevskiã€Vage Egiazarianã€Denis Kuznedelevã€Elias
    Frantarã€Saleh Ashkboosã€Alexander Borzunovã€Torsten Hoefler å’Œ Dan Alistarh. Spqr:
    ä¸€ç§ç¨€ç–é‡åŒ–è¡¨ç¤ºæ–¹æ³•ï¼Œç”¨äºæ¥è¿‘æ— æŸçš„ LLM æƒé‡å‹ç¼©ã€‚arXiv é¢„å°æœ¬ arXiv:2306.03078ï¼Œ2023å¹´ã€‚'
- en: '[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:
    Pre-training of deep bidirectional transformers for language understanding. arXiv
    preprint arXiv:1810.04805, 2018.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Jacob Devlinã€Ming-Wei Changã€Kenton Lee å’Œ Kristina Toutanova. Bert: æ·±åº¦åŒå‘å˜å‹å™¨çš„é¢„è®­ç»ƒç”¨äºè¯­è¨€ç†è§£ã€‚arXiv
    é¢„å°æœ¬ arXiv:1810.04805ï¼Œ2018å¹´ã€‚'
- en: '[15] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq:
    Accurate post-training quantization for generative pre-trained transformers. arXiv
    preprint arXiv:2210.17323, 2022.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Elias Frantarã€Saleh Ashkboosã€Torsten Hoefler å’Œ Dan Alistarh. Gptq: ç”Ÿæˆé¢„è®­ç»ƒå˜å‹å™¨çš„å‡†ç¡®åè®­ç»ƒé‡åŒ–ã€‚arXiv
    é¢„å°æœ¬ arXiv:2210.17323ï¼Œ2022å¹´ã€‚'
- en: '[16] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain LeÂ Noacâ€™h, Haonan
    Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds,
    Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben
    Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation,
    12 2023.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noacâ€™h, Haonan
    Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds,
    Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben
    Wang, Kevin Wang, å’Œ Andy Zouã€‚å°‘æ ·æœ¬è¯­è¨€æ¨¡å‹è¯„ä¼°æ¡†æ¶ï¼Œ2023å¹´12æœˆã€‚'
- en: '[17] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, MichaelÂ W Mahoney, and
    Kurt Keutzer. A survey of quantization methods for efficient neural network inference.
    In Low-Power Computer Vision, pages 291â€“326\. Chapman and Hall/CRC, 2022.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, å’Œ
    Kurt Keutzerã€‚é«˜æ•ˆç¥ç»ç½‘ç»œæ¨æ–­çš„é‡åŒ–æ–¹æ³•ç»¼è¿°ã€‚æ”¶å½•äºã€Šä½åŠŸè€—è®¡ç®—æœºè§†è§‰ã€‹ï¼Œç¬¬291â€“326é¡µã€‚Chapman and Hall/CRCï¼Œ2022å¹´ã€‚'
- en: '[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings
    in deep residual networks. In Computer Visionâ€“ECCV 2016: 14th European Conference,
    Amsterdam, The Netherlands, October 11â€“14, 2016, Proceedings, Part IV 14, pages
    630â€“645\. Springer, 2016.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, å’Œ Jian Sunã€‚æ·±åº¦æ®‹å·®ç½‘ç»œä¸­çš„èº«ä»½æ˜ å°„ã€‚æ”¶å½•äºã€Šè®¡ç®—æœºè§†è§‰â€“ECCV
    2016ï¼šç¬¬14å±Šæ¬§æ´²ä¼šè®®ï¼Œè·å…°é˜¿å§†æ–¯ç‰¹ä¸¹ï¼Œ2016å¹´10æœˆ11â€“14æ—¥ï¼Œè®ºæ–‡é›†ã€‹ç¬¬IVå·ç¬¬14éƒ¨åˆ†ï¼Œç¬¬630â€“645é¡µã€‚Springerï¼Œ2016å¹´ã€‚'
- en: '[19] Benoit Jacob, Skirmantas Kligys, BoÂ Chen, Menglong Zhu, Matthew Tang,
    Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training
    of neural networks for efficient integer-arithmetic-only inference. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 2704â€“2713,
    2018.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang,
    Andrew Howard, Hartwig Adam, å’Œ Dmitry Kalenichenkoã€‚ç”¨äºé«˜æ•ˆæ•´æ•°ç®—æœ¯æ¨æ–­çš„ç¥ç»ç½‘ç»œé‡åŒ–ä¸è®­ç»ƒã€‚æ”¶å½•äº IEEE
    è®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†ï¼Œç¬¬2704â€“2713é¡µï¼Œ2018å¹´ã€‚'
- en: '[20] AlbertÂ Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
    DevendraÂ Singh Chaplot, Diego deÂ las Casas, Florian Bressand, Gianna Lengyel,
    Guillaume Lample, Lucile Saulnier, etÂ al. Mistral 7b. arXiv preprint arXiv:2310.06825,
    2023.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
    Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,
    Guillaume Lample, Lucile Saulnier ç­‰ã€‚Mistral 7bã€‚arXiv é¢„å°æœ¬ arXiv:2310.06825ï¼Œ2023å¹´ã€‚'
- en: '[21] AlbertÂ Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche
    Savary, Chris Bamford, DevendraÂ Singh Chaplot, Diego deÂ las Casas, EmmaÂ Bou Hanna,
    Florian Bressand, etÂ al. Mixtral of experts. arXiv preprint arXiv:2401.04088,
    2024.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche
    Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna,
    Florian Bressand ç­‰ã€‚Mixtral ä¸“å®¶ç³»ç»Ÿã€‚arXiv é¢„å°æœ¬ arXiv:2401.04088ï¼Œ2024å¹´ã€‚'
- en: '[22] Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu
    Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, etÂ al. Solar 10.7 b: Scaling
    large language models with simple yet effective depth up-scaling. arXiv preprint
    arXiv:2312.15166, 2023.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu
    Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim ç­‰ã€‚Solar 10.7 bï¼šé€šè¿‡ç®€å•è€Œæœ‰æ•ˆçš„æ·±åº¦ä¸Šé‡‡æ ·æ¥æ‰©å±•å¤§å‹è¯­è¨€æ¨¡å‹ã€‚arXiv
    é¢„å°æœ¬ arXiv:2312.15166ï¼Œ2023å¹´ã€‚'
- en: '[23] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen,
    MichaelÂ W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization.
    arXiv preprint arXiv:2306.07629, 2023.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen,
    Michael W Mahoney, å’Œ Kurt Keutzerã€‚Squeezellmï¼šå¯†é›†ä¸ç¨€ç–é‡åŒ–ã€‚arXiv é¢„å°æœ¬ arXiv:2306.07629ï¼Œ2023å¹´ã€‚'
- en: '[24] Olga Kovaleva, Saurabh Kulshreshtha, Anna Rogers, and Anna Rumshisky.
    BERT busters: Outlier dimensions that disrupt transformers. In Chengqing Zong,
    Fei Xia, Wenjie Li, and Roberto Navigli, editors, Findings of the Association
    for Computational Linguistics: ACL-IJCNLP 2021, pages 3392â€“3405, Online, August
    2021\. Association for Computational Linguistics.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Olga Kovaleva, Saurabh Kulshreshtha, Anna Rogers, å’Œ Anna Rumshiskyã€‚BERT
    ç ´åè€…ï¼šæ‰°ä¹±å˜å‹å™¨çš„ç¦»ç¾¤ç»´åº¦ã€‚æ”¶å½•äº Chengqing Zong, Fei Xia, Wenjie Li, å’Œ Roberto Navigli ç¼–è¾‘çš„ã€Šè®¡ç®—è¯­è¨€å­¦åä¼šä¼šè®®è®ºæ–‡é›†ï¼šACL-IJCNLP
    2021ã€‹ï¼Œç¬¬3392â€“3405é¡µï¼Œåœ¨çº¿ï¼Œ2021å¹´8æœˆã€‚è®¡ç®—è¯­è¨€å­¦åä¼šã€‚'
- en: '[25] Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. Revealing
    the dark secrets of BERT. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun
    Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP), pages 4365â€“4374, Hong Kong, China, November 2019\.
    Association for Computational Linguistics.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Olga Kovaleva, Alexey Romanov, Anna Rogers, å’Œ Anna Rumshiskyã€‚æ­ç¤ºBERTçš„é»‘æš—ç§˜å¯†ã€‚åœ¨
    Kentaro Inui, Jing Jiang, Vincent Ng, å’Œ Xiaojun Wan ç¼–è¾‘çš„ã€Š2019å¹´è‡ªç„¶è¯­è¨€å¤„ç†ç»éªŒæ–¹æ³•ä¼šè®®æš¨ç¬¬9å±Šå›½é™…è‡ªç„¶è¯­è¨€å¤„ç†è”åˆä¼šè®®ï¼ˆEMNLP-IJCNLPï¼‰ã€‹ä¼šè®®å½•ä¸­ï¼Œé¡µé¢
    4365â€“4374ï¼Œä¸­å›½é¦™æ¸¯ï¼Œ2019å¹´11æœˆã€‚è®¡ç®—è¯­è¨€å­¦åä¼šã€‚'
- en: '[26] JiÂ Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song
    Han. Awq: Activation-aware weight quantization for llm compression and acceleration.
    arXiv preprint arXiv:2306.00978, 2023.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, å’Œ Song Hanã€‚Awq:
    æ¿€æ´»æ„ŸçŸ¥çš„æƒé‡é‡åŒ–ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„å‹ç¼©å’ŒåŠ é€Ÿã€‚arXiv é¢„å°æœ¬ arXiv:2306.00978ï¼Œ2023å¹´ã€‚'
- en: '[27] Ziyang Luo, Artur Kulmizev, and Xiaoxi Mao. Positional artefacts propagate
    through masked language model embeddings. In Chengqing Zong, Fei Xia, Wenjie Li,
    and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association
    for Computational Linguistics and the 11th International Joint Conference on Natural
    Language Processing (Volume 1: Long Papers), pages 5312â€“5327, Online, August 2021\.
    Association for Computational Linguistics.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Ziyang Luo, Artur Kulmizev, å’Œ Xiaoxi Maoã€‚ä½ç½®ä¼ªå½±é€šè¿‡æ©è”½è¯­è¨€æ¨¡å‹åµŒå…¥ä¼ æ’­ã€‚åœ¨ Chengqing
    Zong, Fei Xia, Wenjie Li, å’Œ Roberto Navigli ç¼–è¾‘çš„ã€Šç¬¬59å±Šè®¡ç®—è¯­è¨€å­¦åä¼šå¹´ä¼šæš¨ç¬¬11å±Šå›½é™…è‡ªç„¶è¯­è¨€å¤„ç†è”åˆä¼šè®®ï¼ˆç¬¬1å·ï¼šé•¿ç¯‡è®ºæ–‡ï¼‰ã€‹ä¼šè®®å½•ä¸­ï¼Œé¡µé¢
    5312â€“5327ï¼Œåœ¨çº¿ï¼Œ2021å¹´8æœˆã€‚è®¡ç®—è¯­è¨€å­¦åä¼šã€‚'
- en: '[28] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer
    sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Stephen Merity, Caiming Xiong, James Bradbury, å’Œ Richard Socherã€‚æŒ‡é’ˆå“¨å…µæ··åˆæ¨¡å‹ã€‚arXiv
    é¢„å°æœ¬ arXiv:1609.07843ï¼Œ2016å¹´ã€‚'
- en: '[29] Javaheripi Mojan and Bubeck SÃ©bastien. Phi-2: The surprising power of
    small language models, 2023.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Javaheripi Mojan å’Œ Bubeck SÃ©bastienã€‚Phi-2: å°å‹è¯­è¨€æ¨¡å‹çš„æƒŠäººåŠ›é‡ï¼Œ2023å¹´ã€‚'
- en: '[30] Markus Nagel, Marios Fournarakis, RanaÂ Ali Amjad, Yelysei Bondarenko,
    Mart VanÂ Baalen, and Tijmen Blankevoort. A white paper on neural network quantization.
    arXiv preprint arXiv:2106.08295, 2021.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko,
    Mart Van Baalen, å’Œ Tijmen Blankevoortã€‚å…³äºç¥ç»ç½‘ç»œé‡åŒ–çš„ç™½çš®ä¹¦ã€‚arXiv é¢„å°æœ¬ arXiv:2106.08295ï¼Œ2021å¹´ã€‚'
- en: '[31] Sharan Narang, HyungÂ Won Chung, YiÂ Tay, Liam Fedus, Thibault Fevry, Michael
    Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, Yanqi Zhou,
    Wei Li, Nan Ding, Jake Marcus, Adam Roberts, and Colin Raffel. Do transformer
    modifications transfer across implementations and applications? In Marie-Francine
    Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings
    of the 2021 Conference on Empirical Methods in Natural Language Processing, pages
    5758â€“5773, Online and Punta Cana, Dominican Republic, November 2021\. Association
    for Computational Linguistics.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Sharan Narang, Hyung Won Chung, Yi Tay, Liam Fedus, Thibault Fevry, Michael
    Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, Yanqi Zhou,
    Wei Li, Nan Ding, Jake Marcus, Adam Roberts, å’Œ Colin Raffelã€‚å˜æ¢å™¨ä¿®æ”¹æ˜¯å¦åœ¨å®ç°å’Œåº”ç”¨ä¸­è½¬ç§»ï¼Ÿåœ¨
    Marie-Francine Moens, Xuanjing Huang, Lucia Specia, å’Œ Scott Wen-tau Yih ç¼–è¾‘çš„ã€Š2021å¹´è‡ªç„¶è¯­è¨€å¤„ç†ç»éªŒæ–¹æ³•ä¼šè®®ã€‹ä¼šè®®å½•ä¸­ï¼Œé¡µé¢
    5758â€“5773ï¼Œåœ¨çº¿å’Œå¤šç±³å°¼åŠ å…±å’Œå›½è“¬å¡”å¡çº³ï¼Œ2021å¹´11æœˆã€‚è®¡ç®—è¯­è¨€å­¦åä¼šã€‚'
- en: '[32] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan
    Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for
    sequence modeling. arXiv preprint arXiv:1904.01038, 2019.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan
    Ng, David Grangier, å’Œ Michael Auliã€‚fairseq: ä¸€ä¸ªå¿«é€Ÿã€å¯æ‰©å±•çš„åºåˆ—å»ºæ¨¡å·¥å…·åŒ…ã€‚arXiv é¢„å°æœ¬ arXiv:1904.01038ï¼Œ2019å¹´ã€‚'
- en: '[33] Denis Paperno, GermÃ¡n Kruszewski, Angeliki Lazaridou, NgocÂ Quan Pham,
    Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel FernÃ¡ndez.
    The LAMBADA dataset: Word prediction requiring a broad discourse context. In Katrin
    Erk and NoahÂ A. Smith, editors, Proceedings of the 54th Annual Meeting of the
    Association for Computational Linguistics (Volume 1: Long Papers), pages 1525â€“1534,
    Berlin, Germany, August 2016\. Association for Computational Linguistics.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Denis Paperno, GermÃ¡n Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham,
    Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, å’Œ Raquel FernÃ¡ndezã€‚LAMBADA
    æ•°æ®é›†ï¼šéœ€è¦å¹¿æ³›è¯è¯­èƒŒæ™¯çš„è¯é¢„æµ‹ã€‚åœ¨ Katrin Erk å’Œ Noah A. Smith ç¼–è¾‘çš„ã€Šç¬¬54å±Šè®¡ç®—è¯­è¨€å­¦åä¼šå¹´ä¼šï¼ˆç¬¬1å·ï¼šé•¿ç¯‡è®ºæ–‡ï¼‰ã€‹ä¼šè®®å½•ä¸­ï¼Œé¡µé¢
    1525â€“1534ï¼Œå¾·å›½æŸæ—ï¼Œ2016å¹´8æœˆã€‚è®¡ç®—è¯­è¨€å­¦åä¼šã€‚'
- en: '[34] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James
    Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently
    scaling transformer inference. Proceedings of Machine Learning and Systems, 5,
    2023.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Reiner Popeã€Sholto Douglasã€Aakanksha Chowdheryã€Jacob Devlinã€James Bradburyã€Jonathan
    Heekã€Kefan Xiaoã€Shivani Agrawal å’Œ Jeff Deanã€‚é«˜æ•ˆæ‰©å±•å˜æ¢å™¨æ¨ç†ã€‚æœºå™¨å­¦ä¹ ä¸ç³»ç»Ÿä¼šè®®è®ºæ–‡é›†ï¼Œ5ï¼Œ2023å¹´ã€‚'
- en: '[35] Giovanni Puccetti, Anna Rogers, Aleksandr Drozd, and Felice Dellâ€™Orletta.
    Outlier dimensions that disrupt transformers are driven by frequency. In Yoav
    Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Findings of the Association
    for Computational Linguistics: EMNLP 2022, pages 1286â€“1304, Abu Dhabi, United
    Arab Emirates, December 2022\. Association for Computational Linguistics.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Giovanni Puccetti, Anna Rogers, Aleksandr Drozd å’Œ Felice Dellâ€™Orletta.
    å½±å“å˜æ¢å™¨çš„å¼‚å¸¸ç»´åº¦ç”±é¢‘ç‡é©±åŠ¨ã€‚æ”¶å½•äº Yoav Goldbergã€Zornitsa Kozareva å’Œ Yue Zhang ä¸»ç¼–çš„ã€Šè®¡ç®—è¯­è¨€å­¦åä¼šå‘ç°ï¼šEMNLP
    2022ã€‹ï¼Œç¬¬1286â€“1304é¡µï¼Œé˜¿å¸ƒæ‰æ¯”ï¼Œé˜¿æ‹‰ä¼¯è”åˆé…‹é•¿å›½ï¼Œ2022å¹´12æœˆã€‚è®¡ç®—è¯­è¨€å­¦åä¼šã€‚'
- en: '[36] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
    Sutskever, etÂ al. Language models are unsupervised multitask learners. OpenAI
    blog, 1(8):9, 2019.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Alec Radfordã€Jeffrey Wuã€Rewon Childã€David Luanã€Dario Amodeiã€Ilya Sutskever
    ç­‰äººã€‚è¯­è¨€æ¨¡å‹æ˜¯æ— ç›‘ç£çš„å¤šä»»åŠ¡å­¦ä¹ è€…ã€‚OpenAI åšå®¢ï¼Œ1(8):9ï¼Œ2019å¹´ã€‚'
- en: '[37] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
    Michael Matena, Yanqi Zhou, Wei Li, and PeterÂ J Liu. Exploring the limits of transfer
    learning with a unified text-to-text transformer. Journal of machine learning
    research, 21(140):1â€“67, 2020.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Colin Raffelã€Noam Shazeerã€Adam Robertsã€Katherine Leeã€Sharan Narangã€Michael
    Matenaã€Yanqi Zhouã€Wei Li å’Œ Peter J Liuã€‚æ¢ç´¢ç»Ÿä¸€æ–‡æœ¬åˆ°æ–‡æœ¬å˜æ¢å™¨çš„è¿ç§»å­¦ä¹ æé™ã€‚æœºå™¨å­¦ä¹ ç ”ç©¶æœŸåˆŠï¼Œ21(140):1â€“67ï¼Œ2020å¹´ã€‚'
- en: '[38] Keisuke Sakaguchi, RonanÂ Le Bras, Chandra Bhagavatula, and Yejin Choi.
    Winogrande: An adversarial winograd schema challenge at scale. arXiv preprint
    arXiv:1907.10641, 2019.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Keisuke Sakaguchiã€Ronan Le Brasã€Chandra Bhagavatula å’Œ Yejin Choiã€‚Winograndeï¼šè§„æ¨¡åŒ–çš„å¯¹æŠ—æ€§
    Winograd è¯­æ–™åº“æŒ‘æˆ˜ã€‚arXiv é¢„å°æœ¬ arXiv:1907.10641ï¼Œ2019å¹´ã€‚'
- en: '[39] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian
    Li, Kaipeng Zhang, Peng Gao, YuÂ Qiao, and Ping Luo. Omniquant: Omnidirectionally
    calibrated quantization for large language models. arXiv preprint arXiv:2308.13137,
    2023.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Wenqi Shaoã€Mengzhao Chenã€Zhaoyang Zhangã€Peng Xuã€Lirui Zhaoã€Zhiqian Liã€Kaipeng
    Zhangã€Peng Gaoã€Yu Qiao å’Œ Ping Luoã€‚Omniquantï¼šç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„å…¨æ–¹å‘æ ¡å‡†é‡åŒ–ã€‚arXiv é¢„å°æœ¬ arXiv:2308.13137ï¼Œ2023å¹´ã€‚'
- en: '[40] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202,
    2020.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Noam Shazeer. Glu å˜ä½“æ”¹å–„å˜æ¢å™¨ã€‚arXiv é¢„å°æœ¬ arXiv:2002.05202ï¼Œ2020å¹´ã€‚'
- en: '[41] Jianlin Su, Murtadha Ahmed, YuÂ Lu, Shengfeng Pan, Wen Bo, and Yunfeng
    Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing,
    568:127063, 2024.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Jianlin Suã€Murtadha Ahmedã€Yu Luã€Shengfeng Panã€Wen Bo å’Œ Yunfeng Liuã€‚Roformerï¼šå…·æœ‰æ—‹è½¬ä½ç½®åµŒå…¥çš„å¢å¼ºå˜æ¢å™¨ã€‚ç¥ç»è®¡ç®—ï¼Œ568:127063ï¼Œ2024å¹´ã€‚'
- en: '[42] Mingjie Sun, Xinlei Chen, JÂ Zico Kolter, and Zhuang Liu. Massive activations
    in large language models. arXiv preprint arXiv:2402.17762, 2024.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Mingjie Sunã€Xinlei Chenã€J Zico Kolter å’Œ Zhuang Liuã€‚å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¤§é‡æ¿€æ´»ã€‚arXiv
    é¢„å°æœ¬ arXiv:2402.17762ï¼Œ2024å¹´ã€‚'
- en: '[43] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju,
    Shreya Pathak, Laurent Sifre, Morgane RiviÃ¨re, MihirÂ Sanjay Kale, Juliette Love,
    etÂ al. Gemma: Open models based on gemini research and technology. arXiv preprint
    arXiv:2403.08295, 2024.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Gemma å›¢é˜Ÿã€Thomas Mesnardã€Cassidy Hardinã€Robert Dadashiã€Surya Bhupatirajuã€Shreya
    Pathakã€Laurent Sifreã€Morgane RiviÃ¨reã€Mihir Sanjay Kaleã€Juliette Love ç­‰äººã€‚Gemmaï¼šåŸºäºåŒå­åº§ç ”ç©¶å’ŒæŠ€æœ¯çš„å¼€æ”¾æ¨¡å‹ã€‚arXiv
    é¢„å°æœ¬ arXiv:2403.08295ï¼Œ2024å¹´ã€‚'
- en: '[44] MosaicMLÂ NLP Team. Introducing mpt-7b: A new standard for open-source,
    commercially usable llms, 2023. Accessed: 2023-05-05.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] MosaicML NLP å›¢é˜Ÿã€‚ä»‹ç» mpt-7bï¼šå¼€æ”¾æºä»£ç ã€å•†ä¸šå¯ç”¨çš„ llms çš„æ–°æ ‡å‡†ï¼Œ2023å¹´ã€‚è®¿é—®æ—¶é—´ï¼š2023-05-05ã€‚'
- en: '[45] William Timkey and Marten van Schijndel. All bark and no bite: Rogue dimensions
    in transformer language models obscure representational quality. In Marie-Francine
    Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings
    of the 2021 Conference on Empirical Methods in Natural Language Processing, pages
    4527â€“4546, Online and Punta Cana, Dominican Republic, November 2021\. Association
    for Computational Linguistics.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] William Timkey å’Œ Marten van Schijndelã€‚å…¨å£å·æ— å®è´¨ï¼šå˜æ¢å™¨è¯­è¨€æ¨¡å‹ä¸­çš„å¼‚å¸¸ç»´åº¦æ©ç›–äº†è¡¨å¾è´¨é‡ã€‚æ”¶å½•äº
    Marie-Francine Moensã€Xuanjing Huangã€Lucia Specia å’Œ Scott Wen-tau Yih ä¸»ç¼–çš„ã€Š2021å¹´è‡ªç„¶è¯­è¨€å¤„ç†å®è¯æ–¹æ³•ä¼šè®®è®ºæ–‡é›†ã€‹ï¼Œç¬¬4527â€“4546é¡µï¼Œåœ¨çº¿å’Œå¤šç±³å°¼åŠ å…±å’Œå›½è“¬å¡”å¡çº³ï¼Œ2021å¹´11æœˆã€‚è®¡ç®—è¯­è¨€å­¦åä¼šã€‚'
- en: '[46] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal
    Azhar, etÂ al. Llama: Open and efficient foundation language models. arXiv preprint
    arXiv:2302.13971, 2023.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] ä¹ŒæˆˆÂ·å›¾å¼—é¾™ï¼Œè’‚åšÂ·æ‹‰å¤«é‡Œå°”ï¼Œæˆˆç‰¹è€¶Â·ä¼Šæ‰å¡å°”ï¼Œæ³½ç»´å°”Â·é©¬å°”è’‚å†…ï¼Œç›ä¸½-å®‰Â·æ‹‰ç»ï¼Œè’‚è«ç‰¹Â·æ‹‰å…‹é²ç“¦ï¼Œå·´è’‚æ–¯ç‰¹Â·ç½—é½è€¶ï¼Œçº³æ›¼Â·æˆˆäºšå°”ï¼ŒåŸƒé‡Œå…‹Â·æ±‰å¸ƒç½—ï¼Œè´¹è¨å°”Â·é˜¿æ‰å°”ï¼Œç­‰ã€‚Llamaï¼šå¼€æ”¾ä¸”é«˜æ•ˆçš„åŸºç¡€è¯­è¨€æ¨¡å‹ã€‚arXiv
    é¢„å°æœ¬ arXiv:2302.13971ï¼Œ2023ã€‚'
- en: '[47] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    etÂ al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] ä¹ŒæˆˆÂ·å›¾å¼—é¾™ï¼Œè·¯æ˜“æ–¯Â·é©¬ä¸ï¼Œå‡¯æ–‡Â·æ–¯é€šï¼Œå½¼å¾—Â·é˜¿å°”ä¼¯ç‰¹ï¼Œé˜¿å§†è´¾å¾·Â·é˜¿å°”é©¬èµ«é‡Œï¼Œé›…æ–¯æ•Â·å·´å·´ä¼Šï¼Œå°¼å¤æ‹‰Â·å·´ä»€åˆ©ç§‘å¤«ï¼Œè‹ç±³äºšÂ·å·´ç‰¹æ‹‰ï¼Œæ™®æ‹‰å‰ç“¦å°”Â·å·´å°”åŠ ç“¦ï¼Œèˆ’æÂ·åšè¨å°”ï¼Œç­‰ã€‚Llama
    2ï¼šå¼€æ”¾åŸºç¡€å’Œå¾®è°ƒèŠå¤©æ¨¡å‹ã€‚arXiv é¢„å°æœ¬ arXiv:2307.09288ï¼Œ2023ã€‚'
- en: '[48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    AidanÂ N Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.
    Advances in neural information processing systems, 30, 2017.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] é˜¿å¸Œä»€Â·ç“¦æ–¯ç“¦å°¼ï¼Œè¯ºå§†Â·æ²™æ³½å°”ï¼Œå°¼åŸºÂ·å¸•å°”é©¬å°”ï¼Œé›…å„å¸ƒÂ·ä¹Œæ–¯ç§‘é›·ç‰¹ï¼Œåˆ©æ˜‚Â·ç¼æ–¯ï¼Œè‰¾ä¸¹Â·NÂ·æˆˆéº¦æ–¯ï¼Œå¢å¡æ–¯Â·å‡¯æ³½ï¼Œå’Œä¼Šåˆ©äºšÂ·æ³¢æ´›è‹æ¬£ã€‚æ³¨æ„åŠ›å³ä½ æ‰€éœ€ã€‚ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ï¼Œ30ï¼Œ2017ã€‚'
- en: '[49] Jason Wei, YiÂ Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian
    Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, etÂ al. Emergent
    abilities of large language models. arXiv preprint arXiv:2206.07682, 2022.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] æ°æ£®Â·éŸ¦ï¼Œæ˜“Â·æ³°ï¼Œç‘å¸ŒÂ·åšé©¬è¨å°¼ï¼Œç§‘æ—Â·æ‹‰è´¹å°”ï¼Œå·´é›·ç‰¹Â·ä½æ™®ï¼Œå¡å·´æ–¯è’‚å®‰Â·åšå°”æˆˆï¼Œè¾¾å°¼Â·å°¤åŠ å¡”ç›ï¼Œé©¬å°”æ»•Â·åšæ–¯é©¬ï¼Œä¸¹å°¼Â·å‘¨ï¼Œå”çº³å¾·Â·æ¢…èŒ¨å‹’ï¼Œç­‰ã€‚å¤§å‹è¯­è¨€æ¨¡å‹çš„çªç°èƒ½åŠ›ã€‚arXiv
    é¢„å°æœ¬ arXiv:2206.07682ï¼Œ2022ã€‚'
- en: '[50] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang
    Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization of large language
    models by equivalent and effective shifting and scaling. In Houda Bouamor, Juan
    Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical
    Methods in Natural Language Processing, pages 1648â€“1665, Singapore, December 2023\.
    Association for Computational Linguistics.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] å¾é¢–ï¼Œäº‘è¾°Â·å¼ ï¼Œä½™èˆªÂ·æï¼Œå‘å›½å¼ ï¼Œç‘æµ©Â·é¾šï¼Œé‡‘æ‰¬Â·éƒ­ï¼Œå’Œå‘é¾™Â·åˆ˜ã€‚å¼‚å¸¸å€¼æŠ‘åˆ¶+ï¼šé€šè¿‡ç­‰æ•ˆå’Œæœ‰æ•ˆçš„ç§»ä½ä¸ç¼©æ”¾å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç²¾ç¡®é‡åŒ–ã€‚åœ¨éœè¾¾Â·å¸ƒé˜¿è«å°”ï¼Œèƒ¡å®‰Â·çš®è¯ºï¼Œå’Œå¡åˆ©å¡Â·å·´åˆ©ï¼ˆç¼–è¾‘ï¼‰ï¼Œ2023å¹´è‡ªç„¶è¯­è¨€å¤„ç†å®è¯æ–¹æ³•ä¼šè®®è®ºæ–‡é›†ï¼Œé¡µ1648â€“1665ï¼Œæ–°åŠ å¡ï¼Œ2023å¹´12æœˆã€‚è®¡ç®—è¯­è¨€å­¦åä¼šã€‚'
- en: '[51] Guangxuan Xiao, JiÂ Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song
    Han. SmoothQuant: Accurate and efficient post-training quantization for large
    language models. In Proceedings of the 40th International Conference on Machine
    Learning, 2023.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] å¹¿è½©Â·è‚–ï¼Œå‰Â·æ—ï¼Œç±³å¡åŸƒå°”Â·å¡å…¹å†…å…‹ï¼Œéƒæ­¦ï¼Œæœ±åˆ©å®‰Â·å¾·ç©†æ–¯ï¼Œå’Œå®‹å¯’ã€‚SmoothQuantï¼šé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„ç²¾ç¡®ä¸”é«˜æ•ˆçš„åè®­ç»ƒé‡åŒ–ã€‚åœ¨ç¬¬40å±Šå›½é™…æœºå™¨å­¦ä¹ å¤§ä¼šè®ºæ–‡é›†ä¸­ï¼Œ2023ã€‚'
- en: '[52] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient
    streaming language models with attention sinks. arXiv preprint arXiv:2309.17453,
    2023.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] å¹¿è½©Â·è‚–ï¼Œè¢ä¸œå¤©ï¼Œè´è¿ªÂ·é™ˆï¼Œå®‹å¯’ï¼Œå’Œè¿ˆå…‹Â·åˆ˜æ˜“æ–¯ã€‚é«˜æ•ˆçš„æµå¼è¯­è¨€æ¨¡å‹ä¸æ³¨æ„åŠ›æ±‡èšã€‚arXiv é¢„å°æœ¬ arXiv:2309.17453ï¼Œ2023ã€‚'
- en: '[53] Ruibin Xiong, Yunchang Yang, DiÂ He, Kai Zheng, Shuxin Zheng, Chen Xing,
    Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization
    in the transformer architecture. In International Conference on Machine Learning,
    pages 10524â€“10533\. PMLR, 2020.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] é²å®¾Â·ç†Šï¼Œäº‘ç•…Â·æ¨ï¼Œç‹„Â·èµ«ï¼Œå‡¯Â·éƒ‘ï¼Œèˆ’æ¬£Â·éƒ‘ï¼Œé™ˆæ˜Ÿï¼Œæƒ å¸…Â·å¼ ï¼Œç‡•ç‡•Â·å…°ï¼Œæä¼ŸÂ·ç‹ï¼Œå’Œé“å²©Â·åˆ˜ã€‚åœ¨å˜æ¢å™¨æ¶æ„ä¸­çš„å±‚å½’ä¸€åŒ–ã€‚å›½é™…æœºå™¨å­¦ä¹ å¤§ä¼šè®ºæ–‡é›†ï¼Œé¡µ10524â€“10533ã€‚PMLRï¼Œ2020ã€‚'
- en: '[54] Zhewei Yao, Cheng Li, Xiaoxia Wu, Stephen Youn, and Yuxiong He. A comprehensive
    study on post-training quantization for large language models. arXiv preprint
    arXiv:2303.08302, 2023.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] èµµå“²ä¼Ÿï¼Œææˆï¼Œå´æ™“éœï¼Œæ–¯è’‚èŠ¬Â·æ¨ï¼Œå’Œä½•å®‡é›„ã€‚å¤§å‹è¯­è¨€æ¨¡å‹çš„åè®­ç»ƒé‡åŒ–ç»¼åˆç ”ç©¶ã€‚arXiv é¢„å°æœ¬ arXiv:2303.08302ï¼Œ2023ã€‚'
- en: '[55] Zhewei Yao, Reza YazdaniÂ Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization
    for large-scale transformers. Advances in Neural Information Processing Systems,
    35:27168â€“27183, 2022.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] èµµå“²ä¼Ÿï¼Œé›·æ‰Â·é›…å…¹è¾¾å°¼Â·é˜¿ç±³çº³å·´è¿ªï¼Œå¼ æ•ä½³ï¼Œå´æ™“éœï¼Œæä»é¾™ï¼Œå’Œä½•å®‡é›„ã€‚Zeroquantï¼šé’ˆå¯¹å¤§è§„æ¨¡å˜æ¢å™¨çš„é«˜æ•ˆä¸”ç»æµçš„åè®­ç»ƒé‡åŒ–ã€‚ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ï¼Œ35:27168â€“27183ï¼Œ2022ã€‚'
- en: '[56] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
    HellaSwag: Can a machine really finish your sentence? In Anna Korhonen, David
    Traum, and LluÃ­s MÃ rquez, editors, Proceedings of the 57th Annual Meeting of the
    Association for Computational Linguistics, pages 4791â€“4800, Florence, Italy, July
    2019\. Association for Computational Linguistics.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] ç½—æ¸©Â·æ³½å‹’æ–¯ï¼Œé˜¿é‡ŒÂ·éœå°”èŒ¨æ›¼ï¼Œä¹”çº³å¦Â·æ¯”æ–¯å…‹ï¼Œé˜¿é‡ŒÂ·æ³•èµ«è¿ªï¼Œå’Œå¶æ´¥Â·å´”ã€‚HellaSwagï¼šæœºå™¨çœŸçš„èƒ½å®Œæˆä½ çš„å¥å­å—ï¼Ÿåœ¨å®‰å¨œÂ·ç§‘å°”éœå®ï¼Œå¤§å«Â·ç‰¹åŠ³å§†ï¼Œå’Œå•åˆ©æ–¯Â·é©¬å°”å…‹æ–¯ï¼ˆç¼–è¾‘ï¼‰ï¼Œç¬¬57å±Šè®¡ç®—è¯­è¨€å­¦åä¼šå¹´ä¼šè®ºæ–‡é›†ï¼Œé¡µ4791â€“4800ï¼Œæ„å¤§åˆ©ä½›ç½—ä¼¦è¨ï¼Œ2019å¹´7æœˆã€‚è®¡ç®—è¯­è¨€å­¦åä¼šã€‚'
- en: '[57] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
    Chen, Christopher Dewan, Mona Diab, Xian Li, XiÂ Victoria Lin, etÂ al. Opt: Open
    pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] è‹çŠÂ·å¼ ï¼Œæ–¯è’‚èŠ¬Â·ç½—å‹’ï¼Œçº³æ›¼Â·æˆˆäºšå°”ï¼Œç±³å…‹å°”Â·é˜¿å°”ç‰¹åˆ‡ï¼Œè«é›…Â·é™ˆï¼Œèˆ’è¾‰Â·é™ˆï¼Œå…‹é‡Œæ–¯æ‰˜å¼—Â·å¾·ä¸‡ï¼Œè«å¨œÂ·è¿ªäºšå¸ƒï¼Œè°¢å®‰Â·æï¼Œç»´å¤šåˆ©äºšÂ·æ—ï¼Œç­‰ã€‚Opt:
    å¼€æ”¾é¢„è®­ç»ƒå˜æ¢å™¨è¯­è¨€æ¨¡å‹ã€‚arXiv é¢„å°æœ¬ arXiv:2205.01068ï¼Œ2022å¹´ã€‚'
- en: '[58] WayneÂ Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng
    Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, etÂ al. A survey of
    large language models. arXiv preprint arXiv:2303.18223, 2023.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] éŸ¦æ©Â·è¾›Â·èµµï¼Œæ˜†Â·å‘¨ï¼Œå›æ¯…Â·æï¼Œå¤©æ¯…Â·å”ï¼Œæ™“ç£ŠÂ·ç‹ï¼Œå®‡é¹Â·ä¾¯ï¼Œè¹åƒÂ·é—µï¼Œè´è¾°Â·å¼ ï¼Œä¿Šæ°Â·å¼ ï¼Œè‡ªç¿Â·è‘£ï¼Œç­‰ã€‚å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ç»¼è¿°ã€‚arXiv é¢„å°æœ¬
    arXiv:2303.18223ï¼Œ2023å¹´ã€‚'
- en: Appendix A Additional Calibration Results
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é™„å½• A é¢å¤–çš„æ ¡å‡†ç»“æœ
- en: In this section, we provide details of LLMs when performing calibration, which
    is the step during quantization where the FP16 ranges are computed (AppendixÂ [A.1](#A1.SS1
    "A.1 Detailed Specification of LLMs â€£ Appendix A Additional Calibration Results
    â€£ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs")),
    and additional calibration results (AppendixÂ [A.2](#A1.SS2 "A.2 Other Calibration
    Results on GLU-implementation â€£ Appendix A Additional Calibration Results â€£ Mitigating
    Quantization Errors Due to Activation Spikes in GLU-Based LLMs"), Â [A.3](#A1.SS3
    "A.3 Other Calibration Results on Non GLU-implementation â€£ Appendix A Additional
    Calibration Results â€£ Mitigating Quantization Errors Due to Activation Spikes
    in GLU-Based LLMs")).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†æ‰§è¡Œæ ¡å‡†æ—¶LLMsçš„è¯¦ç»†ä¿¡æ¯ï¼Œè¿™æ˜¯åœ¨é‡åŒ–è¿‡ç¨‹ä¸­è®¡ç®—FP16èŒƒå›´çš„æ­¥éª¤ï¼ˆé™„å½•[A.1](#A1.SS1 "A.1 LLMçš„è¯¦ç»†è§„èŒƒ â€£
    é™„å½• A é¢å¤–çš„æ ¡å‡†ç»“æœ â€£ å‡è½»ç”±äº GLU åŸºäº LLM çš„é‡åŒ–è¯¯å·®")ï¼‰ï¼Œä»¥åŠé¢å¤–çš„æ ¡å‡†ç»“æœï¼ˆé™„å½•[A.2](#A1.SS2 "A.2 GLU å®ç°çš„å…¶ä»–æ ¡å‡†ç»“æœ
    â€£ é™„å½• A é¢å¤–çš„æ ¡å‡†ç»“æœ â€£ å‡è½»ç”±äº GLU åŸºäº LLM çš„é‡åŒ–è¯¯å·®")ï¼Œ[A.3](#A1.SS3 "A.3 é GLU å®ç°çš„å…¶ä»–æ ¡å‡†ç»“æœ â€£
    é™„å½• A é¢å¤–çš„æ ¡å‡†ç»“æœ â€£ å‡è½»ç”±äº GLU åŸºäº LLM çš„é‡åŒ–è¯¯å·®")ï¼‰ã€‚
- en: A.1 Detailed Specification of LLMs
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 LLMçš„è¯¦ç»†è§„èŒƒ
- en: 'In SectionÂ [3.1](#S3.SS1 "3.1 Existence of Activation Spikes in GLU Variants
    â€£ 3 Activation Spikes: Excessive Magnitude of GLU Activations â€£ Mitigating Quantization
    Errors Due to Activation Spikes in GLU-Based LLMs"), we have performed the calibration
    method on various LLMs. We observe the calibration results by categorizing based
    on the presence of GLU in the LLMs. TableÂ [6](#A1.T6 "Table 6 â€£ A.1 Detailed Specification
    of LLMs â€£ Appendix A Additional Calibration Results â€£ Mitigating Quantization
    Errors Due to Activation Spikes in GLU-Based LLMs") shows the detailed structures
    of the LLMs. We refer notations for feed-forward implementiation from [[40](#bib.bib40)].
    In the case of GLU-implemented LLMs, which is LLaMA-2, LLaMA-3, Mistral, Mixtral,
    SOLAR, StableLM-2, and Gemma, most models have SwiGLU for FFN activation, while
    only Gemma has GeGLU. On the other hand, in non GLU-implemented LLMs, most of
    them utilize GeLU for FFN activation, with the exception of OPT, which uses ReLU.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨[3.1](#S3.SS1 "3.1 GLU å˜ä½“ä¸­çš„æ¿€æ´»å³°å€¼çš„å­˜åœ¨ â€£ 3 æ¿€æ´»å³°å€¼: GLU æ¿€æ´»çš„è¿‡åº¦å¹…åº¦ â€£ å‡è½»ç”±äº GLU åŸºäº LLM
    çš„é‡åŒ–è¯¯å·®")èŠ‚ä¸­ï¼Œæˆ‘ä»¬å¯¹å„ç§LLMsè¿›è¡Œäº†æ ¡å‡†æ–¹æ³•çš„åº”ç”¨ã€‚æˆ‘ä»¬é€šè¿‡å¯¹LLMsä¸­çš„GLUå­˜åœ¨æƒ…å†µè¿›è¡Œåˆ†ç±»æ¥è§‚å¯Ÿæ ¡å‡†ç»“æœã€‚è¡¨[6](#A1.T6 "è¡¨ 6 â€£
    A.1 LLMçš„è¯¦ç»†è§„èŒƒ â€£ é™„å½• A é¢å¤–çš„æ ¡å‡†ç»“æœ â€£ å‡è½»ç”±äº GLU åŸºäº LLM çš„é‡åŒ–è¯¯å·®")æ˜¾ç¤ºäº†LLMsçš„è¯¦ç»†ç»“æ„ã€‚æˆ‘ä»¬å‚è€ƒäº†[[40](#bib.bib40)]ä¸­çš„å‰é¦ˆå®ç°ç¬¦å·ã€‚å¯¹äºå®æ–½GLUçš„LLMsï¼Œå¦‚LLaMA-2ï¼ŒLLaMA-3ï¼ŒMistralï¼ŒMixtralï¼ŒSOLARï¼ŒStableLM-2
    å’Œ Gemmaï¼Œå¤§å¤šæ•°æ¨¡å‹çš„FFNæ¿€æ´»ä½¿ç”¨SwiGLUï¼Œåªæœ‰Gemmaä½¿ç”¨GeGLUã€‚å¦ä¸€æ–¹é¢ï¼Œåœ¨æœªå®æ–½GLUçš„LLMsä¸­ï¼Œå¤§å¤šæ•°æ¨¡å‹ä½¿ç”¨GeLUè¿›è¡ŒFFNæ¿€æ´»ï¼Œå”¯ä¸€çš„ä¾‹å¤–æ˜¯OPTï¼Œå®ƒä½¿ç”¨ReLUã€‚'
- en: 'Table 6: Architecture specification of LLMs. We categorize them into two groups
    depending on whether GLU is implemented in the FFN. All LLMs in the table use
    Pre-LN for the LayerNorm position.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¡¨ 6: LLM çš„æ¶æ„è§„èŒƒã€‚æˆ‘ä»¬å°†å®ƒä»¬åˆ†ä¸ºä¸¤ç»„ï¼Œå–å†³äº FFN ä¸­æ˜¯å¦å®ç°äº† GLUã€‚è¡¨ä¸­çš„æ‰€æœ‰LLMséƒ½ä½¿ç”¨ Pre-LN è¿›è¡Œ LayerNorm
    ä½ç½®çš„å¤„ç†ã€‚'
- en: '| Model | Size | FFN Activation | Normalization | PE | Vocabulary Size |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| æ¨¡å‹ | å¤§å° | FFN æ¿€æ´» | å½’ä¸€åŒ– | PE | è¯æ±‡è¡¨å¤§å° |'
- en: '| GLU-implemented LLMs: |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| å®æ–½GLUçš„LLMs: |'
- en: '| LLaMA-2 [[47](#bib.bib47)] | 7B, 13B, 70B | SwiGLU | RMSNorm | RoPE | 32000
    |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2 [[47](#bib.bib47)] | 7B, 13B, 70B | SwiGLU | RMSNorm | RoPE | 32000
    |'
- en: '| LLaMA-3 | 8B, 70B | SwiGLU | RMSNorm | RoPE | 128256 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-3 | 8B, 70B | SwiGLU | RMSNorm | RoPE | 128256 |'
- en: '| Mistral [[20](#bib.bib20)] | 7B | SwiGLU | RMSNorm | RoPE | 32000 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| Mistral [[20](#bib.bib20)] | 7B | SwiGLU | RMSNorm | RoPE | 32000 |'
- en: '| Mixtral [[21](#bib.bib21)] | 8x7B | SwiGLU | RMSNorm | RoPE | 32000 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral [[21](#bib.bib21)] | 8x7B | SwiGLU | RMSNorm | RoPE | 32000 |'
- en: '| SOLAR [[22](#bib.bib22)] | 10.7B | SwiGLU | RMSNorm | RoPE | 32000 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| SOLAR [[22](#bib.bib22)] | 10.7B | SwiGLU | RMSNorm | RoPE | 32000 |'
- en: '| StableLM-2 [[5](#bib.bib5)] | 12B | SwiGLU | LayerNorm | RoPE | 100352 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| StableLM-2 [[5](#bib.bib5)] | 12B | SwiGLU | LayerNorm | RoPE | 100352 |'
- en: '| Gemma [[43](#bib.bib43)] | 7B | GeGLU | RMSNorm | RoPE | 256000 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| Gemma [[43](#bib.bib43)] | 7B | GeGLU | RMSNorm | RoPE | 256000 |'
- en: '| Non GLU-implemented LLMs: |  |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| é GLU å®ç°çš„ LLMs: |  |'
- en: '| OPT [[57](#bib.bib57)] | 6.7B, 13B, 30B, 66B | ReLU | LayerNorm | Learned
    | 50272 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| OPT [[57](#bib.bib57)] | 6.7B, 13B, 30B, 66B | ReLU | LayerNorm | Learned
    | 50272 |'
- en: '| MPT [[44](#bib.bib44)] | 7B, 30B | GeLU | LayerNorm | ALiBi | 50432 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| MPT [[44](#bib.bib44)] | 7B, 30B | GeLU | LayerNorm | ALiBi | 50432 |'
- en: '| Pythia [[6](#bib.bib6)] | 6.9B, 12B | GeLU | LayerNorm | RoPE | 50432, 50688
    |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| Pythia [[6](#bib.bib6)] | 6.9B, 12B | GeLU | LayerNorm | RoPE | 50432, 50688
    |'
- en: '| Falcon [[3](#bib.bib3)] | 7B, 40B | GeLU | LayerNorm | RoPE | 65024 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| Falcon [[3](#bib.bib3)] | 7B, 40B | GeLU | LayerNorm | RoPE | 65024 |'
- en: '| Phi-2 [[29](#bib.bib29)] | 2.7B | GeLU | LayerNorm | RoPE | 51200 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| Phi-2 [[29](#bib.bib29)] | 2.7B | GeLU | LayerNorm | RoPE | 51200 |'
- en: A.2 Other Calibration Results on GLU-implementation
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 å…¶ä»– GLU å®ç°çš„æ ¡å‡†ç»“æœ
- en: 'FigureÂ [8](#A1.F8 "Figure 8 â€£ A.2 Other Calibration Results on GLU-implementation
    â€£ Appendix A Additional Calibration Results â€£ Mitigating Quantization Errors Due
    to Activation Spikes in GLU-Based LLMs"), [9](#A1.F9 "Figure 9 â€£ A.2 Other Calibration
    Results on GLU-implementation â€£ Appendix A Additional Calibration Results â€£ Mitigating
    Quantization Errors Due to Activation Spikes in GLU-Based LLMs") show the calibration
    result examples for various GLU-implemented LLMs that are not shown in the models
    in FigureÂ [1a](#S3.F1 "Figure 1 â€£ 3 Activation Spikes: Excessive Magnitude of
    GLU Activations â€£ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based
    LLMs"). In most GLU-implemented LLMs, we observe that the input activations have
    large values near the first and last layers. Unlike the typical GLU-implemented
    LLM architecture, Mixtral is composed of 8 feed-forward blocks in the single FFN,
    containing multiple gate linear units [[21](#bib.bib21)]. According to this structure,
    we can observe that one of the gates spikes in value in FigureÂ [8](#A1.F8 "Figure
    8 â€£ A.2 Other Calibration Results on GLU-implementation â€£ Appendix A Additional
    Calibration Results â€£ Mitigating Quantization Errors Due to Activation Spikes
    in GLU-Based LLMs").'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾Â [8](#A1.F8 "å›¾ 8 â€£ A.2 å…¶ä»– GLU å®ç°çš„æ ¡å‡†ç»“æœ â€£ é™„å½• A é¢å¤–çš„æ ¡å‡†ç»“æœ â€£ å‡å°‘ GLU åŸºç¡€çš„ LLMs ä¸­ç”±äºæ¿€æ´»å³°å€¼å¼•èµ·çš„é‡åŒ–è¯¯å·®")ï¼Œ[9](#A1.F9
    "å›¾ 9 â€£ A.2 å…¶ä»– GLU å®ç°çš„æ ¡å‡†ç»“æœ â€£ é™„å½• A é¢å¤–çš„æ ¡å‡†ç»“æœ â€£ å‡å°‘ GLU åŸºç¡€çš„ LLMs ä¸­ç”±äºæ¿€æ´»å³°å€¼å¼•èµ·çš„é‡åŒ–è¯¯å·®") æ˜¾ç¤ºäº†å„ç§
    GLU å®ç°çš„ LLMs çš„æ ¡å‡†ç»“æœç¤ºä¾‹ï¼Œè¿™äº›ç¤ºä¾‹åœ¨å›¾ [1a](#S3.F1 "å›¾ 1 â€£ 3 æ¿€æ´»å³°å€¼ï¼šGLU æ¿€æ´»çš„è¿‡å¤§å¹…åº¦ â€£ å‡å°‘ GLU åŸºç¡€çš„
    LLMs ä¸­ç”±äºæ¿€æ´»å³°å€¼å¼•èµ·çš„é‡åŒ–è¯¯å·®") ä¸­æœªæ˜¾ç¤ºã€‚åœ¨å¤§å¤šæ•° GLU å®ç°çš„ LLMs ä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è¾“å…¥æ¿€æ´»åœ¨ç¬¬ä¸€å±‚å’Œæœ€åä¸€å±‚é™„è¿‘æœ‰è¾ƒå¤§çš„å€¼ã€‚ä¸å…¸å‹çš„
    GLU å®ç°çš„ LLM æ¶æ„ä¸åŒï¼ŒMixtral ç”±ä¸€ä¸ª FFN ä¸­çš„ 8 ä¸ªå‰é¦ˆå—ç»„æˆï¼ŒåŒ…å«å¤šä¸ªé—¨æ§çº¿æ€§å•å…ƒ [[21](#bib.bib21)]ã€‚æ ¹æ®è¿™ä¸€ç»“æ„ï¼Œæˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°å›¾
    [8](#A1.F8 "å›¾ 8 â€£ A.2 å…¶ä»– GLU å®ç°çš„æ ¡å‡†ç»“æœ â€£ é™„å½• A é¢å¤–çš„æ ¡å‡†ç»“æœ â€£ å‡å°‘ GLU åŸºç¡€çš„ LLMs ä¸­ç”±äºæ¿€æ´»å³°å€¼å¼•èµ·çš„é‡åŒ–è¯¯å·®")
    ä¸­ä¸€ä¸ªé—¨æ§çš„å€¼å³°å€¼ã€‚
- en: '![Refer to caption](img/18e4d4fbd4246b30523f71181f0eaba4.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/18e4d4fbd4246b30523f71181f0eaba4.png)'
- en: 'Figure 8: Calibration results on GLU-implemented LLMs (Mixtral-8x7B).'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8ï¼šGLU å®ç°çš„ LLMs (Mixtral-8x7B) çš„æ ¡å‡†ç»“æœã€‚
- en: '![Refer to caption](img/2c55f586c7e015b0141efb77032245fd.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/2c55f586c7e015b0141efb77032245fd.png)'
- en: 'Figure 9: Calibration results on GLU-implemented LLMs.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 9ï¼šGLU å®ç°çš„ LLMs çš„æ ¡å‡†ç»“æœã€‚
- en: '![Refer to caption](img/63f769b9bc230b00d39022af4e8a43b5.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/63f769b9bc230b00d39022af4e8a43b5.png)'
- en: 'Figure 10: Calibration results on Non GLU-implemented LLMs.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 10ï¼šé GLU å®ç°çš„ LLMs çš„æ ¡å‡†ç»“æœã€‚
- en: A.3 Other Calibration Results on Non GLU-implementation
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 å…¶ä»–é GLU å®ç°çš„æ ¡å‡†ç»“æœ
- en: 'FigureÂ [10](#A1.F10 "Figure 10 â€£ A.2 Other Calibration Results on GLU-implementation
    â€£ Appendix A Additional Calibration Results â€£ Mitigating Quantization Errors Due
    to Activation Spikes in GLU-Based LLMs") shows the calibration result examples
    for various non GLU-implemented LLMs that were not shown in the models in FigureÂ [1b](#S3.F1
    "Figure 1 â€£ 3 Activation Spikes: Excessive Magnitude of GLU Activations â€£ Mitigating
    Quantization Errors Due to Activation Spikes in GLU-Based LLMs"). There are no
    activation spikes on non GLU-implemented LLMs.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾Â [10](#A1.F10 "å›¾ 10 â€£ A.2 å…¶ä»– GLU å®ç°çš„æ ¡å‡†ç»“æœ â€£ é™„å½• A é¢å¤–çš„æ ¡å‡†ç»“æœ â€£ å‡å°‘ GLU åŸºç¡€çš„ LLMs ä¸­ç”±äºæ¿€æ´»å³°å€¼å¼•èµ·çš„é‡åŒ–è¯¯å·®")
    æ˜¾ç¤ºäº†å„ç§é GLU å®ç°çš„ LLMs çš„æ ¡å‡†ç»“æœç¤ºä¾‹ï¼Œè¿™äº›ç¤ºä¾‹åœ¨å›¾ [1b](#S3.F1 "å›¾ 1 â€£ 3 æ¿€æ´»å³°å€¼ï¼šGLU æ¿€æ´»çš„è¿‡å¤§å¹…åº¦ â€£ å‡å°‘
    GLU åŸºç¡€çš„ LLMs ä¸­ç”±äºæ¿€æ´»å³°å€¼å¼•èµ·çš„é‡åŒ–è¯¯å·®") ä¸­æœªæ˜¾ç¤ºã€‚åœ¨é GLU å®ç°çš„ LLMs ä¸­æ²¡æœ‰æ¿€æ´»å³°å€¼ã€‚
- en: Appendix B BMM Quantization
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é™„å½• B BMM é‡åŒ–
- en: To achieve faster inference latency, BMM operations in the self-attention also
    can be computed as INT8 operation [[51](#bib.bib51)]. This requires a quantization
    on the query, key, and value states including the cached context. Because activation
    spikes produce a large magnitude of latent values, it is important to confirm
    the extent of quantization errors from KV quantization. This confirmation is necessary
    to gain advantages from BMM quantization. In TableÂ [7](#A2.T7 "Table 7 â€£ Appendix
    B BMM Quantization â€£ Mitigating Quantization Errors Due to Activation Spikes in
    GLU-Based LLMs"), we examine the impact of BMM quantization on the W8A8 and QFeM.
    Regardless of the BMM quantization, the QFeM method consistently improves the
    quantization bottleneck. For example, the 13B and 70B models maintain their performance,
    while the 7B model shows a slight decrease. However, this decrease appears to
    be due to inherent quantization errors rather than a quantization bottleneck from
    activation spikes. As a result, we confirm that our QFeM method effectively improves
    the overall performance even in the BMM quantization scenario.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®ç°æ›´å¿«çš„æ¨ç†å»¶è¿Ÿï¼Œè‡ªæ³¨æ„åŠ›ä¸­çš„ BMM æ“ä½œä¹Ÿå¯ä»¥è®¡ç®—ä¸º INT8 æ“ä½œ [[51](#bib.bib51)]ã€‚è¿™è¦æ±‚å¯¹æŸ¥è¯¢ã€é”®å’Œå€¼çŠ¶æ€ï¼ŒåŒ…æ‹¬ç¼“å­˜çš„ä¸Šä¸‹æ–‡ï¼Œè¿›è¡Œé‡åŒ–ã€‚ç”±äºæ¿€æ´»å³°å€¼äº§ç”Ÿçš„å¤§å¹…åº¦æ½œåœ¨å€¼ï¼Œé‡è¦çš„æ˜¯ç¡®è®¤
    KV é‡åŒ–çš„é‡åŒ–è¯¯å·®ç¨‹åº¦ã€‚è¿™ç§ç¡®è®¤æ˜¯ä¸ºäº†ä» BMM é‡åŒ–ä¸­è·å¾—ä¼˜åŠ¿ã€‚åœ¨è¡¨Â [7](#A2.T7 "è¡¨ 7 â€£ é™„å½• B BMM é‡åŒ– â€£ ç¼“è§£ GLU åŸºäº
    LLM çš„æ¿€æ´»å³°å€¼å¼•èµ·çš„é‡åŒ–è¯¯å·®")ä¸­ï¼Œæˆ‘ä»¬æ£€æŸ¥äº† BMM é‡åŒ–å¯¹ W8A8 å’Œ QFeM çš„å½±å“ã€‚ä¸è®º BMM é‡åŒ–å¦‚ä½•ï¼ŒQFeM æ–¹æ³•å§‹ç»ˆèƒ½æ”¹å–„é‡åŒ–ç“¶é¢ˆã€‚ä¾‹å¦‚ï¼Œ13B
    å’Œ 70B æ¨¡å‹ä¿æŒäº†å…¶æ€§èƒ½ï¼Œè€Œ 7B æ¨¡å‹æ˜¾ç¤ºå‡ºè½»å¾®çš„ä¸‹é™ã€‚ç„¶è€Œï¼Œè¿™ç§ä¸‹é™ä¼¼ä¹æ˜¯ç”±äºå›ºæœ‰çš„é‡åŒ–è¯¯å·®ï¼Œè€Œä¸æ˜¯ç”±äºæ¿€æ´»å³°å€¼å¼•èµ·çš„é‡åŒ–ç“¶é¢ˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬ç¡®è®¤æˆ‘ä»¬çš„
    QFeM æ–¹æ³•åœ¨ BMM é‡åŒ–åœºæ™¯ä¸­æœ‰æ•ˆæé«˜äº†æ•´ä½“æ€§èƒ½ã€‚
- en: 'Table 7: BMM quantization results.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¡¨ 7: BMM é‡åŒ–ç»“æœã€‚'
- en: '| Model | Method | BMM Quantization |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| æ¨¡å‹ | æ–¹æ³• | BMMé‡åŒ– |'
- en: '| No | Yes |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| å¦ | æ˜¯ |'
- en: '| 7B | W8A8 | 62.08% | 61.66% |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 7B | W8A8 | 62.08% | 61.66% |'
- en: '| +QFeP | 68.69% | 68.30% |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| +QFeP | 68.69% | 68.30% |'
- en: '| 13B | W8A8 | 55.29% | 55.43% |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 13B | W8A8 | 55.29% | 55.43% |'
- en: '| +QFeP | 69.91% | 69.77% |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| +QFeP | 69.91% | 69.77% |'
- en: '| 70B | W8A8 | 66.87% | 66.75% |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 70B | W8A8 | 66.87% | 66.75% |'
- en: '| +QFeP | 72.62% | 72.69% |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| +QFeP | 72.62% | 72.69% |'
- en: Appendix C Supplementary Experiment Results
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é™„å½• C è¡¥å……å®éªŒç»“æœ
- en: C.1 Additional Results for Combining Outlier Alleviation Methods
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 é¢å¤–ç»“æœï¼šç»„åˆå¼‚å¸¸å€¼ç¼“è§£æ–¹æ³•
- en: In TableÂ [8](#A3.T8 "Table 8 â€£ C.1 Additional Results for Combining Outlier
    Alleviation Methods â€£ Appendix C Supplementary Experiment Results â€£ Mitigating
    Quantization Errors Due to Activation Spikes in GLU-Based LLMs"), we provide additional
    results for SectionÂ [5.3](#S5.SS3 "5.3 Combining Outlier Alleviation Methods â€£
    5 Experiments â€£ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based
    LLMs") with coarse-grained quantization (i.e., per-tensor quantization) scheme
    for weight quantization. Compared to the results obtained with per-channel weight
    quantization in TableÂ [4](#S5.T4 "Table 4 â€£ 5.3 Combining Outlier Alleviation
    Methods â€£ 5 Experiments â€£ Mitigating Quantization Errors Due to Activation Spikes
    in GLU-Based LLMs"), these results elucidate the negative impact of activation
    spikes on the performance of outlier alleviation methods. Furthermore, this suggests
    that the performance of OSP method resort to the weight quantization scheme. Nevertheless,
    the proposed methods, QFeM and QFeP, consistently improve the effectiveness of
    outlier alleviation methods by mitigating the impact of activation spikes.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¡¨Â [8](#A3.T8 "è¡¨ 8 â€£ C.1 é¢å¤–ç»“æœï¼šç»„åˆå¼‚å¸¸å€¼ç¼“è§£æ–¹æ³• â€£ é™„å½• C è¡¥å……å®éªŒç»“æœ â€£ ç¼“è§£ GLU åŸºäº LLM çš„æ¿€æ´»å³°å€¼å¼•èµ·çš„é‡åŒ–è¯¯å·®")ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†ç¬¬
    [5.3](#S5.SS3 "5.3 ç»„åˆå¼‚å¸¸å€¼ç¼“è§£æ–¹æ³• â€£ 5 å®éªŒ â€£ ç¼“è§£ GLU åŸºäº LLM çš„æ¿€æ´»å³°å€¼å¼•èµ·çš„é‡åŒ–è¯¯å·®") èŠ‚çš„ç²—ç²’åº¦é‡åŒ–ï¼ˆå³æ¯å¼ é‡é‡åŒ–ï¼‰æ–¹æ¡ˆçš„é¢å¤–ç»“æœã€‚ä¸è¡¨
    [4](#S5.T4 "è¡¨ 4 â€£ 5.3 ç»„åˆå¼‚å¸¸å€¼ç¼“è§£æ–¹æ³• â€£ 5 å®éªŒ â€£ ç¼“è§£ GLU åŸºäº LLM çš„æ¿€æ´»å³°å€¼å¼•èµ·çš„é‡åŒ–è¯¯å·®") ä¸­æ¯é€šé“æƒé‡é‡åŒ–å¾—åˆ°çš„ç»“æœç›¸æ¯”ï¼Œè¿™äº›ç»“æœé˜æ˜äº†æ¿€æ´»å³°å€¼å¯¹å¼‚å¸¸å€¼ç¼“è§£æ–¹æ³•æ€§èƒ½çš„è´Ÿé¢å½±å“ã€‚æ­¤å¤–ï¼Œè¿™è¡¨æ˜
    OSP æ–¹æ³•çš„æ€§èƒ½ä¾èµ–äºæƒé‡é‡åŒ–æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œæ‰€æå‡ºçš„æ–¹æ³• QFeM å’Œ QFeP ä¸€è‡´åœ°é€šè¿‡å‡è½»æ¿€æ´»å³°å€¼çš„å½±å“æ¥æé«˜å¼‚å¸¸å€¼ç¼“è§£æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚
- en: 'Table 8: Evaluation of outlier alleviation methods with QFeM and QFeP. We report
    perplexity on WikiText-2 and averaged accuracy of four zero-shot tasks. Compared
    to TableÂ [4](#S5.T4 "Table 4 â€£ 5.3 Combining Outlier Alleviation Methods â€£ 5 Experiments
    â€£ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs"),
    per-tensor weight quantization and dynamic per-tensor activation quantization
    are used.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 8ï¼šä½¿ç”¨ QFeM å’Œ QFeP è¯„ä¼°å¼‚å¸¸å€¼ç¼“è§£æ–¹æ³•ã€‚æˆ‘ä»¬æŠ¥å‘Šäº† WikiText-2 ä¸Šçš„å›°æƒ‘åº¦å’Œå››ä¸ªé›¶æ ·æœ¬ä»»åŠ¡çš„å¹³å‡å‡†ç¡®åº¦ã€‚ä¸è¡¨[4](#S5.T4
    "è¡¨ 4 â€£ 5.3 ç»“åˆå¼‚å¸¸å€¼ç¼“è§£æ–¹æ³• â€£ 5 å®éªŒ â€£ ç¼“è§£ GLU åŸºäº LLM ä¸­æ¿€æ´»çªå¢çš„é‡åŒ–è¯¯å·®")ç›¸æ¯”ï¼Œä½¿ç”¨äº†æ¯ä¸ªå¼ é‡æƒé‡é‡åŒ–å’ŒåŠ¨æ€æ¯ä¸ªå¼ é‡æ¿€æ´»é‡åŒ–ã€‚
- en: '| Method | LLaMA-2-7B | LLaMA-2-13B | LLaMA-2-70B |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| æ–¹æ³• | LLaMA-2-7B | LLaMA-2-13B | LLaMA-2-70B |'
- en: '| --- | --- | --- | --- |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| ppl($\downarrow$) |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| ppl($\downarrow$) |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| SQ [[51](#bib.bib51)] | 24.661 | 56.87% | 120.966 | 53.06% | 8.435 | 67.08%
    |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| SQ [[51](#bib.bib51)] | 24.661 | 56.87% | 120.966 | 53.06% | 8.435 | 67.08%
    |'
- en: '| +QFeM | 6.016 | 67.74% | 5.464 | 70.04% | 4.015 | 74.18% |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| +QFeM | 6.016 | 67.74% | 5.464 | 70.04% | 4.015 | 74.18% |'
- en: '| +QFeP | 6.122 | 67.22% | 10.473 | 68.17% | 5.998 | 72.54% |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| +QFeP | 6.122 | 67.22% | 10.473 | 68.17% | 5.998 | 72.54% |'
- en: '| OSP [[50](#bib.bib50)] | 9.131 | 63.61% | 8.997 | 64.03% | 6.492 | 71.13%
    |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| OSP [[50](#bib.bib50)] | 9.131 | 63.61% | 8.997 | 64.03% | 6.492 | 71.13%
    |'
- en: '| +QFeM | 5.951 | 68.65% | 5.284 | 70.67% | 4.434 | 73.30% |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| +QFeM | 5.951 | 68.65% | 5.284 | 70.67% | 4.434 | 73.30% |'
- en: '| +QFeP | 5.821 | 68.25% | 5.868 | 67.96% | 4.976 | 73.57% |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| +QFeP | 5.821 | 68.25% | 5.868 | 67.96% | 4.976 | 73.57% |'
- en: Appendix D Miscellaneous
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é™„å½• D æ‚é¡¹
- en: D.1 Transformer Architecture.
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.1 å˜æ¢å™¨æ¶æ„ã€‚
- en: In FigureÂ [11](#A4.F11 "Figure 11 â€£ D.1 Transformer Architecture. â€£ Appendix
    D Miscellaneous â€£ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based
    LLMs"), we illustrate the Pre-LN transformer architecture and each sub-modules.
    We highlight with the same color the linear modules that accept identical input
    activations. Note that the hidden states are normalized before forwarding into
    the query and up linear modules.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å›¾[11](#A4.F11 "å›¾ 11 â€£ D.1 Transformer æ¶æ„ â€£ é™„å½• D æ‚é¡¹ â€£ ç¼“è§£ GLU åŸºäº LLM ä¸­æ¿€æ´»çªå¢çš„é‡åŒ–è¯¯å·®")ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†
    Pre-LN å˜æ¢å™¨æ¶æ„åŠå…¶å„ä¸ªå­æ¨¡å—ã€‚æˆ‘ä»¬ç”¨ç›¸åŒçš„é¢œè‰²çªå‡ºæ˜¾ç¤ºäº†æ¥å—ç›¸åŒè¾“å…¥æ¿€æ´»çš„çº¿æ€§æ¨¡å—ã€‚æ³¨æ„éšè—çŠ¶æ€åœ¨ä¼ å…¥æŸ¥è¯¢å’Œä¸Šå‡çº¿æ€§æ¨¡å—ä¹‹å‰å·²ç»è¿‡æ ‡å‡†åŒ–ã€‚
- en: '![Refer to caption](img/1635d20e175e23a7191582e614eb0f5d.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/1635d20e175e23a7191582e614eb0f5d.png)'
- en: 'Figure 11: An illustration of Pre-LN transformer block and its sub-modules.
    Two feed-forward implementation, GLU and Non-GLU, are visualized in (c) and (d)
    respectively. In feed-forward network, $\sigma$ denotes non-linear activation
    function, such as GeLU. We highlight the linear modules where input activations
    are quantized.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 11ï¼šPre-LN å˜æ¢å™¨å—åŠå…¶å­æ¨¡å—çš„ç¤ºæ„å›¾ã€‚ä¸¤ä¸ªå‰é¦ˆå®ç°ï¼ŒGLU å’Œé GLUï¼Œåˆ†åˆ«åœ¨ (c) å’Œ (d) ä¸­å¯è§†åŒ–ã€‚åœ¨å‰é¦ˆç½‘ç»œä¸­ï¼Œ$\sigma$
    è¡¨ç¤ºéçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œå¦‚ GeLUã€‚æˆ‘ä»¬çªå‡ºæ˜¾ç¤ºäº†è¾“å…¥æ¿€æ´»è¢«é‡åŒ–çš„çº¿æ€§æ¨¡å—ã€‚
- en: D.2 Additional Results for Token-level Scale Analysis
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.2 ä»¤ç‰Œçº§åˆ«è§„æ¨¡åˆ†æçš„é™„åŠ ç»“æœ
- en: 'We provide additional results for token-level scale analysis (SectionÂ [3.2](#S3.SS2
    "3.2 Token-level Scale Analysis within Activation Spikes â€£ 3 Activation Spikes:
    Excessive Magnitude of GLU Activations â€£ Mitigating Quantization Errors Due to
    Activation Spikes in GLU-Based LLMs")). In FigureÂ [12](#A4.F12 "Figure 12 â€£ D.2
    Additional Results for Token-level Scale Analysis â€£ Appendix D Miscellaneous â€£
    Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs") and
    FigureÂ [13](#A4.F13 "Figure 13 â€£ D.2 Additional Results for Token-level Scale
    Analysis â€£ Appendix D Miscellaneous â€£ Mitigating Quantization Errors Due to Activation
    Spikes in GLU-Based LLMs"), the token for the activation spikes behind the BOS
    token does not exhibit the excessive activation scale.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æä¾›äº†ä»¤ç‰Œçº§åˆ«è§„æ¨¡åˆ†æçš„é™„åŠ ç»“æœï¼ˆç¬¬[3.2](#S3.SS2 "3.2 æ¿€æ´»çªå¢ä¸­çš„ä»¤ç‰Œçº§åˆ«è§„æ¨¡åˆ†æ â€£ 3 æ¿€æ´»çªå¢ï¼šGLU æ¿€æ´»çš„è¿‡åº¦å¹…åº¦ â€£
    ç¼“è§£ GLU åŸºäº LLM ä¸­æ¿€æ´»çªå¢çš„é‡åŒ–è¯¯å·®")èŠ‚ï¼‰ã€‚åœ¨å›¾[12](#A4.F12 "å›¾ 12 â€£ D.2 ä»¤ç‰Œçº§åˆ«è§„æ¨¡åˆ†æçš„é™„åŠ ç»“æœ â€£ é™„å½• D æ‚é¡¹
    â€£ ç¼“è§£ GLU åŸºäº LLM ä¸­æ¿€æ´»çªå¢çš„é‡åŒ–è¯¯å·®")å’Œå›¾[13](#A4.F13 "å›¾ 13 â€£ D.2 ä»¤ç‰Œçº§åˆ«è§„æ¨¡åˆ†æçš„é™„åŠ ç»“æœ â€£ é™„å½• D æ‚é¡¹
    â€£ ç¼“è§£ GLU åŸºäº LLM ä¸­æ¿€æ´»çªå¢çš„é‡åŒ–è¯¯å·®")ä¸­ï¼ŒBOS ä»¤ç‰Œåçš„æ¿€æ´»çªå¢ä»¤ç‰Œæ²¡æœ‰è¡¨ç°å‡ºè¿‡åº¦çš„æ¿€æ´»è§„æ¨¡ã€‚
- en: '![Refer to caption](img/faddb4c3d886a8cca8553da637ea143e.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/faddb4c3d886a8cca8553da637ea143e.png)'
- en: 'Figure 12: Token-wise scales analysis for LLaMA-2-7B. The newline token behind
    the BOS token does not exhibit the activation spikes.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 12ï¼šLLaMA-2-7B çš„ä»¤ç‰Œçº§åˆ«è§„æ¨¡åˆ†æã€‚BOS ä»¤ç‰Œåçš„æ¢è¡Œä»¤ç‰Œæ²¡æœ‰è¡¨ç°å‡ºæ¿€æ´»çªå¢ã€‚
- en: '![Refer to caption](img/f44156b26431f483d384b9e6352a6142.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/f44156b26431f483d384b9e6352a6142.png)'
- en: 'Figure 13: Token-wise scales from the unrolled activation spike of LLaMA-2-70B.
    The apostrophe token behind the BOS token does not exhibit the activation spikes.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾13ï¼šæ¥è‡ªLLaMA-2-70Bå±•å¼€æ¿€æ´»å³°å€¼çš„é€è¯å°ºåº¦ã€‚BOSæ ‡è®°åé¢çš„æ’‡å·æ ‡è®°æ²¡æœ‰è¡¨ç°å‡ºæ¿€æ´»å³°å€¼ã€‚
