- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: æœªåˆ†ç±»'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:04:59'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2402.05406](https://ar5iv.labs.arxiv.org/html/2402.05406)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Lucio Dery â€ƒâ€ƒ Steven Kolawole â€ƒâ€ƒ Jean-FranÃ§ois Kagy â€ƒâ€ƒ Virginia Smith â€ƒâ€ƒ Graham
    Neubig â€ƒâ€ƒ Ameet Talwalkar
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Given the generational gap in available hardware between lay practitioners and
    the most endowed institutions, LLMs are becoming increasingly inaccessible as
    they grow in size. Whilst many approaches have been proposed to compress LLMs
    to make their resource consumption manageable, these methods themselves tend to
    be resource intensive, putting them out of the reach of the very user groups they
    target. In this work, we explore the problem of structured pruning of LLMs using
    only forward passes. We seek to empower practitioners to prune models so large
    that their available hardware has just enough memory to run inference. We develop
    Bonsai, a gradient-free, perturbative pruning method capable of delivering small,
    fast, and accurate pruned models. We observe that BonsaiÂ outputs pruned models
    that (i) outperform those generated by more expensive gradient-based structured
    pruning methods, and (ii) are twice as fast (with comparable accuracy) as those
    generated by semi-structured pruning methods requiring comparable resources as
    Bonsai. We also leverage BonsaiÂ to produce a new sub-2B model using a single A6000
    that yields state-of-the-art performance on 4/6 tasks on the Huggingface Open
    LLM leaderboard.
  prefs: []
  type: TYPE_NORMAL
- en: '[Code for BonsaiÂ can be found here](https://github.com/ldery/Bonsai)'
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning, ICML\addauthor
  prefs: []
  type: TYPE_NORMAL
- en: gnmagenta \addauthoratblue
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As large language models (LLMs) (OpenAI etÂ al., [2023](#bib.bib38); Touvron
    etÂ al., [2023](#bib.bib45); Gemini Team etÂ al., [2023](#bib.bib13)) continue to
    grow in size, the gap between models that achieve state-of-the-art performance
    and those that every-day machine learning (ML) practitioners can feasibly run
    on their available hardware continues to widen (Bender etÂ al., [2021](#bib.bib5);
    Samsi etÂ al., [2023](#bib.bib40)). With the goal of democratizing access to these
    powerful models, previous research has proposed approaches such as pruning (Xia
    etÂ al., [2022](#bib.bib48); Sun etÂ al., [2023](#bib.bib44)), distillation (Hinton
    etÂ al., [2015](#bib.bib18); Gu etÂ al., [2023](#bib.bib14)) and quantization (Xiao
    etÂ al., [2023](#bib.bib49)) to create smaller models from larger pre-trained language
    models.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7cb706285e7f938a089f99d3ee179841.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Perplexity versus inference speed-up of pruned models for methods
    that use only forward passes on the parent model. At any given target perplexity,
    BonsaiÂ produces the fastest model, resulting in improved latency and throughput.
    Post-pruning adaptation is only possible for models below a certain size, given
    the available hardware. Circle sizes are proportional to the modelâ€™s memory footprint.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Landscape of resource consumption (memory and compute) of different
    model compression methods at training time and the inference time resource consumption
    of the models they deliver. âœ—Â  means the method incurs a prohibitive cost to the
    lay practitioner whilst âœ“Â  denotes that it is a viable option with respect to
    that resource.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Regime | Resource | Approaches |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Quantization (Mixed Precision) | Distillation | Unstructured Pruning
    | Gradient-Based Structured Pruning | BonsaiÂ (Ours) |'
  prefs: []
  type: TYPE_TB
- en: '| Train | Memory | âœ“ | âœ“ | âœ“ | âœ— | âœ“ |'
  prefs: []
  type: TYPE_TB
- en: '|  | Compute | âœ“ | âœ— | âœ“ | âœ“ | âœ“ |'
  prefs: []
  type: TYPE_TB
- en: '| Inference | Memory | âœ“ | âœ“ | âœ“ | âœ“ | âœ“ |'
  prefs: []
  type: TYPE_TB
- en: '|  | Compute | âœ— | âœ“ | âœ— | âœ“ | âœ“ |'
  prefs: []
  type: TYPE_TB
- en: 'Unfortunately, so far, these methods have fallen short of the goal of truly
    democratizing access to LLMs. In the process of producing a smaller model out
    of an LLM, methods such as distillation and structured pruning are inaccessible
    to the everyday practitioner due to their prohibitive resource consumption at
    training time. Specifically, pure distillation-based techniques require running
    LLMs to generate large amounts of teacher data (Jiao etÂ al., [2019](#bib.bib22);
    Hsieh etÂ al., [2023](#bib.bib19)) whilst existing structured pruning approaches
    like LLM-Pruner (Ma etÂ al., [2023](#bib.bib31)) and LoRAPrune (Zhang etÂ al., [2023](#bib.bib52))
    require several times more memory than is needed to run inference on the model
    being pruned. Though unstructured pruning (Frantar & Alistarh, [2023](#bib.bib11);
    Sun etÂ al., [2023](#bib.bib44)) and quantization are less restrictive at training
    time, the models they produce are not faster except in the presence of specialized
    hardware for the former (Mishra etÂ al., [2021](#bib.bib36)), whilst the latter
    can actually slow down inference due to added overhead (Dettmers etÂ al., [2022](#bib.bib9)).
    This limits the usefulness of these options for practitioners who are concerned
    with latency-critical applications. Table [1](#S1.T1 "Table 1 â€£ 1 Introduction
    â€£ Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes") summarizes
    the landscape of existing methods and their limitations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We aim to empower ML practitioners to compress LLMs *by themselves* using their
    available resources whilst still producing accurate yet fast and compact models.
    To this end, we propose a novel memory-friendly structured pruning method. We
    observe that the significant memory overhead of prior structured pruning methods
    chiefly comes from having to perform gradient-based optimization: a backward pass
    requires $\gtrapprox 2\times$. To capture the widest range of memory budgets available
    to practitioners, we focus on developing an approach for the following concrete
    setting:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The practitioner only has enough memory on their hardware to run inference
    on the model to be pruned.Â¹Â¹1Specifically we assume a forward pass with a batch
    size of at least 1\. Given the hardware and open source LLM landscape as of January
    2024, for an Nvidia A6000 (48GB) GPU, the largest (full-precision) model this
    would be applicable to is LLaMA-7B*'
  prefs: []
  type: TYPE_NORMAL
- en: The above setting is evergreen. As state-of-the-art models become more compute
    intensive over time, the generational gap in hardware available to the lay practitioner
    versus the most resource endowed institutions is expected to persist or possibly
    widen.
  prefs: []
  type: TYPE_NORMAL
- en: In light of the proposed setting, we present Bonsai, a forward pass-only structured
    pruning approach that is capable of delivering fast, compact, and accurate pruned
    models under the memory limitations that are typical of consumer hardware. To
    decide which modules (attention head, rows in feedforward projection, etc.) of
    the LLM to prune, BonsaiÂ estimates module importances perturbatively by generating
    sub-models and evaluating their performance (running inference). We make this
    approach tractable by contributing multiple techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we treat the problem of inferring each moduleâ€™s importance from the
    performance of generated sub-models as an under-determined regression problem.
    This enables us to estimate the importance of a large number of modules by exploring
    a manageable number of random sub-models. This is unlike past perturbative approaches,
    which prohibitively require roughly as many sub-models as there are modules to
    select from (Ancona etÂ al., [2020](#bib.bib1)), making them intractable for LLMs.
    Next, instead of instantiating sub-models by dropping modules with equal likelihood
    (Kang etÂ al., [2023](#bib.bib23)), we use informative priors derived from work
    on unstructured pruning (Han etÂ al., [2015](#bib.bib15); Sun etÂ al., [2023](#bib.bib44)).
    We thus obtain better estimates of module relevance with fewer evaluated sub-models.
    Finally, unlike past gradient-free approaches that greedily make pruning decisions
    layer-by-layer (Dekhovich etÂ al., [2021](#bib.bib8); Nova etÂ al., [2023](#bib.bib37);
    Sun etÂ al., [2023](#bib.bib44)), BonsaiÂ takes a holistic view to preserve the
    accuracy of the pruned model: modules across layers are removed and evaluated
    together and relevance scores are computed globally to make pruning decisions.
    To the best of our knowledge, these ingredients taken together make BonsaiÂ the
    first successful attempt at scaling gradient-free structured pruning to LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: We conduct several experiments that demonstrate Bonsaiâ€™s efficacy. Bonsai, which
    uses only forward passes for pruning, achieves comparable performance to 2:4 semi-structured
    sparsity with Wanda (Sun etÂ al., [2023](#bib.bib44)) ($0.75\times$1.8B model that
    outperforms the best sub-2B parameter model on the Huggingface Open LLM leaderboard
    on 4 out of 6 tasks as of the time of submission. Based on these strong results
    and its usability under real-world memory constraints, we view BonsaiÂ as a significant
    contribution to unlocking the power of LLMs for a broader spectrum of practitioners
    facing diverse hardware constraints.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Background on Pruning, Problem Definition and Notation Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are given an LLM, $\mathbf{M}_{\theta}$ on available hardware, pruning can
    be critical for achieving latency targets, reducing compute burden, or making
    the model small enough to adapt to new (out-of-domain) tasks by gradient-based
    fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Unstructured pruning approaches compress $\mathbf{M}_{\theta}$ from the model.
    This results in the updated model consisting of sparsified weight matrices with
    a smaller memory footprint. Unfortunately, the updated model does not enjoy inference
    speedups except when specialized hardware is available and thus poses a compute
    burden during inference. Whilst semi-structured variants â€“ those that remove parameters
    in patterns like 2:4 or 4:8 (Mishra etÂ al., [2021](#bib.bib36)) â€“ achieve some
    speedup, these are modest compared to those achieved with structured pruning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Structured pruning takes a more modular view of the units to be removed from
    $\mathbf{M}_{\theta}$, structured pruning can be cast as the following combinatorial
    optimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}\mathbf{m}^{\ast}&amp;=\mathrm{argmax}_{\bar{\mathbf{m}}\in\mathcal{F}_{p}}\quad
    U\left(\mathbf{M}_{&#124;\bar{\mathbf{m}}}\right)\qquad\text{where}\\ \mathcal{F}_{p}&amp;=\bigg{\{}\bar{\mathbf{m}}\subseteq\mathbf{m}~{}\bigg{&#124;}~{}\bigg{(}\sum_{[j:m_{j}\in\bar{\mathbf{m}}]}s_{j}\bigg{)}\leq(1-p)D\bigg{\}}\end{split}$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: $\mathcal{F}_{p}$, it is also faster to run inference on it since it has fewer
    modules.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many structured pruning methods attempt to solve Equation [1](#S2.E1 "Equation
    1 â€£ 2 Background on Pruning, Problem Definition and Notation Setup â€£ Everybody
    Prune Now: Structured Pruning of LLMs with only Forward Passes") by gradient-guided
    optimization (or search) over the space of sub-models. However, since we are interested
    in the memory-constrained setting where computing gradients is not feasible, these
    methods cannot be used. We will thus focus on developing a memory-friendly structured
    pruning technique, but we will compare to semi-structured pruning methods since
    they also have minimal memory overhead.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will develop a structured pruning algorithm that relies
    exclusively on forward passes through the parent model.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Estimating module relevance with only forward passes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have motivated the setting of pruning a model that is so large (relative
    to the amount of memory available to the practitioner), such that we can only
    run forward passes through it. This means that we have to solve Equation [1](#S2.E1
    "Equation 1 â€£ 2 Background on Pruning, Problem Definition and Notation Setup â€£
    Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes") by
    relying on only evaluations of $U$ subsets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We propose a computationally tractable approach where we first perform a small
    number, $n$ modules. We can generate an approximate solution to Equation [1](#S2.E1
    "Equation 1 â€£ 2 Background on Pruning, Problem Definition and Notation Setup â€£
    Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes") as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{m}^{\ast}\approx\mathbf{m}^{\mathrm{approx}}=\mathrm{argmax}_{\bar{\mathbf{m}}\in\mathcal{F}_{p}}\quad\sum_{j\in\bar{\mathbf{m}}}\beta_{j}$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'We note that Equation [2](#S3.E2 "Equation 2 â€£ 3.1 Estimating module relevance
    with only forward passes â€£ 3 Methodology â€£ Everybody Prune Now: Structured Pruning
    of LLMs with only Forward Passes") is straightforward to solve, as it simply requires
    sorting $\beta_{j}$ for our settings of interest, the difference is not significant).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Estimating $\mathbf{\beta}$:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To obtain estimates of the module relevance scores $\mathbf{\beta}=\{\beta_{i}\}_{i\in[N]}$
    as an under-specified regression problem :'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $\alpha_{\bar{\mathbf{m}}_{k}}\in\mathbb{R}^{N}~{}\big{|}~{}\left(\alpha_{\bar{\mathbf{m}}_{k}}\right)_{i}=\mathbf{1}[i\in\bar{\mathbf{m}}_{k}]$,
    is the binary vector that has 0 at indices where modules have been dropped.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a sub-model $\bar{\mathbf{m}}_{k}$ is key to practically realizing
    our approach. We never actually fully instantiate sub-models as this would be
    prohibitively expensive. Instead, we create sub-models *virtually* by zeroing
    out the outputs of the components to be pruned so they have no effect on the final
    model output.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Selecting sub-models for evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An as yet unexplored design choice is how to choose the $n$ are accurate and
    useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a module $m_{i}$ is left unpruned would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\rho_{i}\propto\hat{\mathbf{a}}_{i}=\frac{1}{B}\sum_{b}\bigg{&#124;}\sigma\bigg{(}\big{(}W^{T}[i,:]\big{)}x_{b}\bigg{)}\bigg{&#124;}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: $\sigma\text{ is the nonlinearity}$ which allows us to respect memory constraints.
  prefs: []
  type: TYPE_NORMAL
- en: To enhance the efficiency of our method, instead of considering all active modules
    for pruning, in each layer we consider the bottom $2p$ fraction of entries). Covert
    & Lee ([2020](#bib.bib7)) show that this technique can help reduce the variance
    of the estimator obtained from regression with binary inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Bonsai
  prefs: []
  type: TYPE_NORMAL
- en: '1:Â Â Input: Model [$\mathbf{M}_{\theta}$'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Iterated Pruning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Previous work on gradient-based pruning (Anwar etÂ al., [2017](#bib.bib2); Frankle
    & Carbin, [2018](#bib.bib10)) have shown that taking an iterated approach to pruning
    yields improved results over pruning directly to the target sparsity $p$ sub-models
    to evaluate.
  prefs: []
  type: TYPE_NORMAL
- en: 'We combine the recipes developed in Sections [3.1](#S3.SS1 "3.1 Estimating
    module relevance with only forward passes â€£ 3 Methodology â€£ Everybody Prune Now:
    Structured Pruning of LLMs with only Forward Passes"), [3.2](#S3.SS2 "3.2 Selecting
    sub-models for evaluation â€£ 3 Methodology â€£ Everybody Prune Now: Structured Pruning
    of LLMs with only Forward Passes") and [3.3](#S3.SS3 "3.3 Iterated Pruning â€£ 3
    Methodology â€£ Everybody Prune Now: Structured Pruning of LLMs with only Forward
    Passes") together to produce Bonsaiâ´â´4Structural pruning is a canonical way of
    giving a bonsai tree its shape hence the name., our gradient-free structural pruning
    algorithm. Algorithm [1](#alg1 "Algorithm 1 â€£ 3.2 Selecting sub-models for evaluation
    â€£ 3 Methodology â€£ Everybody Prune Now: Structured Pruning of LLMs with only Forward
    Passes") specifies BonsaiÂ in detail.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Post-pruning adaptation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Depending on the sparsity level $p$, it is possible to obtain a pruned model
    on which it is feasible to run a parameter-efficient finetuning method like LoRA
    (Hu etÂ al., [2021](#bib.bib20)) with the available hardware memory. In this case,
    we can fine-tune the pruned model (result of Algorithm [1](#alg1 "Algorithm 1
    â€£ 3.2 Selecting sub-models for evaluation â€£ 3 Methodology â€£ Everybody Prune Now:
    Structured Pruning of LLMs with only Forward Passes")) on the downstream task
    in order to recover some of the parent model performance that was degraded by
    pruning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Like many past works (Sanh etÂ al., [2020](#bib.bib42); Xia etÂ al., [2022](#bib.bib48)),
    we combine pruning with distillation by incorporating a distillation loss in the
    training objective during fine-tuning of the pruned model. Let $\mathcal{L}_{\mathrm{task}}$
    to index the task data, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: $\mathcal{L}_{\mathrm{distill}}=\sum_{i}D_{\mathrm{KL}}\bigg{(}\mathrm{logits}^{i}\left(\mathbf{M}_{|\mathbf{m}^{\mathrm{approx}}}\right)~{}\|~{}\mathrm{logits}^{i}\left({\mathbf{M}}\right)\bigg{)}$
    instead of hosting the model in memory during fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Wikitext-2 perplexity for 50% sparsity of LLaMA-2 7B with end-to-end
    latency speedups (relative to LLaMA-2 7B). We use Phi-2 (Li etÂ al., [2023](#bib.bib28))
    at the target size as a strong model for comparison. For semi-structured (Wanda
    2:4) pruning, after fine-tuning, the learned LoRA weights cannot be merged with
    the primary model weights else the model reverts back to being dense; this leads
    to the reported slow downs. All methods use forward passes only on the parent
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | $\sim$Size | Fine-tune | PPL | Speedup |'
  prefs: []
  type: TYPE_TB
- en: '| Phi-2 | 3B | âœ“ | $8.69$ |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-2 7B Pruned |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda 2:4 | 3B | âœ— | $10.52$ |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | âœ“ | $8.34$ |'
  prefs: []
  type: TYPE_TB
- en: '| Bonsai | 3B | âœ“ | $8.89$ |'
  prefs: []
  type: TYPE_TB
- en: 5 Experimental Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All experiments are conducted on a single NVIDIA A6000 (48Gb) GPU. In all experiments
    with Bonsai, we prune (1) the heads in the self-attention layers (2) the dimensions
    of the fully connected layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparing Forward Pass Only Methods: We focus our first set of experiments
    on comparing methods that can be run without gradient-based optimization. We consider
    pruning the LLaMA-2 7B model (Touvron etÂ al., [2023](#bib.bib45)) to 50% sparsity.
    We evaluate on the Wikitext-2 (Merity etÂ al., [2016](#bib.bib34)) validation dataset
    and so our signal for pruning, $U$, is the language modelling performance on the
    training set. When measuring speedups, we consider *end-to-end latency* of running
    inference on model.sequence_length chunks of the Wikitext-2 validation set. See
    Appendix [A.2](#A1.SS2 "A.2 Forward Pass Only Experiments â€£ Appendix A Main Experiment
    Details â€£ Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes")
    for details about the hyper-parameters used for all methods in this experiment
    and for specifics about fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparing Structured Pruning Approaches: We compare BonsaiÂ to the following
    gradient-based structured pruning approaches: LLM-Pruner (Ma etÂ al., [2023](#bib.bib31))
    and LoRA-Prune (Zhang etÂ al., [2023](#bib.bib52)). We prune the LLaMA-1 7B model
    (Touvron etÂ al., [2023](#bib.bib45)) to 50% sparsity since this was the model
    version available at time of release of the above methods. We compare these methods
    on Wikitext-2 and also on 6 tasks from the Eleuther LLM Evaluation Harness (Gao
    etÂ al., [2023](#bib.bib12)). Pruning signal for Wikitext-2 task is the same as
    the above experiment. For the Eleuther Harness tasks, we use language modelling
    performance on the C4 (Raffel etÂ al., [2020](#bib.bib39)) dataset as pruning signal.
    We also do parameter efficient finetuning on our pruned model with 30K 512-length
    sequences from this corpus. Specific hyper-parameters for BonsaiÂ for this experiment
    can be found in Appendix [A.3](#A1.SS3 "A.3 Experiments comparing to Gradient
    based structured pruning â€£ Appendix A Main Experiment Details â€£ Everybody Prune
    Now: Structured Pruning of LLMs with only Forward Passes").'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Main Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.1 BonsaiÂ produces fast and performant models with only forward passes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Table [2](#S4.T2 "Table 2 â€£ 4 Post-pruning adaptation â€£ Everybody Prune
    Now: Structured Pruning of LLMs with only Forward Passes"), we explore options
    that are available to practitioners when they can only run forward passes on the
    parent model. Here, LLaMA-2 7B is compressed to 50% sparsity.'
  prefs: []
  type: TYPE_NORMAL
- en: We compare BonsaiÂ to the semi-structured variant of Wanda (Sun etÂ al., [2023](#bib.bib44)).
    Before fine-tuning, the model produced by Wanda 2:4 achieves a speedup over the
    parent model (1.14$\times$) than the model from Bonsai.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this memory-constrained setting, practitioners could alternatively opt for
    a pre-existing model of the target size instead of pruning a larger model. We
    compare the Bonsai-pruned model to Phi-2 (Li etÂ al., [2023](#bib.bib28)), a strong
    representative pre-existing model of similar size. As can be seen in Table [2](#S4.T2
    "Table 2 â€£ 4 Post-pruning adaptation â€£ Everybody Prune Now: Structured Pruning
    of LLMs with only Forward Passes"), Bonsai is able to generate a model that is
    as accurate (0.2 difference in ppl) yet significantly faster (1.58$\times$ speedup),
    thus making it a competitive option to consider even if a model already exists
    at the target size.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: LLaMA-1 (50% sparsity) after post-pruning adaptation with LoRA. ^â€ 
    indicate results as reported by Zhang etÂ al. ([2023](#bib.bib52)).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Foward-only | Wikitext-2 $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA1-7B (Touvron etÂ al., [2023](#bib.bib45)) | - | 5.68 | 75.05 | 56.92
    | 69.93 | 75.34 | 41.89 | 63.83 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LLM-Pruner^â€   (Ma etÂ al., [2023](#bib.bib31)) | âœ— | 16.41 | 60.28 | 47.06
    | 53.43 | 45.96 | 29.18 | 47.18 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRAPrune^â€   (Zhang etÂ al., [2023](#bib.bib52)) | âœ— | 11.60 | 61.88 | 47.86
    | 55.01 | 45.13 | 31.62 | 48.30 |'
  prefs: []
  type: TYPE_TB
- en: '| Bonsai | âœ“ | 10.92 | 67.22 | 43.09 | 61.64 | 54.92 | 26.28 | 50.63 |'
  prefs: []
  type: TYPE_TB
- en: 6.2 BonsaiÂ outperforms other structured pruning approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We next investigate the setting where the practitioner has enough memory to
    run gradient-based structured pruning methods. We compare BonsaiÂ to recent SoTA
    methods: LLM-Pruner (Ma etÂ al., [2023](#bib.bib31)) and LoRA-Prune (Zhang etÂ al.,
    [2023](#bib.bib52)). Since these approaches report their results for the LLaMA-1
    only, we prune LLaMA-1 7B to 50% sparsity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As evinced by Table [3](#S6.T3 "Table 3 â€£ 6.1 Bonsai produces fast and performant
    models with only forward passes â€£ 6 Main Results â€£ Everybody Prune Now: Structured
    Pruning of LLMs with only Forward Passes"), BonsaiÂ outperforms existing structured
    pruning methods for LLMs even though it exclusively uses forward passes in the
    pruning stage. We attribute the superior performance of BonsaiÂ to the fact that
    its pruning decisions are informed by directly exploring the space of sub-models
    whilst the other approaches resort on inaccurate proxies of module relevance in
    order to reduce the memory overhead of a fully gradient-based optimization approach.
    Specifically, LoRA-Pruneâ€™s criterion for deciding which modules to prune uses
    gradient signals not from the modules directly but from a surrogate low-rank matrix.
    On the other hand, LLM-Pruner attempts to model the impact of removing a module
    on the model loss via a Taylor expansion and substitutes the LLMâ€™s intractable
    (memory-wise) Hessian term with the Fisher information matrix which we posit leads
    to sub-optimal pruning decisions.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 BonsaiÂ can produce compressed models with strong zero-shot abilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 4: Phi-2 pruned to 35% sparsity compared to the best sub-2B parameter
    models on the Hugging Face OpenLLM Leaderboard. At the time of submission, $\dagger$
    was the best sub-2B parameter model on the Leaderboard.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | Generation | Multiple Choice (MC) |'
  prefs: []
  type: TYPE_TB
- en: '| Model | Size | GSM8k (5-shot) | ARC-c (25-shot) | Winogrande (5-shot) | Hellaswag
    (10-shot) | Truthful-QA (0-shot) | MMLU (5-shot) | MC Average $\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| Phi-2 (Li etÂ al., [2023](#bib.bib28)) | 2.7B | 54.81 | 61.09 | 74.35 | 75.11
    | 44.47 | 58.11 | 62.63 |'
  prefs: []
  type: TYPE_TB
- en: '| [StableLM-2-1_6b](https://huggingface.co/stabilityai/stablelm-2-1_6b)$\dagger$
    | 36.78 | 38.95 | 50.82 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | 1.8B | 6.37 | $\boldsymbol{47.44}$ |'
  prefs: []
  type: TYPE_TB
- en: Considerable amounts of compute and data, beyond what is available to lay practitioners,
    are needed to train LLMs with strong zero-shot capabilities (OpenAI etÂ al., [2023](#bib.bib38);
    Gemini Team etÂ al., [2023](#bib.bib13)). In this section, we demonstrate that
    BonsaiÂ can empower everyday practitioners to produce strong and compact models
    with competitive zero-shot abilities by simply pruning bigger models on their
    available hardware.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use BonsaiÂ to prune a $\approx$1.8B (35% sparsity). Values for the BonsaiÂ hyper-parameters
    in this experiment are in Appendix [A.4](#A1.SS4 "A.4 Phi-2 pruning experiment
    details â€£ Appendix A Main Experiment Details â€£ Everybody Prune Now: Structured
    Pruning of LLMs with only Forward Passes"). Since its relatively small, the 1.8B
    pruned model can be fully fine-tuned on 1 A6000 GPU over 100k sequences of 2,048
    tokens from the C4 dataset. As can be seen from Table [4](#S6.T4 "Table 4 â€£ 6.3
    Bonsai can produce compressed models with strong zero-shot abilities â€£ 6 Main
    Results â€£ Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes"),
    our pruned model achieves strong zero-shot performance compared to StableLM, the
    leading sub-2B parameter LLM on the Hugging Face OpenLLM leaderboard (Gao etÂ al.,
    [2023](#bib.bib12)) â€” outperforming it on 4 out of 6 tasks as of January 2024\.
    Interestingly, one exception to the general trend of Bonsaiâ€™s superior performance
    is the GSM-8K dataset, which is a mathematical reasoning dataset that requires
    generation of a long reasoning chains. We posit that this is because currently,
    BonsaiÂ prunes with respect to language modeling likelihood, as opposed to reasoning
    accuracy. An interesting avenue for future work is to prune to improve maintain
    reasoning ability.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we conduct various ablative experiments to understand how
    the methodological ingredients from Section [3](#S3 "3 Methodology â€£ Everybody
    Prune Now: Structured Pruning of LLMs with only Forward Passes") contribute to
    make BonsaiÂ effective.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Do we need both perturbative and regressive components of Bonsai? Figure [2](#S7.F2
    "Figure 2 â€£ 7 Analysis â€£ Everybody Prune Now: Structured Pruning of LLMs with
    only Forward Passes") shows that both components are key to obtaining a good pruned
    model. Removing the estimation of module importances via regression leads to a
    degradation in performance ($61.6$ as computed from the unperturbed parent model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7ab05c979f32dafa13204333ec4b8613.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: LLaMA-2 7B pruned to 50% sparsity. Perturbatively evaluating the
    impact of removing modules and then using regression to estimate module importances
    are both key in making BonsaiÂ effective. Details of experiment configuration can
    be found in Appendix [B](#A2 "Appendix B Impact of regression and perturbation
    ablation details â€£ Everybody Prune Now: Structured Pruning of LLMs with only Forward
    Passes"). No post-pruning adaptation is performed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Varying the number of perturbative evaluations. Wikitext-2 perplexity
    of LLaMA-2 7B pruned to 50% sparsity. See Appendix [G](#A7 "Appendix G How many
    perturbative samples are reasonable? â€£ Everybody Prune Now: Structured Pruning
    of LLMs with only Forward Passes") for details. No post-pruning adaptation is
    performed.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathrm{ns}=50$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| PPL ($\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: 'How many perturbative samples are sufficient? We investigate the number of
    perturbative samples required to obtain good estimates of module importances after
    performing regression as in Equation [3](#S3.E3 "Equation 3 â€£ Estimating ð›½: â€£
    3.1 Estimating module relevance with only forward passes â€£ 3 Methodology â€£ Everybody
    Prune Now: Structured Pruning of LLMs with only Forward Passes"). Our results
    are shown in Table [5](#S7.T5 "Table 5 â€£ 7 Analysis â€£ Everybody Prune Now: Structured
    Pruning of LLMs with only Forward Passes"). As expected, performance improves
    as we increase the number of sub-models explored. We note that the number of samples
    we explore, $\mathrm{ns}$), nevertheless BonsaiÂ is able to deliver a performant
    pruned model because of the recipes developed in Section [3](#S3 "3 Methodology
    â€£ Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes").'
  prefs: []
  type: TYPE_NORMAL
- en: 'How much performance is recovered by post-pruning adaptation? During iterative
    pruning, BonsaiÂ damages the parent model by removing modules but does not perform
    intermittent retraining to recover lost performance since even intermediate models
    may be too large for fine-tuning. Even so, as Table [6](#S7.T6 "Table 6 â€£ 7 Analysis
    â€£ Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes") shows,
    the final model produced by BonsaiÂ has reasonable performance without fine-tuning.
    We attribute this to the general robustness of LLMs, the redundancy of modules
    with respect to target end-tasks and Bonsaiâ€™s ability to identify good candidates
    for pruning. If the pruned model is small enough in size, we can perform either
    full fine-tuning or parameter-efficient fine-tunin to recover more performance,
    as can be seen from Table [6](#S7.T6 "Table 6 â€£ 7 Analysis â€£ Everybody Prune Now:
    Structured Pruning of LLMs with only Forward Passes").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Impact of post-pruning adaptation of LLaMA-2 7B pruned to 50% sparsity.
    See Appendix [E](#A5 "Appendix E Post-pruning adaptation â€£ Everybody Prune Now:
    Structured Pruning of LLMs with only Forward Passes") for more details.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Wikitext-2 PPL |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| No Post-Pruning Adaptation | $19.47$ |'
  prefs: []
  type: TYPE_TB
- en: '| Post-Pruning Finetuning | $10.39$ |'
  prefs: []
  type: TYPE_TB
- en: '| â€ƒâ€ƒâ€ƒ+ â€ƒDistillation | $8.89$ |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/9c738ca6f41a278ee7be755755b9a999.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: LLaMA-2 7B pruned to 50% sparsity. See Appendix [F](#A6 "Appendix
    F Impact of prior â€£ Everybody Prune Now: Structured Pruning of LLMs with only
    Forward Passes") for details of experiment configuration and definitions of module-level
    analogues of Wanda and Activation Magnitude.'
  prefs: []
  type: TYPE_NORMAL
- en: 'What is the impact of the choice of metric for the prior $\rho$. Figure [3](#S7.F3
    "Figure 3 â€£ 7 Analysis â€£ Everybody Prune Now: Structured Pruning of LLMs with
    only Forward Passes") shows that using the module-level analogue of Wanda (Sun
    etÂ al., [2023](#bib.bib44)) yields the best performance, both before and after
    post-pruning adaptation. This indicates that Wanda is a strong signal for efficiently
    estimating the importance of model units (whether at the parameter- or module-level).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Should BonsaiÂ prune iteratively? Table [7](#S7.T7 "Table 7 â€£ 7 Analysis â€£ Everybody
    Prune Now: Structured Pruning of LLMs with only Forward Passes") demonstrates
    the benefits of using BonsaiÂ  in an iterative fashion. Pruning slowly ($p_{\mathrm{iter}}=0.05$
    persists even after post-pruning adaptation, indicating that slower pruning allows
    for more accurate estimates of module importance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Varying $p_{\mathrm{iter}}$. Wikitext-2 perplexity of LLaMA-2 7B pruned
    to 50% sparsity. See Appendix [C](#A3 "Appendix C Varying the pruning fraction
    per-iteration â€£ Everybody Prune Now: Structured Pruning of LLMs with only Forward
    Passes") for experiment details.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p_{\mathrm{iter}}=0.05$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| w/o Adapt | $19.47$ |'
  prefs: []
  type: TYPE_TB
- en: '| w Adapt | $8.89$ |'
  prefs: []
  type: TYPE_TB
- en: 8 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 8.1 Pruning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Unstructured pruning:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Whilst structured pruning approaches remove entire model components like layers
    (Xu etÂ al., [2020](#bib.bib50); Xia etÂ al., [2022](#bib.bib48)), dimensions of
    linear layers (Wang etÂ al., [2019](#bib.bib47)) or attention heads (Michel etÂ al.,
    [2019](#bib.bib35); Held & Yang, [2022](#bib.bib17)), unstructured pruning (Han
    etÂ al., [2015](#bib.bib15); Frankle & Carbin, [2018](#bib.bib10); Benbaki etÂ al.,
    [2023](#bib.bib4); Sun etÂ al., [2023](#bib.bib44)) removes individual parameters
    of the model. These approaches achieve memory savings by inducing sparsity in
    the model weights, but they generally do not result in actual model speedups except
    when specialized hardware is available (Mishra etÂ al., [2021](#bib.bib36)). Proposed
    semi-structured sparsity methods (Mishra etÂ al., [2021](#bib.bib36)) such as 2:4
    and 4:8 patterns do result in faster inference, but the speedup gains they achieve
    are far from the idealized $2\times$. There are several gradient-free, unstructured
    pruning approaches. Since, as far as we know, there are no scalable gradient-free
    structured pruning alternatives outside of Bonsai, we compare it with Wanda (Sun
    etÂ al., [2023](#bib.bib44)), a SoTA unstructured/semi-structured pruning technique
    for LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Structured pruning without backward passes:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To the best of our knowledge, all existing structured pruning techniques for
    large (over 1B scale) language models, like LoRAPrune (Zhang etÂ al., [2023](#bib.bib52))
    and LLM-Pruner (Ma etÂ al., [2023](#bib.bib31)), are gradient-based. Thus, under
    the memory setting we consider, these methods cannot be applied since their memory
    requirements well exceed the memory needed for inference on the model to be pruned.
    For smaller language models like BERT, Nova etÂ al. ([2023](#bib.bib37)) recently
    proposed Kernelized Convex Masking (KCM) for gradient-free structured pruning.
    Unfortunately, to prune a fully connected layer with $K$ fraction within its layer
    will not be included in the pruned model â€” a clearly sub-optimal choice. BonsaiÂ enables
    us to obtain globally meaningful estimates of module relevance that are comparable
    across layers.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Model compression beyond pruning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Distillation:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Originally proposed by Hinton etÂ al. ([2015](#bib.bib18)), Sanh etÂ al. ([2019](#bib.bib41))
    have shown that it is possible to reach similar performances on many downstream
    tasks using much smaller language models pre-trained with knowledge distillation.
    The resulting models are lighter and faster at inference time. Unfortunately,
    as noted by Xia etÂ al. ([2022](#bib.bib48)), distilling a pre-trained model to
    a student model that has been initialized from scratch requires lots of compute
    and data to recover reasonable levels of teacher performance. To reduce this resource
    burden, Sanh etÂ al. ([2020](#bib.bib42)); Lagunas etÂ al. ([2021](#bib.bib27));
    Xia etÂ al. ([2022](#bib.bib48)) combine distillation with gradient-based structured
    pruning, so that the student model does not have to be initialized from scratch.
    BonsaiÂ also leverages distillation, but only during the post-pruning adaptation
    phase. This maintains the constraint that the pruning process is gradient-free.
    And since the pruned model is smaller than the parent model, we can safely perform
    distillation and parameter efficient fine-tuning (Hu etÂ al., [2021](#bib.bib20);
    He etÂ al., [2021](#bib.bib16)) within the bounds of available memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Quantization:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Li etÂ al. ([2020](#bib.bib29)) introduced a training strategy that showed that
    heavily compressed, large models achieved higher accuracy than lightly compressed,
    small models, providing both efficiency and high-accuracy results on NLP tasks
    at once. Dettmers etÂ al. ([2022](#bib.bib9)) introduced a new method called â€˜LLM.int8()â€˜,
    which allows loading large models with 16 or 32-bit weights and using them immediately
    for inference without any performance degradation by combining vector-wise quantization
    with mixed-precision decomposition. Xiao etÂ al. ([2023](#bib.bib49)) enabled 8-bit
    weight, 8-bit activation (W8A8) quantization for LLMs with >100B parameters, which
    smoothed activation outliers by migrating the quantization difficulty from activations
    to weights with a mathematically equivalent transformation. Quantization tends
    to be complementary to pruning, allowing independent exploration and subsequent
    combinations of both techniques. Some quantization approaches, specifically mixed
    precision ones like Dettmers etÂ al. ([2022](#bib.bib9)) (Appendix D), can produce
    resulting models that are *slower* than the parent model, though it has also been
    shown that it can result in more accurate models than pruning (Kuzmin etÂ al.,
    [2023](#bib.bib26)). This paper focuses on gradient-free structured pruning. We
    leave combining BonsaiÂ with quantization for future work.
  prefs: []
  type: TYPE_NORMAL
- en: 9 Conclusion, Limitations, and Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we have presented Bonsai, the first tractable gradient-free method
    for structured pruning of LLMs. BonsaiÂ allows practitioners to prune any LLM as
    long as they have enough memory to run inferences on the model. Through a battery
    of experiments, we have shown that the models produced by BonsaiÂ are small, fast,
    and accurate.
  prefs: []
  type: TYPE_NORMAL
- en: 'A primary limitation of BonsaiÂ is its runtime. As Section [7](#S7 "7 Analysis
    â€£ Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes") shows,
    performance improves when we increase the number of sub-models explored, slow
    down pruning, and use more data samples; but this comes at the cost of increased
    runtime. Our result in Table [2](#S4.T2 "Table 2 â€£ 4 Post-pruning adaptation â€£
    Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes") took
    $\sim 40$ hours to obtain, whilst Wanda-2:4 takes on the order of minutes. Since
    the model we produce is twice as fast, this trade-off is worthwhile when amortized
    over inference on the pruned models.'
  prefs: []
  type: TYPE_NORMAL
- en: BonsaiÂ presents several avenues for expansion by future work. First, though
    sub-models are sampled from an informative prior, $\rho$, the sampling process
    is not adaptive. BonsaiÂ could further be strengthened by dynamically exploring
    the space of sub-models. Next, because of memory constraints, BonsaiÂ does not
    fine-tune the model during iterative pruning to recover degraded performance.
    However, forward-pass-only fine-tuning approaches like MeZO (Malladi etÂ al., [2023](#bib.bib32))
    exist that can be used to dynamically update the model during pruning.
  prefs: []
  type: TYPE_NORMAL
- en: 10 Acknowlegements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work was supported in part by the National Science Foundation grants IIS1705121,
    IIS1838017, IIS2046613, IIS2112471, and funding from Meta, Morgan Stanley, Amazon,
    and Google. Any opinions, findings and conclusions or recommendations expressed
    in this material are those of the author(s) and do not necessarily reflect the
    views of any of these funding agencies. We are grateful for helpful feedback from
    Mingjie Sun, Victor Akinwande, Asher Trockman, Afshin Rostamizadeh and Daniel
    Glasner
  prefs: []
  type: TYPE_NORMAL
- en: 11 Broader Impact Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Network pruning is an effective compression strategy to reduce the inference
    costs of large language models. However, existing techniques for structured pruning
    impose expensive memory constraints that may render them out of reach in practice.
    By reducing such constraints, work aims to democratize pruning to provide practitioners
    the ability to produce their own pruned LLMs for real-world applications. While
    we ultimately view this democratization as a benefit, we note that a necessary
    outcome is that it may also make it more feasible for malicious actors to readily
    use LLMs in practice. Finally, we note that our work has primarily focused on
    the axes of efficiency and utility (e.g., perplexity) in assessing performance;
    recent work has shown that compression may also have outsized effects on issues
    such as model fairness and robustness, which would be interesting additional aspects
    to consider in future study.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ancona etÂ al. (2020) Ancona, M., Ã–ztireli, C. and Gross, M. Shapley value as
    principled metric for structured network pruning. *arXiv preprint arXiv:2006.01795*,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anwar etÂ al. (2017) Anwar, S., Hwang, K. and Sung, W. Structured pruning of
    deep convolutional neural networks. *ACM Journal on Emerging Technologies in Computing
    Systems (JETC)*, 13(3):1â€“18, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ba etÂ al. (2016) Ba, J.L., Kiros, J.R. and Hinton, G.E. Layer normalization.
    *arXiv preprint arXiv:1607.06450*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Benbaki etÂ al. (2023) Benbaki, R., Chen, W., Meng, X., Hazimeh, H., Ponomareva,
    N., Zhao, Z. and Mazumder, R. Fast as chita: Neural network pruning with combinatorial
    optimization. *arXiv preprint arXiv:2302.14623*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bender etÂ al. (2021) Bender, E.M., Gebru, T., McMillan-Major, A. and Shmitchell,
    S. On the dangers of stochastic parrots: Can language models be too big. In *Proceedings
    of the 2021 ACM conference on fairness, accountability, and transparency*, pp.Â 
    610â€“623, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bridger (2023) Bridger, P. Pytorch memory tuning, Jul 2023. URL [https://paulbridger.com/posts/pytorch-memory-tuning/](https://paulbridger.com/posts/pytorch-memory-tuning/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Covert & Lee (2020) Covert, I. and Lee, S.I. Improving kernelshap: Practical
    shapley value estimation via linear regression. *arXiv preprint arXiv:2012.01536*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dekhovich etÂ al. (2021) Dekhovich, A., Tax, D.M., Sluiter, M.H. and Bessa,
    M.A. Neural network relief: a pruning algorithm based on neural activity. *arXiv
    preprint arXiv:2109.10795*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers etÂ al. (2022) Dettmers, T., Lewis, M., Belkada, Y. and Zettlemoyer,
    L. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. *arXiv
    preprint arXiv:2208.07339*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frankle & Carbin (2018) Frankle, J. and Carbin, M. The lottery ticket hypothesis:
    Finding sparse, trainable neural networks. *arXiv preprint arXiv:1803.03635*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar & Alistarh (2023) Frantar, E. and Alistarh, D. Sparsegpt: Massive language
    models can be accurately pruned in one-shot. In *International Conference on Machine
    Learning*, pp.Â  10323â€“10337\. PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao etÂ al. (2023) Gao, L. etÂ al. A framework for few-shot language model evaluation,
    12 2023. URL [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gemini Team etÂ al. (2023) Gemini Team etÂ al. Gemini: a family of highly capable
    multimodal models. *arXiv preprint arXiv:2312.11805*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu etÂ al. (2023) Gu, Y., Dong, L., Wei, F. and Huang, M. Knowledge distillation
    of large language models. *arXiv preprint arXiv:2306.08543*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han etÂ al. (2015) Han, S., Mao, H. and Dally, W.J. Deep compression: Compressing
    deep neural networks with pruning, trained quantization and huffman coding. *arXiv
    preprint arXiv:1510.00149*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He etÂ al. (2021) He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T. and Neubig,
    G. Towards a unified view of parameter-efficient transfer learning. *arXiv preprint
    arXiv:2110.04366*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Held & Yang (2022) Held, W. and Yang, D. Shapley head pruning: Identifying
    and removing interference in multilingual transformers. *arXiv preprint arXiv:2210.05709*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton etÂ al. (2015) Hinton, G.E., Vinyals, O. and Dean, J. Distilling the knowledge
    in a neural network. *ArXiv*, abs/1503.02531, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hsieh etÂ al. (2023) Hsieh, C.Y., Li, C.L., Yeh, C.K., Nakhost, H., Fujii, Y.,
    Ratner, A., Krishna, R., Lee, C.Y. and Pfister, T. Distilling step-by-step! outperforming
    larger language models with less training data and smaller model sizes. *arXiv
    preprint arXiv:2305.02301*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu etÂ al. (2021) Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,
    S., Wang, L. and Chen, W. Lora: Low-rank adaptation of large language models.
    *arXiv preprint arXiv:2106.09685*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang etÂ al. (2023) Jiang, A.Q. etÂ al. Mistral 7b. *arXiv preprint arXiv:2310.06825*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiao etÂ al. (2019) Jiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L.,
    Wang, F. and Liu, Q. Tinybert: Distilling bert for natural language understanding.
    *arXiv preprint arXiv:1909.10351*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kang etÂ al. (2023) Kang, M., Li, L. and Li, B. Fashapley: Fast and approximated
    shapley based model pruning towards certifiably robust dnns. In *2023 IEEE Conference
    on Secure and Trustworthy Machine Learning (SaTML)*, pp.Â  575â€“592\. IEEE, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kendall (1948) Kendall, M.G. Rank correlation methods. 1948.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kingma & Ba (2014) Kingma, D.P. and Ba, J. Adam: A method for stochastic optimization.
    *arXiv preprint arXiv:1412.6980*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kuzmin etÂ al. (2023) Kuzmin, A., Nagel, M., VanÂ Baalen, M., Behboodi, A. and
    Blankevoort, T. Pruning vs quantization: Which is better? *arXiv preprint arXiv:2307.02973*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lagunas etÂ al. (2021) Lagunas, F., Charlaix, E., Sanh, V. and Rush, A.M. Block
    pruning for faster transformers. *arXiv preprint arXiv:2109.04838*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li etÂ al. (2023) Li, Y., Bubeck, S., Eldan, R., DelÂ Giorno, A., Gunasekar,
    S. and Lee, Y.T. Textbooks are all you need ii: phi-1.5 technical report. *arXiv
    preprint arXiv:2309.05463*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li etÂ al. (2020) Li, Z., Wallace, E., Shen, S., Lin, K., Keutzer, K., Klein,
    D. and Gonzalez, J. Train big, then compress: Rethinking model size for efficient
    training and inference of transformers. In *International Conference on machine
    learning*, pp.Â  5958â€“5968\. PMLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loshchilov & Hutter (2017) Loshchilov, I. and Hutter, F. Decoupled weight decay
    regularization. *arXiv preprint arXiv:1711.05101*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma etÂ al. (2023) Ma, X., Fang, G. and Wang, X. Llm-pruner: On the structural
    pruning of large language models. *arXiv preprint arXiv:2305.11627*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Malladi etÂ al. (2023) Malladi, S., Gao, T., Nichani, E., Damian, A., Lee, J.D.,
    Chen, D. and Arora, S. Fine-tuning language models with just forward passes. *arXiv
    preprint arXiv:2305.17333*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'McGrath etÂ al. (2023) McGrath, T., Rahtz, M., Kramar, J., Mikulik, V. and Legg,
    S. The hydra effect: Emergent self-repair in language model computations. *arXiv
    preprint arXiv:2307.15771*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merity etÂ al. (2016) Merity, S., Xiong, C., Bradbury, J. and Socher, R. Pointer
    sentinel mixture models. *arXiv preprint arXiv:1609.07843*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Michel etÂ al. (2019) Michel, P., Levy, O. and Neubig, G. Are sixteen heads really
    better than one? *Advances in neural information processing systems*, 32, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mishra etÂ al. (2021) Mishra, A., Latorre, J.A., Pool, J., Stosic, D., Stosic,
    D., Venkatesh, G., Yu, C. and Micikevicius, P. Accelerating sparse deep neural
    networks. *arXiv preprint arXiv:2104.08378*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nova etÂ al. (2023) Nova, A., Dai, H. and Schuurmans, D. Gradient-free structured
    pruning with unlabeled data, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI etÂ al. (2023) OpenAI etÂ al. Gpt-4 technical report, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel etÂ al. (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang,
    S., Matena, M., Zhou, Y., Li, W. and Liu, P.J. Exploring the limits of transfer
    learning with a unified text-to-text transformer. *The Journal of Machine Learning
    Research*, 21(1):5485â€“5551, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Samsi etÂ al. (2023) Samsi, S., Zhao, D., McDonald, J., Li, B., Michaleas, A.,
    Jones, M., Bergeron, W., Kepner, J., Tiwari, D. and Gadepally, V. From words to
    watts: Benchmarking the energy costs of large language model inference. In *2023
    IEEE High Performance Extreme Computing Conference (HPEC)*, pp.Â  1â€“9\. IEEE, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sanh etÂ al. (2019) Sanh, V., Debut, L., Chaumond, J. and Wolf, T. Distilbert,
    a distilled version of bert: smaller, faster, cheaper and lighter. *ArXiv*, abs/1910.01108,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sanh etÂ al. (2020) Sanh, V., Wolf, T. and Rush, A. Movement pruning: Adaptive
    sparsity by fine-tuning. *Advances in Neural Information Processing Systems*,
    33:20378â€“20389, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simonyan & Zisserman (2014) Simonyan, K. and Zisserman, A. Very deep convolutional
    networks for large-scale image recognition. *arXiv preprint arXiv:1409.1556*,
    2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun etÂ al. (2023) Sun, M., Liu, Z., Bair, A. and Kolter, J.Z. A simple and effective
    pruning approach for large language models. *arXiv preprint arXiv:2306.11695*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron etÂ al. (2023) Touvron, H. etÂ al. Llama 2: Open foundation and fine-tuned
    chat models. *arXiv preprint arXiv:2307.09288*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vivek (2023) Vivek, S. The economics of large language models, Sep 2023. URL
    [https://medium.com/emalpha/the-economics-of-large-language-models-2671985b621c](https://medium.com/emalpha/the-economics-of-large-language-models-2671985b621c).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang etÂ al. (2019) Wang, Z., Wohlwend, J. and Lei, T. Structured pruning of
    large language models. *arXiv preprint arXiv:1910.04732*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xia etÂ al. (2022) Xia, M., Zhong, Z. and Chen, D. Structured pruning learns
    compact and accurate models. *arXiv preprint arXiv:2204.00408*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao etÂ al. (2023) Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J. and Han,
    S. Smoothquant: Accurate and efficient post-training quantization for large language
    models. In *International Conference on Machine Learning*, pp.Â  38087â€“38099\.
    PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu etÂ al. (2020) Xu, C., Zhou, W., Ge, T., Wei, F. and Zhou, M. Bert-of-theseus:
    Compressing bert by progressive module replacing. *arXiv preprint arXiv:2002.02925*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zagoruyko & Komodakis (2016) Zagoruyko, S. and Komodakis, N. Wide residual networks.
    *arXiv preprint arXiv:1605.07146*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang etÂ al. (2023) Zhang, M., Shen, C., Yang, Z., Ou, L., Yu, X., Zhuang, B.
    etÂ al. Pruning meets low-rank parameter-efficient fine-tuning. *arXiv preprint
    arXiv:2305.18403*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Main Experiment Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Hyper-parameters for all BonsaiÂ regression during pruning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When using Bonsai, we estimate $\beta$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: BonsaiÂ hyper-parameters for regression. This applies to all experiments
    unless otherwise specified'
  prefs: []
  type: TYPE_NORMAL
- en: '| $\gamma$(Regression Weight) | Learning rate | Batch Size | Epochs |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| {100, 0, 1e-4} | {100, 10, 1, 0.1} | {32, 64, 128} | 50 |'
  prefs: []
  type: TYPE_TB
- en: During cross validation, we choose the model whose predictions have the best
    Kendall rank correlation co-efficient (Kendall, [1948](#bib.bib24)) with the target.
    We do this because we do not care about matching $U_{k}$ reasonably models relative
    module importances.
  prefs: []
  type: TYPE_NORMAL
- en: In general, we use $\ell_{1}$-norm works better.
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Forward Pass Only Experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [10](#A1.T10 "Table 10 â€£ A.2 Forward Pass Only Experiments â€£ Appendix
    A Main Experiment Details â€£ Everybody Prune Now: Structured Pruning of LLMs with
    only Forward Passes") show the BonsaiÂ hyperparameters we used for the experiments
    in Section [6.1](#S6.SS1 "6.1 Bonsai produces fast and performant models with
    only forward passes â€£ 6 Main Results â€£ Everybody Prune Now: Structured Pruning
    of LLMs with only Forward Passes").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 9: BonsaiÂ hyper-params for forward only experiments'
  prefs: []
  type: TYPE_NORMAL
- en: '| $p_{\mathrm{iter}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0.05 | 200 | 32 (per-iter) | Wanda |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: BonsaiÂ  fine-tuning HP for pruned LLaMA family models'
  prefs: []
  type: TYPE_NORMAL
- en: '| LR | rank | LoRA-$\alpha$ (Distill Weight) | LoRA Modules |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1e-4 | 128 | 4$\times$rank | 0.01 | All Modules |'
  prefs: []
  type: TYPE_TB
- en: 'For Wanda(Sun etÂ al., [2023](#bib.bib44)), we use the default hyper-parameters
    specified by [the paper repo here](https://github.com/locuslab/wanda/tree/main)
    for pruning. For fine-tuning, we use rank = 64\. We apply LoRA to only the q_proj
    and v_proj matrices in each layer of the pruned LLaMA model â€“ this is unlike with
    BonsaiÂ where we fine-tune all modules. We cannot do same because since the Wanda
    model just produces sparse matrices, the matrices instantiated during the backward
    pass are the same sizes as the sparsified matrices and thus occupy more memory
    (compared to our approach that actually makes the matrices smaller in dimension
    instead of sparsifying). We are also unable to perform distillation on the Wanda
    models due to this reason. For fine-tuning the Phi-2 model on Wikitext-2, we use
    the same hyper-parameters as BonsaiÂ in Table [10](#A1.T10 "Table 10 â€£ A.2 Forward
    Pass Only Experiments â€£ Appendix A Main Experiment Details â€£ Everybody Prune Now:
    Structured Pruning of LLMs with only Forward Passes").'
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Experiments comparing to Gradient based structured pruning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We compare to LoRA-Prune and LLM-Pruner. We take their performance results directly
    from the LoRA-Prune paper. Whilst we use 1 A6000 GPU (48G) for all experiments,
    LoRA-Prune uses A100 GPU (80G) for pruning LLaMA-1 7B.
  prefs: []
  type: TYPE_NORMAL
- en: 'All BonsaiÂ hyper-parameters are the same as Appendix [A.2](#A1.SS2 "A.2 Forward
    Pass Only Experiments â€£ Appendix A Main Experiment Details â€£ Everybody Prune Now:
    Structured Pruning of LLMs with only Forward Passes") except for $\mathrm{ns}_{\mathrm{sub-models}}$.'
  prefs: []
  type: TYPE_NORMAL
- en: A.4 Phi-2 pruning experiment details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the experiment in Section [6.3](#S6.SS3 "6.3 Bonsai can produce compressed
    models with strong zero-shot abilities â€£ 6 Main Results â€£ Everybody Prune Now:
    Structured Pruning of LLMs with only Forward Passes"), All BonsaiÂ hyper-parameters
    are the same as Appendix [A.2](#A1.SS2 "A.2 Forward Pass Only Experiments â€£ Appendix
    A Main Experiment Details â€£ Everybody Prune Now: Structured Pruning of LLMs with
    only Forward Passes") except for the following changes:'
  prefs: []
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\mathrm{ns}_{\mathrm{sub-models}}=2000$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $p_{\mathrm{iter}}=0.35$. We thus perform 1-shot pruning directly to the target
    sparsity of 35%. We find that this seems to work best for the Phi-2 model. We
    posit that this might be because the Phi-2 models use LayerNorm(Ba etÂ al., [2016](#bib.bib3))
    whilst the other models we explore, LLaMA and Mistral use RMSNorm.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to its relatively small size, the 1.8B pruned model can be fully fine-tuned
    on a single A6000 GPU over 100k sequences of length 2,048 tokens from the C4 dataset
    instead of using LoRA.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Appendix B Impact of regression and perturbation ablation details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For the experiment in Section [3](#S7.F3 "Figure 3 â€£ 7 Analysis â€£ Everybody
    Prune Now: Structured Pruning of LLMs with only Forward Passes"), All BonsaiÂ hyper-parameters
    are the same as Appendix [A.2](#A1.SS2 "A.2 Forward Pass Only Experiments â€£ Appendix
    A Main Experiment Details â€£ Everybody Prune Now: Structured Pruning of LLMs with
    only Forward Passes") except $p_{\mathrm{iter}}=0.1$ to speed up pruning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple alternative to BonsaiÂ is to leverage the prior $\rho$ for this experiment.
    Module level analogues of the unstructured pruning metrics we explore are defined
    in Appendix [F](#A6 "Appendix F Impact of prior â€£ Everybody Prune Now: Structured
    Pruning of LLMs with only Forward Passes").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11: Experiment on linear regression to estimate module importances. Wikitext-2
    Perplexity. LLaMA-2 7B pruned to 50% sparsity'
  prefs: []
  type: TYPE_NORMAL
- en: '| Linear Regression | Relative Speepdup | w/o Post-Pruning Adaptation | w Post-Pruning
    Adaptation |'
  prefs: []
  type: TYPE_TB
- en: '| No | $2.06$ |'
  prefs: []
  type: TYPE_TB
- en: '| Yes | $1.77$ |'
  prefs: []
  type: TYPE_TB
- en: Appendix C Varying the pruning fraction per-iteration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For the experiment in Section [3](#S7.F3 "Figure 3 â€£ 7 Analysis â€£ Everybody
    Prune Now: Structured Pruning of LLMs with only Forward Passes"), All BonsaiÂ hyper-parameters
    are the same as Appendix [A.2](#A1.SS2 "A.2 Forward Pass Only Experiments â€£ Appendix
    A Main Experiment Details â€£ Everybody Prune Now: Structured Pruning of LLMs with
    only Forward Passes") except we vary $p_{\mathrm{iter}}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 12: Varying the fraction pruned at a time. Wikitext-2 Perplexity. LLaMA-2
    7B pruned to 50% sparsity'
  prefs: []
  type: TYPE_NORMAL
- en: '| Prune Frac | Relative Speepdup | w/o Post-Pruning Adaptation | w Post-Pruning
    Adaptation |'
  prefs: []
  type: TYPE_TB
- en: '| 0.05 | $1.58$ |'
  prefs: []
  type: TYPE_TB
- en: '| 0.1 | $1.77$ |'
  prefs: []
  type: TYPE_TB
- en: '| 0.20 | $1.67$ |'
  prefs: []
  type: TYPE_TB
- en: Appendix D Varying the number of calibration data points for pruning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All BonsaiÂ hyper-parameters are the same as Appendix [A.2](#A1.SS2 "A.2 Forward
    Pass Only Experiments â€£ Appendix A Main Experiment Details â€£ Everybody Prune Now:
    Structured Pruning of LLMs with only Forward Passes") except we vary $\mathrm{ns}_{\mathrm{data}}$
    to speed up pruning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 13: How many data-points to consider during forward passes. Wikitext-2
    Perplexity. Llama-2 7B pruned to 50% sparsity'
  prefs: []
  type: TYPE_NORMAL
- en: '| $\mathrm{ns}_{\mathrm{data}}$ | w/o Adapt | w Adapt |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | $130.04$ |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | $61.63$ |'
  prefs: []
  type: TYPE_TB
- en: Appendix E Post-pruning adaptation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this experiment, All BonsaiÂ hyper-parameters are the same as Appendix [A.2](#A1.SS2
    "A.2 Forward Pass Only Experiments â€£ Appendix A Main Experiment Details â€£ Everybody
    Prune Now: Structured Pruning of LLMs with only Forward Passes").'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix F Impact of prior
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this experiment, All BonsaiÂ hyper-parameters are the same as Appendix [A.2](#A1.SS2
    "A.2 Forward Pass Only Experiments â€£ Appendix A Main Experiment Details â€£ Everybody
    Prune Now: Structured Pruning of LLMs with only Forward Passes") except we vary
    $\rho$'
  prefs: []
  type: TYPE_NORMAL
- en: F.1 $\rho$ is Activation Magnitude
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'MLP / Fully Connected Module: Let $d$ and then compute the following averaged
    activation magnitude :'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\left(\rho\in\mathbb{R}^{d}\right)\propto\hat{\mathbf{a}}=\frac{1}{B}\sum_{b}\mathrm{Mean}\bigg{(}\big{&#124;}\mathbf{a}_{b}\big{&#124;},\mathrm{axis=}0\bigg{)}$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'Self-Attention Module: For any data-sample sequence $b$.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\left(\rho\in\mathbb{R}^{h}\right)\propto\hat{\mathbf{a}}$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: F.2 $\rho$ is Wanda (Sun etÂ al., [2023](#bib.bib44))
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'MLP / Fully Connected Module: Let $d$ and then compute the following metric
    which is a module level analogue of Wanda:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 'Self-Attention Module: Let $W\in\mathbb{R}^{d\times o}$.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix G How many perturbative samples are reasonable?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this experiment, All BonsaiÂ hyper-parameters are the same as Appendix [A.2](#A1.SS2
    "A.2 Forward Pass Only Experiments â€£ Appendix A Main Experiment Details â€£ Everybody
    Prune Now: Structured Pruning of LLMs with only Forward Passes") except $p_{\mathrm{iter}}=0.1$
    to speed up pruning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 14: Varying the number of sub-models generated. Wikitext-2 Perplexity.
    LLaMA-2 7B pruned to 50% sparsity'
  prefs: []
  type: TYPE_NORMAL
- en: '| Num Samples | w/o Post-Pruning Adaptation | w Post-Pruning Adaptation |'
  prefs: []
  type: TYPE_TB
- en: '| 1000 | $22.09$ |'
  prefs: []
  type: TYPE_TB
- en: '| 200 | $61.63$ |'
  prefs: []
  type: TYPE_TB
- en: '| 50 | NaN | $9.24$ |'
  prefs: []
  type: TYPE_TB
- en: Using $\mathrm{ns}_{\mathrm{sub-models}}=50$ results in an model with NaN perplexity
    on the Wikitext validation set. We posit that this is because of the LLaMA models
    are half precision, and removing the wrong modules can result in activations going
    outside of the FP16 dynamic range for unique data-points. Note that we are able
    to recover good performance of the model after fine-tuning though (we do not observe
    NaNs with the Wikitext-2 training data). This indicates that BonsaiÂ actually recovers
    good modules even using as few samples as 50 sub-models.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix H Mistral-7B Experiment Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to the primary experiments on the LLaMA and Phi-2 models, supplementary
    experiments were performed on the Mistral-7B (Jiang etÂ al., [2023](#bib.bib21))
    model in comparison with Wanda results on the stated model. We apply BonsaiÂ with
    the same hardware and configuration settings as used for the LLaMA and Phi-2 experiments.
    We target different pruning fractions (0.05, 0.1, and 0.2) across different numbers
    of samples and masks per iteration to evaluate the methodâ€™s performance under
    varying sparsity conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Mistral-7B model architecture differs from the LLaMA architecture in its
    use of group query attention and sliding window attention in lieu of the standard
    self-attention used in most transformer-based models like LLaMA (Jiang etÂ al.,
    [2023](#bib.bib21)). We factor these differences into consideration in the implementation
    of BonsaiÂ for Mistral. For the experiments that produced the results below, all
    BonsaiÂ hyper-parameters are the same as Appendix [A.2](#A1.SS2 "A.2 Forward Pass
    Only Experiments â€£ Appendix A Main Experiment Details â€£ Everybody Prune Now: Structured
    Pruning of LLMs with only Forward Passes").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [15](#A8.T15 "Table 15 â€£ Appendix H Mistral-7B Experiment Details â€£ Everybody
    Prune Now: Structured Pruning of LLMs with only Forward Passes") presents the
    test perplexity results for Mistral-7B under different pruning methods. Considering
    the fully-structured sparsity nature of Bonsai, it achieves a test perplexity
    of 47.5 without post-pruning adaptation, with 1.66$\times$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 15: Test perplexity of Mistral-7B model on Wikitext-2 across fully-structured
    BonsaiÂ and semi-structured Wanda methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Sparsity Level | Method | Test PPL |'
  prefs: []
  type: TYPE_TB
- en: '| Original, unpruned Mistral-7B | N/A | N/A | 5.245 |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | semi-structured 2-4 | magnitude | 13.81 |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | 12.38 |'
  prefs: []
  type: TYPE_TB
- en: '| SparseGPT | 10.46 |'
  prefs: []
  type: TYPE_TB
- en: '| BonsaiÂ (w/o Adaptation) | structured 50% | magnitude | 67.48 |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | 47.50 |'
  prefs: []
  type: TYPE_TB
- en: '| BonsaiÂ (w/ Adaptation) | structured 50% | Wanda | 10.08 |'
  prefs: []
  type: TYPE_TB
- en: 'We further investigate the pruning habits of BonsaiÂ by examining the pruned
    layers of Mistral, as shown in Figure [4](#A8.F4 "Figure 4 â€£ Appendix H Mistral-7B
    Experiment Details â€£ Everybody Prune Now: Structured Pruning of LLMs with only
    Forward Passes"). We notice a recurring theme: when an attention layer is significantly
    altered, it leads to compensation in the next layers within the sequence. This
    adaptive behavior, termed the â€Hydra effectâ€ by McGrath etÂ al. ([2023](#bib.bib33)),
    implies that the layers within a language model interact in a way that changes
    in one layer prompt adjustments in another. McGrath etÂ al. ([2023](#bib.bib33))
    specifically mentioned that when one attention layer was removed from a language
    model, the model was still able to self-repair and produce similar outputs; but
    it did so by relying more heavily on other layers.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/662bb7cfb2dc694c9131df9d6ae50772.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Mistralâ€™s pruned attention layers. The heavily pruned layers are
    usually preceded by or sandwiched between lightly-pruned layers, exhibiting the
    self-repairing â€Hydra effectâ€ (McGrath etÂ al., [2023](#bib.bib33)).'
  prefs: []
  type: TYPE_NORMAL
