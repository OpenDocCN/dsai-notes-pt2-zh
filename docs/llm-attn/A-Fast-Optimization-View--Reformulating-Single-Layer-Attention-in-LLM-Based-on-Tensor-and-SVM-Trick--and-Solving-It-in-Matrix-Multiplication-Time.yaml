- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: æœªåˆ†ç±»'
- en: 'date: 2024-09-08 19:03:24'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 19:03:24'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸€ç§å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§çš„å•å±‚æ³¨æ„åŠ›é‡æ„åŠå…¶çŸ©é˜µä¹˜æ³•æ—¶é—´è§£å†³æ–¹æ¡ˆ
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2309.07418](https://ar5iv.labs.arxiv.org/html/2309.07418)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2309.07418](https://ar5iv.labs.arxiv.org/html/2309.07418)
- en: Yeqi Gao a916755226@gmail.com. The University of Washington. â€ƒâ€ƒ Zhao Song zsong@adobe.com.
    Adobe Research. â€ƒâ€ƒ Weixin Wang wwang176@jh.edu. Johns Hopkins University. â€ƒâ€ƒ Junze
    Yin junze@bu.edu. Boston University.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Yeqi Gao a916755226@gmail.com. åç››é¡¿å¤§å­¦ã€‚ â€ƒâ€ƒ Zhao Song zsong@adobe.com. Adobe Research.
    â€ƒâ€ƒ Weixin Wang wwang176@jh.edu. çº¦ç¿°æ–¯Â·éœæ™®é‡‘æ–¯å¤§å­¦ã€‚ â€ƒâ€ƒ Junze Yin junze@bu.edu. æ³¢å£«é¡¿å¤§å­¦ã€‚
- en: Large language models have played a pivotal role in revolutionizing various
    facets of our daily existence. Serving as the cornerstone of virtual assistants,
    they have seamlessly streamlined information retrieval and task automation. Spanning
    domains from healthcare to education, these models have made an enduring impact,
    elevating productivity, decision-making processes, and accessibility, thereby
    influencing and, to a certain extent, reshaping the lifestyles of people.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å½»åº•æ”¹å˜æˆ‘ä»¬æ—¥å¸¸ç”Ÿæ´»çš„å„ä¸ªæ–¹é¢ä¸­å‘æŒ¥äº†å…³é”®ä½œç”¨ã€‚ä½œä¸ºè™šæ‹ŸåŠ©æ‰‹çš„åŸºçŸ³ï¼Œå®ƒä»¬æ— ç¼åœ°ç®€åŒ–äº†ä¿¡æ¯æ£€ç´¢å’Œä»»åŠ¡è‡ªåŠ¨åŒ–ã€‚ä»åŒ»ç–—ä¿å¥åˆ°æ•™è‚²ï¼Œè¿™äº›æ¨¡å‹äº§ç”Ÿäº†æŒä¹…çš„å½±å“ï¼Œæå‡äº†ç”Ÿäº§åŠ›ã€å†³ç­–è¿‡ç¨‹å’Œå¯åŠæ€§ï¼Œä»è€Œå½±å“å¹¶åœ¨æŸç§ç¨‹åº¦ä¸Šé‡å¡‘äº†äººä»¬çš„ç”Ÿæ´»æ–¹å¼ã€‚
- en: Solving attention regression is a fundamental task in optimizing LLMs. In this
    work, we focus on giving a provable guarantee for the one-layer attention network
    objective function
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è§£å†³æ³¨æ„åŠ›å›å½’æ˜¯ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä¸€ä¸ªåŸºæœ¬ä»»åŠ¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºä¸ºå•å±‚æ³¨æ„åŠ›ç½‘ç»œçš„ç›®æ ‡å‡½æ•°æä¾›å¯è¯æ˜çš„ä¿è¯ã€‚
- en: '|  | $1$2 |  |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Here $\mathsf{A}\in\mathbb{R}^{n^{2}\times d^{2}}$ and $A_{2}\in\mathbb{R}^{n\times
    d}$ is a matrix in $\mathbb{R}^{n\times d}$ is the $j_{0}$. The $X,Y\in\mathbb{R}^{d\times
    d}$ and $b_{j_{0},i_{0}}\in\mathbb{R}$-th row and $i_{0}$, $Y_{*,i_{0}}\in\mathbb{R}^{d}$-column
    vector of $Y$ is the vectorization of $X$.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œ $\mathsf{A}\in\mathbb{R}^{n^{2}\times d^{2}}$ å’Œ $A_{2}\in\mathbb{R}^{n\times
    d}$ æ˜¯ $\mathbb{R}^{n\times d}$ ä¸­çš„ä¸€ä¸ªçŸ©é˜µï¼Œæ˜¯ $j_{0}$ã€‚$X,Y\in\mathbb{R}^{d\times d}$
    å’Œ $b_{j_{0},i_{0}}\in\mathbb{R}$-th è¡Œå’Œ $i_{0}$ï¼Œ$Y_{*,i_{0}}\in\mathbb{R}^{d}$-åˆ—å‘é‡
    $Y$ æ˜¯ $X$ çš„å‘é‡åŒ–ã€‚
- en: In a multi-layer LLM network, the matrix $B\in\mathbb{R}^{n\times d}$ can be
    viewed as the input of a layer. The matrix version of $x$ and $Y$. We provide
    an iterative greedy algorithm to train loss function $L(X,Y)$ that runs in $\widetilde{O}(({\cal
    T}_{\mathrm{mat}}(n,n,d)+{\cal T}_{\mathrm{mat}}(n,d,d)+d^{2\omega})\log(1/\epsilon))$
    denotes the time of multiplying $a\times b$ matrix, and $\omega\approx 2.37$ denotes
    the exponent of matrix multiplication.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤šå±‚ LLM ç½‘ç»œä¸­ï¼ŒçŸ©é˜µ $B\in\mathbb{R}^{n\times d}$ å¯ä»¥è¢«è§†ä¸ºä¸€å±‚çš„è¾“å…¥ã€‚çŸ©é˜µç‰ˆæœ¬çš„ $x$ å’Œ $Y$ã€‚æˆ‘ä»¬æä¾›äº†ä¸€ç§è¿­ä»£è´ªå©ªç®—æ³•æ¥è®­ç»ƒæŸå¤±å‡½æ•°
    $L(X,Y)$ï¼Œå…¶è¿è¡Œæ—¶é—´ä¸º $\widetilde{O}(({\cal T}_{\mathrm{mat}}(n,n,d)+{\cal T}_{\mathrm{mat}}(n,d,d)+d^{2\omega})\log(1/\epsilon))$ï¼Œå…¶ä¸­
    $\omega\approx 2.37$ è¡¨ç¤ºçŸ©é˜µä¹˜æ³•çš„æŒ‡æ•°ã€‚
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 å¼•è¨€
- en: Large language models (LLMs) like GPT-1 [[149](#bib.bib149)], BERT [[49](#bib.bib49)],
    GPT-2 [[154](#bib.bib154)], GPT-3 [[24](#bib.bib24)], ChatGPT [[35](#bib.bib35)],
    GPT-4 [[134](#bib.bib134)], OPT [[209](#bib.bib209)], Llama [[174](#bib.bib174)],
    and Llama 2 [[176](#bib.bib176)] have demonstrated impressive capabilities in
    natural language processing (NLP). These models understand and generate complex
    language, enabling a wide range of applications such as sentiment analysis [[200](#bib.bib200)],
    language translation [[1](#bib.bib1)], question answering [[23](#bib.bib23)],
    and text summarization [[137](#bib.bib137)]. Despite their high-quality performance,
    there remains untapped potential in optimizing and training these massive models,
    making it a challenging endeavor in the present day.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åƒ GPT-1 [[149](#bib.bib149)]ã€BERT [[49](#bib.bib49)]ã€GPT-2 [[154](#bib.bib154)]ã€GPT-3
    [[24](#bib.bib24)]ã€ChatGPT [[35](#bib.bib35)]ã€GPT-4 [[134](#bib.bib134)]ã€OPT [[209](#bib.bib209)]ã€Llama
    [[174](#bib.bib174)] å’Œ Llama 2 [[176](#bib.bib176)] è¿™æ ·çš„è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸå±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚è¿™äº›æ¨¡å‹ç†è§£å¹¶ç”Ÿæˆå¤æ‚çš„è¯­è¨€ï¼Œä½¿å¾—æƒ…æ„Ÿåˆ†æ
    [[200](#bib.bib200)]ã€è¯­è¨€ç¿»è¯‘ [[1](#bib.bib1)]ã€é—®ç­” [[23](#bib.bib23)] å’Œæ–‡æœ¬æ‘˜è¦ [[137](#bib.bib137)]
    ç­‰å¹¿æ³›åº”ç”¨æˆä¸ºå¯èƒ½ã€‚å°½ç®¡å®ƒä»¬çš„è¡¨ç°éå¸¸ä¼˜ç§€ï¼Œä½†åœ¨ä¼˜åŒ–å’Œè®­ç»ƒè¿™äº›å¤§å‹æ¨¡å‹æ–¹é¢ä»æœ‰æœªå¼€å‘çš„æ½œåŠ›ï¼Œä½¿å¾—è¿™ä¸€å·¥ä½œåœ¨å½“ä»Šä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚
- en: The primary technical foundation supporting the capabilities of LLMs is the
    attention matrix [[149](#bib.bib149), [179](#bib.bib179), [24](#bib.bib24), [49](#bib.bib49)].
    The central concept of attention is to learn representations that emphasize the
    most relevant parts of the input. To be more specific, the attention mechanism
    compares the query vectors (the output tokens) with the key vectors (the input
    tokens). The attention weights are then determined based on the similarity of
    this comparison, indicating the relative importance of each input token. These
    attention weights are used to compute weighted averages of the value vectors,
    resulting in the output representation. By leveraging attention, LLMs acquire
    the ability to focus on the crucial aspects of the input, allowing them to gather
    pertinent information more efficiently and precisely. This capability enables
    LLMs to process longer texts effectively and comprehend intricate semantic relationships.
    Notably, the self-attention mechanism enables LLMs to establish connections between
    various segments of the input sequence, enhancing their contextual understanding.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¯æ’‘ LLM åŠŸèƒ½çš„ä¸»è¦æŠ€æœ¯åŸºç¡€æ˜¯æ³¨æ„åŠ›çŸ©é˜µ [[149](#bib.bib149), [179](#bib.bib179), [24](#bib.bib24),
    [49](#bib.bib49)]ã€‚æ³¨æ„åŠ›çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯å­¦ä¹ å¼ºè°ƒè¾“å…¥ä¸­æœ€ç›¸å…³éƒ¨åˆ†çš„è¡¨ç¤ºã€‚æ›´å…·ä½“åœ°è¯´ï¼Œæ³¨æ„åŠ›æœºåˆ¶å°†æŸ¥è¯¢å‘é‡ï¼ˆè¾“å‡ºæ ‡è®°ï¼‰ä¸é”®å‘é‡ï¼ˆè¾“å…¥æ ‡è®°ï¼‰è¿›è¡Œæ¯”è¾ƒã€‚ç„¶åï¼Œæ ¹æ®è¿™ç§æ¯”è¾ƒçš„ç›¸ä¼¼æ€§æ¥ç¡®å®šæ³¨æ„åŠ›æƒé‡ï¼Œè¡¨ç¤ºæ¯ä¸ªè¾“å…¥æ ‡è®°çš„ç›¸å¯¹é‡è¦æ€§ã€‚è¿™äº›æ³¨æ„åŠ›æƒé‡ç”¨äºè®¡ç®—å€¼å‘é‡çš„åŠ æƒå¹³å‡ï¼Œä»è€Œå¾—åˆ°è¾“å‡ºè¡¨ç¤ºã€‚é€šè¿‡åˆ©ç”¨æ³¨æ„åŠ›ï¼ŒLLMs
    èƒ½å¤Ÿä¸“æ³¨äºè¾“å…¥çš„å…³é”®æ–¹é¢ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿæ›´æœ‰æ•ˆå’Œå‡†ç¡®åœ°æ”¶é›†ç›¸å…³ä¿¡æ¯ã€‚è¿™ä¸€èƒ½åŠ›ä½¿ LLMs èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†æ›´é•¿çš„æ–‡æœ¬ï¼Œå¹¶ç†è§£å¤æ‚çš„è¯­ä¹‰å…³ç³»ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶ä½¿
    LLMs èƒ½å¤Ÿåœ¨è¾“å…¥åºåˆ—çš„å„ä¸ªéƒ¨åˆ†ä¹‹é—´å»ºç«‹è”ç³»ï¼Œå¢å¼ºå…¶ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ã€‚
- en: We start with defining the general Attention forward layer,
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»å®šä¹‰ä¸€èˆ¬çš„æ³¨æ„åŠ›å‰å‘å±‚å¼€å§‹ï¼Œ
- en: Definition 1.1  ($\ell$-th layer forward computation).
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å®šä¹‰ 1.1 ï¼ˆ$\ell$-å±‚å‰å‘è®¡ç®—ï¼‰ã€‚
- en: 'Let ${\bf 1}_{n}$-dimensional vector whose entries are all $1$ be a function:
    each entry of the vector in $\mathbb{R}^{n}$ and other entries of this matrix
    are all $0$, let $X_{\ell}\in\mathbb{R}^{n\times d}$-th layer input and $X_{\ell+1}\in\mathbb{R}^{n\times
    d}$'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ ${\bf 1}_{n}$-ç»´å‘é‡ï¼Œå…¶æ‰€æœ‰æ¡ç›®å‡ä¸º $1$ ä¸ºä¸€ä¸ªå‡½æ•°ï¼šå‘é‡åœ¨ $\mathbb{R}^{n}$ ä¸­çš„æ¯ä¸ªæ¡ç›®åŠè¯¥çŸ©é˜µçš„å…¶ä»–æ¡ç›®å‡ä¸º
    $0$ï¼Œè®¾ $X_{\ell}\in\mathbb{R}^{n\times d}$-å±‚è¾“å…¥å’Œ $X_{\ell+1}\in\mathbb{R}^{n\times
    d}$
- en: '|  | $\displaystyle X_{\ell+1}\leftarrow D^{-1}\exp(X_{\ell}QK^{\top}X_{\ell}^{\top})X_{\ell}V$
    |  |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle X_{\ell+1}\leftarrow D^{-1}\exp(X_{\ell}QK^{\top}X_{\ell}^{\top})X_{\ell}V$
    |  |'
- en: where $D:=\operatorname{diag}(\exp(X_{\ell}QK^{\top}X_{\ell}^{\top}){\bf 1}_{n})$
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $D:=\operatorname{diag}(\exp(X_{\ell}QK^{\top}X_{\ell}^{\top}){\bf 1}_{n})$
- en: 'Mathematically, a general optimization with respect to attention computation
    is defined as:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ•°å­¦ä¸Šè®²ï¼Œå…³äºæ³¨æ„åŠ›è®¡ç®—çš„ä¸€èˆ¬ä¼˜åŒ–å®šä¹‰ä¸ºï¼š
- en: Definition 1.2  (Attention optimization).
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å®šä¹‰ 1.2 ï¼ˆæ³¨æ„åŠ›ä¼˜åŒ–ï¼‰ã€‚
- en: 'Let $A_{1},A_{2},A_{3},B\in\mathbb{R}^{n\times d}$. The attention computation
    is defined as:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ $A_{1},A_{2},A_{3},B\in\mathbb{R}^{n\times d}$ã€‚æ³¨æ„åŠ›è®¡ç®—å®šä¹‰ä¸ºï¼š
- en: '|  | $1$2 |  |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where $D(X)\in\mathbb{R}^{n\times n}$.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $D(X)\in\mathbb{R}^{n\times n}$ã€‚
- en: '![Refer to caption](img/4299be57d79d62ad6bdc597ce50480f9.png)![Refer to caption](img/00d76d08bdd817b3626571f58ccdc732.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/4299be57d79d62ad6bdc597ce50480f9.png)![å‚è§æ ‡é¢˜](img/00d76d08bdd817b3626571f58ccdc732.png)'
- en: 'Figure 1: The visualization of the attention optimization (see DefinitionÂ [1.2](#S1.Thmtheorem2
    "Definition 1.2 (Attention optimization). â€£ 1 Introduction â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")). Let $A_{1},A_{2},A_{3},B\in\mathbb{R}^{n\times
    d}$. We first get $\exp(A_{1}XA_{2}^{\top})\in\mathbb{R}^{n\times n}$, $X$. Then,
    we have $D(X)\in\mathbb{R}^{n\times n}$. After that, we multiply $D(X)^{-1}$,
    $A_{3}$ and subtract $B$ matrices, the purple rectangle represents the $n$ matrices,
    and the green squares represent the $n\times n$ diagonal matrices.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 1ï¼šæ³¨æ„åŠ›ä¼˜åŒ–çš„å¯è§†åŒ–ï¼ˆè§å®šä¹‰ [1.2](#S1.Thmtheorem2 "å®šä¹‰ 1.2ï¼ˆæ³¨æ„åŠ›ä¼˜åŒ–ï¼‰ã€‚ â€£ 1 å¼•è¨€ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§çš„å•å±‚æ³¨æ„åŠ›é‡æ„åŠçŸ©é˜µä¹˜æ³•æ—¶é—´æ±‚è§£"ï¼‰ï¼‰ã€‚è®¾
    $A_{1},A_{2},A_{3},B\in\mathbb{R}^{n\times d}$ã€‚æˆ‘ä»¬é¦–å…ˆå¾—åˆ° $\exp(A_{1}XA_{2}^{\top})\in\mathbb{R}^{n\times
    n}$ï¼Œ$X$ã€‚ç„¶åï¼Œæˆ‘ä»¬å¾—åˆ° $D(X)\in\mathbb{R}^{n\times n}$ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å¯¹ $D(X)^{-1}$ã€$A_{3}$ è¿›è¡Œä¹˜æ³•æ“ä½œï¼Œå¹¶å‡å»
    $B$ çŸ©é˜µï¼Œç´«è‰²çŸ©å½¢è¡¨ç¤º $n$ çŸ©é˜µï¼Œç»¿è‰²æ–¹å—è¡¨ç¤º $n\times n$ å¯¹è§’çŸ©é˜µã€‚
- en: '![Refer to caption](img/e77a6fab7b9b8d8aa84a3ca6114da1a5.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/e77a6fab7b9b8d8aa84a3ca6114da1a5.png)'
- en: 'Figure 2: The visualization of a variation of DefinitionÂ [1.2](#S1.Thmtheorem2
    "Definition 1.2 (Attention optimization). â€£ 1 Introduction â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"). Let $A_{1},A_{2},A_{3},B\in\mathbb{R}^{n\times
    d}$, $D(X)\in\mathbb{R}^{n\times n}$. $\mathrm{mat}:\mathbb{R}^{n^{2}}\to\mathbb{R}^{n\times
    n}$, and $\operatorname{vec}=\mathrm{mat}^{-1}$ and multiply $\operatorname{\mathsf{A}}$.
    Then, we multiply $(D(X)\otimes I_{n})^{-1}\in\mathbb{R}^{n^{2}\times n^{2}}$,
    which gives us a vector in $\mathbb{R}^{n^{2}}$ to transform that into a matrix
    in $\mathbb{R}^{n\times n}$. Finally, we compute the minimum of the Frobenius
    norm of $\mathrm{mat}((D(X)\otimes I_{n})^{-1}\cdot\exp(\operatorname{\mathsf{A}}\operatorname{vec}(X)))A_{3}Y-B$:
    in the matrix $D(X)\otimes I_{n}$. The red rectangle represents the matrix in
    $\mathbb{R}^{d\times d}$.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 2ï¼šå®šä¹‰Â [1.2](#S1.Thmtheorem2 "Definition 1.2 (Attention optimization). â€£ 1
    Introduction â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") çš„å˜ä½“çš„å¯è§†åŒ–ã€‚ä»¤ $A_{1},A_{2},A_{3},B\in\mathbb{R}^{n\times d}$ï¼Œ$D(X)\in\mathbb{R}^{n\times
    n}$ã€‚$\mathrm{mat}:\mathbb{R}^{n^{2}}\to\mathbb{R}^{n\times n}$ï¼Œ$\operatorname{vec}=\mathrm{mat}^{-1}$
    å¹¶ä¹˜ä»¥ $\operatorname{\mathsf{A}}$ã€‚ç„¶åï¼Œæˆ‘ä»¬ä¹˜ä»¥ $(D(X)\otimes I_{n})^{-1}\in\mathbb{R}^{n^{2}\times
    n^{2}}$ï¼Œè¿™ç»™æˆ‘ä»¬ä¸€ä¸ªå‘é‡ $\mathbb{R}^{n^{2}}$ï¼Œå°†å…¶è½¬æ¢ä¸ºçŸ©é˜µ $\mathbb{R}^{n\times n}$ã€‚æœ€åï¼Œæˆ‘ä»¬è®¡ç®—
    $\mathrm{mat}((D(X)\otimes I_{n})^{-1}\cdot\exp(\operatorname{\mathsf{A}}\operatorname{vec}(X)))A_{3}Y-B$
    åœ¨çŸ©é˜µ $D(X)\otimes I_{n}$ ä¸­çš„å¼—ç½—è´çº½æ–¯èŒƒæ•°çš„æœ€å°å€¼ã€‚çº¢è‰²çŸ©å½¢è¡¨ç¤º $\mathbb{R}^{d\times d}$ ä¸­çš„çŸ©é˜µã€‚'
- en: 'Here $X=QK^{\top},Y=V$ are the input of a layer $X_{\ell}$ are the output layer
    $X_{\ell+1}$. Attention computation has been analyzed in many recent works [[203](#bib.bib203),
    [11](#bib.bib11), [29](#bib.bib29), [75](#bib.bib75), [57](#bib.bib57), [171](#bib.bib171),
    [175](#bib.bib175), [139](#bib.bib139), [125](#bib.bib125), [208](#bib.bib208),
    [140](#bib.bib140), [159](#bib.bib159)], but none of them give a complete analysis
    of the full version of the attention computation problem. They all simplify this
    problem by different strategies (see details in TableÂ [1](#S4.T1 "Table 1 â€£ 4
    Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¿™é‡Œ $X=QK^{\top},Y=V$ æ˜¯å±‚çš„è¾“å…¥ï¼Œ$X_{\ell}$ æ˜¯è¾“å‡ºå±‚ï¼Œ$X_{\ell+1}$ã€‚æ³¨æ„åŠ›è®¡ç®—åœ¨è®¸å¤šè¿‘æœŸçš„å·¥ä½œä¸­è¿›è¡Œäº†åˆ†æ
    [[203](#bib.bib203), [11](#bib.bib11), [29](#bib.bib29), [75](#bib.bib75), [57](#bib.bib57),
    [171](#bib.bib171), [175](#bib.bib175), [139](#bib.bib139), [125](#bib.bib125),
    [208](#bib.bib208), [140](#bib.bib140), [159](#bib.bib159)]ï¼Œä½†å…¶ä¸­æ²¡æœ‰ä¸€ä¸ªç»™å‡ºå®Œæ•´çš„æ³¨æ„åŠ›è®¡ç®—é—®é¢˜çš„åˆ†æã€‚å®ƒä»¬éƒ½é€šè¿‡ä¸åŒçš„ç­–ç•¥ç®€åŒ–äº†è¿™ä¸ªé—®é¢˜ï¼ˆè¯¦ç»†ä¿¡æ¯è§è¡¨Â [1](#S4.T1
    "Table 1 â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")ï¼‰ã€‚'
- en: However, simplifying this problem may lead to a significant decrease in the
    model performance, which may require extra model training or fine-tuning. This
    results in deployment obstacles.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œç®€åŒ–è¿™ä¸ªé—®é¢˜å¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œè¿™å¯èƒ½éœ€è¦é¢å¤–çš„æ¨¡å‹è®­ç»ƒæˆ–å¾®è°ƒã€‚è¿™å¯¼è‡´äº†éƒ¨ç½²éšœç¢ã€‚
- en: 'In this paper, our focus is on optimizing the attention mechanism. Our goal
    is to present a complete, un-simplified analysis of the attention problem defined
    in DefinitionÂ [1.2](#S1.Thmtheorem2 "Definition 1.2 (Attention optimization).
    â€£ 1 Introduction â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), a task that, to the best of our knowledge, has not been done before. We
    provide a provable guarantee for optimizing the attention function in the case
    of a single-layer attention network. Our motivation stems from the critical role
    of the attention optimization problem in the functionality of LLMs, and we firmly
    believe that our theoretical analysis will significantly influence the development
    of LLMs.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 'æœ¬æ–‡çš„é‡ç‚¹æ˜¯ä¼˜åŒ–æ³¨æ„åŠ›æœºåˆ¶ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¯¹å®šä¹‰åœ¨å®šä¹‰Â [1.2](#S1.Thmtheorem2 "Definition 1.2 (Attention
    optimization). â€£ 1 Introduction â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time") ä¸­çš„æ³¨æ„åŠ›é—®é¢˜è¿›è¡Œå®Œæ•´ã€æœªç®€åŒ–çš„åˆ†æï¼Œè¿™é¡¹ä»»åŠ¡åœ¨æˆ‘ä»¬çœ‹æ¥å°šæœªå®Œæˆã€‚æˆ‘ä»¬ä¸ºå•å±‚æ³¨æ„åŠ›ç½‘ç»œä¸­çš„æ³¨æ„åŠ›å‡½æ•°ä¼˜åŒ–æä¾›äº†å¯è¯æ˜çš„ä¿è¯ã€‚æˆ‘ä»¬çš„åŠ¨æœºæ¥æºäºæ³¨æ„åŠ›ä¼˜åŒ–é—®é¢˜åœ¨
    LLM åŠŸèƒ½ä¸­çš„å…³é”®ä½œç”¨ï¼Œæˆ‘ä»¬åšä¿¡æˆ‘ä»¬çš„ç†è®ºåˆ†æå°†å¯¹ LLM çš„å‘å±•äº§ç”Ÿé‡å¤§å½±å“ã€‚'
- en: As [[11](#bib.bib11)], they show that one step forward computation of attention
    can be done in $o(n^{2})$ matrix. However, it is still an open problem about how
    fast we optimize the loss function via the iterative method.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ [[11](#bib.bib11)] æ‰€ç¤ºï¼Œä»–ä»¬è¡¨æ˜ï¼Œæ³¨æ„åŠ›çš„å•æ­¥å‰å‘è®¡ç®—å¯ä»¥åœ¨ $o(n^{2})$ çŸ©é˜µä¸­å®Œæˆã€‚ç„¶è€Œï¼Œå¦‚ä½•é€šè¿‡è¿­ä»£æ–¹æ³•ä¼˜åŒ–æŸå¤±å‡½æ•°ä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚
- en: 'How fast can we optimize the training process of attention matrix (See DefinitionÂ [1.2](#S1.Thmtheorem2
    "Definition 1.2 (Attention optimization). â€£ 1 Introduction â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"))?'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 'æˆ‘ä»¬èƒ½å¤šå¿«ä¼˜åŒ–æ³¨æ„åŠ›çŸ©é˜µçš„è®­ç»ƒè¿‡ç¨‹ï¼ˆå‚è§å®šä¹‰Â [1.2](#S1.Thmtheorem2 "Definition 1.2 (Attention optimization).
    â€£ 1 Introduction â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")ï¼‰ï¼Ÿ'
- en: In this study, we make progress towards this fundamental question.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬åœ¨è¿™ä¸ªåŸºæœ¬é—®é¢˜ä¸Šå–å¾—äº†è¿›å±•ã€‚
- en: To establish the correctness of our algorithm, we conduct a comprehensive analysis
    of the positive semi-definite (PSD) property and the Lipschitz continuity of the
    Hessian matrix constructed from the attention matrix. These two properties provide
    the necessary assurance for employing TensorSRHT and Newtonâ€™s method, ensuring
    both fast computation and convergence, respectively.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†éªŒè¯æˆ‘ä»¬ç®—æ³•çš„æ­£ç¡®æ€§ï¼Œæˆ‘ä»¬å¯¹ä»æ³¨æ„åŠ›çŸ©é˜µæ„é€ çš„æµ·æ£®çŸ©é˜µçš„æ­£åŠå®šï¼ˆPSDï¼‰æ€§è´¨å’Œ Lipschitz è¿ç»­æ€§è¿›è¡Œäº†å…¨é¢åˆ†æã€‚è¿™ä¸¤ä¸ªæ€§è´¨ä¸ºä½¿ç”¨ TensorSRHT
    å’Œç‰›é¡¿æ³•æä¾›äº†å¿…è¦çš„ä¿è¯ï¼Œç¡®ä¿äº†å¿«é€Ÿè®¡ç®—å’Œæ”¶æ•›ã€‚
- en: Now, we will present our main result as follows.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å°†å±•ç¤ºæˆ‘ä»¬çš„ä¸»è¦ç»“æœå¦‚ä¸‹ã€‚
- en: Theorem 1.3  (Informal version of our main theorem).
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å®šç† 1.3ï¼ˆæˆ‘ä»¬ä¸»è¦å®šç†çš„éæ­£å¼ç‰ˆæœ¬ï¼‰ã€‚
- en: Let $A_{1},A_{2},A_{2}\in\mathbb{R}^{n\times d}$ solves to the attention problem
    up to $\epsilon$. Here $\omega\approx 2.37$.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ $A_{1},A_{2},A_{2}\in\mathbb{R}^{n\times d}$ è§£å†³æ³¨æ„åŠ›é—®é¢˜è‡³ $\epsilon$ã€‚è¿™é‡Œ $\omega\approx
    2.37$ã€‚
- en: 'Here $\omega$ denotes the time of multiplying an $a\times b$ size matrix, and
    ${\cal T}_{\mathrm{mat}}(n,n,n)=n^{\omega}$. See more details of matrix multiplication
    notation in SectionÂ [4.7](#S4.SS7 "4.7 Fast Matrix Multiplication â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¿™é‡Œ $\omega$ è¡¨ç¤ºä¹˜æ³•ä¸€ä¸ª $a\times b$ å¤§å°çŸ©é˜µçš„æ—¶é—´ï¼Œè€Œ ${\cal T}_{\mathrm{mat}}(n,n,n)=n^{\omega}$ã€‚æœ‰å…³çŸ©é˜µä¹˜æ³•ç¬¦å·çš„æ›´å¤šç»†èŠ‚è¯·å‚è§ç¬¬
    [4.7](#S4.SS7 "4.7 Fast Matrix Multiplication â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") èŠ‚ã€‚'
- en: Relationship with the Softmax Regression Problem
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ä¸ Softmax å›å½’é—®é¢˜çš„å…³ç³»
- en: 'Moreover, the attention weight can be viewed as the output of a softmax regression
    model, which is defined as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæ³¨æ„åŠ›æƒé‡å¯ä»¥è§†ä¸º softmax å›å½’æ¨¡å‹çš„è¾“å‡ºï¼Œå®šä¹‰å¦‚ä¸‹ï¼š
- en: Definition 1.4  (Single softmax regression [[55](#bib.bib55)] and multiple softmax
    regression [[72](#bib.bib72)]).
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å®šä¹‰ 1.4ï¼ˆå•ä¸€ softmax å›å½’ [[55](#bib.bib55)] å’Œå¤šé‡ softmax å›å½’ [[72](#bib.bib72)]ï¼‰ã€‚
- en: Given a matrix $A\in\mathbb{R}^{n\times d}$, the single softmax regression problem
    is defined as
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šä¸€ä¸ªçŸ©é˜µ $A\in\mathbb{R}^{n\times d}$ï¼Œå•ä¸€ softmax å›å½’é—®é¢˜å®šä¹‰ä¸º
- en: '|  | $1$2 |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Let $D(X)\in\mathbb{R}^{n\times n}$ and $X\in\mathbb{R}^{d\times d}$
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ $D(X)\in\mathbb{R}^{n\times n}$ å’Œ $X\in\mathbb{R}^{d\times d}$
- en: '|  | $1$2 |  |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 'On the one hand, due to the observation in [[72](#bib.bib72), [73](#bib.bib73)],
    the equation in Part 1 of DefinitionÂ [1.4](#S1.Thmtheorem4 "Definition 1.4 (Single
    softmax regression [55] and multiple softmax regression [72]). â€£ Relationship
    with the Softmax Regression Problem â€£ 1 Introduction â€£ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time") can be viewed as one row of the equation
    in Part 2 of DefinitionÂ [1.4](#S1.Thmtheorem4 "Definition 1.4 (Single softmax
    regression [55] and multiple softmax regression [72]). â€£ Relationship with the
    Softmax Regression Problem â€£ 1 Introduction â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¸€æ–¹é¢ï¼Œç”±äºåœ¨ [[72](#bib.bib72), [73](#bib.bib73)] çš„è§‚å¯Ÿï¼Œå®šä¹‰Â [1.4](#S1.Thmtheorem4 "Definition
    1.4 (Single softmax regression [55] and multiple softmax regression [72]). â€£ Relationship
    with the Softmax Regression Problem â€£ 1 Introduction â€£ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time") ç¬¬1éƒ¨åˆ†çš„æ–¹ç¨‹å¯ä»¥è¢«è§†ä¸ºå®šä¹‰Â [1.4](#S1.Thmtheorem4
    "Definition 1.4 (Single softmax regression [55] and multiple softmax regression
    [72]). â€£ Relationship with the Softmax Regression Problem â€£ 1 Introduction â€£ A
    Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time") ç¬¬2éƒ¨åˆ†æ–¹ç¨‹çš„ä¸€è¡Œã€‚'
- en: 'On the other hand, due to the well-known tensor trickÂ¹Â¹1Given matrices $A_{1},A_{2}\in\mathbb{R}^{n\times
    d}$, the well-known tensor-trick suggests that $\operatorname{vec}(A_{1}XA_{2}^{\top})=(A_{1}\otimes
    A_{2})\operatorname{vec}(X)\in\mathbb{R}^{n^{2}}$. (see [[58](#bib.bib58), [52](#bib.bib52)]
    as an example), the Part 2 equation DefinitionÂ [1.4](#S1.Thmtheorem4 "Definition
    1.4 (Single softmax regression [55] and multiple softmax regression [72]). â€£ Relationship
    with the Softmax Regression Problem â€£ 1 Introduction â€£ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time") is equivalent to'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢ï¼Œç”±äºè‘—åçš„å¼ é‡æŠ€å·§Â¹Â¹1 ç»™å®šçŸ©é˜µ $A_{1},A_{2}\in\mathbb{R}^{n\times d}$ï¼Œè‘—åçš„å¼ é‡æŠ€å·§å»ºè®® $\operatorname{vec}(A_{1}XA_{2}^{\top})=(A_{1}\otimes
    A_{2})\operatorname{vec}(X)\in\mathbb{R}^{n^{2}}$ã€‚ï¼ˆå‚è§ [[58](#bib.bib58), [52](#bib.bib52)]
    ä½œä¸ºä¾‹å­ï¼‰ï¼Œç¬¬äºŒéƒ¨åˆ†æ–¹ç¨‹å®šä¹‰Â [1.4](#S1.Thmtheorem4 "å®šä¹‰ 1.4ï¼ˆå•ä¸€ softmax å›å½’ [55] å’Œå¤šé‡ softmax å›å½’
    [72]ï¼‰ã€‚ â€£ ä¸ softmax å›å½’é—®é¢˜çš„å…³ç³» â€£ 1 å¼•è¨€ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿°å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")
    ç›¸å½“äº
- en: '|  | $1$2 |  | (1) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: 'which can be a slightly more complicated version of the Part 1 equation in
    DefinitionÂ [1.4](#S1.Thmtheorem4 "Definition 1.4 (Single softmax regression [55]
    and multiple softmax regression [72]). â€£ Relationship with the Softmax Regression
    Problem â€£ 1 Introduction â€£ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"). In particular, instead of one re-scaling factor, we will have $n$ into
    $n$. For each chunk, we use the same rescaling factor.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯ä»¥æ˜¯å®šä¹‰Â [1.4](#S1.Thmtheorem4 "å®šä¹‰ 1.4ï¼ˆå•ä¸€ softmax å›å½’ [55] å’Œå¤šé‡ softmax å›å½’ [72]ï¼‰ã€‚
    â€£ ä¸ softmax å›å½’é—®é¢˜çš„å…³ç³» â€£ 1 å¼•è¨€ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿°å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³") ä¸­ç¬¬ä¸€éƒ¨åˆ†æ–¹ç¨‹çš„ç¨å¾®å¤æ‚çš„ç‰ˆæœ¬ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å°†ä»ä¸€ä¸ªé‡æ–°ç¼©æ”¾å› å­æ”¹ä¸º
    $n$ã€‚å¯¹äºæ¯ä¸ªå—ï¼Œæˆ‘ä»¬ä½¿ç”¨ç›¸åŒçš„é‡æ–°ç¼©æ”¾å› å­ã€‚
- en: '![Refer to caption](img/bfca81b3da52836ce3d9be7b8fb0c63c.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/bfca81b3da52836ce3d9be7b8fb0c63c.png)'
- en: 'Figure 3: The visualization of Eq.Â ([1](#S1.E1 "In Relationship with the Softmax
    Regression Problem â€£ 1 Introduction â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")). Let $A_{1},A_{2}\in\mathbb{R}^{n\times d}$, $C,D(X)\in\mathbb{R}^{n\times
    n}$. We first get that $(D(X)\otimes I_{n})^{-1}\in\mathbb{R}^{n^{2}\times n^{2}}$
    with $\operatorname{vec}(X)$ with $\operatorname{\mathsf{A}}\cdot\operatorname{vec}(X)\in\mathbb{R}^{n^{2}}$
    and subtract it from $(D(X)\otimes I_{n})^{-1}\exp(\operatorname{\mathsf{A}}\cdot\operatorname{vec}(X))$
    norm of $(D(X)\otimes I_{n})^{-1}\exp(\operatorname{\mathsf{A}}\cdot\operatorname{vec}(X))-c$:
    in the matrix $D(X)\otimes I_{n}$.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3ï¼šæ–¹ç¨‹ ([1](#S1.E1 "ä¸ softmax å›å½’é—®é¢˜çš„å…³ç³» â€£ 1 å¼•è¨€ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿°å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³"))
    çš„å¯è§†åŒ–ã€‚ä»¤ $A_{1},A_{2}\in\mathbb{R}^{n\times d}$ï¼Œ$C,D(X)\in\mathbb{R}^{n\times n}$ã€‚æˆ‘ä»¬é¦–å…ˆå¾—åˆ°
    $(D(X)\otimes I_{n})^{-1}\in\mathbb{R}^{n^{2}\times n^{2}}$ å’Œ $\operatorname{vec}(X)$
    ä¸ $\operatorname{\mathsf{A}}\cdot\operatorname{vec}(X)\in\mathbb{R}^{n^{2}}$ çš„è¿ç®—ï¼Œç„¶åä»
    $(D(X)\otimes I_{n})^{-1}\exp(\operatorname{\mathsf{A}}\cdot\operatorname{vec}(X))$
    ä¸­å‡å» $(D(X)\otimes I_{n})^{-1}\exp(\operatorname{\mathsf{A}}\cdot\operatorname{vec}(X))-c$
    çš„èŒƒæ•°ï¼šåœ¨çŸ©é˜µ $D(X)\otimes I_{n}$ ä¸­ã€‚
- en: 'Note that the multiple softmax regression problem is a simplified version of
    what we study in DefinitionÂ [1.2](#S1.Thmtheorem2 "Definition 1.2 (Attention optimization).
    â€£ 1 Introduction â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"). We believe that our work can also support the study of softmax regression.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå¤šé‡ softmax å›å½’é—®é¢˜æ˜¯æˆ‘ä»¬åœ¨å®šä¹‰Â [1.2](#S1.Thmtheorem2 "å®šä¹‰ 1.2ï¼ˆæ³¨æ„åŠ›ä¼˜åŒ–ï¼‰ã€‚ â€£ 1 å¼•è¨€ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ
    SVM æŠ€å·§é‡æ–°è¡¨è¿°å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³") ä¸­ç ”ç©¶çš„ç®€åŒ–ç‰ˆæœ¬ã€‚æˆ‘ä»¬ç›¸ä¿¡æˆ‘ä»¬çš„å·¥ä½œä¹Ÿå¯ä»¥æ”¯æŒ softmax å›å½’çš„ç ”ç©¶ã€‚
- en: Relatinship with Support Vector Machines (SVM)
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ä¸æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰çš„å…³ç³»
- en: The usual SVM [[91](#bib.bib91), [37](#bib.bib37), [77](#bib.bib77), [175](#bib.bib175)]
    objective function in optimization can be viewed as a product of a summation of
    a batch of inner product. Inspired by that, we can define $n$ for each $j_{0}\in[n]$
    functions $h(Y)_{i_{0}}\in\mathbb{R}^{n}$ is the vectorization of $X$ is the vectorization
    of $Y$ can be turned into
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸çš„ SVM [[91](#bib.bib91), [37](#bib.bib37), [77](#bib.bib77), [175](#bib.bib175)]
    ä¼˜åŒ–ä¸­çš„ç›®æ ‡å‡½æ•°å¯ä»¥çœ‹ä½œæ˜¯è‹¥å¹²å†…ç§¯çš„æ±‚å’Œçš„ä¹˜ç§¯ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºæ¯ä¸ª $j_{0}\in[n]$ å®šä¹‰ $n$ ä¸ªå‡½æ•° $h(Y)_{i_{0}}\in\mathbb{R}^{n}$ï¼Œå…¶ä¸­
    $X$ çš„å‘é‡åŒ–å¯ä»¥è½¬æ¢ä¸º $Y$ çš„å‘é‡åŒ–ã€‚
- en: '|  | $1$2 |  | (2) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: where $b_{j_{0},i_{0}}$. We call this formulation SVM-inspired formulation.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $b_{j_{0},i_{0}}$ã€‚æˆ‘ä»¬ç§°è¿™ç§å½¢å¼ä¸º SVM çµæ„Ÿçš„å…¬å¼ã€‚
- en: '![Refer to caption](img/d73e25196093f8ef6e4aad48d830bd59.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/d73e25196093f8ef6e4aad48d830bd59.png)'
- en: 'Figure 4: The visualization of Eq.Â ([2](#S1.E2 "In Relatinship with Support
    Vector Machines (SVM) â€£ 1 Introduction â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")). Let $A_{1},A_{2},A_{3},B\in\mathbb{R}^{n\times
    d}$. We have $\operatorname{\mathsf{A}}=A_{1}\otimes A_{2}\in\mathbb{R}^{n^{2}\times
    d^{2}}$ is the $j_{0}$. $x=\operatorname{vec}(X)\in\mathbb{R}^{d^{2}}$ (see DefinitionÂ [4.10](#S4.Thmtheorem10
    "Definition 4.10\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")) and $h(Y)_{i_{0}}\in\mathbb{R}^{n}$
    at $j_{0}$-column from the inner produce. Finally, we compute the square of this
    difference and add all of them from $i_{0}=1$ and from $j_{0}=1$. In this figure,
    we use blue rectangles to represent vectors, where the dark blue represents $f(x)_{j_{0}}$,
    and the light blue represents the terms used to compute $f(x)_{j_{0}}$. The green
    square represents the scalar. The red rectangle represents the matrix.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4ï¼šæ–¹ç¨‹çš„å¯è§†åŒ–ï¼ˆ[2](#S1.E2 "ä¸æ”¯æŒå‘é‡æœº (SVM) çš„å…³ç³» â€£ 1 å¼•è¨€ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿°å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´ä¸­æ±‚è§£"ï¼‰ã€‚è®¾
    $A_{1},A_{2},A_{3},B\in\mathbb{R}^{n\times d}$ã€‚æˆ‘ä»¬æœ‰ $\operatorname{\mathsf{A}}=A_{1}\otimes
    A_{2}\in\mathbb{R}^{n^{2}\times d^{2}}$ æ˜¯ $j_{0}$ã€‚$x=\operatorname{vec}(X)\in\mathbb{R}^{d^{2}}$ï¼ˆè§å®šä¹‰Â [4.10](#S4.Thmtheorem10
    "å®šä¹‰ 4.10. â€£ 4.3 ä¸ ğ‘‹ ç›¸å…³çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿°å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´ä¸­æ±‚è§£"ï¼‰ï¼‰å’Œ
    $h(Y)_{i_{0}}\in\mathbb{R}^{n}$ åœ¨ $j_{0}$ åˆ—æ¥è‡ªå†…ç§¯ã€‚æœ€åï¼Œæˆ‘ä»¬è®¡ç®—è¿™ä¸ªå·®å¼‚çš„å¹³æ–¹ï¼Œå¹¶å°† $i_{0}=1$ å’Œ
    $j_{0}=1$ çš„æ‰€æœ‰ç»“æœç›¸åŠ ã€‚åœ¨æ­¤å›¾ä¸­ï¼Œæˆ‘ä»¬ç”¨è“è‰²çŸ©å½¢è¡¨ç¤ºå‘é‡ï¼Œå…¶ä¸­æ·±è“è‰²è¡¨ç¤º $f(x)_{j_{0}}$ï¼Œæµ…è“è‰²è¡¨ç¤ºç”¨äºè®¡ç®— $f(x)_{j_{0}}$
    çš„é¡¹ã€‚ç»¿è‰²æ–¹å—è¡¨ç¤ºæ ‡é‡ã€‚çº¢è‰²çŸ©å½¢è¡¨ç¤ºçŸ©é˜µã€‚
- en: Roadmap
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è·¯çº¿å›¾
- en: 'In SectionÂ [2](#S2 "2 Related Work â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), we introduce related research work. In SectionÂ [3](#S3
    "3 Technique Overview â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), we provide an overview of the techniques we will use throughout the rest
    of the paper. In SectionÂ [4](#S4 "4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), we present the basic notations we use, some mathematical
    facts, and helpful definitions that support the following proof. In SectionÂ [5](#S5
    "5 Gradient â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    we compute the gradients of the helpful functions defined earlier. In SectionÂ [6](#S6
    "6 Hessian â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    we define the Hessian for further discussion. In SectionÂ [7](#S7 "7 Hessian for
    ğ‘‹ â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), we compute
    the Hessian matrix with respect to $X$ is Lipschitz. In SectionÂ [9](#S9 "9 Hessian
    for ğ‘‹ Is PSD â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), we show that the Hessian matrix with respect to $X$ and show that it is
    Lipschitz and positive semidefinite (PSD). In SectionÂ [11](#S11 "11 Hessian for
    ğ‘‹ and ğ‘Œ â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    we compute the Hessian matrix with respect to both $X$. In SectionÂ [12](#S12 "12
    Lipschitz for Hessian of ğ‘¥,ğ‘¦ â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), we demonstrate that the Hessian matrix with respect to
    both $X$ is Lipschitz. In SectionÂ [13](#S13 "13 Generating a Spectral Sparsifier
    via TensorSketch â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), we introduce some tensor sketch techniques to obtain fast approximations
    of the Hessian. In SectionÂ [14](#S14 "14 Analysis Of Algorithm 1 â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), we introduce the Newton step.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬[2](#S2 "2 ç›¸å…³å·¥ä½œ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§é‡æ„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ç›¸å…³çš„ç ”ç©¶å·¥ä½œã€‚åœ¨ç¬¬[3](#S3
    "3 æŠ€æœ¯æ¦‚è¿° â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§é‡æ„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")èŠ‚ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†æˆ‘ä»¬å°†åœ¨è®ºæ–‡å…¶ä½™éƒ¨åˆ†ä¸­ä½¿ç”¨çš„æŠ€æœ¯æ¦‚è¿°ã€‚åœ¨ç¬¬[4](#S4
    "4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§é‡æ„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†æˆ‘ä»¬ä½¿ç”¨çš„åŸºæœ¬ç¬¦å·ï¼Œä¸€äº›æ•°å­¦äº‹å®ï¼Œä»¥åŠæ”¯æŒåç»­è¯æ˜çš„æœ‰ç”¨å®šä¹‰ã€‚åœ¨ç¬¬[5](#S5
    "5 æ¢¯åº¦ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§é‡æ„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—äº†ä¹‹å‰å®šä¹‰çš„æœ‰ç”¨å‡½æ•°çš„æ¢¯åº¦ã€‚åœ¨ç¬¬[6](#S6
    "6 Hessian â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§é‡æ„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")èŠ‚ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†HessiançŸ©é˜µä»¥ä¾›è¿›ä¸€æ­¥è®¨è®ºã€‚åœ¨ç¬¬[7](#S7
    "7 å…³äº $X$ çš„ Hessian â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§é‡æ„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—äº†å…³äº$X$çš„HessiançŸ©é˜µï¼Œä¸”å…¶ä¸ºLipschitzã€‚
    åœ¨ç¬¬[9](#S9 "9 å…³äº $X$ çš„Hessianä¸ºPSD â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§é‡æ„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")èŠ‚ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å…³äº$X$çš„HessiançŸ©é˜µï¼Œå¹¶è¯æ˜å…¶ä¸ºLipschitzä¸”ä¸ºæ­£åŠå®šï¼ˆPSDï¼‰ã€‚åœ¨ç¬¬[11](#S11
    "11 å…³äº $X$ å’Œ $Y$ çš„Hessian â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§é‡æ„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—äº†å…³äº$X$å’Œ$Y$çš„HessiançŸ©é˜µã€‚åœ¨ç¬¬[12](#S12
    "12 å…³äº $x, y$ çš„Hessian Lipschitz â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§é‡æ„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")èŠ‚ä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†å…³äº$X$å’Œ$Y$çš„HessiançŸ©é˜µä¸ºLipschitzã€‚åœ¨ç¬¬[13](#S13
    "13 é€šè¿‡TensorSketchç”Ÿæˆå…‰è°±ç¨€ç–åŒ–å™¨ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§é‡æ„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€äº›å¼ é‡è‰å›¾æŠ€æœ¯ä»¥è·å¾—Hessiançš„å¿«é€Ÿè¿‘ä¼¼ã€‚åœ¨ç¬¬[14](#S14
    "14 ç®—æ³• 1 çš„åˆ†æ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§é‡æ„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ç‰›é¡¿æ­¥éª¤ã€‚
- en: 2 Related Work
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 ç›¸å…³å·¥ä½œ
- en: Attention
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›
- en: '[[18](#bib.bib18)] represents one of the earliest works that employed attention
    in NLP. They assumed that a fixed-length vector could enhance the performance
    of the encoder-decoder design by incorporating an attention mechanism. This mechanism
    allows the decoder to focus on relevant words in the source sentence while generating
    translations. Consequently, this approach significantly improves the performance
    of machine translation models compared to those without an attention mechanism.
    Subsequently, [[113](#bib.bib113)] explained two variants of attention: local
    attention, which considers a subset of source words at a time, and global attention,
    which attends to all source words.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[[18](#bib.bib18)] ä»£è¡¨äº†æœ€æ—©é‡‡ç”¨æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†çš„ç ”ç©¶ä¹‹ä¸€ã€‚ä»–ä»¬å‡è®¾ä¸€ä¸ªå›ºå®šé•¿åº¦çš„å‘é‡é€šè¿‡å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥æé«˜ç¼–ç å™¨-è§£ç å™¨è®¾è®¡çš„æ€§èƒ½ã€‚è¯¥æœºåˆ¶å…è®¸è§£ç å™¨åœ¨ç”Ÿæˆç¿»è¯‘æ—¶ä¸“æ³¨äºæºå¥å­ä¸­çš„ç›¸å…³è¯è¯­ã€‚å› æ­¤ï¼Œä¸æ²¡æœ‰æ³¨æ„åŠ›æœºåˆ¶çš„æ¨¡å‹ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•æ˜¾è‘—æé«˜äº†æœºå™¨ç¿»è¯‘æ¨¡å‹çš„æ€§èƒ½ã€‚éšåï¼Œ[[113](#bib.bib113)]
    è§£é‡Šäº†æ³¨æ„åŠ›çš„ä¸¤ç§å˜ä½“ï¼šå±€éƒ¨æ³¨æ„åŠ›ï¼Œå®ƒä¸€æ¬¡è€ƒè™‘æºè¯è¯­çš„ä¸€ä¸ªå­é›†ï¼Œä»¥åŠå…¨å±€æ³¨æ„åŠ›ï¼Œå®ƒå…³æ³¨æ‰€æœ‰æºè¯è¯­ã€‚'
- en: Attention finds extensive applications across various domains. In image captioning,
    [[192](#bib.bib192)] utilizes attention matrices to align specific parts of an
    image with words in a caption. In the context of the Transformer model [[179](#bib.bib179)],
    attention matrices capture differences between words in a sentence. In the realm
    of graph neural networks, [[177](#bib.bib177)] investigates these neural network
    architectures designed for graph-structured data, computing attention matrices
    between each node and its neighbors.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›æœºåˆ¶åœ¨å„ä¸ªé¢†åŸŸä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚åœ¨å›¾åƒæè¿°ä¸­ï¼Œ[[192](#bib.bib192)] åˆ©ç”¨æ³¨æ„åŠ›çŸ©é˜µå°†å›¾åƒçš„ç‰¹å®šéƒ¨åˆ†ä¸æè¿°ä¸­çš„è¯è¯­å¯¹é½ã€‚åœ¨ Transformer
    æ¨¡å‹çš„èƒŒæ™¯ä¸‹ï¼Œ[[179](#bib.bib179)] æ³¨æ„åŠ›çŸ©é˜µæ•æ‰äº†å¥å­ä¸­è¯è¯­ä¹‹é—´çš„å·®å¼‚ã€‚åœ¨å›¾ç¥ç»ç½‘ç»œé¢†åŸŸï¼Œ[[177](#bib.bib177)] ç ”ç©¶äº†è¿™äº›ä¸ºå›¾ç»“æ„æ•°æ®è®¾è®¡çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œè®¡ç®—äº†æ¯ä¸ªèŠ‚ç‚¹ä¸å…¶é‚»å±…ä¹‹é—´çš„æ³¨æ„åŠ›çŸ©é˜µã€‚
- en: On the theoretical side, after the emergence of LLMs, there has been a substantial
    body of work dedicated to studying attention computation [[57](#bib.bib57), [11](#bib.bib11),
    [203](#bib.bib203), [40](#bib.bib40), [118](#bib.bib118), [29](#bib.bib29), [99](#bib.bib99)].
    Notably, recent research by [[203](#bib.bib203), [40](#bib.bib40), [99](#bib.bib99)]
    employs Locality Sensitive Hashing (LSH) techniques to approximate attention mechanisms.
    In particular, [[203](#bib.bib203)] introduces $\mathsf{KDEformer}$, $\sinh$.
    Lastly, [[57](#bib.bib57)] proposes randomized and deterministic algorithms for
    reducing the dimensionality of attention matrices in LLMs, achieving high accuracy
    while significantly reducing feature dimensions.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç†è®ºæ–¹é¢ï¼Œéšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‡ºç°ï¼Œå·²ç»æœ‰å¤§é‡çš„å·¥ä½œè‡´åŠ›äºç ”ç©¶æ³¨æ„åŠ›è®¡ç®—[[57](#bib.bib57), [11](#bib.bib11),
    [203](#bib.bib203), [40](#bib.bib40), [118](#bib.bib118), [29](#bib.bib29), [99](#bib.bib99)]ã€‚ç‰¹åˆ«æ˜¯ï¼Œ[[203](#bib.bib203),
    [40](#bib.bib40), [99](#bib.bib99)] é‡‡ç”¨å±€éƒ¨æ•æ„Ÿå“ˆå¸Œï¼ˆLSHï¼‰æŠ€æœ¯æ¥è¿‘ä¼¼æ³¨æ„åŠ›æœºåˆ¶ã€‚ç‰¹åˆ«æ˜¯ï¼Œ[[203](#bib.bib203)]
    ä»‹ç»äº† $\mathsf{KDEformer}$ å’Œ $\sinh$ã€‚æœ€åï¼Œ[[57](#bib.bib57)] æå‡ºäº†éšæœºå’Œç¡®å®šæ€§ç®—æ³•æ¥å‡å°‘ LLMs ä¸­æ³¨æ„åŠ›çŸ©é˜µçš„ç»´åº¦ï¼ŒåŒæ—¶åœ¨æ˜¾è‘—é™ä½ç‰¹å¾ç»´åº¦çš„åŒæ—¶å®ç°é«˜ç²¾åº¦ã€‚
- en: Additionally, numerous studies have attempted to analyze theoretical attention
    from the perspectives of optimization and convergence [[111](#bib.bib111), [69](#bib.bib69),
    [172](#bib.bib172), [205](#bib.bib205)]. [[111](#bib.bib111)] investigated how
    transformers acquire knowledge about word co-occurrence patterns. [[69](#bib.bib69)]
    focused on studying regression problems inspired by neural networks that employ
    exponential activation functions. [[172](#bib.bib172)] analyzed why models occasionally
    prioritize significant words and explained how the attention mechanism evolves
    during the training process. [[205](#bib.bib205)] demonstrated that the presence
    of a heavy-tailed noise distribution contributes to the bad performance of stochastic
    gradient descent (SGD) compared to adaptive methods.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œè®¸å¤šç ”ç©¶è¯•å›¾ä»ä¼˜åŒ–å’Œæ”¶æ•›çš„è§’åº¦åˆ†æç†è®ºä¸Šçš„æ³¨æ„åŠ›[[111](#bib.bib111), [69](#bib.bib69), [172](#bib.bib172),
    [205](#bib.bib205)]ã€‚[[111](#bib.bib111)] ç ”ç©¶äº†å˜æ¢å™¨å¦‚ä½•è·å–è¯è¯­å…±ç°æ¨¡å¼çš„çŸ¥è¯†ã€‚[[69](#bib.bib69)]
    é‡ç‚¹ç ”ç©¶äº†å—ç¥ç»ç½‘ç»œå¯å‘çš„å›å½’é—®é¢˜ï¼Œè¿™äº›ç¥ç»ç½‘ç»œä½¿ç”¨æŒ‡æ•°æ¿€æ´»å‡½æ•°ã€‚[[172](#bib.bib172)] åˆ†æäº†æ¨¡å‹ä¸ºä½•å¶å°”ä¼˜å…ˆè€ƒè™‘é‡è¦è¯è¯­ï¼Œå¹¶è§£é‡Šäº†æ³¨æ„åŠ›æœºåˆ¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¼”å˜ã€‚[[205](#bib.bib205)]
    è¯æ˜äº†é‡å°¾å™ªå£°åˆ†å¸ƒçš„å­˜åœ¨å¯¼è‡´éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰çš„æ€§èƒ½å·®äºè‡ªé€‚åº”æ–¹æ³•ã€‚
- en: Theoretical LLMs
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç†è®º LLMs
- en: There are numerous amount of works focusing on the theoretical aspects of LLMs.
    In [[155](#bib.bib155)], the syntactic representations of the attention matrix
    and the individual word embeddings are presented, together with the mathematical
    justification of elucidating the geometrical properties of these representations.
    [[84](#bib.bib84)] introduces a structural probe that analyzes, under the linear
    transformation of a word representation space of a neural network, whether or
    not syntax trees are embedded.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šå·¥ä½œä¸“æ³¨äº LLM çš„ç†è®ºæ–¹é¢ã€‚ åœ¨ [[155](#bib.bib155)] ä¸­ï¼Œå±•ç¤ºäº†æ³¨æ„åŠ›çŸ©é˜µå’Œå•è¯åµŒå…¥çš„å¥æ³•è¡¨ç¤ºï¼Œå¹¶æä¾›äº†é˜æ˜è¿™äº›è¡¨ç¤ºå‡ ä½•å±æ€§çš„æ•°å­¦è¯æ˜ã€‚
    [[84](#bib.bib84)] ä»‹ç»äº†ä¸€ä¸ªç»“æ„æ¢æµ‹å™¨ï¼Œè¯¥æ¢æµ‹å™¨åˆ†æåœ¨ç¥ç»ç½‘ç»œçš„å•è¯è¡¨ç¤ºç©ºé—´çš„çº¿æ€§å˜æ¢ä¸‹ï¼Œè¯­æ³•æ ‘æ˜¯å¦è¢«åµŒå…¥ã€‚
- en: '[[39](#bib.bib39), [110](#bib.bib110), [151](#bib.bib151), [100](#bib.bib100)]
    study the optimization of LLMs. [[39](#bib.bib39)] proposes a new algorithm called
    ZO-BCD. It has favorable overall query complexity and a smaller computational
    complexity in each iteration. [[110](#bib.bib110)] creates a simple scalable second-order
    optimizer, called Sophia. In different parts of the parameter, Sophia adapts to
    the curvature. This may be strongly heterogeneous for language modeling tasks.
    The bound of the running time does not rely on the condition number of the loss.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[[39](#bib.bib39), [110](#bib.bib110), [151](#bib.bib151), [100](#bib.bib100)]
    ç ”ç©¶äº† LLM çš„ä¼˜åŒ–é—®é¢˜ã€‚ [[39](#bib.bib39)] æå‡ºäº†ä¸€ä¸ªæ–°ç®—æ³•ï¼Œç§°ä¸º ZO-BCDã€‚è¯¥ç®—æ³•å…·æœ‰è‰¯å¥½çš„æ•´ä½“æŸ¥è¯¢å¤æ‚åº¦ï¼Œå¹¶ä¸”åœ¨æ¯æ¬¡è¿­ä»£ä¸­è®¡ç®—å¤æ‚åº¦è¾ƒå°ã€‚
    [[110](#bib.bib110)] åˆ›å»ºäº†ä¸€ä¸ªç®€å•çš„å¯æ‰©å±•äºŒé˜¶ä¼˜åŒ–å™¨ï¼Œç§°ä¸º Sophiaã€‚åœ¨å‚æ•°çš„ä¸åŒéƒ¨åˆ†ï¼ŒSophia é€‚åº”æ›²ç‡ã€‚è¿™å¯¹äºè¯­è¨€å»ºæ¨¡ä»»åŠ¡å¯èƒ½å…·æœ‰å¼ºçƒˆçš„å¼‚è´¨æ€§ã€‚è¿è¡Œæ—¶é—´çš„ç•Œé™ä¸ä¾èµ–äºæŸå¤±çš„æ¡ä»¶æ•°ã€‚'
- en: Other theoretical LLM papers study the knowledge and skills of LLMs. [[188](#bib.bib188)]
    analyzes distinct â€œskillâ€ neurons, which are regarded as robust indicators of
    downstream tasks when employing the process of soft prompt-tuning, as discussed
    in [[108](#bib.bib108)], for language models. [[50](#bib.bib50)] find a positive
    relationship between the activation of these neurons and the expression of their
    corresponding facts, through analyzing BERT. Simultaneously, [[32](#bib.bib32)]
    employs a fully unsupervised approach to extract latent knowledge from a language
    modelâ€™s internal activations. In addition, [[79](#bib.bib79)] and [[124](#bib.bib124)]
    show that in the feed-forward layers of pre-trained models, language models localize
    knowledge. [[194](#bib.bib194)] explores the feasibility of selecting a specific
    subset of layers for modification and determining the optimal location for integrating
    a classifier. [[123](#bib.bib123)] demonstrate that large trained transformers
    exhibit sparsity in their feedforward activations. Zero-th order algorithm for
    training LLM has been analyzed [[125](#bib.bib125), [54](#bib.bib54), [204](#bib.bib204)].
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä»–ç†è®º LLM è®ºæ–‡ç ”ç©¶äº† LLM çš„çŸ¥è¯†å’ŒæŠ€èƒ½ã€‚ [[188](#bib.bib188)] åˆ†æäº†ä¸åŒçš„â€œæŠ€èƒ½â€ç¥ç»å…ƒï¼Œè¿™äº›ç¥ç»å…ƒè¢«è§†ä¸ºåœ¨é‡‡ç”¨è½¯æç¤ºè°ƒä¼˜è¿‡ç¨‹æ—¶ï¼Œä½œä¸ºä¸‹æ¸¸ä»»åŠ¡çš„ç¨³å¥æŒ‡æ ‡ï¼Œå¦‚
    [[108](#bib.bib108)] ä¸­è®¨è®ºçš„è¯­è¨€æ¨¡å‹ã€‚ [[50](#bib.bib50)] é€šè¿‡åˆ†æ BERT å‘ç°è¿™äº›ç¥ç»å…ƒçš„æ¿€æ´»ä¸å…¶å¯¹åº”äº‹å®çš„è¡¨è¾¾ä¹‹é—´å­˜åœ¨æ­£ç›¸å…³å…³ç³»ã€‚åŒæ—¶ï¼Œ[[32](#bib.bib32)]
    é‡‡ç”¨å®Œå…¨æ— ç›‘ç£çš„æ–¹æ³•ä»è¯­è¨€æ¨¡å‹çš„å†…éƒ¨æ¿€æ´»ä¸­æå–æ½œåœ¨çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œ[[79](#bib.bib79)] å’Œ [[124](#bib.bib124)] è¡¨æ˜ï¼Œåœ¨é¢„è®­ç»ƒæ¨¡å‹çš„å‰é¦ˆå±‚ä¸­ï¼Œè¯­è¨€æ¨¡å‹å±€éƒ¨åŒ–çŸ¥è¯†ã€‚
    [[194](#bib.bib194)] æ¢ç´¢äº†é€‰æ‹©ç‰¹å®šå±‚å­é›†è¿›è¡Œä¿®æ”¹å’Œç¡®å®šæ•´åˆåˆ†ç±»å™¨çš„æœ€ä½³ä½ç½®çš„å¯è¡Œæ€§ã€‚ [[123](#bib.bib123)] è¯æ˜äº†å¤§å‹è®­ç»ƒå˜æ¢å™¨åœ¨å…¶å‰é¦ˆæ¿€æ´»ä¸­è¡¨ç°å‡ºç¨€ç–æ€§ã€‚é›¶é˜¶ç®—æ³•ç”¨äºè®­ç»ƒ
    LLM å·²è¢«åˆ†æ [[125](#bib.bib125), [54](#bib.bib54), [204](#bib.bib204)]ã€‚
- en: LLMs Application and Evaluation
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLMs åº”ç”¨ä¸è¯„ä¼°
- en: Recently, there has been much interest in developing LLM-based systems for conversational
    AI and task-oriented dialogue, like Googleâ€™s Meena chatbot [[148](#bib.bib148)],
    Microsoft 365 Copilot [[161](#bib.bib161)], Adobe firefly, Adobe Photoshop, GPT
    series [[149](#bib.bib149), [154](#bib.bib154), [24](#bib.bib24), [35](#bib.bib35),
    [134](#bib.bib134)], and BERT [[49](#bib.bib49)].
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘ï¼Œå¼€å‘åŸºäº LLM çš„å¯¹è¯ AI å’Œä»»åŠ¡å¯¼å‘å¯¹è¯ç³»ç»Ÿå¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œå¦‚è°·æ­Œçš„ Meena èŠå¤©æœºå™¨äºº [[148](#bib.bib148)]ã€Microsoft
    365 Copilot [[161](#bib.bib161)]ã€Adobe Fireflyã€Adobe Photoshopã€GPT ç³»åˆ— [[149](#bib.bib149),
    [154](#bib.bib154), [24](#bib.bib24), [35](#bib.bib35), [134](#bib.bib134)] å’Œ
    BERT [[49](#bib.bib49)]ã€‚
- en: Moreover, LLM evaluation is also a popular research area. Within the field of
    NLP, LLMs are evaluated based on natural language understanding [[20](#bib.bib20),
    [102](#bib.bib102), [103](#bib.bib103), [45](#bib.bib45)], reasoning [[23](#bib.bib23),
    [187](#bib.bib187), [193](#bib.bib193)], natural language generation [[183](#bib.bib183),
    [147](#bib.bib147), [137](#bib.bib137), [36](#bib.bib36), [47](#bib.bib47)], and
    multilingual tasks [[10](#bib.bib10), [5](#bib.bib5), [112](#bib.bib112), [199](#bib.bib199)].
    Robustness [[109](#bib.bib109), [181](#bib.bib181), [207](#bib.bib207)], ethics
    [[48](#bib.bib48)], biases [[65](#bib.bib65)], and trustworthiness [[80](#bib.bib80)]
    are also important aspects. More specifically, the abilities of LLMs in social
    science [[51](#bib.bib51), [66](#bib.bib66), [131](#bib.bib131)], mathematics
    [[12](#bib.bib12), [53](#bib.bib53), [184](#bib.bib184), [19](#bib.bib19)], science
    [[42](#bib.bib42), [67](#bib.bib67)], engineering [[19](#bib.bib19), [122](#bib.bib122),
    [138](#bib.bib138), [160](#bib.bib160)], and medical applications [[38](#bib.bib38),
    [87](#bib.bib87)] are evaluated.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼ŒLLM è¯„ä¼°ä¹Ÿæ˜¯ä¸€ä¸ªçƒ­é—¨çš„ç ”ç©¶é¢†åŸŸã€‚åœ¨ NLP é¢†åŸŸï¼ŒLLM çš„è¯„ä¼°åŸºäºè‡ªç„¶è¯­è¨€ç†è§£ [[20](#bib.bib20), [102](#bib.bib102),
    [103](#bib.bib103), [45](#bib.bib45)]ï¼Œæ¨ç† [[23](#bib.bib23), [187](#bib.bib187),
    [193](#bib.bib193)]ï¼Œè‡ªç„¶è¯­è¨€ç”Ÿæˆ [[183](#bib.bib183), [147](#bib.bib147), [137](#bib.bib137),
    [36](#bib.bib36), [47](#bib.bib47)]ï¼Œä»¥åŠå¤šè¯­è¨€ä»»åŠ¡ [[10](#bib.bib10), [5](#bib.bib5),
    [112](#bib.bib112), [199](#bib.bib199)]ã€‚é²æ£’æ€§ [[109](#bib.bib109), [181](#bib.bib181),
    [207](#bib.bib207)]ï¼Œä¼¦ç† [[48](#bib.bib48)]ï¼Œåè§ [[65](#bib.bib65)]ï¼Œä»¥åŠå¯ä¿¡åº¦ [[80](#bib.bib80)]
    ä¹Ÿæ˜¯é‡è¦æ–¹é¢ã€‚æ›´å…·ä½“åœ°è¯´ï¼ŒLLM åœ¨ç¤¾ä¼šç§‘å­¦ [[51](#bib.bib51), [66](#bib.bib66), [131](#bib.bib131)]ï¼Œæ•°å­¦
    [[12](#bib.bib12), [53](#bib.bib53), [184](#bib.bib184), [19](#bib.bib19)]ï¼Œç§‘å­¦
    [[42](#bib.bib42), [67](#bib.bib67)]ï¼Œå·¥ç¨‹ [[19](#bib.bib19), [122](#bib.bib122),
    [138](#bib.bib138), [160](#bib.bib160)]ï¼Œå’ŒåŒ»å­¦åº”ç”¨ [[38](#bib.bib38), [87](#bib.bib87)]
    çš„èƒ½åŠ›ä¹Ÿåœ¨è¯„ä¼°ä¹‹ä¸­ã€‚
- en: Sketching
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**è‰å›¾**'
- en: 'Sketching is a powerful tool that is used to accelerate the performance of
    machine learning algorithms and optimization processes. The fundamental concept
    of sketching is to partition a large input matrix into a significantly smaller
    sketching matrix but still preserve the main characteristics of the original matrix.
    Therefore, the algorithms may work with the smaller matrix instead of the huge
    original, which leads to a substantial reduction in processing time. Many previous
    works have studied sketching, proposed sketching algorithms, and supported these
    algorithms with robust theoretical guarantees. For example, the Johnson-Lindenstrauss
    lemma is proposed by [[89](#bib.bib89)]: it shows that under a certain high-dimensional
    space, projecting points to a lower-dimensional subspace may preserve the pairwise
    distances between these points. This mathematical property becomes the foundation
    of the development of faster algorithms for tasks such as nearest neighbor search.
    In addition, as explained in [[2](#bib.bib2)], the Fast Johnson-Lindenstrauss
    Transform (FJLT) introduces a specific family of structured random projections
    that can be applied to a matrix in input sparsity time.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**è‰å›¾** æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ï¼Œç”¨äºåŠ é€Ÿæœºå™¨å­¦ä¹ ç®—æ³•å’Œä¼˜åŒ–è¿‡ç¨‹çš„æ€§èƒ½ã€‚è‰å›¾çš„åŸºæœ¬æ¦‚å¿µæ˜¯å°†ä¸€ä¸ªå¤§è¾“å…¥çŸ©é˜µåˆ’åˆ†ä¸ºä¸€ä¸ªæ˜¾è‘—è¾ƒå°çš„è‰å›¾çŸ©é˜µï¼Œä½†ä»ç„¶ä¿ç•™åŸå§‹çŸ©é˜µçš„ä¸»è¦ç‰¹å¾ã€‚å› æ­¤ï¼Œç®—æ³•å¯ä»¥ä½¿ç”¨è¾ƒå°çš„çŸ©é˜µè€Œä¸æ˜¯å·¨å¤§çš„åŸå§‹çŸ©é˜µï¼Œä»è€Œå¤§å¹…å‡å°‘å¤„ç†æ—¶é—´ã€‚è®¸å¤šä¹‹å‰çš„å·¥ä½œç ”ç©¶äº†è‰å›¾æŠ€æœ¯ï¼Œæå‡ºäº†è‰å›¾ç®—æ³•ï¼Œå¹¶ç”¨åšå®çš„ç†è®ºä¿è¯æ”¯æŒè¿™äº›ç®—æ³•ã€‚ä¾‹å¦‚ï¼Œ**Johnson-Lindenstrauss
    å¼•ç†** ç”± [[89](#bib.bib89)] æå‡ºï¼šå®ƒè¡¨æ˜ï¼Œåœ¨æŸäº›é«˜ç»´ç©ºé—´ä¸‹ï¼Œå°†ç‚¹æŠ•å½±åˆ°ä½ç»´å­ç©ºé—´å¯èƒ½ä¿ç•™è¿™äº›ç‚¹ä¹‹é—´çš„å¯¹è·ç¦»ã€‚è¿™ä¸€æ•°å­¦æ€§è´¨æˆä¸ºäº†å‘å±•æ›´å¿«ç®—æ³•çš„åŸºç¡€ï¼Œå¦‚æœ€è¿‘é‚»æœç´¢ã€‚æ­¤å¤–ï¼Œå¦‚
    [[2](#bib.bib2)] æ‰€è§£é‡Šçš„ï¼Œ**å¿«é€Ÿ Johnson-Lindenstrauss å˜æ¢ï¼ˆFJLTï¼‰** å¼•å…¥äº†ä¸€ç±»ç‰¹å®šçš„ç»“æ„åŒ–éšæœºæŠ•å½±ï¼Œè¿™äº›æŠ•å½±å¯ä»¥åœ¨è¾“å…¥ç¨€ç–æ—¶é—´å†…åº”ç”¨äºçŸ©é˜µã€‚'
- en: More recently, sketching has been applied to many numerical linear algebra tasks,
    such as linear regression [[46](#bib.bib46), [133](#bib.bib133)], dynamic kernel
    estimation [[143](#bib.bib143)], submodular maximization [[144](#bib.bib144)],
    matrix sensing [[145](#bib.bib145)], gradient-based algorithm [[195](#bib.bib195)],
    clustering [[59](#bib.bib59), [64](#bib.bib64)], convex programming [[167](#bib.bib167),
    [146](#bib.bib146), [93](#bib.bib93), [90](#bib.bib90), [117](#bib.bib117)], online
    optimization problems [[150](#bib.bib150)], training neural networks [[173](#bib.bib173),
    [197](#bib.bib197), [169](#bib.bib169), [70](#bib.bib70), [25](#bib.bib25)], reinforcement
    learning [[191](#bib.bib191), [196](#bib.bib196)], tensor decomposition [[165](#bib.bib165),
    [60](#bib.bib60)], relational database [[141](#bib.bib141)], low-rank approximation
    [[30](#bib.bib30), [128](#bib.bib128), [126](#bib.bib126), [8](#bib.bib8), [164](#bib.bib164)],
    distributed problems [[31](#bib.bib31), [190](#bib.bib190)], weighted low rank
    approximation [[152](#bib.bib152), [76](#bib.bib76), [168](#bib.bib168)], CP decomposition
    [[129](#bib.bib129)], regression inspired by softmax [[118](#bib.bib118), [74](#bib.bib74),
    [162](#bib.bib162), [55](#bib.bib55)], matrix sensing [[145](#bib.bib145)], and
    Kronecker product regression [[153](#bib.bib153)].
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘ï¼Œç´ ææŠ€æœ¯å·²è¢«åº”ç”¨äºè®¸å¤šæ•°å€¼çº¿æ€§ä»£æ•°ä»»åŠ¡ï¼Œä¾‹å¦‚çº¿æ€§å›å½’ [[46](#bib.bib46), [133](#bib.bib133)]ï¼ŒåŠ¨æ€æ ¸ä¼°è®¡ [[143](#bib.bib143)]ï¼Œå­æ¨¡é‡æœ€å¤§åŒ–
    [[144](#bib.bib144)]ï¼ŒçŸ©é˜µæ„ŸçŸ¥ [[145](#bib.bib145)]ï¼ŒåŸºäºæ¢¯åº¦çš„ç®—æ³• [[195](#bib.bib195)]ï¼Œèšç±»
    [[59](#bib.bib59), [64](#bib.bib64)]ï¼Œå‡¸ç¼–ç¨‹ [[167](#bib.bib167), [146](#bib.bib146),
    [93](#bib.bib93), [90](#bib.bib90), [117](#bib.bib117)]ï¼Œåœ¨çº¿ä¼˜åŒ–é—®é¢˜ [[150](#bib.bib150)]ï¼Œç¥ç»ç½‘ç»œè®­ç»ƒ
    [[173](#bib.bib173), [197](#bib.bib197), [169](#bib.bib169), [70](#bib.bib70),
    [25](#bib.bib25)]ï¼Œå¼ºåŒ–å­¦ä¹  [[191](#bib.bib191), [196](#bib.bib196)]ï¼Œå¼ é‡åˆ†è§£ [[165](#bib.bib165),
    [60](#bib.bib60)]ï¼Œå…³ç³»æ•°æ®åº“ [[141](#bib.bib141)]ï¼Œä½ç§©è¿‘ä¼¼ [[30](#bib.bib30), [128](#bib.bib128),
    [126](#bib.bib126), [8](#bib.bib8), [164](#bib.bib164)]ï¼Œåˆ†å¸ƒå¼é—®é¢˜ [[31](#bib.bib31),
    [190](#bib.bib190)]ï¼ŒåŠ æƒä½ç§©è¿‘ä¼¼ [[152](#bib.bib152), [76](#bib.bib76), [168](#bib.bib168)]ï¼ŒCPåˆ†è§£
    [[129](#bib.bib129)]ï¼Œå—softmaxå¯å‘çš„å›å½’ [[118](#bib.bib118), [74](#bib.bib74), [162](#bib.bib162),
    [55](#bib.bib55)]ï¼ŒçŸ©é˜µæ„ŸçŸ¥ [[145](#bib.bib145)]ï¼Œä»¥åŠKroneckerç§¯å›å½’ [[153](#bib.bib153)]ã€‚
- en: Second-order Method
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: äºŒé˜¶æ–¹æ³•
- en: Second-order method have been used for solving many convex optimization and
    non-convex optimization problems, such as linear programming [[41](#bib.bib41),
    [26](#bib.bib26), [93](#bib.bib93), [167](#bib.bib167), [71](#bib.bib71), [83](#bib.bib83)],
    empirical risk minimization [[117](#bib.bib117), [146](#bib.bib146)], support
    vector machines [[77](#bib.bib77)], cutting plan method [[116](#bib.bib116), [90](#bib.bib90)],
    semi-definite programming [[88](#bib.bib88), [81](#bib.bib81), [71](#bib.bib71),
    [170](#bib.bib170)], hyperbolic programming/polynomials [[61](#bib.bib61), [211](#bib.bib211)],
    streaming algorithm [[119](#bib.bib119), [27](#bib.bib27), [170](#bib.bib170)],
    federated learning [[28](#bib.bib28)].
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: äºŒé˜¶æ–¹æ³•å·²è¢«ç”¨äºè§£å†³è®¸å¤šå‡¸ä¼˜åŒ–å’Œéå‡¸ä¼˜åŒ–é—®é¢˜ï¼Œä¾‹å¦‚çº¿æ€§è§„åˆ’ [[41](#bib.bib41), [26](#bib.bib26), [93](#bib.bib93),
    [167](#bib.bib167), [71](#bib.bib71), [83](#bib.bib83)]ï¼Œç»éªŒé£é™©æœ€å°åŒ– [[117](#bib.bib117),
    [146](#bib.bib146)]ï¼Œæ”¯æŒå‘é‡æœº [[77](#bib.bib77)]ï¼Œå‰ªåˆ‡å¹³é¢æ–¹æ³• [[116](#bib.bib116), [90](#bib.bib90)]ï¼ŒåŠæ­£å®šè§„åˆ’
    [[88](#bib.bib88), [81](#bib.bib81), [71](#bib.bib71), [170](#bib.bib170)]ï¼ŒåŒæ›²è§„åˆ’/å¤šé¡¹å¼
    [[61](#bib.bib61), [211](#bib.bib211)]ï¼Œæµå¼ç®—æ³• [[119](#bib.bib119), [27](#bib.bib27),
    [170](#bib.bib170)]ï¼Œè”é‚¦å­¦ä¹  [[28](#bib.bib28)]ã€‚
- en: Convergence and Deep Neural Network Optimization
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ”¶æ•›æ€§ä¸æ·±åº¦ç¥ç»ç½‘ç»œä¼˜åŒ–
- en: Many works focus on analyzing optimization, convergence guarantees, and training
    improvement. [[107](#bib.bib107)] shows that stochastic gradient descent optimizes
    over-parameterized neural networks on structured data, while [[63](#bib.bib63)]
    demonstrates that gradient descent optimizes over-parameterized neural networks.
    In [[16](#bib.bib16)], a convergence theory for over-parameterized deep neural
    networks via gradient descent is developed. [[17](#bib.bib17)] analyzes the convergence
    rate of training recurrent neural networks. [[3](#bib.bib3)] provides a fine-grained
    analysis of optimization and generalization for over-parameterized two-layer neural
    networks. [[4](#bib.bib4)] studies exact computation with an infinitely wide neural
    network. [[33](#bib.bib33)] proposes a Gram-Gauss-Newton method for optimizing
    over-parameterized neural networks. [[201](#bib.bib201)] improves the analysis
    of the global convergence of stochastic gradient descent when training deep neural
    networks, requiring a milder over-parameterization compared to prior research.
    Other research, such as [[135](#bib.bib135), [96](#bib.bib96), [206](#bib.bib206)],
    focuses on optimization and generalization, while [[69](#bib.bib69), [118](#bib.bib118)]
    emphasize the convergence rate and stability. Works like [[25](#bib.bib25), [173](#bib.bib173),
    [9](#bib.bib9), [127](#bib.bib127), [202](#bib.bib202)] concentrate on specialized
    optimization algorithms and techniques for training neural networks, and [[115](#bib.bib115),
    [82](#bib.bib82)] concentrate on leveraging neural network structure.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šç ”ç©¶ä¸“æ³¨äºåˆ†æä¼˜åŒ–ã€æ”¶æ•›ä¿è¯å’Œè®­ç»ƒæ”¹è¿›ã€‚[[107](#bib.bib107)] æ˜¾ç¤ºäº†éšæœºæ¢¯åº¦ä¸‹é™åœ¨ç»“æ„åŒ–æ•°æ®ä¸Šçš„ä¼˜åŒ–æ•ˆæœï¼Œè€Œ [[63](#bib.bib63)]
    è¯æ˜äº†æ¢¯åº¦ä¸‹é™åœ¨è¿‡å‚æ•°åŒ–ç¥ç»ç½‘ç»œä¸Šçš„ä¼˜åŒ–æ•ˆæœã€‚åœ¨ [[16](#bib.bib16)] ä¸­ï¼Œå¼€å‘äº†ä¸€ç§é€šè¿‡æ¢¯åº¦ä¸‹é™å¯¹è¿‡å‚æ•°åŒ–æ·±åº¦ç¥ç»ç½‘ç»œçš„æ”¶æ•›ç†è®ºã€‚[[17](#bib.bib17)]
    åˆ†æäº†è®­ç»ƒé€’å½’ç¥ç»ç½‘ç»œçš„æ”¶æ•›é€Ÿåº¦ã€‚[[3](#bib.bib3)] æä¾›äº†å¯¹è¿‡å‚æ•°åŒ–åŒå±‚ç¥ç»ç½‘ç»œçš„ä¼˜åŒ–å’Œæ³›åŒ–çš„ç»†è‡´åˆ†æã€‚[[4](#bib.bib4)] ç ”ç©¶äº†å…·æœ‰æ— é™å®½åº¦ç¥ç»ç½‘ç»œçš„ç²¾ç¡®è®¡ç®—ã€‚[[33](#bib.bib33)]
    æå‡ºäº†ä¼˜åŒ–è¿‡å‚æ•°åŒ–ç¥ç»ç½‘ç»œçš„Gram-Gauss-Newtonæ–¹æ³•ã€‚[[201](#bib.bib201)] æ”¹è¿›äº†åœ¨è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œæ—¶éšæœºæ¢¯åº¦ä¸‹é™çš„å…¨å±€æ”¶æ•›åˆ†æï¼Œä¸ä¹‹å‰çš„ç ”ç©¶ç›¸æ¯”ï¼Œè¦æ±‚çš„è¿‡å‚æ•°åŒ–ç¨‹åº¦è¾ƒä½ã€‚å…¶ä»–ç ”ç©¶ï¼Œå¦‚
    [[135](#bib.bib135), [96](#bib.bib96), [206](#bib.bib206)]ï¼Œä¸“æ³¨äºä¼˜åŒ–å’Œæ³›åŒ–ï¼Œè€Œ [[69](#bib.bib69),
    [118](#bib.bib118)] å¼ºè°ƒæ”¶æ•›é€Ÿåº¦å’Œç¨³å®šæ€§ã€‚åƒ [[25](#bib.bib25), [173](#bib.bib173), [9](#bib.bib9),
    [127](#bib.bib127), [202](#bib.bib202)] è¿™æ ·çš„ç ”ç©¶é›†ä¸­äºè®­ç»ƒç¥ç»ç½‘ç»œçš„ä¸“é—¨ä¼˜åŒ–ç®—æ³•å’ŒæŠ€æœ¯ï¼Œè€Œ [[115](#bib.bib115),
    [82](#bib.bib82)] åˆ™ä¸“æ³¨äºåˆ©ç”¨ç¥ç»ç½‘ç»œç»“æ„ã€‚
- en: Algorithmic Regularization
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç®—æ³•æ­£åˆ™åŒ–
- en: There is a significant body of research exploring the latent bias inherent in
    gradient descent when applied to separable classification tasks. This research
    typically employs logistic or exponentially-tailed loss functions to maximize
    margins, as demonstrated in previous studies [[97](#bib.bib97), [68](#bib.bib68),
    [101](#bib.bib101), [98](#bib.bib98), [158](#bib.bib158), [130](#bib.bib130),
    [132](#bib.bib132)]. These novel findings have also been applied to non-separable
    data through the utilization of gradient-based techniques [[86](#bib.bib86), [95](#bib.bib95),
    [94](#bib.bib94)]. Analysis of implicit bias in regression problems and associated
    loss functions is carried out using methods such as mirror descent [[198](#bib.bib198),
    [13](#bib.bib13), [14](#bib.bib14), [178](#bib.bib178), [157](#bib.bib157), [180](#bib.bib180),
    [7](#bib.bib7), [68](#bib.bib68)] and stochastic gradient descent [[85](#bib.bib85),
    [120](#bib.bib120), [114](#bib.bib114), [210](#bib.bib210), [56](#bib.bib56),
    [121](#bib.bib121), [22](#bib.bib22)]. These findings extend to the implicit bias
    of adaptive and momentum-based optimization methods [[92](#bib.bib92), [185](#bib.bib185),
    [186](#bib.bib186), [142](#bib.bib142)].
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å¤§é‡ç ”ç©¶æ¢è®¨äº†åœ¨å¯åˆ†ç¦»åˆ†ç±»ä»»åŠ¡ä¸­åº”ç”¨æ¢¯åº¦ä¸‹é™æ—¶å›ºæœ‰çš„æ½œåœ¨åå·®ã€‚è¿™äº›ç ”ç©¶é€šå¸¸ä½¿ç”¨é€»è¾‘å›å½’æˆ–æŒ‡æ•°å°¾æŸå¤±å‡½æ•°æ¥æœ€å¤§åŒ–è¾¹ç•Œï¼Œå¦‚å…ˆå‰çš„ç ”ç©¶æ‰€ç¤º [[97](#bib.bib97),
    [68](#bib.bib68), [101](#bib.bib101), [98](#bib.bib98), [158](#bib.bib158), [130](#bib.bib130),
    [132](#bib.bib132)]ã€‚è¿™äº›æ–°å‘ç°è¿˜é€šè¿‡åˆ©ç”¨åŸºäºæ¢¯åº¦çš„æŠ€æœ¯åº”ç”¨äºä¸å¯åˆ†æ•°æ® [[86](#bib.bib86), [95](#bib.bib95),
    [94](#bib.bib94)]ã€‚å¯¹å›å½’é—®é¢˜åŠç›¸å…³æŸå¤±å‡½æ•°ä¸­éšå«åå·®çš„åˆ†æä½¿ç”¨äº†å¦‚é•œåƒä¸‹é™ [[198](#bib.bib198), [13](#bib.bib13),
    [14](#bib.bib14), [178](#bib.bib178), [157](#bib.bib157), [180](#bib.bib180),
    [7](#bib.bib7), [68](#bib.bib68)] å’Œéšæœºæ¢¯åº¦ä¸‹é™ [[85](#bib.bib85), [120](#bib.bib120),
    [114](#bib.bib114), [210](#bib.bib210), [56](#bib.bib56), [121](#bib.bib121),
    [22](#bib.bib22)] ç­‰æ–¹æ³•ã€‚è¿™äº›å‘ç°è¿˜æ‰©å±•åˆ°è‡ªé€‚åº”å’ŒåŸºäºåŠ¨é‡çš„ä¼˜åŒ–æ–¹æ³•çš„éšå«åå·® [[92](#bib.bib92), [185](#bib.bib185),
    [186](#bib.bib186), [142](#bib.bib142)]ã€‚
- en: 3 Technique Overview
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 æŠ€æœ¯æ¦‚è¿°
- en: 'In this section, we will introduce the primary technique employed in this paper.
    The notations used in this section are presented in Preliminary (SectionÂ [4](#S4
    "4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»æœ¬æ–‡é‡‡ç”¨çš„ä¸»è¦æŠ€æœ¯ã€‚æœ¬èŠ‚ä¸­ä½¿ç”¨çš„ç¬¦å·åœ¨åˆæ­¥éƒ¨åˆ†ï¼ˆç¬¬[4](#S4 "4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")èŠ‚ï¼‰ä¸­ç»™å‡ºã€‚'
- en: 3.1 Analysis
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 åˆ†æ
- en: Split Hessian into blocks ($X,Y$)
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å°†Hessianåˆ†å‰²æˆå—ï¼ˆ$X,Y$ï¼‰
- en: 'In the fast approximation and convergence guarantee of the training process
    for the attention matrix, the positive semi-definite property is a key focus in
    SectionÂ [6](#S6 "6 Hessian â€£ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"). In comparison to single/multiple softmax regression, both the weights
    $X$ (refer to DefinitionÂ [1.2](#S1.Thmtheorem2 "Definition 1.2 (Attention optimization).
    â€£ 1 Introduction â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")) need to be considered. Therefore, our Hessian matrix discussed in SectionÂ [6](#S6
    "6 Hessian â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")
    has the following format'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹æ³¨æ„åŠ›çŸ©é˜µçš„å¿«é€Ÿè¿‘ä¼¼å’Œæ”¶æ•›ä¿è¯ä¸­ï¼Œæ­£åŠå®šæ€§è´¨æ˜¯ç¬¬[6](#S6 "6 Hessian â€£ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time")èŠ‚çš„å…³é”®å…³æ³¨ç‚¹ã€‚ä¸å•ä¸€/å¤šé‡softmaxå›å½’ç›¸æ¯”ï¼Œéœ€è¦è€ƒè™‘æƒé‡$X$ï¼ˆå‚è§å®šä¹‰[1.2](#S1.Thmtheorem2
    "Definition 1.2 (Attention optimization). â€£ 1 Introduction â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨ç¬¬[6](#S6 "6 Hessian â€£ A
    Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time")èŠ‚è®¨è®ºçš„HessiançŸ©é˜µå…·æœ‰ä»¥ä¸‹æ ¼å¼'
- en: '|  | $\displaystyle H=\begin{bmatrix}H_{x,x}&amp;H_{x,y}\\ H_{y,x}&amp;H_{y,y}\end{bmatrix}$
    |  |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle H=\begin{bmatrix}H_{x,x}&amp;H_{x,y}\\ H_{y,x}&amp;H_{y,y}\end{bmatrix}$
    |  |'
- en: To establish the positive semi-definite property, we will examine the properties
    of the matrix above individually.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å»ºç«‹æ­£åŠå®šæ€§è´¨ï¼Œæˆ‘ä»¬å°†é€ä¸ªæ£€æŸ¥ä¸Šè¿°çŸ©é˜µçš„æ€§è´¨ã€‚
- en: Positive Semi-Definite For Hessian $H_{x,x}$
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å¯¹äºHessian $H_{x,x}$çš„æ­£åŠå®šæ€§
- en: 'The positive semi-definite of the Hessian, denoted as ${H_{x,x},H_{y,y}}$,
    constitutes a crucial initial step in the proof outlined in LemmaÂ [6.1](#S6.Thmtheorem1
    "Lemma 6.1\. â€£ 6 Hessian â€£ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"). These Hessian are discussed in detail in SectionÂ [9](#S9 "9 Hessian for
    ğ‘‹ Is PSD â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")
    and SectionÂ [10](#S10 "10 Hessian for ğ‘Œ â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 'Hessiançš„æ­£åŠå®šæ€§ï¼Œè®°ä½œ${H_{x,x},H_{y,y}}$ï¼Œæ„æˆäº†å¼•ç†[6.1](#S6.Thmtheorem1 "Lemma 6.1\.
    â€£ 6 Hessian â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")ä¸­è¯æ˜çš„å…³é”®åˆæ­¥æ­¥éª¤ã€‚è¿™äº›Hessianåœ¨ç¬¬[9](#S9
    "9 Hessian for ğ‘‹ Is PSD â€£ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")èŠ‚å’Œç¬¬[10](#S10 "10 Hessian for ğ‘Œ â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")èŠ‚ä¸­è¯¦ç»†è®¨è®ºã€‚'
- en: 'Leveraging LemmaÂ [10.1](#S10.Thmtheorem1 "Lemma 10.1\. â€£ 10.1 Hessian Property
    â€£ 10 Hessian for ğ‘Œ â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") and LemmaÂ [9.1](#S9.Thmtheorem1 "Lemma 9.1\. â€£ 9.1 Main Result â€£ 9 Hessian
    for ğ‘‹ Is PSD â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), we can establish the following results if the regularization weight sufficiently
    large (see SectionÂ [9](#S9 "9 Hessian for ğ‘‹ Is PSD â€£ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time") in details), then'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 'åˆ©ç”¨å¼•ç† [10.1](#S10.Thmtheorem1 "Lemma 10.1\. â€£ 10.1 Hessian Property â€£ 10 Hessian
    for ğ‘Œ â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")
    å’Œå¼•ç† [9.1](#S9.Thmtheorem1 "Lemma 9.1\. â€£ 9.1 Main Result â€£ 9 Hessian for ğ‘‹ Is
    PSD â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")ï¼Œå¦‚æœæ­£åˆ™åŒ–æƒé‡è¶³å¤Ÿå¤§ï¼ˆè¯¦è§ç¬¬
    [9](#S9 "9 Hessian for ğ‘‹ Is PSD â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time") èŠ‚ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥å»ºç«‹ä»¥ä¸‹ç»“æœï¼š'
- en: '|  | $\displaystyle H(x)\succeq l\cdot I_{d^{2}}~{}~{}\text{and}~{}~{}H(y)\succeq
    l\cdot I_{d^{2}}$ |  |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle H(x)\succeq l\cdot I_{d^{2}}~{}~{}\text{and}~{}~{}H(y)\succeq
    l\cdot I_{d^{2}}$ |  |'
- en: Spectral upper bound for $H_{x,y}$
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: $H_{x,y}$ çš„å…‰è°±ä¸Šç•Œ
- en: To establish the spectral upper bound of $H_{x,y}$ into $\{{G_{i}}\}_{i=1}^{4}$.
    The spectral upper bound for $H_{x,y}$.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å»ºç«‹ $H_{x,y}$ çš„å…‰è°±ä¸Šç•Œä¸º $\{{G_{i}}\}_{i=1}^{4}$ã€‚$H_{x,y}$ çš„å…‰è°±ä¸Šç•Œã€‚
- en: Given this upper bound, our final focus in the proof of the positive semi-definite
    property (PSD) will be as follows.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ­¤ä¸Šç•Œä¸‹ï¼Œæˆ‘ä»¬åœ¨æ­£åŠå®šæ€§è´¨ (PSD) è¯æ˜ä¸­çš„æœ€ç»ˆå…³æ³¨ç‚¹å¦‚ä¸‹ã€‚
- en: PSD for Hessian $H$
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Hessian $H$ çš„ PSD
- en: 'The Hessian matrix $H$ in SectionÂ [9](#S9 "9 Hessian for ğ‘‹ Is PSD â€£ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time") and $H_{y,y}$ and
    $H_{y,x}$, $a_{2}$ as the bound of the matrix above respectively in LemmaÂ [6.1](#S6.Thmtheorem1
    "Lemma 6.1\. â€£ 6 Hessian â€£ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), we have the following result'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¬¬ [9](#S9 "9 Hessian for ğ‘‹ Is PSD â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") èŠ‚ä¸­çš„ Hessian çŸ©é˜µ $H$ å’Œ $H_{y,y}$ åŠ $H_{y,x}$ï¼Œ$a_{2}$
    ä½œä¸ºä¸Šç•ŒçŸ©é˜µçš„ç•Œé™ï¼Œåˆ†åˆ«åœ¨å¼•ç† [6.1](#S6.Thmtheorem1 "Lemma 6.1\. â€£ 6 Hessian â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") ä¸­ï¼Œæˆ‘ä»¬å¾—å‡ºä»¥ä¸‹ç»“æœï¼š'
- en: '|  | $\displaystyle H\succeq\{\alpha_{1}-\alpha_{3},\alpha_{2}-\alpha_{3}\}\cdot
    I_{2d^{2}}$ |  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle H\succeq\{\alpha_{1}-\alpha_{3},\alpha_{2}-\alpha_{3}\}\cdot
    I_{2d^{2}}$ |  |'
- en: Given the relationship of $\{a_{i}\}_{i=1}^{3}$ as discussed above, the positive
    semi-definite property of the Hessian matrix is established.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®ä¸Šè¿°è®¨è®ºçš„ $\{a_{i}\}_{i=1}^{3}$ å…³ç³»ï¼ŒHessian çŸ©é˜µçš„æ­£åŠå®šæ€§è´¨è¢«å»ºç«‹ã€‚
- en: Lipschitz property for Hessian
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Hessian çš„ Lipschitz æ€§è´¨
- en: 'The Lipschitz property of the Hessian is determined by the upper bound and
    Lipschitz property of the basic functions that constitute the Hessian matrix $H$
    has three parts $H_{x,x}$ and $H_{y,y}$ is independent of $y$, the Lipschitz property
    can be easily established. For details of others, we refer the readers to read
    SectionÂ [12](#S12 "12 Lipschitz for Hessian of ğ‘¥,ğ‘¦ â€£ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time").'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 'Hessian çš„ Lipschitz æ€§è´¨ç”±æ„æˆ Hessian çŸ©é˜µ $H$ çš„åŸºæœ¬å‡½æ•°çš„ä¸Šç•Œå’Œ Lipschitz æ€§è´¨å†³å®šï¼Œå…¶ä¸­ $H_{x,x}$
    å’Œ $H_{y,y}$ ä¸ $y$ æ— å…³ï¼ŒLipschitz æ€§è´¨å¯ä»¥å¾ˆå®¹æ˜“åœ°å»ºç«‹ã€‚æœ‰å…³å…¶ä»–ç»†èŠ‚ï¼Œæˆ‘ä»¬è¯·è¯»è€…æŸ¥é˜…ç¬¬ [12](#S12 "12 Lipschitz
    for Hessian of ğ‘¥,ğ‘¦ â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") èŠ‚ã€‚'
- en: 'To compute the Lipschitz continuity of $H_{x,x}$, $c(x)$ in LemmaÂ [8.4](#S8.Thmtheorem4
    "Lemma 8.4 (Basic Functions Upper Bound). â€£ 8.3 A Core Tool: Upper Bound for Several
    Basic Functions â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time"), which together form the matrix $H_{x,x}$
    into four distinct parts denoted as $\{G_{k}\}_{k=1}^{4}$), we want to bound'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¸ºäº†è®¡ç®— $H_{x,x}$ çš„ Lipschitz è¿ç»­æ€§ï¼Œ$c(x)$ åœ¨å¼•ç†Â [8.4](#S8.Thmtheorem4 "Lemma 8.4
    (Basic Functions Upper Bound). â€£ 8.3 A Core Tool: Upper Bound for Several Basic
    Functions â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") ä¸­ï¼Œç»“åˆå½¢æˆçŸ©é˜µ $H_{x,x}$ çš„å››ä¸ªä¸åŒéƒ¨åˆ† $\{G_{k}\}_{k=1}^{4}$ï¼Œæˆ‘ä»¬å¸Œæœ›ç•Œå®š'
- en: '|  | $\displaystyle&#124;\prod_{i=1}^{t}\beta_{i}(x)-\prod_{i=1}^{t}\beta_{i}(\widetilde{x})&#124;,$
    |  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle&#124;\prod_{i=1}^{t}\beta_{i}(x)-\prod_{i=1}^{t}\beta_{i}(\widetilde{x})&#124;,$
    |  |'
- en: which can be upper bounded by
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯ä»¥è¢«ä¸Šç•Œ
- en: '|  | $1$2 |  |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where assume that $\beta_{0}(x)=1$ for convenient. We will then proceed to establish
    the Lipschitz continuity of $H_{x,x}$
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ–¹ä¾¿çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å‡è®¾ $\beta_{0}(x)=1$ã€‚ç„¶åæˆ‘ä»¬å°†ç»§ç»­å»ºç«‹ $H_{x,x}$ çš„ Lipschitz è¿ç»­æ€§ã€‚
- en: '|  | $1$2 |  |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 3.2 Algorithm
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 ç®—æ³•
- en: Forward Computation
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å‰å‘è®¡ç®—
- en: 'To simplify the computation of the attention matrix, we can decompose the computation
    process into three components: $f$, and $h$ time, as stated in LemmaÂ [5.3](#S5.Thmtheorem3
    "Lemma 5.3\. â€£ 5.3 Computation of ğ‘,ğ‘“,â„ â€£ 5 Gradient â€£ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time").'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¸ºäº†ç®€åŒ–æ³¨æ„åŠ›çŸ©é˜µçš„è®¡ç®—ï¼Œæˆ‘ä»¬å¯ä»¥å°†è®¡ç®—è¿‡ç¨‹åˆ†è§£ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼š$f$ å’Œ $h$ æ—¶é—´ï¼Œå¦‚å¼•ç†Â [5.3](#S5.Thmtheorem3 "Lemma
    5.3\. â€£ 5.3 Computation of ğ‘,ğ‘“,â„ â€£ 5 Gradient â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") ä¸­æ‰€è¿°ã€‚'
- en: Gradient Computation
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ¢¯åº¦è®¡ç®—
- en: 'We can compute the gradient in SectionÂ [5](#S5 "5 Gradient â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 'æˆ‘ä»¬å¯ä»¥åœ¨ç¬¬Â [5](#S5 "5 Gradient â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time") èŠ‚è®¡ç®—æ¢¯åº¦ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š'
- en: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)}{\mathrm{d}x}$ |  |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)}{\mathrm{d}x}$ |  |'
- en: for some matrix $p(x,y)\in\mathbb{R}^{n\times n}$ can be computed in ${\cal
    T}_{\mathrm{mat}}(n,d,n)+{\cal T}_{\mathrm{mat}}(d,n,d)$ time. Similarly,
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæŸäº›çŸ©é˜µ $p(x,y)\in\mathbb{R}^{n\times n}$ å¯ä»¥åœ¨ ${\cal T}_{\mathrm{mat}}(n,d,n)+{\cal
    T}_{\mathrm{mat}}(d,n,d)$ æ—¶é—´å†…è®¡ç®—ã€‚ç±»ä¼¼åœ°ï¼Œ
- en: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)}{\mathrm{d}y}$ |  |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)}{\mathrm{d}y}$ |  |'
- en: which also takes ${\cal T}_{\mathrm{mat}}(n,n,d)+{\cal T}_{\mathrm{mat}}(n,d,d)$
    and $g(y(t))$ time.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¹Ÿéœ€è¦ ${\cal T}_{\mathrm{mat}}(n,n,d)+{\cal T}_{\mathrm{mat}}(n,d,d)$ å’Œ $g(y(t))$
    æ—¶é—´ã€‚
- en: Straightforward Hessian Computation
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç›´æ¥è®¡ç®— Hessian çŸ©é˜µ
- en: Computing the Hessian in straightforward way would take ${\cal T}_{\mathrm{mat}}(d^{2},n^{2},d^{2})$
    where $\operatorname{\mathsf{A}}\in\mathbb{R}^{n^{2}\times d^{2}}$. This is too
    slow, we will use sketching ideas to speed up this running time. Using sketching
    matrices to speed up the Hessian computation has been extensively studied in convex
    and non-convex optimization [[93](#bib.bib93), [117](#bib.bib117), [167](#bib.bib167),
    [71](#bib.bib71), [77](#bib.bib77), [146](#bib.bib146)].
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´æ¥è®¡ç®— Hessian å°†éœ€è¦ ${\cal T}_{\mathrm{mat}}(d^{2},n^{2},d^{2})$ æ—¶é—´ï¼Œå…¶ä¸­ $\operatorname{\mathsf{A}}\in\mathbb{R}^{n^{2}\times
    d^{2}}$ã€‚è¿™å¤ªæ…¢äº†ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è‰å›¾æ–¹æ³•æ¥åŠ å¿«è¿™ä¸€è¿è¡Œæ—¶é—´ã€‚ä½¿ç”¨è‰å›¾çŸ©é˜µåŠ é€Ÿ Hessian è®¡ç®—å·²ç»åœ¨å‡¸ä¼˜åŒ–å’Œéå‡¸ä¼˜åŒ–ä¸­å¾—åˆ°äº†å¹¿æ³›ç ”ç©¶ [[93](#bib.bib93),
    [117](#bib.bib117), [167](#bib.bib167), [71](#bib.bib71), [77](#bib.bib77), [146](#bib.bib146)]ã€‚
- en: TensorSRHT Fast Approximation for Hessian
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TensorSRHT å¿«é€Ÿè¿‘ä¼¼ Hessian
- en: 'Building upon the aforementioned properties, we can apply the Newton Method
    in SectionÂ [14](#S14 "14 Analysis Of Algorithm 1 â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") to establish convergence for the regression problem.
    Now, letâ€™s delve into the primary contribution of this paper. Given that $\operatorname{\mathsf{A}}\in\mathbb{R}^{n^{2}\times
    d^{2}}$ down to $\widetilde{O}(nd)+{\cal T}_{\mathrm{mat}}(d^{2},d^{2},d^{2})$
    in the paper which is the most common setting in practice because $n$ is feature
    dimension).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºä¸Šè¿°ç‰¹æ€§ï¼Œæˆ‘ä»¬å¯ä»¥åº”ç”¨ç¬¬ [14](#S14 "14 ç®—æ³• 1 çš„åˆ†æ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿°å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")
    èŠ‚ä¸­çš„ç‰›é¡¿æ³•æ¥å»ºç«‹å›å½’é—®é¢˜çš„æ”¶æ•›æ€§ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ·±å…¥æ¢è®¨æœ¬æ–‡çš„ä¸»è¦è´¡çŒ®ã€‚é‰´äº $\operatorname{\mathsf{A}}\in\mathbb{R}^{n^{2}\times
    d^{2}}$ åœ¨æœ¬æ–‡ä¸­ä¸‹é™ä¸º $\widetilde{O}(nd)+{\cal T}_{\mathrm{mat}}(d^{2},d^{2},d^{2})$ï¼Œè¿™æ˜¯å®é™…ä¸­æœ€å¸¸è§çš„è®¾ç½®ï¼Œå› ä¸º
    $n$ æ˜¯ç‰¹å¾ç»´åº¦ï¼‰ã€‚
- en: Overall Time
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ€»ä½“æ—¶é—´
- en: In Summary, we know that
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“æ¥è¯´ï¼Œæˆ‘ä»¬çŸ¥é“
- en: â€¢
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Computing forward function ${\cal T}_{\mathrm{mat}}(n,n,d)+{\cal T}_{\mathrm{mat}}(n,d,d)$
    time (LemmaÂ [5.3](#S5.Thmtheorem3 "Lemma 5.3\. â€£ 5.3 Computation of ğ‘,ğ‘“,â„ â€£ 5
    Gradient â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"))'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¡ç®—å‰å‘å‡½æ•°çš„æ—¶é—´ä¸º ${\cal T}_{\mathrm{mat}}(n,n,d)+{\cal T}_{\mathrm{mat}}(n,d,d)$ï¼ˆå¼•ç†
    [5.3](#S5.Thmtheorem3 "å¼•ç† 5.3 â€£ 5.3 è®¡ç®— ğ‘,ğ‘“,â„ â€£ 5 æ¢¯åº¦ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿°å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")ï¼‰ã€‚
- en: â€¢
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Computing gradient takes ${\cal T}_{\mathrm{mat}}(n,n,d)+{\cal T}_{\mathrm{mat}}(n,d,d)$
    time (LemmaÂ [5.4](#S5.Thmtheorem4 "Lemma 5.4\. â€£ 5.4 Reformulating Gradient (ğ‘¥)
    in Matrix View â€£ 5 Gradient â€£ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") and LemmaÂ [5.5](#S5.Thmtheorem5 "Lemma 5.5\. â€£ 5.5 Reformulating Gradient
    (ğ‘¦) in Matrix View â€£ 5 Gradient â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"))'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¡ç®—æ¢¯åº¦çš„æ—¶é—´ä¸º ${\cal T}_{\mathrm{mat}}(n,n,d)+{\cal T}_{\mathrm{mat}}(n,d,d)$ï¼ˆå¼•ç†
    [5.4](#S5.Thmtheorem4 "å¼•ç† 5.4 â€£ 5.4 åœ¨çŸ©é˜µè§†è§’ä¸­é‡æ–°è¡¨è¿°æ¢¯åº¦ï¼ˆğ‘¥ï¼‰ â€£ 5 æ¢¯åº¦ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM
    æŠ€å·§é‡æ–°è¡¨è¿°å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³") å’Œå¼•ç† [5.5](#S5.Thmtheorem5 "å¼•ç† 5.5 â€£ 5.5 åœ¨çŸ©é˜µè§†è§’ä¸­é‡æ–°è¡¨è¿°æ¢¯åº¦ï¼ˆğ‘¦ï¼‰
    â€£ 5 æ¢¯åº¦ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿°å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")ï¼‰ã€‚
- en: â€¢
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Compute Hessian takes $\widetilde{O}(nd)+{\cal T}_{\mathrm{mat}}(d^{2},d^{2},d^{2})$
    (LemmaÂ [13.6](#S13.Thmtheorem6 "Lemma 13.6\. â€£ 13.4 Fast Approximation for Hessian
    via Sketching â€£ 13 Generating a Spectral Sparsifier via TensorSketch â€£ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time"))'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¡ç®— Hessian çš„æ—¶é—´å¤æ‚åº¦ä¸º $\widetilde{O}(nd)+{\cal T}_{\mathrm{mat}}(d^{2},d^{2},d^{2})$ï¼ˆå¼•ç†
    [13.6](#S13.Thmtheorem6 "å¼•ç† 13.6 â€£ 13.4 é€šè¿‡ sketching å¿«é€Ÿé€¼è¿‘ Hessian â€£ 13 é€šè¿‡ TensorSketch
    ç”Ÿæˆè°±ç¨€ç–åŒ–å™¨ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿°å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")ï¼‰ã€‚
- en: â€¢
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Compute $g$
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¡ç®— $g$
- en: The total time can be expressed as $\widetilde{O}({\cal T}_{\mathrm{mat}}(n,d,n)+{\cal
    T}_{\mathrm{mat}}(n,d,d)+d^{2\omega})\log(1/\epsilon)$ is the exponent of matrix
    multiplication.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»æ—¶é—´å¯ä»¥è¡¨ç¤ºä¸º $\widetilde{O}({\cal T}_{\mathrm{mat}}(n,d,n)+{\cal T}_{\mathrm{mat}}(n,d,d)+d^{2\omega})\log(1/\epsilon)$ï¼Œå…¶ä¸­
    $\omega$ æ˜¯çŸ©é˜µä¹˜æ³•çš„æŒ‡æ•°ã€‚
- en: 4 Preliminary
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 åˆæ­¥
- en: '| Previous works | Simplified version of Def.Â [1.2](#S1.Thmtheorem2 "Definition
    1.2 (Attention optimization). â€£ 1 Introduction â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") | How Def.Â [1.2](#S1.Thmtheorem2 "Definition 1.2
    (Attention optimization). â€£ 1 Introduction â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") is simplified |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| å…ˆå‰çš„å·¥ä½œ | å®šä¹‰ [1.2](#S1.Thmtheorem2 "å®šä¹‰ 1.2ï¼ˆæ³¨æ„åŠ›ä¼˜åŒ–ï¼‰ã€‚ â€£ 1 ä»‹ç» â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM
    æŠ€å·§é‡æ–°è¡¨è¿°å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³") çš„ç®€åŒ–ç‰ˆæœ¬ | å®šä¹‰ [1.2](#S1.Thmtheorem2 "å®šä¹‰ 1.2ï¼ˆæ³¨æ„åŠ›ä¼˜åŒ–ï¼‰ã€‚ â€£ 1
    ä»‹ç» â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿°å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³") æ˜¯å¦‚ä½•ç®€åŒ–çš„ |'
- en: '| --- | --- | --- |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| [[203](#bib.bib203), [11](#bib.bib11), [29](#bib.bib29)] | $D^{-1}\exp(A_{1}XA_{2}^{\top})A_{3}Y$,
    $K=A_{2}$, both $X,Y$ are not considered |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| [[203](#bib.bib203), [11](#bib.bib11), [29](#bib.bib29)] | $D^{-1}\exp(A_{1}XA_{2}^{\top})A_{3}Y$ï¼Œ$K=A_{2}$ï¼Œ$X$
    å’Œ $Y$ éƒ½ä¸è€ƒè™‘ |'
- en: '| [[55](#bib.bib55)] | $(D^{-1}\exp(A_{1}XA_{2}^{\top}))_{i,*}$ are not considered
    |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| [[55](#bib.bib55)] | $(D^{-1}\exp(A_{1}XA_{2}^{\top}))_{i,*}$ ä¸è¢«è€ƒè™‘ |'
- en: '| [[72](#bib.bib72), [73](#bib.bib73)] | $D^{-1}\exp(A_{1}XA_{2}^{\top})$ are
    not considered |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| [[72](#bib.bib72), [73](#bib.bib73)] | $D^{-1}\exp(A_{1}XA_{2}^{\top})$ æœªè¢«è€ƒè™‘
    |'
- en: '| [[75](#bib.bib75)] | $D^{-1}\exp(A_{1}XA_{2}^{\top})$ is not considered and
    need the symmetric assumption for matrix |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| [[75](#bib.bib75)] | $D^{-1}\exp(A_{1}XA_{2}^{\top})$ æœªè¢«è€ƒè™‘ï¼Œéœ€è¦å¯¹çŸ©é˜µè¿›è¡Œå¯¹ç§°å‡è®¾ |'
- en: '| [[57](#bib.bib57)] | $D^{-1}\exp(A_{2}A_{2}^{\top})$ is not considered |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| [[57](#bib.bib57)] | $D^{-1}\exp(A_{2}A_{2}^{\top})$ æœªè¢«è€ƒè™‘ |'
- en: '| [[171](#bib.bib171)] | $A_{1}XA_{2}^{\top}A_{3}$ and $\exp$ is not considered
    |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| [[171](#bib.bib171)] | $A_{1}XA_{2}^{\top}A_{3}$ å’Œ $\exp$ æœªè¢«è€ƒè™‘ |'
- en: 'Table 1: Here $D:=\operatorname{diag}(\exp(A_{1}XA_{2}^{\top}){\bf 1}_{n})$.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 1ï¼šè¿™é‡Œ $D:=\operatorname{diag}(\exp(A_{1}XA_{2}^{\top}){\bf 1}_{n})$ã€‚
- en: 'In SectionÂ [4.1](#S4.SS1 "4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), we present the basic mathematical
    properties of vectors, norms and matrices. In sectionÂ [4.2](#S4.SS2 "4.2 General
    Definitions â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), we provide a definition of $L(X,Y)$. In sectionÂ [4.4](#S4.SS4 "4.4 A Helpful
    Definition With Respect to ğ‘Œ â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), we define a series of helpful functions with respect
    to $Y$ and $Y$. In SectionÂ [4.6](#S4.SS6 "4.6 Regularization â€£ 4 Preliminary â€£
    A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on
    Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), we define
    the regularization function. In SectionÂ [4.7](#S4.SS7 "4.7 Fast Matrix Multiplication
    â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), we introduce facts related to fast matrix multiplication.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç« èŠ‚ [4.1](#S4.SS1 "4.1 åŸºç¡€äº‹å® â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´ä¸­æ±‚è§£")
    ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†å‘é‡ã€èŒƒæ•°å’ŒçŸ©é˜µçš„åŸºæœ¬æ•°å­¦å±æ€§ã€‚åœ¨ç« èŠ‚ [4.2](#S4.SS2 "4.2 ä¸€èˆ¬å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´ä¸­æ±‚è§£")
    ä¸­ï¼Œæˆ‘ä»¬æä¾›äº† $L(X,Y)$ çš„å®šä¹‰ã€‚åœ¨ç« èŠ‚ [4.4](#S4.SS4 "4.4 å…³äº ğ‘Œ çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM
    æŠ€å·§é‡æ„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´ä¸­æ±‚è§£") ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ç³»åˆ—ä¸ $Y$ å’Œ $Y$ ç›¸å…³çš„æœ‰ç”¨å‡½æ•°ã€‚åœ¨ç« èŠ‚ [4.6](#S4.SS6 "4.6 æ­£åˆ™åŒ–
    â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´ä¸­æ±‚è§£") ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†æ­£åˆ™åŒ–å‡½æ•°ã€‚åœ¨ç« èŠ‚ [4.7](#S4.SS7
    "4.7 å¿«é€ŸçŸ©é˜µä¹˜æ³• â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´ä¸­æ±‚è§£") ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸å¿«é€ŸçŸ©é˜µä¹˜æ³•ç›¸å…³çš„äº‹å®ã€‚
- en: Notation
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç¬¦å·
- en: Now we define the basic notations we use in this paper.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å®šä¹‰äº†æœ¬æ–‡ä¸­ä½¿ç”¨çš„åŸºæœ¬ç¬¦å·ã€‚
- en: First, we define the notations related to the sets. We use $\mathbb{N}$. Let
    $n$ be in $\mathbb{N}$. We use $\mathbb{R},\mathbb{R}^{n},\mathbb{R}^{n\times
    d}$-dimensional vectors, and $n\times d$. We use $\mathbb{R}_{+}$ to denote the
    set containing all positive real numbers.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬å®šä¹‰ä¸é›†åˆç›¸å…³çš„ç¬¦å·ã€‚æˆ‘ä»¬ä½¿ç”¨ $\mathbb{N}$ã€‚è®¾ $n$ å±äº $\mathbb{N}$ã€‚æˆ‘ä»¬ä½¿ç”¨ $\mathbb{R},\mathbb{R}^{n},\mathbb{R}^{n\times
    d}$ ç»´å‘é‡å’Œ $n\times d$ã€‚æˆ‘ä»¬ä½¿ç”¨ $\mathbb{R}_{+}$ è¡¨ç¤ºåŒ…å«æ‰€æœ‰æ­£å®æ•°çš„é›†åˆã€‚
- en: Then, we define the notations related to vectors. Let $x,y\in\mathbb{R}^{d}$,
    we define $x_{i}\in\mathbb{R}$-th entry of $x$ as $\langle x,y\rangle:=\sum_{i=1}^{d}x_{i}y_{i}$
    and $y$ as $(x\circ y)_{i}:=x_{i}\cdot y_{i}$. For all $p\in\{1,2,\infty\}$, which
    is the $\ell_{p}$. We use ${\bf 1}_{d}$ to denote the $d$â€™s and $0$â€™s, respectively.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸å‘é‡ç›¸å…³çš„ç¬¦å·ã€‚è®¾ $x,y\in\mathbb{R}^{d}$ï¼Œæˆ‘ä»¬å®šä¹‰ $x_{i}\in\mathbb{R}$-th é¡¹ä¸º $\langle
    x,y\rangle:=\sum_{i=1}^{d}x_{i}y_{i}$ï¼Œ$y$ ä¸º $(x\circ y)_{i}:=x_{i}\cdot y_{i}$ã€‚å¯¹äºæ‰€æœ‰
    $p\in\{1,2,\infty\}$ï¼Œå³ $\ell_{p}$ã€‚æˆ‘ä»¬ç”¨ ${\bf 1}_{d}$ è¡¨ç¤º $d$ ä¸ª $1$ å’Œ $0$ã€‚
- en: After that, we define the notations related to matrices. Let $A\in\mathbb{R}^{n\times
    d}$ and $j\in[d]$ to denote the entry of $A$-th row and $j$ and $A_{*,j}\in\mathbb{R}^{n}$.
    We use $A^{\top}\in\mathbb{R}^{d\times n}$, where $A_{i,j}^{\top}=A_{j,i}$, we
    define $x=\operatorname{vec}(X)\in\mathbb{R}^{d^{2}}$. For $x\in\mathbb{R}^{d}$
    as $\operatorname{diag}(x)_{i,i}=x_{i}$ and other entries of $\operatorname{diag}(x)$â€™s.
    $\|A\|_{F}\in\mathbb{R}$ denote the Frobenius norm and the spectral norm of $A\in\mathbb{R}^{n\times
    d}$ and $\|A\|:=\max_{x\in\mathbb{R}^{d}}\|Ax\|_{2}/\|x\|_{2}$. For each $j_{1}\in[n]$
    to denote one $n\times d^{2}$. Let $C,D\in\mathbb{R}^{d\times d}$ if for all $y\in\mathbb{R}^{d}$.
    $C$. We use $I_{d}$ identity matrix. $\operatorname{nnz}(A)$ that are not equal
    to zero. ${\bf 0}_{n\times n}\in\mathbb{R}^{n\times n}$, $({\bf 0}_{n\times n})_{i,j}=0$.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å®šä¹‰ä¸çŸ©é˜µç›¸å…³çš„ç¬¦å·ã€‚è®¾ $A\in\mathbb{R}^{n\times d}$ å’Œ $j\in[d]$ è¡¨ç¤º $A$ ç¬¬ $j$ åˆ—çš„æ¡ç›®ï¼Œä»¥åŠ
    $A_{*,j}\in\mathbb{R}^{n}$ã€‚æˆ‘ä»¬ä½¿ç”¨ $A^{\top}\in\mathbb{R}^{d\times n}$ï¼Œå…¶ä¸­ $A_{i,j}^{\top}=A_{j,i}$ï¼Œæˆ‘ä»¬å®šä¹‰
    $x=\operatorname{vec}(X)\in\mathbb{R}^{d^{2}}$ã€‚å¯¹äº $x\in\mathbb{R}^{d}$ï¼Œå®šä¹‰ $\operatorname{diag}(x)_{i,i}=x_{i}$
    å’Œ $\operatorname{diag}(x)$ çš„å…¶ä»–æ¡ç›®ã€‚$\|A\|_{F}\in\mathbb{R}$ è¡¨ç¤º Frobenius èŒƒæ•°å’Œ $A\in\mathbb{R}^{n\times
    d}$ çš„è°±èŒƒæ•°ï¼Œ$\|A\|:=\max_{x\in\mathbb{R}^{d}}\|Ax\|_{2}/\|x\|_{2}$ã€‚å¯¹äºæ¯ä¸ª $j_{1}\in[n]$ï¼Œè¡¨ç¤ºä¸€ä¸ª
    $n\times d^{2}$ã€‚è®¾ $C,D\in\mathbb{R}^{d\times d}$ï¼Œå¦‚æœå¯¹äºæ‰€æœ‰ $y\in\mathbb{R}^{d}$ éƒ½æˆç«‹
    $C$ã€‚æˆ‘ä»¬ä½¿ç”¨ $I_{d}$ ä½œä¸ºå•ä½çŸ©é˜µã€‚$\operatorname{nnz}(A)$ è¡¨ç¤ºä¸ç­‰äºé›¶çš„æ¡ç›®ã€‚${\bf 0}_{n\times n}\in\mathbb{R}^{n\times
    n}$ï¼Œ$({\bf 0}_{n\times n})_{i,j}=0$ã€‚
- en: Let $n_{1},n_{2},d_{1},d_{2}$ and $B\in\mathbb{R}^{n_{2}\times d_{2}}$ and $B$,
    as $(A\otimes B)_{(i_{1}-1)n_{2}+i_{2},(j_{1}-1)d_{2}+j_{2}}$, where $i_{1}\in[n_{1}],j_{1}\in[d_{1}],i_{2}\in[n_{2}],j_{2}\in[d_{2}]$
    is defined by $X_{i,j}=\mathrm{mat}(x)_{i,j}:=x_{(i-1)\cdot n+j}$.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ $n_{1},n_{2},d_{1},d_{2}$ å’Œ $B\in\mathbb{R}^{n_{2}\times d_{2}}$ï¼Œä»¥åŠ $B$ï¼Œå…¶ä¸­
    $(A\otimes B)_{(i_{1}-1)n_{2}+i_{2},(j_{1}-1)d_{2}+j_{2}}$ï¼Œå…¶ä¸­ $i_{1}\in[n_{1}],j_{1}\in[d_{1}],i_{2}\in[n_{2}],j_{2}\in[d_{2}]$
    ç”± $X_{i,j}=\mathrm{mat}(x)_{i,j}:=x_{(i-1)\cdot n+j}$ å®šä¹‰ã€‚
- en: '![Refer to caption](img/6c1c1e971b27b856ec67768788f9b1ad.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/6c1c1e971b27b856ec67768788f9b1ad.png)'
- en: 'Figure 5: The visualization of the functions $\mathrm{mat}:\mathbb{R}^{n^{2}}\to\mathbb{R}^{n\times
    n}$. We have $x\in\mathbb{R}^{n^{2}}$. In this figure, we give an example of $n=3$,
    the first three entries of the vector $x$, $X_{1,2}$ respectively, the second
    three entries of the vector $x$, $X_{2,2}$ respectively, and the third three entries
    of the vector $x$, $X_{3,2}$ respectively. For the right figure, every entry in
    $X$ by $\operatorname{vec}$.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5ï¼šå‡½æ•° $\mathrm{mat}:\mathbb{R}^{n^{2}}\to\mathbb{R}^{n\times n}$ çš„å¯è§†åŒ–ã€‚æˆ‘ä»¬æœ‰ $x\in\mathbb{R}^{n^{2}}$ã€‚åœ¨æ­¤å›¾ä¸­ï¼Œæˆ‘ä»¬ç»™å‡ºäº†
    $n=3$ çš„ç¤ºä¾‹ï¼Œå‘é‡ $x$ çš„å‰ä¸‰ä¸ªæ¡ç›®ï¼Œåˆ†åˆ«æ˜¯ $X_{1,2}$ï¼Œå‘é‡ $x$ çš„ç¬¬äºŒç»„ä¸‰ä¸ªæ¡ç›®ï¼Œåˆ†åˆ«æ˜¯ $X_{2,2}$ï¼Œä»¥åŠå‘é‡ $x$ çš„ç¬¬ä¸‰ç»„ä¸‰ä¸ªæ¡ç›®ï¼Œåˆ†åˆ«æ˜¯
    $X_{3,2}$ã€‚å¯¹äºå³ä¾§çš„å›¾å½¢ï¼Œ$X$ ä¸­çš„æ¯ä¸ªæ¡ç›®éƒ½ç”± $\operatorname{vec}$ è¡¨ç¤ºã€‚
- en: 4.1 Basic Facts
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 åŸºæœ¬äº‹å®
- en: In this section, we will introduce the basic mathematical facts.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»åŸºæœ¬çš„æ•°å­¦äº‹å®ã€‚
- en: Fact 4.1.
  id: totrans-155
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: äº‹å® 4.1ã€‚
- en: Let $a,b\in\mathbb{R}$.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ $a,b\in\mathbb{R}$ã€‚
- en: For all vectors $u,v,w\in\mathbb{R}^{n}$, we have
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ‰€æœ‰å‘é‡ $u,v,w\in\mathbb{R}^{n}$ï¼Œæˆ‘ä»¬æœ‰
- en: â€¢
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\langle u,v\rangle=\langle u\circ v,{\bf 1}_{n}\rangle=u^{\top}\mathrm{diag}(v){\bf
    1}_{n}$
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\langle u,v\rangle=\langle u\circ v,{\bf 1}_{n}\rangle=u^{\top}\mathrm{diag}(v){\bf
    1}_{n}$
- en: â€¢
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\langle u\circ v,w\rangle=\langle u\circ w,v\rangle$
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\langle u\circ v,w\rangle=\langle u\circ w,v\rangle$
- en: â€¢
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\langle u\circ v,w\rangle=\langle u\circ v\circ w,{\bf 1}_{n}\rangle=u^{\top}\operatorname{diag}(v)w$
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\langle u\circ v,w\rangle=\langle u\circ v\circ w,{\bf 1}_{n}\rangle=u^{\top}\operatorname{diag}(v)w$
- en: â€¢
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\langle u\circ v\circ w\circ z,{\bf 1}_{n}\rangle=u^{\top}\operatorname{diag}(v\circ
    w)z$
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\langle u\circ v\circ w\circ z,{\bf 1}_{n}\rangle=u^{\top}\operatorname{diag}(v\circ
    w)z$
- en: â€¢
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $u\circ v=v\circ u=\operatorname{diag}(u)\cdot v=\operatorname{diag}(v)\cdot
    u$
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $u\circ v=v\circ u=\operatorname{diag}(u)\cdot v=\operatorname{diag}(v)\cdot
    u$
- en: â€¢
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $1$2
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2
- en: â€¢
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\operatorname{diag}(u)^{\top}=\operatorname{diag}(u)$
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\operatorname{diag}(u)^{\top}=\operatorname{diag}(u)$
- en: â€¢
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\operatorname{diag}(u)\cdot\operatorname{diag}(v)\cdot{\bf 1}_{n}=\operatorname{diag}(u)v$
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\operatorname{diag}(u)\cdot\operatorname{diag}(v)\cdot{\bf 1}_{n}=\operatorname{diag}(u)v$
- en: â€¢
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\operatorname{diag}(u\circ v)=\operatorname{diag}(u)\operatorname{diag}(v)$
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\operatorname{diag}(u\circ v)=\operatorname{diag}(u)\operatorname{diag}(v)$
- en: â€¢
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\operatorname{diag}(u)+\operatorname{diag}(v)=\operatorname{diag}(u+v)$
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\operatorname{diag}(u)+\operatorname{diag}(v)=\operatorname{diag}(u+v)$
- en: â€¢
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\langle u,v\rangle=\langle v,u\rangle$
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\langle u,v\rangle=\langle v,u\rangle$
- en: â€¢
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\langle u,v\rangle=u^{\top}v=v^{\top}u$
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\langle u,v\rangle=u^{\top}v=v^{\top}u$
- en: â€¢
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $1$2.
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2ã€‚
- en: Fact 4.2.
  id: totrans-184
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: äº‹å® 4.2ã€‚
- en: Let .
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ ã€‚
- en: For vectors $x,y\in\mathbb{R}^{n}$ we have
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå‘é‡ $x,y\in\mathbb{R}^{n}$ï¼Œæˆ‘ä»¬æœ‰
- en: â€¢
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|x\circ y\|_{2}\leq\|x\|_{\infty}\cdot\|y\|_{2}$
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|x\circ y\|_{2}\leq\|x\|_{\infty}\cdot\|y\|_{2}$
- en: â€¢
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|x\|_{\infty}\leq\|x\|_{2}\leq\sqrt{n}\|x\|_{\infty}$
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|x\|_{\infty}\leq\|x\|_{2}\leq\sqrt{n}\|x\|_{\infty}$
- en: â€¢
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|\exp(x)\|_{\infty}\leq\exp(\|x\|_{2})$
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|\exp(x)\|_{\infty}\leq\exp(\|x\|_{2})$
- en: â€¢
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|x+y\|_{2}\leq\|x\|_{2}+\|y\|_{2}$
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|x+y\|_{2}\leq\|x\|_{2}+\|y\|_{2}$
- en: â€¢
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|\alpha x\|_{2}\leq|\alpha|\cdot\|x\|_{2}$
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|\alpha x\|_{2}\leq|\alpha|\cdot\|x\|_{2}$
- en: â€¢
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: For any $\|x\|_{2},\|y\|_{2}\leq R$
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯¹äºä»»ä½• $\|x\|_{2},\|y\|_{2}\leq R$
- en: Fact 4.3.
  id: totrans-199
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: äº‹å® 4.3ã€‚
- en: For matrices $X,Y\in\mathbb{R}^{n\times n}$, we have
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºçŸ©é˜µ $X,Y\in\mathbb{R}^{n\times n}$ï¼Œæˆ‘ä»¬æœ‰
- en: â€¢
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|X^{\top}\|=\|X\|$
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|X^{\top}\|=\|X\|$
- en: â€¢
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|X\|\geq\|Y\|-\|X-Y\|$
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|X\|\geq\|Y\|-\|X-Y\|$
- en: â€¢
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|X+Y\|\leq\|X\|+\|Y\|$
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|X+Y\|\leq\|X\|+\|Y\|$
- en: â€¢
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|X\cdot Y\|\leq\|X\|\cdot\|Y\|$
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|X\cdot Y\|\leq\|X\|\cdot\|Y\|$
- en: â€¢
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: If $X\preceq\alpha\cdot Y$
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœ $X\preceq\alpha\cdot Y$
- en: â€¢
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|Yx\|_{2}\leq\|Y\|\cdot\|x\|_{2}$
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|Yx\|_{2}\leq\|Y\|\cdot\|x\|_{2}$
- en: Fact 4.4.
  id: totrans-213
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: äº‹å® 4.4ã€‚
- en: For any vectors $u,v\in\mathbb{R}^{n}$, we have
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä»»æ„å‘é‡ $u,v\in\mathbb{R}^{n}$ï¼Œæˆ‘ä»¬æœ‰
- en: â€¢
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 1\. $uu^{\top}\preceq\|u\|_{2}^{2}\cdot I_{n}$
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬1éƒ¨åˆ† $uu^{\top}\preceq\|u\|_{2}^{2}\cdot I_{n}$
- en: â€¢
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 2\. $\operatorname{diag}(u)\preceq\|u\|_{2}\cdot I_{n}$
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬2éƒ¨åˆ† $\operatorname{diag}(u)\preceq\|u\|_{2}\cdot I_{n}$
- en: â€¢
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 3\. $\operatorname{diag}(u\circ u)\preceq\|u\|_{2}^{2}\cdot I_{n}$
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬3éƒ¨åˆ† $\operatorname{diag}(u\circ u)\preceq\|u\|_{2}^{2}\cdot I_{n}$
- en: â€¢
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 4\. $uv^{\top}+vu^{\top}\preceq uu^{\top}+vv^{\top}$
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬4éƒ¨åˆ† $uv^{\top}+vu^{\top}\preceq uu^{\top}+vv^{\top}$
- en: â€¢
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 5\. $uv^{\top}+vu^{\top}\succeq-(uu^{\top}+vv^{\top})$
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬5éƒ¨åˆ† $uv^{\top}+vu^{\top}\succeq-(uu^{\top}+vv^{\top})$
- en: â€¢
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 6\. $(v\circ u)(v\circ u)^{\top}\preceq\|v\|^{2}_{\infty}uu^{\top}$
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬6éƒ¨åˆ† $(v\circ u)(v\circ u)^{\top}\preceq\|v\|^{2}_{\infty}uu^{\top}$
- en: â€¢
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 7\. $\operatorname{diag}(u\circ v)\preceq\|u\|_{2}\|v\|_{2}\cdot I_{n}$
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬7éƒ¨åˆ† $\operatorname{diag}(u\circ v)\preceq\|u\|_{2}\|v\|_{2}\cdot I_{n}$
- en: Fact 4.5.
  id: totrans-229
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: äº‹å® 4.5ã€‚
- en: Let $g,f:\mathbb{R}^{d}\to\mathbb{R}^{n}$.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ $g,f:\mathbb{R}^{d}\to\mathbb{R}^{n}$ã€‚
- en: Let $x\in\mathbb{R}^{d}$ be an arbitrary vector.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ $x\in\mathbb{R}^{d}$ ä¸ºä»»æ„å‘é‡ã€‚
- en: Let $a\in\mathbb{R}$ be an arbitrary real number.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ $a\in\mathbb{R}$ ä¸ºä»»æ„å®æ•°ã€‚
- en: Then, we have
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬æœ‰
- en: â€¢
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\frac{\mathrm{d}q(x)^{a}}{\mathrm{d}x}=a\cdot q(x)^{a-1}\cdot\frac{\mathrm{d}q(x)}{\mathrm{d}x}$
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\frac{\mathrm{d}q(x)^{a}}{\mathrm{d}x}=a\cdot q(x)^{a-1}\cdot\frac{\mathrm{d}q(x)}{\mathrm{d}x}$
- en: â€¢
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\frac{\mathrm{d}\|f(x)\|^{2}_{2}}{\mathrm{d}t}=2\langle f(x),\frac{\mathrm{d}f(x)}{\mathrm{d}t}\rangle$
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\frac{\mathrm{d}\|f(x)\|^{2}_{2}}{\mathrm{d}t}=2\langle f(x),\frac{\mathrm{d}f(x)}{\mathrm{d}t}\rangle$
- en: â€¢
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $1$2
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2
- en: â€¢
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $1$2 (product rule for Hadamard product)
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2ï¼ˆHadamardä¹˜ç§¯çš„ä¹˜æ³•è§„åˆ™ï¼‰
- en: 4.2 General Definitions
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 ä¸€èˆ¬å®šä¹‰
- en: In this section, we introduce some general definitions.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å¼•å…¥ä¸€äº›ä¸€èˆ¬å®šä¹‰ã€‚
- en: Definition 4.6  (Index summary).
  id: totrans-244
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å®šä¹‰ 4.6ï¼ˆç´¢å¼•æ€»ç»“ï¼‰ã€‚
- en: We use $i$ range, and $j$ range.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨ $i$ èŒƒå›´å’Œ $j$ èŒƒå›´ã€‚
- en: We use $i_{0},i_{1},i_{2}$, and $j_{0},j_{1},j_{2}$.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨ $i_{0},i_{1},i_{2}$ å’Œ $j_{0},j_{1},j_{2}$ã€‚
- en: Definition 4.7.
  id: totrans-247
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å®šä¹‰ 4.7ã€‚
- en: If the following conditions hold
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶
- en: â€¢
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $A_{1}\in\mathbb{R}^{n\times d}$
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $A_{1}\in\mathbb{R}^{n\times d}$
- en: â€¢
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $A_{2}\in\mathbb{R}^{n\times d}$
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $A_{2}\in\mathbb{R}^{n\times d}$
- en: â€¢
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $\mathsf{A}\in\mathbb{R}^{n^{2}\times d^{2}}$
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $\mathsf{A}\in\mathbb{R}^{n^{2}\times d^{2}}$
- en: â€“
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€“
- en: For each $j_{0}\in[n]$ to be one $n\times d^{2}$
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ª $j_{0}\in[n]$ï¼Œæ˜¯ä¸€ä¸ª $n\times d^{2}$
- en: â€¢
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $A_{3}\in\mathbb{R}^{n\times d}$
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $A_{3}\in\mathbb{R}^{n\times d}$
- en: â€¢
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $B\in\mathbb{R}^{n\times d}$ denote the $(j_{0},i_{0})$ for each $j_{0}\in[n]$
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $B\in\mathbb{R}^{n\times d}$ è¡¨ç¤ºæ¯ä¸ª $j_{0}\in[n]$ çš„ $(j_{0},i_{0})$
- en: â€¢
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $X\in\mathbb{R}^{d\times d}$
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $X\in\mathbb{R}^{d\times d}$
- en: 'Our final goal is to study the loss function, defined as:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„æœ€ç»ˆç›®æ ‡æ˜¯ç ”ç©¶æŸå¤±å‡½æ•°ï¼Œå®šä¹‰ä¸ºï¼š
- en: '|  | $1$2 |  |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­
- en: â€¢
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: we define $D(X)\in\mathbb{R}^{n\times n}$
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®šä¹‰ $D(X)\in\mathbb{R}^{n\times n}$
- en: â€¢
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: For each $j_{0}\in[n]$ to be $\langle\exp(\operatorname{\mathsf{A}}_{j_{0}}x),{\bf
    1}_{n}\rangle$ is the $j_{0}$ and $x\in\mathbb{R}^{d^{2}}$
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ª $j_{0}\in[n]$ï¼Œä½¿å¾— $\langle\exp(\operatorname{\mathsf{A}}_{j_{0}}x),{\bf
    1}_{n}\rangle$ æ˜¯ $j_{0}$ å’Œ $x\in\mathbb{R}^{d^{2}}$
- en: 'Further, for each $j_{0}\in[n],i_{0}\in[d]$ as follows:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œå¯¹äºæ¯ä¸ª $j_{0}\in[n],i_{0}\in[d]$ å¦‚ä¸‹ï¼š
- en: '|  | $1$2 |  |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Using tensor-trick in [[72](#bib.bib72), [73](#bib.bib73)], we can see that
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ [[72](#bib.bib72), [73](#bib.bib73)] ä¸­çš„å¼ é‡æŠ€å·§ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°
- en: '|  | $\displaystyle L(X,Y)=\sum_{j_{0}=1}^{n}\sum_{i_{0}=1}^{d}L(X,Y)_{j_{0},i_{0}}.$
    |  |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L(X,Y)=\sum_{j_{0}=1}^{n}\sum_{i_{0}=1}^{d}L(X,Y)_{j_{0},i_{0}}.$
    |  |'
- en: 4.3 Helpful Definitions With Respect to $X$
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 å…³äº $X$ çš„æœ‰ç”¨å®šä¹‰
- en: Now, we introduce a few helpful definitions related to $X\in\mathbb{R}^{d\times
    d}$.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å¼•å…¥ä¸€äº›ä¸ $X\in\mathbb{R}^{d\times d}$ ç›¸å…³çš„æœ‰ç”¨å®šä¹‰ã€‚
- en: Definition 4.8.
  id: totrans-276
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å®šä¹‰ 4.8ã€‚
- en: Let $\operatorname{\mathsf{A}}=A_{1}\otimes A_{2}\in\mathbb{R}^{n^{2}\times
    d^{2}}$, and $\operatorname{\mathsf{A}}_{j_{0}}\in\mathbb{R}^{n\times d^{2}}$
    block from $\operatorname{\mathsf{A}}$.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ $\operatorname{\mathsf{A}}=A_{1}\otimes A_{2}\in\mathbb{R}^{n^{2}\times d^{2}}$ï¼Œä¸”
    $\operatorname{\mathsf{A}}_{j_{0}}\in\mathbb{R}^{n\times d^{2}}$ æ˜¯ $\operatorname{\mathsf{A}}$
    çš„ä¸€ä¸ªå—ã€‚
- en: 'We define $u(x)_{j_{0}}:\mathbb{R}^{d^{2}}\rightarrow\mathbb{R}^{n}$ as follows:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®šä¹‰ $u(x)_{j_{0}}:\mathbb{R}^{d^{2}}\rightarrow\mathbb{R}^{n}$ å¦‚ä¸‹ï¼š
- en: '|  | $\displaystyle u(x)_{j_{0}}:=\underbrace{\exp(\operatorname{\mathsf{A}}_{j_{0}}x)}_{n\times
    1}.$ |  |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle u(x)_{j_{0}}:=\underbrace{\exp(\operatorname{\mathsf{A}}_{j_{0}}x)}_{n\times
    1}.$ |  |'
- en: Definition 4.9.
  id: totrans-280
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å®šä¹‰ 4.9ã€‚
- en: Let $\operatorname{\mathsf{A}}=A_{1}\otimes A_{2}\in\mathbb{R}^{n^{2}\times
    d^{2}}$, and $\operatorname{\mathsf{A}}_{j_{0}}\in\mathbb{R}^{n\times d^{2}}$
    block from $\operatorname{\mathsf{A}}$.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ $\operatorname{\mathsf{A}}=A_{1}\otimes A_{2}\in\mathbb{R}^{n^{2}\times d^{2}}$ï¼Œè€Œ
    $\operatorname{\mathsf{A}}_{j_{0}}\in\mathbb{R}^{n\times d^{2}}$ æ˜¯ä» $\operatorname{\mathsf{A}}$
    ä¸­çš„ä¸€ä¸ªå—ã€‚
- en: 'We define $\alpha(x)_{j_{0}}:\mathbb{R}^{d^{2}}\rightarrow\mathbb{R}$ as:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®šä¹‰ $\alpha(x)_{j_{0}}:\mathbb{R}^{d^{2}}\rightarrow\mathbb{R}$ å¦‚ä¸‹ï¼š
- en: '|  | $\displaystyle\alpha(x)_{j_{0}}:=\langle\underbrace{\exp(\operatorname{\mathsf{A}}_{j_{0}}x)}_{n\times
    1},\underbrace{{\bf 1}_{n}}_{n\times 1}\rangle.$ |  |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\alpha(x)_{j_{0}}:=\langle\underbrace{\exp(\operatorname{\mathsf{A}}_{j_{0}}x)}_{n\times
    1},\underbrace{{\bf 1}_{n}}_{n\times 1}\rangle.$ |  |'
- en: Definition 4.10.
  id: totrans-284
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å®šä¹‰ 4.10ã€‚
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as in DefinitionÂ [4.9](#S4.Thmtheorem9
    "Definition 4.9\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 'è®¾ $\alpha(x)_{j_{0}}\in\mathbb{R}$ æŒ‰å®šä¹‰Â [4.9](#S4.Thmtheorem9 "Definition 4.9\.
    â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") å®šä¹‰ã€‚'
- en: 'Let $u(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as in DefinitionÂ [4.8](#S4.Thmtheorem8
    "Definition 4.8\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 'è®¾ $u(x)_{j_{0}}\in\mathbb{R}^{n}$ æŒ‰å®šä¹‰Â [4.8](#S4.Thmtheorem8 "Definition 4.8\.
    â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") å®šä¹‰ã€‚'
- en: We define $f(x)_{j_{0}}:\mathbb{R}^{d^{2}}\rightarrow\mathbb{R}^{n}$
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®šä¹‰ $f(x)_{j_{0}}:\mathbb{R}^{d^{2}}\rightarrow\mathbb{R}^{n}$
- en: '|  | $\displaystyle f(x)_{j_{0}}:=\underbrace{\alpha(x)_{j_{0}}^{-1}}_{\mathrm{scalar}}\underbrace{u(x)_{j_{0}}}_{n\times
    1}.$ |  |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle f(x)_{j_{0}}:=\underbrace{\alpha(x)_{j_{0}}^{-1}}_{\mathrm{scalar}}\underbrace{u(x)_{j_{0}}}_{n\times
    1}.$ |  |'
- en: 4.4 A Helpful Definition With Respect to $Y$
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 å…³äº $Y$ çš„æœ‰ç”¨å®šä¹‰
- en: In this section, we introduce a helpful definition related to $Y\in\mathbb{R}^{d\times
    d}$.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸ $Y\in\mathbb{R}^{d\times d}$ ç›¸å…³çš„æœ‰ç”¨å®šä¹‰ã€‚
- en: Definition 4.11.
  id: totrans-291
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å®šä¹‰ 4.11ã€‚
- en: 'For each $i_{0}\in[d]$ as:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ª $i_{0}\in[d]$ï¼Œå¦‚ä¸‹ï¼š
- en: '|  | $\displaystyle h(Y)_{i_{0}}:=\underbrace{A_{3}}_{n\times d}\underbrace{Y_{*,i_{0}}}_{d\times
    1}.$ |  |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h(Y)_{i_{0}}:=\underbrace{A_{3}}_{n\times d}\underbrace{Y_{*,i_{0}}}_{d\times
    1}.$ |  |'
- en: 4.5 Helpful Definitions With Respect to Both $X$
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 å…³äº $X$ çš„æœ‰ç”¨å®šä¹‰
- en: In this section, we introduce some helpful definitions related to both $X\in\mathbb{R}^{d\times
    d}$.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€äº›ä¸ $X\in\mathbb{R}^{d\times d}$ ç›¸å…³çš„æœ‰ç”¨å®šä¹‰ã€‚
- en: Definition 4.12.
  id: totrans-296
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å®šä¹‰ 4.12ã€‚
- en: 'We define $c(x,y)_{j_{0},i_{0}}:\mathbb{R}^{d^{2}}\times\mathbb{R}^{d^{2}}\rightarrow\mathbb{R}$
    as follows:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®šä¹‰ $c(x,y)_{j_{0},i_{0}}:\mathbb{R}^{d^{2}}\times\mathbb{R}^{d^{2}}\rightarrow\mathbb{R}$
    å¦‚ä¸‹ï¼š
- en: '|  | $\displaystyle c(x,y)_{j_{0},i_{0}}:=\langle f(x)_{j_{0}},h(y)_{i_{0}}\rangle-b_{j_{0},i_{0}}.$
    |  |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle c(x,y)_{j_{0},i_{0}}:=\langle f(x)_{j_{0}},h(y)_{i_{0}}\rangle-b_{j_{0},i_{0}}.$
    |  |'
- en: Furthermore, we define $c(x,:)_{j_{0},i_{0}}$ as follows
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæˆ‘ä»¬å®šä¹‰ $c(x,:)_{j_{0},i_{0}}$ å¦‚ä¸‹
- en: '|  | $\displaystyle c(x,:)_{j_{0},i_{0}}:=\langle f(x)_{j_{0}},v\rangle-b_{j_{0},i_{0}}$
    |  |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle c(x,:)_{j_{0},i_{0}}:=\langle f(x)_{j_{0}},v\rangle-b_{j_{0},i_{0}}$
    |  |'
- en: for some fixed vector $v\in\mathbb{R}^{n}$ and also doesnâ€™t depend on $y$.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæŸä¸ªå›ºå®šå‘é‡ $v\in\mathbb{R}^{n}$ï¼Œä¸”ä¹Ÿä¸ä¾èµ–äº $y$ã€‚
- en: Similarly, we also define $c(:,y)_{j_{0},i_{0}}$ as follows
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼åœ°ï¼Œæˆ‘ä»¬è¿˜å®šä¹‰ $c(:,y)_{j_{0},i_{0}}$ å¦‚ä¸‹
- en: '|  | $\displaystyle c(:,y)_{j_{0},i_{0}}:=\langle v,h(y)_{i_{0}}\rangle-b_{j_{0},i_{0}}$
    |  |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle c(:,y)_{j_{0},i_{0}}:=\langle v,h(y)_{i_{0}}\rangle-b_{j_{0},i_{0}}$
    |  |'
- en: for some fixed vector $v\in\mathbb{R}^{n}$ and also doesnâ€™t depend on $y$.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæŸä¸ªå›ºå®šå‘é‡ $v\in\mathbb{R}^{n}$ï¼Œä¸”ä¹Ÿä¸ä¾èµ–äº $y$ã€‚
- en: Definition 4.13.
  id: totrans-305
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å®šä¹‰ 4.13ã€‚
- en: We define
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®šä¹‰
- en: '|  | $\displaystyle L(x,:)_{j_{0},i_{0}}:=0.5c(x,:)_{j_{0},i_{0}}^{2}$ |  |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L(x,:)_{j_{0},i_{0}}:=0.5c(x,:)_{j_{0},i_{0}}^{2}$ |  |'
- en: and
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: å’Œ
- en: '|  | $\displaystyle L(:,y)_{j_{0},i_{0}}:=0.5c(:,y)_{j_{0},i_{0}}^{2}$ |  |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L(:,y)_{j_{0},i_{0}}:=0.5c(:,y)_{j_{0},i_{0}}^{2}$ |  |'
- en: and
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: å’Œ
- en: '|  | $\displaystyle L(x,y)_{j_{0},i_{0}}:=0.5c(x,y)_{j_{0},i_{0}}^{2}$ |  |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L(x,y)_{j_{0},i_{0}}:=0.5c(x,y)_{j_{0},i_{0}}^{2}$ |  |'
- en: 4.6 Regularization
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 æ­£åˆ™åŒ–
- en: In this section, we define the regularization loss we use.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰æˆ‘ä»¬ä½¿ç”¨çš„æ­£åˆ™åŒ–æŸå¤±ã€‚
- en: Definition 4.14.
  id: totrans-314
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å®šä¹‰ 4.14ã€‚
- en: Let $W\in\mathbb{R}^{n\times n}$ denote a positive diagonal matrix. We use the
    following regularization loss
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ $W\in\mathbb{R}^{n\times n}$ ä¸ºæ­£å¯¹è§’çŸ©é˜µã€‚æˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹æ­£åˆ™åŒ–æŸå¤±
- en: '|  | $\displaystyle\&#124;(W\otimes I)(A_{1}\otimes A_{2})x\&#124;_{2}^{2}+\&#124;WA_{3}y\&#124;_{F}^{2}$
    |  |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;(W\otimes I)(A_{1}\otimes A_{2})x\&#124;_{2}^{2}+\&#124;WA_{3}y\&#124;_{F}^{2}$
    |  |'
- en: Note that $1$2.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ $1$2ã€‚
- en: 4.7 Fast Matrix Multiplication
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7 å¿«é€ŸçŸ©é˜µä¹˜æ³•
- en: We use ${\cal T}_{\mathrm{mat}}(a,b,c)$ matrix with another $b\times c$ matrix.
    Fast matrix multiplication [[44](#bib.bib44), [182](#bib.bib182), [105](#bib.bib105),
    [78](#bib.bib78), [34](#bib.bib34), [15](#bib.bib15), [62](#bib.bib62), [106](#bib.bib106),
    [189](#bib.bib189)] is a fundamental tool in theoretical computer science.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨ ${\cal T}_{\mathrm{mat}}(a,b,c)$ çŸ©é˜µä¸å¦ä¸€ä¸ª $b\times c$ çŸ©é˜µã€‚å¿«é€ŸçŸ©é˜µä¹˜æ³• [[44](#bib.bib44),
    [182](#bib.bib182), [105](#bib.bib105), [78](#bib.bib78), [34](#bib.bib34), [15](#bib.bib15),
    [62](#bib.bib62), [106](#bib.bib106), [189](#bib.bib189)] æ˜¯ç†è®ºè®¡ç®—æœºç§‘å­¦ä¸­çš„ä¸€ä¸ªåŸºæœ¬å·¥å…·ã€‚
- en: Fact 4.15.
  id: totrans-320
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: äº‹å® 4.15ã€‚
- en: ${\cal T}_{\mathrm{mat}}(a,b,c)=O({\cal T}_{\mathrm{mat}}(b,a,c))=O({\cal T}_{\mathrm{mat}}(a,c,b))$.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: ${\cal T}_{\mathrm{mat}}(a,b,c)=O({\cal T}_{\mathrm{mat}}(b,a,c))=O({\cal T}_{\mathrm{mat}}(a,c,b))$ã€‚
- en: For $k\in\mathbb{R}_{+}$ to be the value such that $\forall n\in\mathbb{N}$.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº $k\in\mathbb{R}_{+}$ï¼Œä½¿å¾— $\forall n\in\mathbb{N}$ã€‚
- en: For convenience, we define three special values of $\omega(k)$ to be the fast
    matrix multiplication exponent, i.e., $\omega:=\omega(1)$ to be the dual exponent
    of matrix multiplication, i.e., $\omega(\alpha)=2$.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œæˆ‘ä»¬å®šä¹‰ $\omega(k)$ çš„ä¸‰ä¸ªç‰¹æ®Šå€¼ä¸ºå¿«é€ŸçŸ©é˜µä¹˜æ³•çš„æŒ‡æ•°ï¼Œå³ $\omega:=\omega(1)$ ä½œä¸ºçŸ©é˜µä¹˜æ³•çš„å¯¹å¶æŒ‡æ•°ï¼Œå³
    $\omega(\alpha)=2$ã€‚
- en: The following fact can be found in Lemma 3.6 of [[88](#bib.bib88)], also see
    [[21](#bib.bib21)].
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢çš„äº‹å®å¯ä»¥åœ¨ [[88](#bib.bib88)] çš„å¼•ç† 3.6 ä¸­æ‰¾åˆ°ï¼Œä¹Ÿå‚è§ [[21](#bib.bib21)]ã€‚
- en: Fact 4.16  (Convexity of $\omega(k)$).
  id: totrans-325
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: äº‹å® 4.16  (å‡½æ•° $\omega(k)$ çš„å‡¸æ€§)ã€‚
- en: The function $\omega(k)$ is convex.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: å‡½æ•° $\omega(k)$ æ˜¯å‡¸çš„ã€‚
- en: 5 Gradient
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 æ¢¯åº¦
- en: 'In SectionÂ [5.1](#S5.SS1 "5.1 Gradient for ğ‘¥ â€£ 5 Gradient â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), we show the gradient with respect
    to variables $x$. In SectionÂ [5.3](#S5.SS3 "5.3 Computation of ğ‘,ğ‘“,â„ â€£ 5 Gradient
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), we compute
    running time of $c,f,h$ to compute time complexity. In SectionÂ [5.5](#S5.SS5 "5.5
    Reformulating Gradient (ğ‘¦) in Matrix View â€£ 5 Gradient â€£ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time"), we reformulate the gradient with respect
    to $Y$ to compute time complexity.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨ç¬¬ [5.1](#S5.SS1 "5.1 Gradient for ğ‘¥ â€£ 5 Gradient â€£ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time") èŠ‚ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å…³äºå˜é‡ $x$ çš„æ¢¯åº¦ã€‚åœ¨ç¬¬ [5.3](#S5.SS3
    "5.3 Computation of ğ‘,ğ‘“,â„ â€£ 5 Gradient â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—äº† $c,f,h$ çš„è¿è¡Œæ—¶é—´ä»¥è®¡ç®—æ—¶é—´å¤æ‚åº¦ã€‚åœ¨ç¬¬ [5.5](#S5.SS5 "5.5
    Reformulating Gradient (ğ‘¦) in Matrix View â€£ 5 Gradient â€£ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time") èŠ‚ä¸­ï¼Œæˆ‘ä»¬é‡æ–°è¡¨è¿°äº†å…³äº $Y$ çš„æ¢¯åº¦ä»¥è®¡ç®—æ—¶é—´å¤æ‚åº¦ã€‚'
- en: 5.1 Gradient for $x$
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 å¯¹ $x$ çš„æ¢¯åº¦
- en: In this section, we compute the gradient for $x$. Most of the following gradient
    computations can be found in [[72](#bib.bib72), [73](#bib.bib73)].
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®¡ç®— $x$ çš„æ¢¯åº¦ã€‚å¤§å¤šæ•°åç»­çš„æ¢¯åº¦è®¡ç®—å¯ä»¥åœ¨ [[72](#bib.bib72), [73](#bib.bib73)] ä¸­æ‰¾åˆ°ã€‚
- en: Lemma 5.1  (Gradient with respect to $x$).
  id: totrans-331
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 5.1  (å…³äº $x$ çš„æ¢¯åº¦)ã€‚
- en: If the following conditions hold
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶
- en: â€¢
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: For each $i\in[d^{2}]$ denote the $i$
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ª $i\in[d^{2}]$ï¼Œè¡¨ç¤º $i$
- en: â€¢
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $u(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.8](#S4.Thmtheorem8
    "Definition 4.8\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ä»¤ $u(x)_{j_{0}}\in\mathbb{R}^{n}$ å¦‚å®šä¹‰[4.8](#S4.Thmtheorem8 "Definition 4.8\.
    â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") å®šä¹‰ã€‚'
- en: â€¢
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ä»¤ $\alpha(x)_{j_{0}}\in\mathbb{R}$ å¦‚å®šä¹‰[4.9](#S4.Thmtheorem9 "Definition 4.9\.
    â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") å®šä¹‰ã€‚'
- en: â€¢
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $f(x)_{j_{0}}\in\mathbb{R}^{n}$ å¦‚å®šä¹‰ [4.10](#S4.Thmtheorem10 "å®šä¹‰ 4.10ã€‚ â€£ 4.3
    ä¸ ğ‘‹ ç›¸å…³çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°æ„å»º LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")
- en: â€¢
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4
    Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ å¦‚å®šä¹‰ [4.12](#S4.Thmtheorem12 "å®šä¹‰ 4.12ã€‚
    â€£ 4.5 ä¸ ğ‘‹ å’Œ ğ‘Œ ç›¸å…³çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°æ„å»º LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")
- en: â€¢
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $L(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.13](#S4.Thmtheorem13
    "Definition 4.13\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4
    Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $L(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ å¦‚å®šä¹‰ [4.13](#S4.Thmtheorem13 "å®šä¹‰ 4.13ã€‚
    â€£ 4.5 ä¸ ğ‘‹ å’Œ ğ‘Œ ç›¸å…³çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°æ„å»º LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")
- en: Then, for each $i\in[d^{2}]$, we have
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œå¯¹äºæ¯ä¸ª $i\in[d^{2}]$ï¼Œæˆ‘ä»¬æœ‰
- en: â€¢
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 1.
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ 1 éƒ¨åˆ†ã€‚
- en: '|  | $\displaystyle\frac{\mathrm{d}u(x)_{j_{0}}}{\mathrm{d}x_{i}}=u(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i}$
    |  |'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}u(x)_{j_{0}}}{\mathrm{d}x_{i}}=u(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i}$
    |  |'
- en: â€¢
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 2.
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ 2 éƒ¨åˆ†ã€‚
- en: '|  | $\displaystyle\frac{\mathrm{d}\alpha(x)_{j_{0}}}{\mathrm{d}x_{i}}=\langle
    u(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{i_{0},i},{\bf 1}_{n}\rangle$ |  |'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}\alpha(x)_{j_{0}}}{\mathrm{d}x_{i}}=\langle
    u(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{i_{0},i},{\bf 1}_{n}\rangle$ |  |'
- en: â€¢
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 3.
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ 3 éƒ¨åˆ†ã€‚
- en: '|  | $1$2 |  |'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: â€¢
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 4. For a fixed vector $v\in\mathbb{R}^{n}$), we have
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ 4 éƒ¨åˆ†ã€‚å¯¹äºå›ºå®šå‘é‡ $v\in\mathbb{R}^{n}$ï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $1$2 |  |'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '![Refer to caption](img/1863170410eedf795b70b046b0768e9c.png)'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![å‚è€ƒæ ‡é¢˜](img/1863170410eedf795b70b046b0768e9c.png)'
- en: 'Figure 6: The visualization of Part 4 of LemmaÂ [5.1](#S5.Thmtheorem1 "Lemma
    5.1 (Gradient with respect to ğ‘¥). â€£ 5.1 Gradient for ğ‘¥ â€£ 5 Gradient â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"). We are given $f(x)_{j_{0}},v,\operatorname{\mathsf{A}}_{j_{0},i}\in\mathbb{R}^{n}$
    and $v$. For the right-hand side, we have three steps. Step 1: we compute the
    Hadamard product of $f(x)_{j_{0}}$. Step 2: We find the inner product of this
    Hadamard product and $v$ and $v$ and $\operatorname{\mathsf{A}}_{j_{0},i}$. The
    red rectangles represent the vector $v$.'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'å›¾ 6: å¼•ç”¨å®šç†Â [5.1](#S5.Thmtheorem1 "å®šç† 5.1ï¼ˆç›¸å¯¹äº ğ‘¥ çš„æ¢¯åº¦ï¼‰ã€‚ â€£ 5.1 ğ‘¥ çš„æ¢¯åº¦ â€£ 5 æ¢¯åº¦ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ
    SVM æŠ€å·§é‡æ–°æ„å»º LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£") çš„ç¬¬ 4 éƒ¨åˆ†å¯è§†åŒ–ã€‚æˆ‘ä»¬ç»™å®š $f(x)_{j_{0}},v,\operatorname{\mathsf{A}}_{j_{0},i}\in\mathbb{R}^{n}$
    å’Œ $v$ã€‚å¯¹äºå³ä¾§ï¼Œæœ‰ä¸‰ä¸ªæ­¥éª¤ã€‚æ­¥éª¤ 1ï¼šè®¡ç®— $f(x)_{j_{0}}$ çš„ Hadamard ä¹˜ç§¯ã€‚æ­¥éª¤ 2ï¼šæ‰¾åˆ°è¯¥ Hadamard ä¹˜ç§¯ä¸ $v$
    çš„å†…ç§¯ï¼Œä»¥åŠ $v$ å’Œ $\operatorname{\mathsf{A}}_{j_{0},i}$ çš„å†…ç§¯ã€‚çº¢è‰²çŸ©å½¢è¡¨ç¤ºå‘é‡ $v$ã€‚'
- en: â€¢
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 5. For each $i_{0}\in[d]$
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ 5 éƒ¨åˆ†ã€‚å¯¹äºæ¯ä¸ª $i_{0}\in[d]$
- en: '|  | $1$2 |  |'
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: â€¢
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 6.
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ 6 éƒ¨åˆ†ã€‚
- en: '|  | $1$2 |  |'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: â€¢
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 7. (for hessian diagonal term)
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ 7 éƒ¨åˆ†ã€‚ï¼ˆå¯¹äº Hessian å¯¹è§’çº¿é¡¹ï¼‰
- en: '|  | $1$2 |  |'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '![Refer to caption](img/506215cafa5031425b4de08e4ebc4f1b.png)'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![å‚è€ƒæ ‡é¢˜](img/506215cafa5031425b4de08e4ebc4f1b.png)'
- en: 'Figure 7: The visualization of Part 7 of LemmaÂ [5.1](#S5.Thmtheorem1 "Lemma
    5.1 (Gradient with respect to ğ‘¥). â€£ 5.1 Gradient for ğ‘¥ â€£ 5 Gradient â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"). We are given $f(x)_{j_{0}},v,\operatorname{\mathsf{A}}_{j_{0},i}\in\mathbb{R}^{n}$
    and $\operatorname{\mathsf{A}}_{j_{0},i}$ with respect to $x_{i}\in\mathbb{R}$
    and $v$ and $v$ and $\operatorname{\mathsf{A}}_{j_{0},i}$. The red rectangles
    represent the vector $v$.'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'å›¾ 7: å¼•ç”¨å®šç†Â [5.1](#S5.Thmtheorem1 "å®šç† 5.1ï¼ˆç›¸å¯¹äº ğ‘¥ çš„æ¢¯åº¦ï¼‰ã€‚ â€£ 5.1 ğ‘¥ çš„æ¢¯åº¦ â€£ 5 æ¢¯åº¦ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ
    SVM æŠ€å·§é‡æ–°æ„å»º LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£") çš„ç¬¬ 7 éƒ¨åˆ†å¯è§†åŒ–ã€‚æˆ‘ä»¬ç»™å®š $f(x)_{j_{0}},v,\operatorname{\mathsf{A}}_{j_{0},i}\in\mathbb{R}^{n}$
    å’Œ $\operatorname{\mathsf{A}}_{j_{0},i}$ å…³äº $x_{i}\in\mathbb{R}$ å’Œ $v$ ä»¥åŠ $v$ å’Œ
    $\operatorname{\mathsf{A}}_{j_{0},i}$ã€‚çº¢è‰²çŸ©å½¢è¡¨ç¤ºå‘é‡ $v$ã€‚'
- en: â€¢
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 8. (for hessian off-diagonal term)
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ 8 éƒ¨åˆ†ã€‚ï¼ˆå¯¹äº Hessian éå¯¹è§’çº¿é¡¹ï¼‰
- en: '|  | $1$2 |  |'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: â€¢
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 9 (for hessian diagonal term, this can be obtained by using Part 4 as a
    black-box)
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬9éƒ¨åˆ†ï¼ˆå¯¹äº Hessian å¯¹è§’çº¿é¡¹ï¼Œè¿™å¯ä»¥é€šè¿‡ä½¿ç”¨ç¬¬4éƒ¨åˆ†ä½œä¸ºé»‘ç®±è·å¾—ï¼‰
- en: '|  | $1$2 |  |'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '![Refer to caption](img/7789ea533c8b22818e08c27ec76bbfba.png)'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/7789ea533c8b22818e08c27ec76bbfba.png)'
- en: 'Figure 8: The visualization of Part 9 of LemmaÂ [5.1](#S5.Thmtheorem1 "Lemma
    5.1 (Gradient with respect to ğ‘¥). â€£ 5.1 Gradient for ğ‘¥ â€£ 5 Gradient â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"). We are given $f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\in\mathbb{R}^{n}$
    and $\operatorname{\mathsf{A}}_{j_{0},i}$. For the right-hand side, we have three
    steps. Step 1: we compute the Hadamard product of $\operatorname{\mathsf{A}}_{j_{0},i}$.
    Step 2: We find the inner product of $f(x)_{j_{0}}$ and $\operatorname{\mathsf{A}}_{j_{0},i}$.
    The green rectangles represent the vector $\operatorname{\mathsf{A}}_{j_{0},i}$.'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'å›¾8ï¼šå¼•ç†Â [5.1](#S5.Thmtheorem1 "Lemma 5.1 (Gradient with respect to ğ‘¥). â€£ 5.1
    Gradient for ğ‘¥ â€£ 5 Gradient â€£ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") ç¬¬9éƒ¨åˆ†çš„å¯è§†åŒ–ã€‚æˆ‘ä»¬ç»™å‡º $f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\in\mathbb{R}^{n}$
    å’Œ $\operatorname{\mathsf{A}}_{j_{0},i}$ã€‚å¯¹äºå³ä¾§ï¼Œæœ‰ä¸‰ä¸ªæ­¥éª¤ã€‚æ­¥éª¤1ï¼šæˆ‘ä»¬è®¡ç®— $\operatorname{\mathsf{A}}_{j_{0},i}$
    çš„ Hadamard ä¹˜ç§¯ã€‚æ­¥éª¤2ï¼šæˆ‘ä»¬æ‰¾åˆ° $f(x)_{j_{0}}$ å’Œ $\operatorname{\mathsf{A}}_{j_{0},i}$
    çš„å†…ç§¯ã€‚ç»¿è‰²çŸ©å½¢è¡¨ç¤ºå‘é‡ $\operatorname{\mathsf{A}}_{j_{0},i}$ã€‚'
- en: â€¢
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 10 (for hessian off-diagonal term, this can be obtained by using Part 4
    as a black-box)
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬10éƒ¨åˆ†ï¼ˆå¯¹äº Hessian éå¯¹è§’çº¿é¡¹ï¼Œè¿™å¯ä»¥é€šè¿‡ä½¿ç”¨ç¬¬4éƒ¨åˆ†ä½œä¸ºé»‘ç®±è·å¾—ï¼‰
- en: '|  | $1$2 |  |'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Proof.
  id: totrans-382
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: Proof of Part 1. See Part 4 of Proof of Lemma 5.18 in [[72](#bib.bib72)] (Page
    14).
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬1éƒ¨åˆ†çš„è¯æ˜ã€‚å‚è§ [[72](#bib.bib72)] çš„ç¬¬4éƒ¨åˆ†çš„å¼•ç† 5.18 çš„è¯æ˜ï¼ˆç¬¬14é¡µï¼‰ã€‚
- en: Proof of Part 2. See Part 5 of Proof of Lemma 5.18 in [[72](#bib.bib72)] (Page
    14).
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬2éƒ¨åˆ†çš„è¯æ˜ã€‚å‚è§ [[72](#bib.bib72)] çš„ç¬¬5éƒ¨åˆ†çš„å¼•ç† 5.18 çš„è¯æ˜ï¼ˆç¬¬14é¡µï¼‰ã€‚
- en: Proof of Part 3. See Part 9 of Proof of Lemma 5.18 in [[72](#bib.bib72)] (page
    15).
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬3éƒ¨åˆ†çš„è¯æ˜ã€‚å‚è§ [[72](#bib.bib72)] çš„ç¬¬9éƒ¨åˆ†çš„å¼•ç† 5.18 çš„è¯æ˜ï¼ˆç¬¬15é¡µï¼‰ã€‚
- en: Proof of Part 4. See Part 14 of Proof of Lemma 5.18 in [[72](#bib.bib72)] (page
    15).
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬4éƒ¨åˆ†çš„è¯æ˜ã€‚å‚è§ [[72](#bib.bib72)] çš„ç¬¬14éƒ¨åˆ†çš„å¼•ç† 5.18 çš„è¯æ˜ï¼ˆç¬¬15é¡µï¼‰ã€‚
- en: Proof of Part 5.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬5éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: 'Note that by DefinitionÂ [4.12](#S4.Thmtheorem12 "Definition 4.12\. â€£ 4.5 Helpful
    Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), we have'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ³¨æ„ï¼Œæ ¹æ®å®šä¹‰Â [4.12](#S4.Thmtheorem12 "Definition 4.12\. â€£ 4.5 Helpful Definitions
    With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")ï¼Œæˆ‘ä»¬æœ‰'
- en: '|  | $\displaystyle c(x,:)_{j_{0},i_{0}}:=\langle f(x)_{j_{0}},v\rangle-b_{j_{0},i_{0}}$
    |  | (3) |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle c(x,:)_{j_{0},i_{0}}:=\langle f(x)_{j_{0}},v\rangle-b_{j_{0},i_{0}}$
    |  | (3) |'
- en: Therefore, we have
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\frac{\mathrm{d}c(x,:)_{j_{0},i_{0}}}{\mathrm{d}x_{i}}=$
    |  |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}c(x,:)_{j_{0},i_{0}}}{\mathrm{d}x_{i}}=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step comes from Eq.Â ([3](#S5.E3 "In Proof. â€£ 5.1 Gradient for
    ğ‘¥ â€£ 5 Gradient â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")), the second step follows from $\frac{\mathrm{d}b_{j_{0},i_{0}}}{\mathrm{d}x_{i}}=0$,
    and the third step is due to Part 4.'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ç¬¬ä¸€æ­¥æ¥è‡ªäº Eq.Â ([3](#S5.E3 "In Proof. â€£ 5.1 Gradient for ğ‘¥ â€£ 5 Gradient â€£ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time"))ï¼Œç¬¬äºŒæ­¥æ˜¯å› ä¸º $\frac{\mathrm{d}b_{j_{0},i_{0}}}{\mathrm{d}x_{i}}=0$ï¼Œç¬¬ä¸‰æ­¥æ˜¯ç”±äºç¬¬4éƒ¨åˆ†ã€‚'
- en: 'Proof of Part 6. Noted that by DefinitionÂ [4.13](#S4.Thmtheorem13 "Definition
    4.13\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), we have'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¬¬6éƒ¨åˆ†çš„è¯æ˜ã€‚æ³¨æ„ï¼Œæ ¹æ®å®šä¹‰Â [4.13](#S4.Thmtheorem13 "Definition 4.13\. â€£ 4.5 Helpful Definitions
    With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")ï¼Œæˆ‘ä»¬æœ‰'
- en: '|  | $\displaystyle L(x,:)_{j_{0},i_{0}}=0.5c(x,:)_{j_{0},i_{0}}^{2}$ |  |
    (4) |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L(x,:)_{j_{0},i_{0}}=0.5c(x,:)_{j_{0},i_{0}}^{2}$ |  |
    (4) |'
- en: Therefore, we have
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\frac{\mathrm{d}L(x,:)_{j_{0},i_{0}}}{\mathrm{d}x_{i}}=$
    |  |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}L(x,:)_{j_{0},i_{0}}}{\mathrm{d}x_{i}}=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step is due to Eq.Â ([4](#S5.E4 "In Proof. â€£ 5.1 Gradient for
    ğ‘¥ â€£ 5 Gradient â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")), the second step is because of chain rule of derivative, the last step
    comes from Part 5.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ç¬¬ä¸€æ­¥æ˜¯ç”±äºå…¬å¼Â ([4](#S5.E4 "åœ¨è¯æ˜ â€£ 5.1 æ¢¯åº¦å¯¹äº ğ‘¥ â€£ 5 æ¢¯åº¦ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§çš„å•å±‚æ³¨æ„åŠ›é‡æ„åŠçŸ©é˜µä¹˜æ³•æ—¶é—´è§£å†³æ–¹æ¡ˆ")),
    ç¬¬äºŒæ­¥æ˜¯ç”±äºå¯¼æ•°é“¾å¼æ³•åˆ™ï¼Œæœ€åä¸€æ­¥æ¥è‡ªç¬¬5éƒ¨åˆ†ã€‚
- en: Proof of Part 7.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬7éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: We have
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\frac{\mathrm{d}\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle}{\mathrm{d}x_{i}}=$
    |  |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle}{\mathrm{d}x_{i}}=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step is due to FactÂ [4.5](#S4.Thmtheorem5 "Fact 4.5\. â€£ 4.1
    Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), the second step comes from FactÂ [4.5](#S4.Thmtheorem5 "Fact 4.5\. â€£ 4.1
    Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), the third step is because of Part 4, the fourth step is owing to simple
    algebra, the fifth step follows from FactÂ [4.1](#S4.Thmtheorem1 "Fact 4.1\. â€£
    4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), and the last step comes from FactÂ [4.1](#S4.Thmtheorem1
    "Fact 4.1\. â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ç¬¬ä¸€æ­¥æ˜¯ç”±äºäº‹å®Â [4.5](#S4.Thmtheorem5 "äº‹å® 4.5\. â€£ 4.1 åŸºæœ¬äº‹å® â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§çš„å•å±‚æ³¨æ„åŠ›é‡æ„åŠçŸ©é˜µä¹˜æ³•æ—¶é—´è§£å†³æ–¹æ¡ˆ"),
    ç¬¬äºŒæ­¥æ¥è‡ªäº‹å®Â [4.5](#S4.Thmtheorem5 "äº‹å® 4.5\. â€£ 4.1 åŸºæœ¬äº‹å® â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§çš„å•å±‚æ³¨æ„åŠ›é‡æ„åŠçŸ©é˜µä¹˜æ³•æ—¶é—´è§£å†³æ–¹æ¡ˆ"),
    ç¬¬ä¸‰æ­¥æ˜¯ç”±äºç¬¬4éƒ¨åˆ†ï¼Œç¬¬å››æ­¥æ˜¯ç”±äºç®€å•çš„ä»£æ•°ï¼Œç¬¬äº”æ­¥ç”±äº‹å®Â [4.1](#S4.Thmtheorem1 "äº‹å® 4.1\. â€£ 4.1 åŸºæœ¬äº‹å® â€£ 4 åˆæ­¥
    â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§çš„å•å±‚æ³¨æ„åŠ›é‡æ„åŠçŸ©é˜µä¹˜æ³•æ—¶é—´è§£å†³æ–¹æ¡ˆ")ï¼Œæœ€åä¸€æ­¥æ¥è‡ªäº‹å®Â [4.1](#S4.Thmtheorem1 "äº‹å®
    4.1\. â€£ 4.1 åŸºæœ¬äº‹å® â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§çš„å•å±‚æ³¨æ„åŠ›é‡æ„åŠçŸ©é˜µä¹˜æ³•æ—¶é—´è§£å†³æ–¹æ¡ˆ")ã€‚
- en: Proof of Part 8.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬8éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: We have
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\frac{\mathrm{d}\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle}{\mathrm{d}x_{l}}=$
    |  |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle}{\mathrm{d}x_{l}}=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step comes from FactÂ [4.5](#S4.Thmtheorem5 "Fact 4.5\. â€£ 4.1
    Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), the second step is because of FactÂ [4.5](#S4.Thmtheorem5 "Fact 4.5\. â€£
    4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), the third step follows from Part 4, the fourth step is
    due to simple algebra, the fifth step is owing to FactÂ [4.1](#S4.Thmtheorem1 "Fact
    4.1\. â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), and the last step comes from FactÂ [4.1](#S4.Thmtheorem1
    "Fact 4.1\. â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¬¬ä¸€æ­¥æ¥æºäºäº‹å®[4.5](#S4.Thmtheorem5 "Fact 4.5\. â€£ 4.1 Basic Facts â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")ï¼Œç¬¬äºŒæ­¥å› ä¸ºäº‹å®[4.5](#S4.Thmtheorem5
    "Fact 4.5\. â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")ï¼Œç¬¬ä¸‰æ­¥æ¥è‡ªç¬¬4éƒ¨åˆ†ï¼Œç¬¬å››æ­¥ç”±äºç®€å•çš„ä»£æ•°ï¼Œç¬¬äº”æ­¥ç”±äºäº‹å®[4.1](#S4.Thmtheorem1
    "Fact 4.1\. â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")ï¼Œæœ€åä¸€æ­¥æ¥æºäºäº‹å®[4.1](#S4.Thmtheorem1 "Fact 4.1\. â€£ 4.1
    Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")ã€‚'
- en: Proof of Part 9.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬9éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: We have
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\frac{\mathrm{d}\langle f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle}{\mathrm{d}x_{i}}=$
    |  |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}\langle f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle}{\mathrm{d}x_{i}}=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step is due to FactÂ [4.5](#S4.Thmtheorem5 "Fact 4.5\. â€£ 4.1
    Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), the second step comes from Part 4, and the last step is because of FactÂ [4.1](#S4.Thmtheorem1
    "Fact 4.1\. â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¬¬ä¸€æ­¥ç”±äºäº‹å®[4.5](#S4.Thmtheorem5 "Fact 4.5\. â€£ 4.1 Basic Facts â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")ï¼Œç¬¬äºŒæ­¥æ¥æºäºç¬¬4éƒ¨åˆ†ï¼Œè€Œæœ€åä¸€æ­¥å› ä¸ºäº‹å®[4.1](#S4.Thmtheorem1
    "Fact 4.1\. â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")ã€‚'
- en: Proof of Part 10. We have
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬10éƒ¨åˆ†çš„è¯æ˜ã€‚æˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\frac{\mathrm{d}\langle f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle}{\mathrm{d}x_{l}}=$
    |  |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}\langle f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle}{\mathrm{d}x_{l}}=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step comes from FactÂ [4.5](#S4.Thmtheorem5 "Fact 4.5\. â€£ 4.1
    Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), the second step is owing to Part 4, and the last step is due to FactÂ [4.1](#S4.Thmtheorem1
    "Fact 4.1\. â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"). âˆ'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¬¬ä¸€æ­¥æ¥æºäºäº‹å®[4.5](#S4.Thmtheorem5 "Fact 4.5\. â€£ 4.1 Basic Facts â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")ï¼Œç¬¬äºŒæ­¥ç”±äºç¬¬4éƒ¨åˆ†ï¼Œè€Œæœ€åä¸€æ­¥ç”±äºäº‹å®[4.1](#S4.Thmtheorem1
    "Fact 4.1\. â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")ã€‚âˆ'
- en: 5.2 Gradient With Respect to $y$
  id: totrans-431
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 å…³äº$y$çš„æ¢¯åº¦
- en: In this section, we compute the gradient with respect to $y$.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—å…³äº$y$çš„æ¢¯åº¦ã€‚
- en: Lemma 5.2.
  id: totrans-433
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç†5.2ã€‚
- en: If the following conditions hold
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶
- en: â€¢
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $v\in\mathbb{R}^{n}$ which doesnâ€™t depend on x and also doesnâ€™t depend on
    y.
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾$v\in\mathbb{R}^{n}$ä¸ä¾èµ–äº$x$ï¼Œä¹Ÿä¸ä¾èµ–äº$y$ã€‚
- en: â€¢
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $c(:,y)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4
    Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $c(:,y)_{j_{0},i_{0}}\in\mathbb{R}$ å¦‚å®šä¹‰[4.12](#S4.Thmtheorem12 "å®šä¹‰ 4.12\.
    â€£ 4.5 å…³äº ğ‘‹ å’Œ ğ‘Œ çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ ä¸€ç§å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§çš„å•å±‚æ³¨æ„åŠ›çš„é‡æ–°è¡¨è¿°ï¼Œä»¥åŠåœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´ä¸­çš„è§£å†³")ã€‚
- en: â€¢
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $L(:,y)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.13](#S4.Thmtheorem13
    "Definition 4.13\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4
    Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $L(:,y)_{j_{0},i_{0}}\in\mathbb{R}$ å¦‚å®šä¹‰[4.13](#S4.Thmtheorem13 "å®šä¹‰ 4.13\.
    â€£ 4.5 å…³äº ğ‘‹ å’Œ ğ‘Œ çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ ä¸€ç§å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§çš„å•å±‚æ³¨æ„åŠ›çš„é‡æ–°è¡¨è¿°ï¼Œä»¥åŠåœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´ä¸­çš„è§£å†³")ã€‚
- en: â€¢
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $h(y_{i_{0}}):=\underbrace{A_{3}}_{n\times d}\underbrace{y_{i_{0}}}_{d\times
    1}.$
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $h(y_{i_{0}}):=\underbrace{A_{3}}_{n\times d}\underbrace{y_{i_{0}}}_{d\times
    1}.$
- en: â€¢
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $h(y_{i_{0}})=h(y)_{i_{0}}$ for convenient
  id: totrans-444
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä¸ºæ–¹ä¾¿èµ·è§ï¼Œè®¾ $h(y_{i_{0}})=h(y)_{i_{0}}$
- en: â€¢
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $A_{3,*,i_{2}}\in\mathbb{R}^{n}$-th column of matrix $A_{3}\in\mathbb{R}^{n\times
    d}$
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $A_{3,*,i_{2}}\in\mathbb{R}^{n}$ æ˜¯çŸ©é˜µ $A_{3}\in\mathbb{R}^{n\times d}$ çš„ç¬¬ $n$
    åˆ—
- en: Then, we have
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¾—åˆ°
- en: â€¢
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 1. If $i_{1}=i_{0}$
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬1éƒ¨åˆ†ã€‚å¦‚æœ $i_{1}=i_{0}$
- en: '|  | $\displaystyle\frac{\mathrm{d}h(y_{i_{0}})}{\mathrm{d}y_{i_{1},i_{2}}}=A_{3,*,i_{2}}$
    |  |'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}h(y_{i_{0}})}{\mathrm{d}y_{i_{1},i_{2}}}=A_{3,*,i_{2}}$
    |  |'
- en: â€¢
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 2. If $i_{1}\neq i_{0}$
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬2éƒ¨åˆ†ã€‚å¦‚æœ $i_{1}\neq i_{0}$
- en: '|  | $\displaystyle\frac{\mathrm{d}h(y_{i_{0}})}{\mathrm{d}y_{i_{1},i_{2}}}={\bf
    0}_{n}$ |  |'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}h(y_{i_{0}})}{\mathrm{d}y_{i_{1},i_{2}}}={\bf
    0}_{n}$ |  |'
- en: â€¢
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 3. If $i_{1}=i_{0}$
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬3éƒ¨åˆ†ã€‚å¦‚æœ $i_{1}=i_{0}$
- en: '|  | $\displaystyle\frac{\mathrm{d}\langle v,h(y)_{i_{0}}\rangle}{\mathrm{d}y_{i_{1},i_{2}}}=\langle
    v,A_{3,*,i_{2}}\rangle$ |  |'
  id: totrans-456
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}\langle v,h(y)_{i_{0}}\rangle}{\mathrm{d}y_{i_{1},i_{2}}}=\langle
    v,A_{3,*,i_{2}}\rangle$ |  |'
- en: â€¢
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 4. If $i_{1}\neq i_{0}$
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬4éƒ¨åˆ†ã€‚å¦‚æœ $i_{1}\neq i_{0}$
- en: '|  | $\displaystyle\frac{\mathrm{d}\langle v,h(y)_{i_{0}}\rangle}{\mathrm{d}y_{i_{1},i_{2}}}=0$
    |  |'
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}\langle v,h(y)_{i_{0}}\rangle}{\mathrm{d}y_{i_{1},i_{2}}}=0$
    |  |'
- en: â€¢
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 5. If $i_{1}=i_{0}$
  id: totrans-461
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬5éƒ¨åˆ†ã€‚å¦‚æœ $i_{1}=i_{0}$
- en: '|  | $\displaystyle\frac{\mathrm{d}c(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=\langle
    v,A_{3,*,i_{2}}\rangle$ |  |'
  id: totrans-462
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}c(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=\langle
    v,A_{3,*,i_{2}}\rangle$ |  |'
- en: â€¢
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 6. If $i_{1}\neq i_{0}$
  id: totrans-464
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬6éƒ¨åˆ†ã€‚å¦‚æœ $i_{1}\neq i_{0}$
- en: '|  | $\displaystyle\frac{\mathrm{d}c(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=0$
    |  |'
  id: totrans-465
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}c(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=0$
    |  |'
- en: â€¢
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 7. If $i_{1}=i_{0}$
  id: totrans-467
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬7éƒ¨åˆ†ã€‚å¦‚æœ $i_{1}=i_{0}$
- en: '|  | $1$2 |  |'
  id: totrans-468
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: â€¢
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 8. If $i_{1}\neq i_{0}$
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬8éƒ¨åˆ†ã€‚å¦‚æœ $i_{1}\neq i_{0}$
- en: '|  | $\displaystyle\frac{\mathrm{d}L(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=0$
    |  |'
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}L(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=0$
    |  |'
- en: Proof.
  id: totrans-472
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: Proof of Part 1.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬1éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: '|  | $\displaystyle\frac{\mathrm{d}h(y_{i_{0}})}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}h(y_{i_{0}})}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: where the first step is due to the definition of $h(y_{i_{0}})$.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ç¬¬ä¸€æ­¥æ˜¯ç”±äº $h(y_{i_{0}})$ çš„å®šä¹‰ã€‚
- en: Proof of Part 2.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬2éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: '|  | $\displaystyle\frac{\mathrm{d}h(y_{i_{0}})}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}h(y_{i_{0}})}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
- en: where the first step is due to $i_{1}\neq i_{2}$.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ç¬¬ä¸€æ­¥æ˜¯ç”±äº $i_{1}\neq i_{2}$ã€‚
- en: Proof of Part 3.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬3éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: '|  | $\displaystyle\frac{\mathrm{d}\langle v,h(y)_{i_{0}}\rangle}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}\langle v,h(y)_{i_{0}}\rangle}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step comes from FactÂ [4.5](#S4.Thmtheorem5 "Fact 4.5\. â€£ 4.1
    Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), the second step is due to the result of Part 1.'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ç¬¬ä¸€æ­¥æ¥æºäºäº‹å®[4.5](#S4.Thmtheorem5 "äº‹å® 4.5\. â€£ 4.1 åŸºæœ¬äº‹å® â€£ 4 åˆæ­¥ â€£ ä¸€ç§å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ
    SVM æŠ€å·§çš„å•å±‚æ³¨æ„åŠ›çš„é‡æ–°è¡¨è¿°ï¼Œä»¥åŠåœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´ä¸­çš„è§£å†³")ï¼Œç¬¬äºŒæ­¥æ˜¯ç”±äºç¬¬1éƒ¨åˆ†çš„ç»“æœã€‚
- en: Proof of Part 4.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬4éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: '|  | $\displaystyle\frac{\mathrm{d}\langle v,h(y)_{i_{0}}\rangle}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}\langle v,h(y)_{i_{0}}\rangle}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step is becaues of FactÂ [4.5](#S4.Thmtheorem5 "Fact 4.5\. â€£
    4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), the second step comes from the result of Part 2.'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 'é¦–å…ˆçš„æ­¥éª¤æ˜¯å› ä¸ºäº‹å® [4.5](#S4.Thmtheorem5 "Fact 4.5\. â€£ 4.1 Basic Facts â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")ï¼Œç¬¬äºŒæ­¥æ¥è‡ªäºç¬¬2éƒ¨åˆ†çš„ç»“æœã€‚'
- en: Proof of Part 5.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬5éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: '|  | $\displaystyle\frac{\mathrm{d}c(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}c(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
- en: '|  | $\displaystyle\ =$ |  |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\ =$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step comes from the DefinitionÂ [4.12](#S4.Thmtheorem12 "Definition
    4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the second
    step is because of $\frac{\mathrm{d}b_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=0$,
    and the last step is due to Part 3.'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 'é¦–å…ˆçš„æ­¥éª¤æ¥è‡ªäºå®šä¹‰ [4.12](#S4.Thmtheorem12 "Definition 4.12\. â€£ 4.5 Helpful Definitions
    With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")ï¼Œç¬¬äºŒæ­¥æ˜¯å› ä¸º $\frac{\mathrm{d}b_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=0$ï¼Œæœ€åä¸€æ­¥æ˜¯ç”±äºç¬¬3éƒ¨åˆ†ã€‚'
- en: Proof of Part 6.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬6éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: '|  | $\displaystyle\frac{\mathrm{d}c(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}c(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
- en: '|  | $\displaystyle\ =$ |  |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\ =$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step is due to the DefinitionÂ [4.12](#S4.Thmtheorem12 "Definition
    4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the second
    step comes from $\frac{\mathrm{d}b_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=0$,
    and the last step is owing to Part 4.'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 'é¦–å…ˆçš„æ­¥éª¤æºè‡ªäºå®šä¹‰ [4.12](#S4.Thmtheorem12 "Definition 4.12\. â€£ 4.5 Helpful Definitions
    With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")ï¼Œç¬¬äºŒæ­¥æ¥è‡ªäº $\frac{\mathrm{d}b_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=0$ï¼Œæœ€åä¸€æ­¥æ˜¯ç”±äºç¬¬4éƒ¨åˆ†ã€‚'
- en: Proof of Part 7.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬7éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: '|  | $\displaystyle\frac{\mathrm{d}L(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}L(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step is due to the DefinitionÂ [4.13](#S4.Thmtheorem13 "Definition
    4.13\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the second
    step comes from the chain rule of derivative, and the last step is owing to Part
    5.'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 'é¦–å…ˆçš„æ­¥éª¤æºè‡ªäºå®šä¹‰ [4.13](#S4.Thmtheorem13 "Definition 4.13\. â€£ 4.5 Helpful Definitions
    With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")ï¼Œç¬¬äºŒæ­¥æ¥è‡ªäºå¯¼æ•°çš„é“¾å¼æ³•åˆ™ï¼Œæœ€åä¸€æ­¥æ˜¯ç”±äºç¬¬5éƒ¨åˆ†ã€‚'
- en: Proof of Part 8.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬8éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: '|  | $\displaystyle\frac{\mathrm{d}L(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}L(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step is because of the DefinitionÂ [4.13](#S4.Thmtheorem13 "Definition
    4.13\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the second
    step is due to the chain rule of derivative, and the last step comes from Part
    6. âˆ'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 'é¦–å…ˆçš„æ­¥éª¤æ˜¯ç”±äºå®šä¹‰ [4.13](#S4.Thmtheorem13 "Definition 4.13\. â€£ 4.5 Helpful Definitions
    With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")ï¼Œç¬¬äºŒæ­¥æ˜¯ç”±äºå¯¼æ•°çš„é“¾å¼æ³•åˆ™ï¼Œæœ€åä¸€æ­¥æ¥è‡ªäºç¬¬6éƒ¨åˆ†ã€‚âˆ'
- en: 5.3 Computation of $c,f,h$
  id: totrans-508
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 è®¡ç®— $c,f,h$
- en: In this section, we explain how to compute $c(x,y),f(x),h(y)$.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬è§£é‡Šå¦‚ä½•è®¡ç®— $c(x,y),f(x),h(y)$ã€‚
- en: Lemma 5.3.
  id: totrans-510
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç†5.3ã€‚
- en: If the following conditions hold
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä»¥ä¸‹æ¡ä»¶æˆç«‹
- en: â€¢
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: For each $j_{0}\in[n]$, let $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ as an $n\times
    d$ matrix)
  id: totrans-513
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ª $j_{0}\in[n]$ï¼Œè®¾ $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ ä½œä¸ºä¸€ä¸ª $n\times d$ çŸ©é˜µ
- en: â€¢
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'For each $j_{0}\in[n]$ be defined as Definition[4.10](#S4.Thmtheorem10 "Definition
    4.10\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"). (We can view $f(x)$ matrix)'
  id: totrans-515
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'å¯¹äºæ¯ä¸ª$j_{0}\in[n]$ï¼Œå®šä¹‰ä¸ºå®šä¹‰[4.10](#S4.Thmtheorem10 "Definition 4.10\. â€£ 4.3 Helpful
    Definitions With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")ã€‚ ï¼ˆæˆ‘ä»¬å¯ä»¥æŸ¥çœ‹$f(x)$çŸ©é˜µï¼‰'
- en: â€¢
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'For each $i_{0}\in[d]$ be defined as Definition[4.11](#S4.Thmtheorem11 "Definition
    4.11\. â€£ 4.4 A Helpful Definition With Respect to ğ‘Œ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"). (We can view $h(y)$ matrix)'
  id: totrans-517
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'å¯¹äºæ¯ä¸ª$i_{0}\in[d]$ï¼Œå®šä¹‰ä¸ºå®šä¹‰[4.11](#S4.Thmtheorem11 "Definition 4.11\. â€£ 4.4 A Helpful
    Definition With Respect to ğ‘Œ â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")ã€‚ ï¼ˆæˆ‘ä»¬å¯ä»¥æŸ¥çœ‹$h(y)$çŸ©é˜µï¼‰'
- en: â€¢
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $A_{3}\in\mathbb{R}^{n\times d}$
  id: totrans-519
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤$A_{3}\in\mathbb{R}^{n\times d}$
- en: â€¢
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: We can view $y$ matrix
  id: totrans-521
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥æŸ¥çœ‹$y$çŸ©é˜µ
- en: Then, we can compute $f,h,c$ time.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—$f,h,c$çš„æ—¶é—´ã€‚
- en: Proof.
  id: totrans-523
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: 'By definitionÂ [4.11](#S4.Thmtheorem11 "Definition 4.11\. â€£ 4.4 A Helpful Definition
    With Respect to ğ‘Œ â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), we have'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ ¹æ®å®šä¹‰[4.11](#S4.Thmtheorem11 "Definition 4.11\. â€£ 4.4 A Helpful Definition With
    Respect to ğ‘Œ â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time")ï¼Œæˆ‘ä»¬æœ‰'
- en: '|  | $\displaystyle\underbrace{h(y)}_{n\times d}=\underbrace{A_{3}}_{n\times
    d}\underbrace{y}_{d\times d}.$ |  | (5) |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underbrace{h(y)}_{n\times d}=\underbrace{A_{3}}_{n\times
    d}\underbrace{y}_{d\times d}.$ |  | (5) |'
- en: First $h(y)\in\mathbb{R}^{n\times d}$ matrix ($A_{3}$ matrix ($y$.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆæ˜¯$h(y)\in\mathbb{R}^{n\times d}$çŸ©é˜µï¼ˆ$A_{3}$çŸ©é˜µï¼ˆ$y$ã€‚
- en: '![Refer to caption](img/2d3e4154621daaadac8b25aad9a865c1.png)'
  id: totrans-527
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/2d3e4154621daaadac8b25aad9a865c1.png)'
- en: 'Figure 9: The visualization of Eq.Â ([5](#S5.E5 "In Proof. â€£ 5.3 Computation
    of ğ‘,ğ‘“,â„ â€£ 5 Gradient â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")). We have $A_{3}\in\mathbb{R}^{n\times d}$ is a function, which maps the
    matrix $y\in\mathbb{R}^{d\times d}$ by multiplying $A_{3}$. The red rectangles
    represent matrices which are the factors, and the blue rectangle represents the
    matrix which is the product.'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 9: Eq. ([5](#S5.E5 "In Proof. â€£ 5.3 Computation of ğ‘,ğ‘“,â„ â€£ 5 Gradient â€£ A
    Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time"))çš„å¯è§†åŒ–ã€‚æˆ‘ä»¬æœ‰$A_{3}\in\mathbb{R}^{n\times
    d}$æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒé€šè¿‡ä¹˜ä»¥$A_{3}$æ¥æ˜ å°„çŸ©é˜µ$y\in\mathbb{R}^{d\times d}$ã€‚çº¢è‰²çŸ©å½¢è¡¨ç¤ºå› å­çŸ©é˜µï¼Œè“è‰²çŸ©å½¢è¡¨ç¤ºä¹˜ç§¯çŸ©é˜µã€‚'
- en: We also have
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å¯ä»¥
- en: '|  | $1$2 |  | (6) |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (6) |'
- en: Then the computation of $f(x)\in\mathbb{R}^{n\times n}$.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åè®¡ç®—$f(x)\in\mathbb{R}^{n\times n}$ã€‚
- en: '![Refer to caption](img/c05b86816f745fcf485d105c5f191095.png)'
  id: totrans-532
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/c05b86816f745fcf485d105c5f191095.png)'
- en: 'Figure 10: The visualization of Eq.Â ([6](#S5.E6 "In Proof. â€£ 5.3 Computation
    of ğ‘,ğ‘“,â„ â€£ 5 Gradient â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")). We have $A_{1},A_{2}\in\mathbb{R}^{n\times d}$, and $D(X)\in\mathbb{R}^{n\times
    n}$ and compute $\exp(A_{1}XA_{2}^{\top})\in\mathbb{R}^{n\times n}$ and $\exp(A_{1}XA_{2}^{\top})$.
    The green squares represent the square matrices in $\mathbb{R}^{n\times n}$ (the
    dark blue denotes the transpose of the matrix in $\mathbb{R}^{n\times d}$.'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 10: Eq. ([6](#S5.E6 "In Proof. â€£ 5.3 Computation of ğ‘,ğ‘“,â„ â€£ 5 Gradient â€£
    A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on
    Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"))çš„å¯è§†åŒ–ã€‚æˆ‘ä»¬æœ‰$A_{1},A_{2}\in\mathbb{R}^{n\times
    d}$ï¼Œä»¥åŠ$D(X)\in\mathbb{R}^{n\times n}$ï¼Œå¹¶è®¡ç®—$\exp(A_{1}XA_{2}^{\top})\in\mathbb{R}^{n\times
    n}$å’Œ$\exp(A_{1}XA_{2}^{\top})$ã€‚ç»¿è‰²çš„æ–¹å—è¡¨ç¤º$\mathbb{R}^{n\times n}$ä¸­çš„æ–¹é˜µï¼ˆæ·±è“è‰²è¡¨ç¤º$\mathbb{R}^{n\times
    d}$ä¸­çš„çŸ©é˜µçš„è½¬ç½®ï¼‰ã€‚'
- en: Given that
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®š
- en: '|  | $\displaystyle\underbrace{c(x,y)}_{n\times d}=\underbrace{f(x)}_{n\times
    n}\underbrace{h(y)}_{n\times d}-\underbrace{B}_{n\times d}$ |  | (7) |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underbrace{c(x,y)}_{n\times d}=\underbrace{f(x)}_{n\times
    n}\underbrace{h(y)}_{n\times d}-\underbrace{B}_{n\times d}$ |  | (7) |'
- en: Then $c$.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæ˜¯$c$ã€‚
- en: '![Refer to caption](img/1858c5740c54c01100bd045fbd858479.png)'
  id: totrans-537
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/1858c5740c54c01100bd045fbd858479.png)'
- en: 'Figure 11: The visualization of Eq.Â ([7](#S5.E7 "In Proof. â€£ 5.3 Computation
    of ğ‘,ğ‘“,â„ â€£ 5 Gradient â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")). Let $f(x)\in\mathbb{R}^{n\times n}$ (see FigureÂ [9](#S5.F9 "Figure 9
    â€£ Proof. â€£ 5.3 Computation of ğ‘,ğ‘“,â„ â€£ 5 Gradient â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")). We have $B\in\mathbb{R}^{n\times d}$ with $h(y)$
    from their product to get $c(x,y)\in\mathbb{R}^{n\times d}$. The blue rectangles
    represent the matrix in $\mathbb{R}^{n\times d}$.'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 11ï¼šæ–¹ç¨‹ ([7](#S5.E7 "åœ¨è¯æ˜ â€£ 5.3 è®¡ç®— ğ‘,ğ‘“,â„ â€£ 5 æ¢¯åº¦ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§çš„å•å±‚æ³¨æ„åŠ›çš„é‡æ–°è¡¨è¿°ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³"))
    çš„å¯è§†åŒ–ã€‚ä»¤ $f(x)\in\mathbb{R}^{n\times n}$ï¼ˆè§å›¾ [9](#S5.F9 "å›¾ 9 â€£ è¯æ˜ â€£ 5.3 è®¡ç®— ğ‘,ğ‘“,â„
    â€£ 5 æ¢¯åº¦ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§çš„å•å±‚æ³¨æ„åŠ›çš„é‡æ–°è¡¨è¿°ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")ï¼‰ã€‚æˆ‘ä»¬æœ‰ $B\in\mathbb{R}^{n\times
    d}$ï¼Œé€šè¿‡å®ƒä»¬çš„ä¹˜ç§¯å¾—åˆ° $c(x,y)\in\mathbb{R}^{n\times d}$ã€‚è“è‰²çŸ©å½¢è¡¨ç¤º $\mathbb{R}^{n\times d}$
    ä¸­çš„çŸ©é˜µã€‚
- en: âˆ
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ
- en: 5.4 Reformulating Gradient ($x$) in Matrix View
  id: totrans-540
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 ä»çŸ©é˜µè§†è§’é‡æ–°è¡¨è¿°æ¢¯åº¦ï¼ˆ$x$ï¼‰
- en: In this section, we reformulate the gradient $x$ in the matrixâ€™s view.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»çŸ©é˜µçš„è§†è§’é‡æ–°è¡¨è¿°æ¢¯åº¦ $x$ã€‚
- en: Lemma 5.4.
  id: totrans-542
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 5.4ã€‚
- en: If the following conditions hold
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶
- en: â€¢
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $1$2
  id: totrans-545
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2
- en: â€¢
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $c(x,y)\in\mathbb{R}^{n\times d}$
  id: totrans-547
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $c(x,y)\in\mathbb{R}^{n\times d}$
- en: â€¢
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$
  id: totrans-549
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $f(x)_{j_{0}}\in\mathbb{R}^{n}$
- en: â€¢
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $h(y)_{i_{0}}\in\mathbb{R}^{n}$
  id: totrans-551
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $h(y)_{i_{0}}\in\mathbb{R}^{n}$
- en: â€¢
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $1$2
  id: totrans-553
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $1$2
- en: â€¢
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let
  id: totrans-555
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤
- en: '|  | $\displaystyle q(x,y)_{j_{0}}=\sum_{i_{0}=1}^{d}c(x,y)_{j_{0},i_{0}}h(y)_{i_{0}}$
    |  |'
  id: totrans-556
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle q(x,y)_{j_{0}}=\sum_{i_{0}=1}^{d}c(x,y)_{j_{0},i_{0}}h(y)_{i_{0}}$
    |  |'
- en: then, we have
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬æœ‰
- en: â€¢
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 1.
  id: totrans-559
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ 1 éƒ¨åˆ†ã€‚
- en: '|  | $1$2 |  |'
  id: totrans-560
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: â€¢
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 2. Suppose $c(x,y),\operatorname{\mathsf{A}},f(x),h(y)$ can be computed
    in $O(nd^{2})$ time.
  id: totrans-562
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ 2 éƒ¨åˆ†ã€‚å‡è®¾ $c(x,y),\operatorname{\mathsf{A}},f(x),h(y)$ å¯ä»¥åœ¨ $O(nd^{2})$ æ—¶é—´å†…è®¡ç®—ã€‚
- en: â€¢
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 3.
  id: totrans-564
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ 3 éƒ¨åˆ†ã€‚
- en: '|  | $1$2 |  |'
  id: totrans-565
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: â€¢
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 4. Suppose $c(x,y),\operatorname{\mathsf{A}},f(x),h(y)$ can be computed
    in ${\cal T}_{\mathrm{mat}}(n,d,n)+{\cal T}_{\mathrm{mat}}(n,d,d)$ time
  id: totrans-567
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ 4 éƒ¨åˆ†ã€‚å‡è®¾ $c(x,y),\operatorname{\mathsf{A}},f(x),h(y)$ å¯ä»¥åœ¨ ${\cal T}_{\mathrm{mat}}(n,d,n)+{\cal
    T}_{\mathrm{mat}}(n,d,d)$ æ—¶é—´å†…è®¡ç®—ã€‚
- en: Proof.
  id: totrans-568
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: Proof of Part 1.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ 1 éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: 'Note that by FactÂ [4.1](#S4.Thmtheorem1 "Fact 4.1\. â€£ 4.1 Basic Facts â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), we have'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œç”±äºäº‹å® [4.1](#S4.Thmtheorem1 "äº‹å® 4.1ã€‚ â€£ 4.1 åŸºæœ¬äº‹å® â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM
    æŠ€å·§çš„å•å±‚æ³¨æ„åŠ›çš„é‡æ–°è¡¨è¿°ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")ï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $1$2 |  |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: and
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: å’Œ
- en: '|  | $1$2 |  |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Thus, we complete the proof.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬å®Œæˆäº†è¯æ˜ã€‚
- en: Proof of Part 2.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ 2 éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: We first compute $(\operatorname{diag}(f(x)_{j_{0}})-f(x)_{j_{0}}f(x)_{j_{0}}^{\top})h(y)_{i_{0}}$
    time.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆè®¡ç®— $(\operatorname{diag}(f(x)_{j_{0}})-f(x)_{j_{0}}f(x)_{j_{0}}^{\top})h(y)_{i_{0}}$
    æ—¶é—´ã€‚
- en: Then we can compute the rest, it takes $O(nd^{2})$ time.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥è®¡ç®—å…¶ä½™éƒ¨åˆ†ï¼Œè¿™éœ€è¦ $O(nd^{2})$ æ—¶é—´ã€‚
- en: Proof of Part 3 and Part 4.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ 3 éƒ¨åˆ†å’Œç¬¬ 4 éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: Firstly, we can compute $q(x,y)_{j_{0}}\in\mathbb{R}^{n}$.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®— $q(x,y)_{j_{0}}\in\mathbb{R}^{n}$ã€‚
- en: Recall from the Lemma statement, we have
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å¼•ç†é™ˆè¿°ä¸­å›é¡¾ï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle q(x,y)_{j_{0}}=\sum_{i_{0}=1}^{d}c(x,y)_{j_{0},i_{0}}h(y)_{i_{0}}.$
    |  | (8) |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle q(x,y)_{j_{0}}=\sum_{i_{0}=1}^{d}c(x,y)_{j_{0},i_{0}}h(y)_{i_{0}}.$
    |  | (8) |'
- en: Let $q(x,y)_{j_{0}}\in\mathbb{R}^{n}$-th column of $q(x,y)$.
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¤ $q(x,y)_{j_{0}}\in\mathbb{R}^{n}$ ä¸º $q(x,y)$ çš„ç¬¬ $j_{0}$ åˆ—ã€‚
- en: Then we have
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle q(x,y)=\underbrace{h(y)}_{n\times d}\underbrace{c(x,y)^{\top}}_{d\times
    n}$ |  |'
  id: totrans-584
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle q(x,y)=\underbrace{h(y)}_{n\times d}\underbrace{c(x,y)^{\top}}_{d\times
    n}$ |  |'
- en: This takes ${\cal T}_{\mathrm{mat}}(n,d,n)$ time.
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™éœ€è¦ ${\cal T}_{\mathrm{mat}}(n,d,n)$ æ—¶é—´ã€‚
- en: Then, we compute
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬è®¡ç®—
- en: '|  | $1$2 |  | (9) |'
  id: totrans-587
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (9) |'
- en: This takes $O(n^{2})$ time in total.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ€»å…±éœ€è¦ $O(n^{2})$ æ—¶é—´ã€‚
- en: We can show that
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è¯æ˜
- en: '|  |  | $\displaystyle~{}\frac{\mathrm{d}L(x,y)}{\mathrm{d}x}$ |  |'
  id: totrans-590
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}\frac{\mathrm{d}L(x,y)}{\mathrm{d}x}$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-591
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-592
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-593
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-594
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-595
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step is based on DefinitionÂ [4.7](#S4.Thmtheorem7 "Definition
    4.7\. â€£ 4.2 General Definitions â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), the second step is because of Part 1, the third
    step is due to Eq.Â ([8](#S5.E8 "In Proof. â€£ 5.4 Reformulating Gradient (ğ‘¥) in
    Matrix View â€£ 5 Gradient â€£ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")), the fourth step follows from Eq.Â ([9](#S5.E9 "In Proof. â€£ 5.4 Reformulating
    Gradient (ğ‘¥) in Matrix View â€£ 5 Gradient â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")), and the last step due to tensor-trick.'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ç¬¬ä¸€æ­¥åŸºäºå®šä¹‰ [4.7](#S4.Thmtheorem7 "å®šä¹‰ 4.7ã€‚â€£ 4.2 ä¸€èˆ¬å®šä¹‰ â€£ 4 åˆæ­¥ â€£ ä¸€ç§å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§çš„å•å±‚æ³¨æ„åŠ›çš„é‡æ„ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")ï¼Œç¬¬äºŒæ­¥ç”±äºç¬¬
    1 éƒ¨åˆ†ï¼Œç¬¬ä¸‰æ­¥ä¾æ®å…¬å¼ ([8](#S5.E8 "è¯æ˜ä¸­ã€‚â€£ 5.4 åœ¨çŸ©é˜µè§†è§’ä¸‹é‡æ„æ¢¯åº¦ (ğ‘¥) â€£ 5 æ¢¯åº¦ â€£ ä¸€ç§å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§çš„å•å±‚æ³¨æ„åŠ›çš„é‡æ„ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£"))ï¼Œç¬¬å››æ­¥æ¥è‡ªå…¬å¼
    ([9](#S5.E9 "è¯æ˜ä¸­ã€‚â€£ 5.4 åœ¨çŸ©é˜µè§†è§’ä¸‹é‡æ„æ¢¯åº¦ (ğ‘¥) â€£ 5 æ¢¯åº¦ â€£ ä¸€ç§å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§çš„å•å±‚æ³¨æ„åŠ›çš„é‡æ„ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£"))ï¼Œæœ€åä¸€æ­¥ç”±äºå¼ é‡æŠ€å·§ã€‚
- en: Note that $A_{1}^{\top}p(x,y)A_{2}$ time. âˆ
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ $A_{1}^{\top}p(x,y)A_{2}$ æ—¶é—´ã€‚âˆ
- en: 5.5 Reformulating Gradient ($y$) in Matrix View
  id: totrans-598
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 åœ¨çŸ©é˜µè§†è§’ä¸‹é‡æ„æ¢¯åº¦ ($y$)
- en: In this section, we reformulate the gradient $y$ in the matrixâ€™s view.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»çŸ©é˜µè§†è§’é‡æ–°æ„é€ æ¢¯åº¦ $y$ã€‚
- en: Lemma 5.5.
  id: totrans-600
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 5.5ã€‚
- en: If the following conditions hold
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶
- en: â€¢
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: if $i_{1}=i_{0}$
  id: totrans-603
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœ $i_{1}=i_{0}$
- en: â€¢
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: if $i_{1}\neq i_{0}$
  id: totrans-605
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœ $i_{1}\neq i_{0}$
- en: â€¢
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $1$2
  id: totrans-607
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $1$2
- en: â€¢
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $\widetilde{q}(x,y)_{i_{0}}=\sum_{j_{0}=1}^{n}f(x)_{j_{0}}c(x,y)_{j_{0},i_{0}}$
  id: totrans-609
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $\widetilde{q}(x,y)_{i_{0}}=\sum_{j_{0}=1}^{n}f(x)_{j_{0}}c(x,y)_{j_{0},i_{0}}$
- en: Then we have
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬æœ‰
- en: â€¢
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 1.
  id: totrans-612
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ 1 éƒ¨åˆ†ã€‚
- en: '|  | $1$2 |  |'
  id: totrans-613
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: â€¢
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 2.
  id: totrans-615
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ 2 éƒ¨åˆ†ã€‚
- en: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)}{\mathrm{d}y_{i_{0},i_{2}}}=A_{3,*,i_{2}}^{\top}\widetilde{q}(x,y)_{i_{0}}$
    |  |'
  id: totrans-616
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)}{\mathrm{d}y_{i_{0},i_{2}}}=A_{3,*,i_{2}}^{\top}\widetilde{q}(x,y)_{i_{0}}$
    |  |'
- en: â€¢
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 3.
  id: totrans-618
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ 3 éƒ¨åˆ†ã€‚
- en: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)}{\mathrm{d}y}=\operatorname{vec}(\underbrace{A_{3}^{\top}}_{d\times
    n}\underbrace{\widetilde{q}(x,y)}_{n\times d})$ |  |'
  id: totrans-619
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)}{\mathrm{d}y}=\operatorname{vec}(\underbrace{A_{3}^{\top}}_{d\times
    n}\underbrace{\widetilde{q}(x,y)}_{n\times d})$ |  |'
- en: â€¢
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 4. Computing $\frac{\mathrm{d}L(x,y)}{\mathrm{d}y}$
  id: totrans-621
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ 4 éƒ¨åˆ†ã€‚è®¡ç®— $\frac{\mathrm{d}L(x,y)}{\mathrm{d}y}$
- en: Proof.
  id: totrans-622
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: Proof of Part 1.
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ 1 éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0},i_{2}}}=$
    |  |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0},i_{2}}}=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-625
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step comes from the assumption from the Lemma statement and
    the second step is based on FactÂ [4.1](#S4.Thmtheorem1 "Fact 4.1\. â€£ 4.1 Basic
    Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ç¬¬ä¸€æ­¥æ¥è‡ªå¼•ç†å£°æ˜çš„å‡è®¾ï¼Œç¬¬äºŒæ­¥åŸºäºäº‹å® [4.1](#S4.Thmtheorem1 "äº‹å® 4.1ã€‚â€£ 4.1 åŸºæœ¬äº‹å® â€£ 4 åˆæ­¥ â€£ ä¸€ç§å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§çš„å•å±‚æ³¨æ„åŠ›çš„é‡æ„ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")ã€‚
- en: Proof of Part 2.
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ 2 éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)}{\mathrm{d}y_{i_{0},i_{2}}}=$ |  |'
  id: totrans-628
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)}{\mathrm{d}y_{i_{0},i_{2}}}=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-629
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-630
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step is due to the assumption from the Lemma statement, the
    second step is because of FactÂ [4.1](#S4.Thmtheorem1 "Fact 4.1\. â€£ 4.1 Basic Facts
    â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), and the last step comes from the definition of $\widetilde{q}(x,y)_{i_{0}}$
    (see from the Lemma statement).'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ç¬¬ä¸€æ­¥æ¥è‡ªå¼•ç†å£°æ˜çš„å‡è®¾ï¼Œç¬¬äºŒæ­¥ç”±äºäº‹å® [4.1](#S4.Thmtheorem1 "äº‹å® 4.1ã€‚â€£ 4.1 åŸºæœ¬äº‹å® â€£ 4 åˆæ­¥ â€£ ä¸€ç§å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§çš„å•å±‚æ³¨æ„åŠ›çš„é‡æ„ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")ï¼Œæœ€åä¸€æ­¥æ¥è‡ª
    $\widetilde{q}(x,y)_{i_{0}}$ çš„å®šä¹‰ï¼ˆè§å¼•ç†å£°æ˜ï¼‰ã€‚
- en: Proof of Part 3.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ 3 éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)}{\mathrm{d}y}=\operatorname{vec}(A_{3}^{\top}\widetilde{q}(x,y))$
    |  |'
  id: totrans-633
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)}{\mathrm{d}y}=\operatorname{vec}(A_{3}^{\top}\widetilde{q}(x,y))$
    |  |'
- en: where the first step comes from tensor trick based on Part 2.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ç¬¬ä¸€æ­¥æ¥è‡ªåŸºäºç¬¬ 2 éƒ¨åˆ†çš„å¼ é‡æŠ€å·§ã€‚
- en: Proof of Part 4. Computing $\widetilde{q}(x,y)\in\mathbb{R}^{n\times d}$ time.
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ 4 éƒ¨åˆ†çš„è¯æ˜ã€‚è®¡ç®— $\widetilde{q}(x,y)\in\mathbb{R}^{n\times d}$ æ—¶é—´ã€‚
- en: Computing $A_{3}^{\top}\widetilde{q}(x,y)$ time.
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—$A_{3}^{\top}\widetilde{q}(x,y)$çš„æ—¶é—´ã€‚
- en: âˆ
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ
- en: 6 Hessian
  id: totrans-638
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 Hessian
- en: In this section, we provide more details related to Hessian.
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æä¾›ä¸Hessianç›¸å…³çš„æ›´å¤šç»†èŠ‚ã€‚
- en: Finally the hessian $H\in\mathbb{R}^{2d^{2}\times 2d^{2}}$ which can be written
    as
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆçš„HessiançŸ©é˜µ$H\in\mathbb{R}^{2d^{2}\times 2d^{2}}$å¯ä»¥å†™ä½œ
- en: '|  | $\displaystyle H=\begin{bmatrix}H_{x,x}&amp;H_{x,y}\\ H_{y,x}&amp;H_{y,y}\end{bmatrix}$
    |  |'
  id: totrans-641
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle H=\begin{bmatrix}H_{x,x}&amp;H_{x,y}\\ H_{y,x}&amp;H_{y,y}\end{bmatrix}$
    |  |'
- en: where
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­
- en: â€¢
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: '$H_{x,x}\in\mathbb{R}^{d^{2}\times d^{2}}$ (see details in SectionÂ [7](#S7
    "7 Hessian for ğ‘‹ â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"))'
  id: totrans-644
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$H_{x,x}\in\mathbb{R}^{d^{2}\times d^{2}}$ï¼ˆè¯¦è§ç¬¬[7](#S7 "7 Hessian for ğ‘‹ â€£ A
    Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time")èŠ‚ï¼‰'
- en: â€¢
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: '$H_{x,y}$ is $\frac{\mathrm{d}^{2}L}{\mathrm{d}x\mathrm{d}y}$ (see details
    in SectionÂ [11](#S11 "11 Hessian for ğ‘‹ and ğ‘Œ â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"))'
  id: totrans-646
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$H_{x,y}$æ˜¯$\frac{\mathrm{d}^{2}L}{\mathrm{d}x\mathrm{d}y}$ï¼ˆè¯¦è§ç¬¬[11](#S11 "11
    Hessian for ğ‘‹ and ğ‘Œ â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")èŠ‚ï¼‰'
- en: â€¢
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: '$H_{y,y}\in\mathbb{R}^{d^{2}\times d^{2}}$ (see details in SectionÂ [10](#S10
    "10 Hessian for ğ‘Œ â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"))'
  id: totrans-648
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$H_{y,y}\in\mathbb{R}^{d^{2}\times d^{2}}$ï¼ˆè¯¦è§ç¬¬[10](#S10 "10 Hessian for ğ‘Œ â€£
    A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on
    Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")èŠ‚ï¼‰'
- en: â€“
  id: totrans-649
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€“
- en: We can view $$H_{y,y}=\begin{bmatrix}H_{y,y,1,1}&amp;0&amp;0&amp;\cdots&amp;0\\
  id: totrans-650
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°$$H_{y,y}=\begin{bmatrix}H_{y,y,1,1}&amp;0&amp;0&amp;\cdots&amp;0\\
- en: 0&amp;H_{y,y,2,2}&amp;0&amp;\cdots&amp;0\\
  id: totrans-651
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 0&amp;H_{y,y,2,2}&amp;0&amp;\cdots&amp;0\\
- en: 0&amp;0&amp;H_{y,y,3,3}&amp;\cdots&amp;0\\
  id: totrans-652
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 0&amp;0&amp;H_{y,y,3,3}&amp;\cdots&amp;0\\
- en: \vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
  id: totrans-653
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: \vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
- en: 0&amp;0&amp;0&amp;\cdots&amp;H_{y,y,d,d}\end{bmatrix}$$
  id: totrans-654
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 0&amp;0&amp;0&amp;\cdots&amp;H_{y,y,d,d}\end{bmatrix}$$
- en: â€“
  id: totrans-655
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€“
- en: where $H_{y,y,i_{0},i_{0}}=\sum_{j_{0}=1}^{n}\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0},*}\mathrm{d}y_{i_{0},*}}\in\mathbb{R}^{d\times
    d}$
  id: totrans-656
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: å…¶ä¸­$H_{y,y,i_{0},i_{0}}=\sum_{j_{0}=1}^{n}\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0},*}\mathrm{d}y_{i_{0},*}}\in\mathbb{R}^{d\times
    d}$
- en: Lemma 6.1.
  id: totrans-657
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 6.1.
- en: If the following conditions hold
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä»¥ä¸‹æ¡ä»¶æˆç«‹
- en: â€¢
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $H_{x,x}\succeq\alpha_{1}I_{d^{2}}$
  id: totrans-660
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $H_{x,x}\succeq\alpha_{1}I_{d^{2}}$
- en: â€¢
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $H_{y,y}\succeq\alpha_{2}I_{d^{2}}$
  id: totrans-662
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $H_{y,y}\succeq\alpha_{2}I_{d^{2}}$
- en: â€¢
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|H_{x,y}\|\leq\alpha_{3}$
  id: totrans-664
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|H_{x,y}\|\leq\alpha_{3}$
- en: â€¢
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|H_{y,x}\|\leq\alpha_{3}$
  id: totrans-666
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|H_{y,x}\|\leq\alpha_{3}$
- en: â€¢
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let 
  id: totrans-668
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾
- en: Then we have
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¾—åˆ°
- en: '|  | $\displaystyle H\succeq\{\alpha_{1}-\alpha_{3},\alpha_{2}-\alpha_{3}\}\cdot
    I_{2d^{2}}$ |  |'
  id: totrans-670
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle H\succeq\{\alpha_{1}-\alpha_{3},\alpha_{2}-\alpha_{3}\}\cdot
    I_{2d^{2}}$ |  |'
- en: Proof.
  id: totrans-671
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: Let $u,v\in\mathbb{R}^{d^{2}}$, then we have
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾$u,v\in\mathbb{R}^{d^{2}}$ï¼Œåˆ™æˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\begin{bmatrix}u^{\top}&amp;v^{\top}\end{bmatrix}H\begin{bmatrix}u\\
    v\end{bmatrix}=$ |  |'
  id: totrans-673
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\begin{bmatrix}u^{\top}&amp;v^{\top}\end{bmatrix}H\begin{bmatrix}u\\
    v\end{bmatrix}=$ |  |'
- en: '|  | $\displaystyle\geq$ |  |'
  id: totrans-674
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\geq$ |  |'
- en: '|  | $\displaystyle\geq$ |  |'
  id: totrans-675
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\geq$ |  |'
- en: '|  | $\displaystyle\geq$ |  |'
  id: totrans-676
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\geq$ |  |'
- en: '|  | $\displaystyle\geq$ |  |'
  id: totrans-677
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\geq$ |  |'
- en: '|  | $\displaystyle\geq$ |  |'
  id: totrans-678
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\geq$ |  |'
- en: 'where the first step is based on the expansion of $H$, the third step comes
    from FactÂ [4.2](#S4.Thmtheorem2 "Fact 4.2\. â€£ 4.1 Basic Facts â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time") and FactÂ [4.3](#S4.Thmtheorem3
    "Fact 4.3\. â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") , the fourth step is because of $\|H_{x,y}\|\leq\alpha_{3},\|H_{y,x}\|\leq\alpha_{3}$,
    and the last step is based on the simple algebra.'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ç¬¬ä¸€æ­¥åŸºäº$H$çš„å±•å¼€ï¼Œç¬¬ä¸‰æ­¥æ¥è‡ªäºäº‹å®[4.2](#S4.Thmtheorem2 "Fact 4.2\. â€£ 4.1 Basic Facts â€£
    4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")å’Œäº‹å®[4.3](#S4.Thmtheorem3 "Fact 4.3\. â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£
    A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on
    Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")ï¼Œç¬¬å››æ­¥ç”±äº$\|H_{x,y}\|\leq\alpha_{3},\|H_{y,x}\|\leq\alpha_{3}$ï¼Œæœ€åä¸€æ­¥åŸºäºç®€å•çš„ä»£æ•°è¿ç®—ã€‚'
- en: Thus, it implies
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå®ƒæ„å‘³ç€
- en: '|  | $\displaystyle H\succeq\{\alpha_{1}-\alpha_{3},\alpha_{2}-\alpha_{3}\}\cdot
    I_{2d^{2}}$ |  |'
  id: totrans-681
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle H\succeq\{\alpha_{1}-\alpha_{3},\alpha_{2}-\alpha_{3}\}\cdot
    I_{2d^{2}}$ |  |'
- en: âˆ
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ
- en: Algorithm 1 Our Algorithm
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: ç®—æ³• 1 æˆ‘ä»¬çš„ç®—æ³•
- en: '1:procedureÂ TrainingAlgorithm($A_{1},A_{2},A_{3},B\in\mathbb{R}^{n\times d}$
    TheoremÂ [1.3](#S1.Thmtheorem3 "Theorem 1.3 (Informal version of our main theorem).
    â€£ 1 Introduction â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")2:Â Â Â Â Â Let $x(0),y(0)\in\mathbb{R}^{d^{2}}$Â do4:Â Â Â Â Â Â Â Â Â  /*Forward*/5:Â Â Â Â Â Â Â Â Â Compute
    $h(y(t))\in\mathbb{R}^{n\times d}$ ${\cal T}_{\mathrm{mat}}(n,d,d)$ $\triangleright$
    time7:Â Â Â Â Â Â Â Â Â Compute $c(x(t),y(t))\in\mathbb{R}^{n\times d}$, $h(y(t))$ ${\cal
    T}_{\mathrm{mat}}(n,d,d)$ based on LemmaÂ [5.4](#S5.Thmtheorem4 "Lemma 5.4\. â€£
    5.4 Reformulating Gradient (ğ‘¥) in Matrix View â€£ 5 Gradient â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") $\triangleright$ time10:Â Â Â Â Â Â Â Â Â Compute
    $g(y(t))$ ${\cal T}_{\mathrm{mat}}(n,d,n)+{\cal T}_{\mathrm{mat}}(n,d,d)$ via
    TensorSRHT $\triangleright$13:Â Â Â Â Â Â Â Â Â  /*Update*/14:Â Â Â Â Â Â Â Â Â $$\begin{bmatrix}x(t+1)\\'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: '1:procedureÂ TrainingAlgorithm($A_{1},A_{2},A_{3},B\in\mathbb{R}^{n\times d}$
    å®šç†Â [1.3](#S1.Thmtheorem3 "å®šç† 1.3 (æˆ‘ä»¬ä¸»è¦å®šç†çš„éæ­£å¼ç‰ˆæœ¬). â€£ 1 ä»‹ç» â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")2:Â Â Â Â Â ä»¤ $x(0),y(0)\in\mathbb{R}^{d^{2}}$
    æ‰§è¡Œ4:Â Â Â Â Â Â Â Â Â  /*å‰å‘*/5:Â Â Â Â Â Â Â Â Â è®¡ç®— $h(y(t))\in\mathbb{R}^{n\times d}$ ${\cal T}_{\mathrm{mat}}(n,d,d)$
    $\triangleright$ æ—¶é—´7:Â Â Â Â Â Â Â Â Â è®¡ç®— $c(x(t),y(t))\in\mathbb{R}^{n\times d}$, $h(y(t))$
    ${\cal T}_{\mathrm{mat}}(n,d,d)$ åŸºäºå¼•ç†Â [5.4](#S5.Thmtheorem4 "å¼•ç† 5.4\. â€£ 5.4 Reformulating
    Gradient (ğ‘¥) in Matrix View â€£ 5 Gradient â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") $\triangleright$ æ—¶é—´10:Â Â Â Â Â Â Â Â Â è®¡ç®— $g(y(t))$ ${\cal
    T}_{\mathrm{mat}}(n,d,n)+{\cal T}_{\mathrm{mat}}(n,d,d)$ é€šè¿‡TensorSRHT $\triangleright$13:Â Â Â Â Â Â Â Â Â 
    /*æ›´æ–°*/14:Â Â Â Â Â Â Â Â Â $$\begin{bmatrix}x(t+1)\\'
- en: y(t+1)\end{bmatrix}\leftarrow\begin{bmatrix}x(t)\\
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: y(t+1)\end{bmatrix}\leftarrow\begin{bmatrix}x(t)\\
- en: y(t)\end{bmatrix}-\begin{bmatrix}g(x(t))\\
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: y(t)\end{bmatrix}-\begin{bmatrix}g(x(t))\\
- en: g(y(t))\end{bmatrix}\widetilde{H}^{-1}$$ $\triangleright$15:Â Â Â Â Â endÂ for16:endÂ procedure
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: g(y(t))\end{bmatrix}\widetilde{H}^{-1}$$ $\triangleright$15:Â Â Â Â Â ç»“æŸÂ for16:ç»“æŸÂ procedure
- en: 7 Hessian for $X$
  id: totrans-688
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 Hessian å¯¹äº $X$
- en: 'In SectionÂ [7.1](#S7.SS1 "7.1 Hessian â€£ 7 Hessian for ğ‘‹ â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), we compute the Hessian matrix
    with respect to $x$, representing the Hessian.'
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨ç¬¬[7.1](#S7.SS1 "7.1 Hessian â€£ 7 Hessian for ğ‘‹ â€£ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time")èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—äº†å…³äº$x$çš„HessiançŸ©é˜µï¼Œè¡¨ç¤ºHessianã€‚'
- en: 7.1 Hessian
  id: totrans-690
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 Hessian
- en: Now, we start to compute the Hessian matrix with respect to $x$.
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å¼€å§‹è®¡ç®—å…³äº$x$çš„HessiançŸ©é˜µã€‚
- en: Lemma 7.1.
  id: totrans-692
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 7.1.
- en: If the following conditions hold
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä»¥ä¸‹æ¡ä»¶æˆç«‹
- en: â€¢
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $\gamma(x)_{j_{0}}:=\langle f(x)_{j_{0}},v\rangle$ (We define this notation
    for easy of writing proofs.)
  id: totrans-695
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $\gamma(x)_{j_{0}}:=\langle f(x)_{j_{0}},v\rangle$ï¼ˆæˆ‘ä»¬å®šä¹‰è¿™ä¸ªç¬¦å·ä»¥ä¾¿äºä¹¦å†™è¯æ˜ã€‚ï¼‰
- en: Then we have for each $i\in[d^{2}]$
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå¯¹äºæ¯ä¸ª$i\in[d^{2}]$æˆ‘ä»¬æœ‰
- en: â€¢
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 1\. $i=l$ Hessian diagonal term
  id: totrans-698
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬1éƒ¨åˆ†\. $i=l$ Hessiançš„å¯¹è§’çº¿é¡¹
- en: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}x_{i}\mathrm{d}x_{i}}=$
    |  |'
  id: totrans-699
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}x_{i}\mathrm{d}x_{i}}=$
    |  |'
- en: '|  |  | $\displaystyle~{}+c(x,:)_{j_{0},i_{0}}\cdot$ |  |'
  id: totrans-700
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}+c(x,:)_{j_{0},i_{0}}\cdot$ |  |'
- en: '|  |  | $\displaystyle~{}($ |  |'
  id: totrans-701
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}($ |  |'
- en: '|  |  | $\displaystyle~{}+\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle(1-\gamma_{j_{0}}(x))$
    |  |'
  id: totrans-702
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}+\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle(1-\gamma_{j_{0}}(x))$
    |  |'
- en: '|  |  | $\displaystyle~{}-2\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle\cdot\langle
    f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle$ |  |'
  id: totrans-703
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}-2\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle\cdot\langle
    f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle$ |  |'
- en: '|  |  | $\displaystyle~{}+2\langle f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle^{2}\cdot\gamma_{j_{0}}(x)$
    |  |'
  id: totrans-704
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}+2\langle f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle^{2}\cdot\gamma_{j_{0}}(x)$
    |  |'
- en: '|  |  | $\displaystyle~{})$ |  |'
  id: totrans-705
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{})$ |  |'
- en: â€¢
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 2\. $i\neq l$ Hessian off-diagonal term
  id: totrans-707
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬2éƒ¨åˆ†\. $i\neq l$ Hessiançš„éå¯¹è§’çº¿é¡¹
- en: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}x_{i}\mathrm{d}x_{l}}=$
    |  |'
  id: totrans-708
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}x_{i}\mathrm{d}x_{l}}=$
    |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-709
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle~{}+c(x,:)_{j_{0},i_{0}}\cdot$ |  |'
  id: totrans-710
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}+c(x,:)_{j_{0},i_{0}}\cdot$ |  |'
- en: '|  |  | $\displaystyle~{}($ |  |'
  id: totrans-711
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}($ |  |'
- en: '|  |  | $\displaystyle~{}+\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i}\circ\operatorname{\mathsf{A}}_{j_{0},l},v\rangle(1-\langle
    f(x)_{j_{0}},v\rangle))$ |  |'
  id: totrans-712
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}+\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i}\circ\operatorname{\mathsf{A}}_{j_{0},l},v\rangle(1-\langle
    f(x)_{j_{0}},v\rangle))$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-713
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-714
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle~{})$ |  |'
  id: totrans-715
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{})$ |  |'
- en: Proof.
  id: totrans-716
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: Proof of Part 1.
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: è¯æ˜ç¬¬1éƒ¨åˆ†ã€‚
- en: At first, we have
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬æœ‰
- en: '|  |  | $1$2 |  |'
  id: totrans-719
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-720
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-721
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-722
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-723
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-724
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-725
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-726
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  |  | $\displaystyle~{}-2\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle\cdot\langle
    f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle$ |  |'
  id: totrans-727
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}-2\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle\cdot\langle
    f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle$ |  |'
- en: '|  |  | $\displaystyle~{}+2\langle f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle^{2}\cdot\langle
    f(x)_{j_{0}},v\rangle$ |  |'
  id: totrans-728
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}+2\langle f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle^{2}\cdot\langle
    f(x)_{j_{0}},v\rangle$ |  |'
- en: '|  |  | $\displaystyle~{}-\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle\cdot\langle
    f(x)_{j_{0}},v\rangle$ |  |'
  id: totrans-729
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}-\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle\cdot\langle
    f(x)_{j_{0}},v\rangle$ |  |'
- en: 'where the first step is based on the product rule of derivative, the second
    step comes from Part 4, Part 7, and Part 9 of LemmaÂ [5.1](#S5.Thmtheorem1 "Lemma
    5.1 (Gradient with respect to ğ‘¥). â€£ 5.1 Gradient for ğ‘¥ â€£ 5 Gradient â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), and the last step is due to simple
    algebra.'
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ç¬¬ä¸€æ­¥åŸºäºå¯¼æ•°çš„ä¹˜ç§¯è§„åˆ™ï¼Œç¬¬äºŒæ­¥æ¥è‡ªå¼•ç†[5.1](#S5.Thmtheorem1 "Lemma 5.1 (Gradient with respect
    to ğ‘¥). â€£ 5.1 Gradient for ğ‘¥ â€£ 5 Gradient â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")çš„ç¬¬4éƒ¨åˆ†ã€ç¬¬7éƒ¨åˆ†å’Œç¬¬9éƒ¨åˆ†ï¼Œæœ€åä¸€æ­¥ç”±äºç®€å•çš„ä»£æ•°ã€‚'
- en: Then we can show that
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥è¯æ˜
- en: '|  |  | $\displaystyle~{}\frac{\mathrm{d}}{\mathrm{d}x_{i}}(\frac{\mathrm{d}}{\mathrm{d}x_{i}}L_{j_{0},i_{0}})$
    |  |'
  id: totrans-732
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}\frac{\mathrm{d}}{\mathrm{d}x_{i}}(\frac{\mathrm{d}}{\mathrm{d}x_{i}}L_{j_{0},i_{0}})$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-733
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-734
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-735
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: 'where the first step comes from Part 6 of LemmaÂ [5.1](#S5.Thmtheorem1 "Lemma
    5.1 (Gradient with respect to ğ‘¥). â€£ 5.1 Gradient for ğ‘¥ â€£ 5 Gradient â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") and the second step is due to Part
    5 of LemmaÂ [5.1](#S5.Thmtheorem1 "Lemma 5.1 (Gradient with respect to ğ‘¥). â€£ 5.1
    Gradient for ğ‘¥ â€£ 5 Gradient â€£ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ç¬¬ä¸€æ­¥æ¥è‡ªå¼•ç†[5.1](#S5.Thmtheorem1 "Lemma 5.1 (Gradient with respect to ğ‘¥). â€£ 5.1
    Gradient for ğ‘¥ â€£ 5 Gradient â€£ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")çš„ç¬¬6éƒ¨åˆ†ï¼Œç¬¬äºŒæ­¥ç”±äºå¼•ç†[5.1](#S5.Thmtheorem1 "Lemma 5.1 (Gradient with respect to
    ğ‘¥). â€£ 5.1 Gradient for ğ‘¥ â€£ 5 Gradient â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")çš„ç¬¬5éƒ¨åˆ†ã€‚'
- en: Combining the above two equations, we complete the proof.
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“åˆä¸Šè¿°ä¸¤ä¸ªæ–¹ç¨‹ï¼Œæˆ‘ä»¬å®Œæˆäº†è¯æ˜ã€‚
- en: Proof of Part 2.
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: è¯æ˜ç¬¬2éƒ¨åˆ†ã€‚
- en: Firstly, we can show that
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥è¯æ˜
- en: '|  |  | $1$2 |  |'
  id: totrans-740
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-741
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-742
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-743
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-744
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-745
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-746
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-747
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-748
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-749
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle~{}-\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i}\circ\operatorname{\mathsf{A}}_{j_{0},l},v\rangle\cdot\langle
    f(x)_{j_{0}},v\rangle$ |  |'
  id: totrans-750
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}-\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i}\circ\operatorname{\mathsf{A}}_{j_{0},l},v\rangle\cdot\langle
    f(x)_{j_{0}},v\rangle$ |  |'
- en: 'where the first step is owing to the product rule of derivative, the second
    step is based on Part 4, Part 8, and Part 10 of LemmaÂ [5.1](#S5.Thmtheorem1 "Lemma
    5.1 (Gradient with respect to ğ‘¥). â€£ 5.1 Gradient for ğ‘¥ â€£ 5 Gradient â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), and the last step comes from simple
    algebra.'
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ç¬¬ä¸€æ­¥æ˜¯ç”±äºå¯¼æ•°çš„ä¹˜ç§¯è§„åˆ™ï¼Œç¬¬äºŒæ­¥åŸºäºå¼•ç†[5.1](#S5.Thmtheorem1 "Lemma 5.1 (Gradient with respect
    to ğ‘¥). â€£ 5.1 Gradient for ğ‘¥ â€£ 5 Gradient â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")çš„ç¬¬4éƒ¨åˆ†ã€ç¬¬8éƒ¨åˆ†å’Œç¬¬10éƒ¨åˆ†ï¼Œæœ€åä¸€æ­¥æ¥è‡ªç®€å•çš„ä»£æ•°ã€‚'
- en: We have
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰
- en: '|  |  | $\displaystyle~{}\frac{\mathrm{d}}{\mathrm{d}x_{l}}(\frac{\mathrm{d}}{\mathrm{d}x_{i}}L_{j_{0},i_{0}})$
    |  |'
  id: totrans-753
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}\frac{\mathrm{d}}{\mathrm{d}x_{l}}(\frac{\mathrm{d}}{\mathrm{d}x_{i}}L_{j_{0},i_{0}})$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-754
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-755
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-756
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-757
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: Combining the above two equations, we complete the proof. âˆ
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“åˆä¸Šè¿°ä¸¤ä¸ªæ–¹ç¨‹ï¼Œæˆ‘ä»¬å®Œæˆäº†è¯æ˜ã€‚âˆ
- en: 7.2 A Helpful Lemma
  id: totrans-759
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 æœ‰ç”¨çš„å¼•ç†
- en: In this section, we present a helpful Lemma.
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªæœ‰ç”¨çš„å¼•ç†ã€‚
- en: Lemma 7.2.
  id: totrans-761
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 7.2ã€‚
- en: We have
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰
- en: â€¢
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 1.
  id: totrans-764
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬1éƒ¨åˆ†ã€‚
- en: '|  | $1$2 |  |'
  id: totrans-765
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: â€¢
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 2.
  id: totrans-767
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬2éƒ¨åˆ†ã€‚
- en: '|  |  | $1$2 |  |'
  id: totrans-768
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-769
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: â€¢
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 3.
  id: totrans-771
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬3éƒ¨åˆ†ã€‚
- en: '|  | $1$2 |  |'
  id: totrans-772
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: â€¢
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 4.
  id: totrans-774
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬4éƒ¨åˆ†ã€‚
- en: '|  | $1$2 |  |'
  id: totrans-775
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Proof.
  id: totrans-776
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: Proof of Part 1. We have
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬1éƒ¨åˆ†çš„è¯æ˜ã€‚æˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i}\circ\operatorname{\mathsf{A}}_{j_{0},l},v\rangle=$
    |  |'
  id: totrans-778
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i}\circ\operatorname{\mathsf{A}}_{j_{0},l},v\rangle=$
    |  |'
- en: 'where the first step follows from FactÂ [4.1](#S4.Thmtheorem1 "Fact 4.1\. â€£
    4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time").'
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¬¬ä¸€æ­¥æ¥è‡ªäºäº‹å®[4.1](#S4.Thmtheorem1 "Fact 4.1\. â€£ 4.1 Basic Facts â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")ã€‚'
- en: Proof of Part 2. We have
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬2éƒ¨åˆ†çš„è¯æ˜ã€‚æˆ‘ä»¬æœ‰
- en: '|  |  | $1$2 |  |'
  id: totrans-781
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-782
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  |  | $\displaystyle~{}+\langle f(x)_{j_{0}}\circ v,\operatorname{\mathsf{A}}_{j_{0},l}\rangle\cdot\operatorname{\mathsf{A}}_{j_{0},i}^{\top}\cdot
    f(x)_{j_{0}}$ |  |'
  id: totrans-783
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}+\langle f(x)_{j_{0}}\circ v,\operatorname{\mathsf{A}}_{j_{0},l}\rangle\cdot\operatorname{\mathsf{A}}_{j_{0},i}^{\top}\cdot
    f(x)_{j_{0}}$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-784
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  |  | $\displaystyle~{}+\operatorname{\mathsf{A}}_{j_{0},i}^{\top}f(x)_{j_{0}}(f(x)_{j_{0}}\circ
    v)^{\top}\operatorname{\mathsf{A}}_{j_{0},l}$ |  |'
  id: totrans-785
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}+\operatorname{\mathsf{A}}_{j_{0},i}^{\top}f(x)_{j_{0}}(f(x)_{j_{0}}\circ
    v)^{\top}\operatorname{\mathsf{A}}_{j_{0},l}$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-786
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  |  | $\displaystyle~{}+f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top})\operatorname{\mathsf{A}}_{j_{0},l}$
    |  |'
  id: totrans-787
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}+f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top})\operatorname{\mathsf{A}}_{j_{0},l}$
    |  |'
- en: 'where the first step follows from FactÂ [4.1](#S4.Thmtheorem1 "Fact 4.1\. â€£
    4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), the second step follows from FactÂ [4.1](#S4.Thmtheorem1
    "Fact 4.1\. â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), and the last step follows from the simple algebra.'
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¬¬ä¸€æ­¥æ¥è‡ªäºäº‹å®[4.1](#S4.Thmtheorem1 "Fact 4.1\. â€£ 4.1 Basic Facts â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")ï¼Œç¬¬äºŒæ­¥æ¥è‡ªäºäº‹å®[4.1](#S4.Thmtheorem1
    "Fact 4.1\. â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")ï¼Œæœ€åä¸€æ­¥æ¥è‡ªäºç®€å•çš„ä»£æ•°è¿ç®—ã€‚'
- en: Proof of Part 3. We have
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬3éƒ¨åˆ†çš„è¯æ˜ã€‚æˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle\cdot\langle
    f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},l},v\rangle=$ |  |'
  id: totrans-790
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle\cdot\langle
    f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},l},v\rangle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-791
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step follows from FactÂ [4.1](#S4.Thmtheorem1 "Fact 4.1\. â€£
    4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), and the last step follows from FactÂ [4.1](#S4.Thmtheorem1
    "Fact 4.1\. â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¬¬ä¸€æ­¥æ¥è‡ªäºäº‹å®[4.1](#S4.Thmtheorem1 "Fact 4.1\. â€£ 4.1 Basic Facts â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")ï¼Œæœ€åä¸€æ­¥æ¥è‡ªäºäº‹å®[4.1](#S4.Thmtheorem1
    "Fact 4.1\. â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")ã€‚'
- en: Proof of Part 4. We have
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬4éƒ¨åˆ†çš„è¯æ˜ã€‚æˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\langle f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle\cdot\langle
    f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},l}\rangle=$ |  |'
  id: totrans-794
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\langle f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle\cdot\langle
    f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},l}\rangle=$ |  |'
- en: 'where the first step follows from FactÂ [4.1](#S4.Thmtheorem1 "Fact 4.1\. â€£
    4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"). âˆ'
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ç¬¬ä¸€æ­¥éµå¾ªäº‹å® [4.1](#S4.Thmtheorem1 "Fact 4.1\. â€£ 4.1 Basic Facts â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")ã€‚âˆ'
- en: 7.3 Defining $B(x)$
  id: totrans-796
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 å®šä¹‰ $B(x)$
- en: In this section, we formally define $B(x)$.
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æ­£å¼å®šä¹‰ $B(x)$ã€‚
- en: Definition 7.3.
  id: totrans-798
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å®šä¹‰ 7.3ã€‚
- en: If the following conditions hold
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä»¥ä¸‹æ¡ä»¶æˆç«‹
- en: â€¢
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $\gamma_{j_{0}}(x)=\langle f(x)_{j_{0}},v\rangle$
  id: totrans-801
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $\gamma_{j_{0}}(x)=\langle f(x)_{j_{0}},v\rangle$
- en: We define $B(x)\in\mathbb{R}^{n\times n}$ as follows
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®šä¹‰ $B(x)\in\mathbb{R}^{n\times n}$ å¦‚ä¸‹
- en: '|  | $\displaystyle B(x):=$ |  |'
  id: totrans-803
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle B(x):=$ |  |'
- en: '|  |  | $\displaystyle~{}+B_{\operatorname{rank}}^{1}+B_{\operatorname{rank}}^{2}+B_{\operatorname{rank}}^{3}$
    |  |'
  id: totrans-804
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}+B_{\operatorname{rank}}^{1}+B_{\operatorname{rank}}^{2}+B_{\operatorname{rank}}^{3}$
    |  |'
- en: where
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å“ªé‡Œ
- en: â€¢
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $1$2
  id: totrans-807
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2
- en: and
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: å’Œ
- en: â€¢
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $1$2
  id: totrans-810
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2
- en: â€¢
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $1$2
  id: totrans-812
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2
- en: â€¢
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $B_{\operatorname{rank}}^{3}:=(f(x)_{j_{0}}\circ v)\cdot(f(x)_{j_{0}}\circ v)^{\top}$
  id: totrans-814
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $B_{\operatorname{rank}}^{3}:=(f(x)_{j_{0}}\circ v)\cdot(f(x)_{j_{0}}\circ v)^{\top}$
- en: Lemma 7.4.
  id: totrans-815
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 7.4ã€‚
- en: 'Let $B(x)$ be defined as DefinitionÂ [7.3](#S7.Thmtheorem3 "Definition 7.3\.
    â€£ 7.3 Defining ğµâ¢(ğ‘¥) â€£ 7 Hessian for ğ‘‹ â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), then we have'
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä»¤ $B(x)$ å¦‚å®šä¹‰ [7.3](#S7.Thmtheorem3 "Definition 7.3\. â€£ 7.3 Defining ğµâ¢(ğ‘¥) â€£
    7 Hessian for ğ‘‹ â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") æ‰€å®šä¹‰ï¼Œåˆ™æˆ‘ä»¬æœ‰'
- en: '|  | $1$2 |  |'
  id: totrans-817
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Proof.
  id: totrans-818
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: 'The proof follows by combining LemmaÂ [7.1](#S7.Thmtheorem1 "Lemma 7.1\. â€£ 7.1
    Hessian â€£ 7 Hessian for ğ‘‹ â€£ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") and LemmaÂ [7.2](#S7.Thmtheorem2 "Lemma 7.2\. â€£ 7.2 A Helpful Lemma â€£ 7
    Hessian for ğ‘‹ â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"). âˆ'
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¯æ˜é€šè¿‡ç»“åˆå¼•ç† [7.1](#S7.Thmtheorem1 "Lemma 7.1\. â€£ 7.1 Hessian â€£ 7 Hessian for ğ‘‹
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time") å’Œå¼•ç† [7.2](#S7.Thmtheorem2
    "Lemma 7.2\. â€£ 7.2 A Helpful Lemma â€£ 7 Hessian for ğ‘‹ â€£ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time")ã€‚âˆ'
- en: 8 Lipschitz Property of $H_{x,x}$
  id: totrans-820
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 Lipschitz æ€§è´¨çš„ $H_{x,x}$
- en: 'In SectionÂ [8.1](#S8.SS1 "8.1 Main Result â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥}
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), we present
    the main results of the Lipschitz property of $H_{x,x}$. In SectionÂ [8.6](#S8.SS6
    "8.6 Calculation: Step 2 Lipschitz for Matrix Function -ğ›¾_ğ‘—â‚€â¢(ğ‘¥)â‹…ğ‘â¢(ğ‘¥,:)_{ğ‘—â‚€,ğ‘–â‚€}â‹…diag(ğ‘“â¢(ğ‘¥)_ğ‘—â‚€âˆ˜ğ‘£)
    â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), we analyze the second step of Lipschitz function $-\gamma_{j_{0}}(x)\cdot
    c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ v)$. In SectionÂ [8.8](#S8.SS8
    "8.8 Calculation: Step 4 Lipschitz for Matrix Function -ğ‘â¢(ğ‘¥,:)_{ğ‘—â‚€,ğ‘–â‚€}â‹…(ğ‘“â¢(ğ‘¥)_ğ‘—â‚€âˆ˜ğ‘£)â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€^âŠ¤
    â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), we analyze the fourth step of Lipschitz function $-c(x,:)_{j_{0},i_{0}}\cdot(f(x)_{j_{0}}\circ
    v)f(x)_{j_{0}}^{\top}$. In SectionÂ [8.10](#S8.SS10 "8.10 Calculation: Step 6 Lipschitz
    for Matrix Function -ğ‘â¢(ğ‘¥,:)_{ğ‘—â‚€,ğ‘–â‚€}â‹…ğ‘“â¢(ğ‘¥)_ğ‘—â‚€â¢(ğ‘“â¢(ğ‘¥)_ğ‘—â‚€âˆ˜ğ‘£)^âŠ¤ â€£ 8 Lipschitz Property
    of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    we analyze the sixth step of Lipschitz function $-c(x,:)_{j_{0},i_{0}})\cdot f(x)_{j_{0}}(f(x)_{j_{0}}\circ
    v)^{\top}$. In SectionÂ [8.12](#S8.SS12 "8.12 Calculation: Step 8 Lipschitz for
    Matrix Function ğ›¾_ğ‘—â‚€â¢(ğ‘¥)Â²â‹…ğ‘“â¢(ğ‘¥)_ğ‘—â‚€â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€^âŠ¤ â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥}
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), we analyze
    the eighth step of Lipschitz function $\gamma_{j_{0}}(x)^{2}\cdot f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$.'
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬[8.1](#S8.SS1 "8.1 ä¸»è¦ç»“æœ â€£ 8 Lipschitz æ€§è´¨çš„ ğ»_{ğ‘¥,ğ‘¥} â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§çš„ LLM
    å•å±‚æ³¨æ„åŠ›é‡æ„åŠå…¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´ä¸­çš„è§£å†³")èŠ‚ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº† $H_{x,x}$ çš„ Lipschitz æ€§è´¨çš„ä¸»è¦ç»“æœã€‚åœ¨ç¬¬[8.6](#S8.SS6 "8.6
    è®¡ç®—ï¼šæ­¥éª¤ 2 Lipschitz å¯¹çŸ©é˜µå‡½æ•° -ğ›¾_ğ‘—â‚€â¢(ğ‘¥)â‹…ğ‘â¢(ğ‘¥,:)_{ğ‘—â‚€,ğ‘–â‚€}â‹…diag(ğ‘“â¢(ğ‘¥)_ğ‘—â‚€âˆ˜ğ‘£) â€£ 8 Lipschitz
    æ€§è´¨çš„ ğ»_{ğ‘¥,ğ‘¥} â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§çš„ LLM å•å±‚æ³¨æ„åŠ›é‡æ„åŠå…¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´ä¸­çš„è§£å†³")èŠ‚ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº† Lipschitz
    å‡½æ•° $-\gamma_{j_{0}}(x)\cdot c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ
    v)$ çš„ç¬¬äºŒæ­¥ã€‚åœ¨ç¬¬[8.8](#S8.SS8 "8.8 è®¡ç®—ï¼šæ­¥éª¤ 4 Lipschitz å¯¹çŸ©é˜µå‡½æ•° -ğ‘â¢(ğ‘¥,:)_{ğ‘—â‚€,ğ‘–â‚€}â‹…(ğ‘“â¢(ğ‘¥)_ğ‘—â‚€âˆ˜ğ‘£)â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€^âŠ¤
    â€£ 8 Lipschitz æ€§è´¨çš„ ğ»_{ğ‘¥,ğ‘¥} â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§çš„ LLM å•å±‚æ³¨æ„åŠ›é‡æ„åŠå…¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´ä¸­çš„è§£å†³")èŠ‚ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†
    Lipschitz å‡½æ•° $-c(x,:)_{j_{0},i_{0}}\cdot(f(x)_{j_{0}}\circ v)f(x)_{j_{0}}^{\top}$
    çš„ç¬¬å››æ­¥ã€‚åœ¨ç¬¬[8.10](#S8.SS10 "8.10 è®¡ç®—ï¼šæ­¥éª¤ 6 Lipschitz å¯¹çŸ©é˜µå‡½æ•° -ğ‘â¢(ğ‘¥,:)_{ğ‘—â‚€,ğ‘–â‚€}â‹…ğ‘“â¢(ğ‘¥)_ğ‘—â‚€â¢(ğ‘“â¢(ğ‘¥)_ğ‘—â‚€âˆ˜ğ‘£)^âŠ¤
    â€£ 8 Lipschitz æ€§è´¨çš„ ğ»_{ğ‘¥,ğ‘¥} â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§çš„ LLM å•å±‚æ³¨æ„åŠ›é‡æ„åŠå…¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´ä¸­çš„è§£å†³")èŠ‚ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†
    Lipschitz å‡½æ•° $-c(x,:)_{j_{0},i_{0}})\cdot f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top}$
    çš„ç¬¬å…­æ­¥ã€‚åœ¨ç¬¬[8.12](#S8.SS12 "8.12 è®¡ç®—ï¼šæ­¥éª¤ 8 Lipschitz å¯¹çŸ©é˜µå‡½æ•° ğ›¾_ğ‘—â‚€â¢(ğ‘¥)Â²â‹…ğ‘“â¢(ğ‘¥)_ğ‘—â‚€â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€^âŠ¤
    â€£ 8 Lipschitz æ€§è´¨çš„ ğ»_{ğ‘¥,ğ‘¥} â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§çš„ LLM å•å±‚æ³¨æ„åŠ›é‡æ„åŠå…¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´ä¸­çš„è§£å†³")èŠ‚ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†
    Lipschitz å‡½æ•° $\gamma_{j_{0}}(x)^{2}\cdot f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$ çš„ç¬¬å…«æ­¥ã€‚
- en: 8.1 Main Result
  id: totrans-822
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1 ä¸»è¦ç»“æœ
- en: In this section, we present the main result of the Lipschitz property.
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº† Lipschitz æ€§è´¨çš„ä¸»è¦ç»“æœã€‚
- en: Lemma 8.1.
  id: totrans-824
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 8.1ã€‚
- en: If the following conditions hold
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶
- en: â€¢
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $1$2
  id: totrans-827
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $1$2
- en: â€¢
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $H=\sum_{j_{0}=1}^{n}\sum_{i_{0}=1}^{d}H_{j_{0},i_{0}}$)
  id: totrans-829
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $H=\sum_{j_{0}=1}^{n}\sum_{i_{0}=1}^{d}H_{j_{0},i_{0}}$)
- en: â€¢
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $\operatorname{\mathsf{A}}\in\mathbb{R}^{n^{2}\times d^{2}}$ be defined
    as Definition[4.8](#S4.Thmtheorem8 "Definition 4.8\. â€£ 4.3 Helpful Definitions
    With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time")'
  id: totrans-831
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $\operatorname{\mathsf{A}}\in\mathbb{R}^{n^{2}\times d^{2}}$ å¦‚å®šä¹‰[4.8](#S4.Thmtheorem8
    "å®šä¹‰ 4.8\. â€£ 4.3 ä¸ ğ‘‹ ç›¸å…³çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§çš„ LLM å•å±‚æ³¨æ„åŠ›é‡æ„åŠå…¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´ä¸­çš„è§£å†³")æ‰€å®šä¹‰ã€‚
- en: â€¢
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-833
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $\alpha(x)_{j_{0}}\in\mathbb{R}$ å¦‚å®šä¹‰[4.9](#S4.Thmtheorem9 "å®šä¹‰ 4.9\. â€£ 4.3
    ä¸ ğ‘‹ ç›¸å…³çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§çš„ LLM å•å±‚æ³¨æ„åŠ›é‡æ„åŠå…¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´ä¸­çš„è§£å†³")æ‰€å®šä¹‰ã€‚
- en: â€¢
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-835
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾$f(x)_{j_{0}}\in\mathbb{R}^{n}$å®šä¹‰ä¸ºå®šä¹‰[4.10](#S4.Thmtheorem10 "å®šä¹‰ 4.10\. â€£ 4.3
    å…³äº ğ‘‹ çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’ŒSVMæŠ€å·§é‡æ„LLMä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")
- en: â€¢
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4
    Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-837
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾$c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$å®šä¹‰ä¸ºå®šä¹‰[4.12](#S4.Thmtheorem12 "å®šä¹‰ 4.12\.
    â€£ 4.5 å…³äº ğ‘‹ å’Œ ğ‘Œ çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’ŒSVMæŠ€å·§é‡æ„LLMä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")ã€‚
- en: â€¢
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  id: totrans-839
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾$\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
- en: â€¢
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  id: totrans-841
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$ï¼Œ$\|x\|_{2}\leq R$ï¼Œ$\|v\|_{2}\leq R^{2}$
- en: â€¢
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $R\geq 4$
  id: totrans-843
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾$R\geq 4$
- en: â€¢
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $M:=\exp(O(R^{2}+\log(nd)))$
  id: totrans-845
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾$M:=\exp(O(R^{2}+\log(nd)))$
- en: Then, we have for all $x,\widetilde{x}\in\mathbb{R}^{d^{2}}$
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œå¯¹äºæ‰€æœ‰$x,\widetilde{x}\in\mathbb{R}^{d^{2}}$æˆ‘ä»¬æœ‰
- en: â€¢
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 1\. For each $j_{0}\in[n]$
  id: totrans-848
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬1éƒ¨åˆ†\. å¯¹äºæ¯ä¸ª$j_{0}\in[n]$
- en: '|  | $\displaystyle\&#124;H_{j_{0},i_{0}}(x)-H_{j_{0},i_{0}}(\widetilde{x})\&#124;\leq
    M\cdot\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
  id: totrans-849
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;H_{j_{0},i_{0}}(x)-H_{j_{0},i_{0}}(\widetilde{x})\&#124;\leq
    M\cdot\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
- en: â€¢
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 2.
  id: totrans-851
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬2éƒ¨åˆ†ã€‚
- en: '|  | $\displaystyle\&#124;H(x)-H(\widetilde{x})\&#124;\leq M\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  id: totrans-852
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;H(x)-H(\widetilde{x})\&#124;\leq M\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
- en: Proof.
  id: totrans-853
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: Proof of Part 1. We have
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬1éƒ¨åˆ†çš„è¯æ˜ã€‚æˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;H_{j_{0},i_{0}}(x)-H_{j_{0},i_{0}}(\widetilde{x})\&#124;\leq$
    |  |'
  id: totrans-855
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;H_{j_{0},i_{0}}(x)-H_{j_{0},i_{0}}(\widetilde{x})\&#124;\leq$
    |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-856
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-857
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step follows from definition of $H_{j_{0},i_{0}}(x)$, the second
    step follows from LemmaÂ [8.2](#S8.Thmtheorem2 "Lemma 8.2\. â€£ 8.2 Summary of Nine
    Steps â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), and last step follows from simple algebra.'
  id: totrans-858
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ç¬¬ä¸€æ­¥ç”±$H_{j_{0},i_{0}}(x)$çš„å®šä¹‰å¾—å‡ºï¼Œç¬¬äºŒæ­¥ç”±å¼•ç†[8.2](#S8.Thmtheorem2 "å¼•ç† 8.2\. â€£ 8.2
    ä¹æ­¥æ€»ç»“ â€£ 8 ğ»_{ğ‘¥,ğ‘¥}çš„Lipschitzæ€§è´¨ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’ŒSVMæŠ€å·§é‡æ„LLMä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")å¾—å‡ºï¼Œæœ€åä¸€æ­¥ç”±ç®€å•ä»£æ•°å¾—å‡ºã€‚
- en: Proof of Part 2.
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒéƒ¨åˆ†çš„è¯æ˜ã€‚
- en: Then, we have
  id: totrans-860
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;H(x)-H(\widetilde{x})\&#124;\leq$ |  |'
  id: totrans-861
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;H(x)-H(\widetilde{x})\&#124;\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-862
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: where the first step follows from triangle inequality and $H=\sum_{j_{0}=1}^{n}\sum_{i_{0}=1}^{d}H_{j_{0},i_{0}}$,
    and the second step follows from Part 1. âˆ
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ç¬¬ä¸€æ­¥ç”±ä¸‰è§’ä¸ç­‰å¼å¾—å‡ºï¼Œ$H=\sum_{j_{0}=1}^{n}\sum_{i_{0}=1}^{d}H_{j_{0},i_{0}}$ï¼Œç¬¬äºŒæ­¥ç”±ç¬¬1éƒ¨åˆ†å¾—å‡ºã€‚âˆ
- en: 8.2 Summary of Nine Steps
  id: totrans-864
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2 ä¹æ­¥æ€»ç»“
- en: In this section, we provide a summary of the nine-step calculation of Lipschitz
    for different matrix functions.
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æ€»ç»“äº†ä¸åŒçŸ©é˜µå‡½æ•°Lipschitzçš„ä¹æ­¥è®¡ç®—æ–¹æ³•ã€‚
- en: Lemma 8.2.
  id: totrans-866
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 8.2ã€‚
- en: If the following conditions hold
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶
- en: â€¢
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $G_{1}(x)=c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ v)$
  id: totrans-869
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $G_{1}(x)=c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ v)$
- en: â€¢
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $1$2
  id: totrans-871
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2
- en: â€¢
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $1$2
  id: totrans-873
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2
- en: â€¢
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $1$2
  id: totrans-875
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2
- en: â€¢
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $G_{5}(x)=-2\gamma_{j_{0}}(x)\cdot f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top}$)
  id: totrans-877
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $G_{5}(x)=-2\gamma_{j_{0}}(x)\cdot f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top}$)
- en: â€¢
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $G_{6}(x)=-c(x,:)_{j_{0},i_{0}}\cdot f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top}$)
  id: totrans-879
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $G_{6}(x)=-c(x,:)_{j_{0},i_{0}}\cdot f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top}$)
- en: â€¢
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $1$2
  id: totrans-881
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2
- en: â€¢
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $G_{8}(x)=\gamma_{j_{0}}(x)^{2}\cdot f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
  id: totrans-883
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $G_{8}(x)=\gamma_{j_{0}}(x)^{2}\cdot f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
- en: â€¢
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $G_{9}(x)=(f(x)_{j_{0}}\circ v)\cdot(f(x)_{j_{0}}\circ v)^{\top}$
  id: totrans-885
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $G_{9}(x)=(f(x)_{j_{0}}\circ v)\cdot(f(x)_{j_{0}}\circ v)^{\top}$
- en: Then, we have
  id: totrans-886
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\max_{k\in[9]}\&#124;G_{k}(x)-G_{k}(\widetilde{x})\&#124;\leq
    n^{1.5}\exp(20R^{2}).$ |  |'
  id: totrans-887
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{k\in[9]}\&#124;G_{k}(x)-G_{k}(\widetilde{x})\&#124;\leq
    n^{1.5}\exp(20R^{2}).$ |  |'
- en: Proof.
  id: totrans-888
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: 'The proof follows from LemmaÂ [8.7](#S8.Thmtheorem7 "Lemma 8.7\. â€£ 8.5 Calculation:
    Step 1 Lipschitz for Matrix Function ğ‘â¢(ğ‘¥,:)_{ğ‘—â‚€,ğ‘–â‚€}â‹…diag(ğ‘“â¢(ğ‘¥)_ğ‘—â‚€âˆ˜ğ‘£) â€£ 8 Lipschitz
    Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), LemmaÂ [8.8](#S8.Thmtheorem8 "Lemma 8.8\. â€£ 8.6 Calculation: Step 2 Lipschitz
    for Matrix Function -ğ›¾_ğ‘—â‚€â¢(ğ‘¥)â‹…ğ‘â¢(ğ‘¥,:)_{ğ‘—â‚€,ğ‘–â‚€}â‹…diag(ğ‘“â¢(ğ‘¥)_ğ‘—â‚€âˆ˜ğ‘£) â€£ 8 Lipschitz Property
    of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    LemmaÂ [8.9](#S8.Thmtheorem9 "Lemma 8.9\. â€£ 8.7 Calculation: Step 3 Lipschitz for
    Matrix Function -2â¢ğ›¾_ğ‘—â‚€â¢(ğ‘¥)â‹…(ğ‘“â¢(ğ‘¥)_ğ‘—â‚€âˆ˜ğ‘£)â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€^âŠ¤ â€£ 8 Lipschitz Property of
    ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    LemmaÂ [8.10](#S8.Thmtheorem10 "Lemma 8.10\. â€£ 8.8 Calculation: Step 4 Lipschitz
    for Matrix Function -ğ‘â¢(ğ‘¥,:)_{ğ‘—â‚€,ğ‘–â‚€}â‹…(ğ‘“â¢(ğ‘¥)_ğ‘—â‚€âˆ˜ğ‘£)â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€^âŠ¤ â€£ 8 Lipschitz Property
    of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    LemmaÂ [8.11](#S8.Thmtheorem11 "Lemma 8.11\. â€£ 8.9 Calculation: Step 5 Lipschitz
    for Matrix Function -2â¢ğ›¾_ğ‘—â‚€â¢(ğ‘¥)â‹…ğ‘“â¢(ğ‘¥)_ğ‘—â‚€â¢(ğ‘“â¢(ğ‘¥)_ğ‘—â‚€âˆ˜ğ‘£)^âŠ¤ â€£ 8 Lipschitz Property
    of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    LemmaÂ [8.12](#S8.Thmtheorem12 "Lemma 8.12\. â€£ 8.10 Calculation: Step 6 Lipschitz
    for Matrix Function -ğ‘â¢(ğ‘¥,:)_{ğ‘—â‚€,ğ‘–â‚€}â‹…ğ‘“â¢(ğ‘¥)_ğ‘—â‚€â¢(ğ‘“â¢(ğ‘¥)_ğ‘—â‚€âˆ˜ğ‘£)^âŠ¤ â€£ 8 Lipschitz Property
    of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    LemmaÂ [8.13](#S8.Thmtheorem13 "Lemma 8.13\. â€£ 8.11 Calculation: Step 7 Lipschitz
    for Matrix Function 2â¢ğ›¾_ğ‘—â‚€â¢(ğ‘¥)â¢ğ‘â¢(ğ‘¥,:)_{ğ‘—â‚€,ğ‘–â‚€}â‹…ğ‘“â¢(ğ‘¥)_ğ‘—â‚€â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€^âŠ¤ â€£ 8 Lipschitz
    Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), LemmaÂ [8.14](#S8.Thmtheorem14 "Lemma 8.14\. â€£ 8.12 Calculation: Step 8
    Lipschitz for Matrix Function ğ›¾_ğ‘—â‚€â¢(ğ‘¥)Â²â‹…ğ‘“â¢(ğ‘¥)_ğ‘—â‚€â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€^âŠ¤ â€£ 8 Lipschitz Property
    of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    and LemmaÂ [8.15](#S8.Thmtheorem15 "Lemma 8.15\. â€£ 8.13 Calculation: Step 9 Lipschitz
    for Matrix Function (ğ‘“â¢(ğ‘¥)_ğ‘—â‚€âˆ˜ğ‘£)â‹…(ğ‘“â¢(ğ‘¥)_ğ‘—â‚€âˆ˜ğ‘£)^âŠ¤ â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥}
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"). âˆ'
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
  zh: è¯æ˜æ¥è‡ªå¼•ç† [8.7](#S8.Thmtheorem7 "å¼•ç† 8.7\. â€£ 8.5 è®¡ç®—ï¼šæ­¥éª¤ 1 çŸ©é˜µå‡½æ•° ğ‘â¢(ğ‘¥,:)_{ğ‘—â‚€,ğ‘–â‚€}â‹…diag(ğ‘“â¢(ğ‘¥)_ğ‘—â‚€âˆ˜ğ‘£)
    â€£ 8 Lipschitz æ€§è´¨çš„ ğ»_{ğ‘¥,ğ‘¥} â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿° LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")ï¼Œå¼•ç†
    [8.8](#S8.Thmtheorem8 "å¼•ç† 8.8\. â€£ 8.6 è®¡ç®—ï¼šæ­¥éª¤ 2 çŸ©é˜µå‡½æ•° -ğ›¾_ğ‘—â‚€â¢(ğ‘¥)â‹…ğ‘â¢(ğ‘¥,:)_{ğ‘—â‚€,ğ‘–â‚€}â‹…diag(ğ‘“â¢(ğ‘¥)_ğ‘—â‚€âˆ˜ğ‘£)
    â€£ 8 Lipschitz æ€§è´¨çš„ ğ»_{ğ‘¥,ğ‘¥} â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿° LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")ï¼Œå¼•ç†
    [8.9](#S8.Thmtheorem9 "å¼•ç† 8.9\. â€£ 8.7 è®¡ç®—ï¼šæ­¥éª¤ 3 çŸ©é˜µå‡½æ•° -2â¢ğ›¾_ğ‘—â‚€â¢(ğ‘¥)â‹…(ğ‘“â¢(ğ‘¥)_ğ‘—â‚€âˆ˜ğ‘£)â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€^âŠ¤
    â€£ 8 Lipschitz æ€§è´¨çš„ ğ»_{ğ‘¥,ğ‘¥} â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿° LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")ï¼Œå¼•ç†
    [8.10](#S8.Thmtheorem10 "å¼•ç† 8.10\. â€£ 8.8 è®¡ç®—ï¼šæ­¥éª¤ 4 çŸ©é˜µå‡½æ•° -ğ‘â¢(ğ‘¥,:)_{ğ‘—â‚€,ğ‘–â‚€}â‹…(ğ‘“â¢(ğ‘¥)_ğ‘—â‚€âˆ˜ğ‘£)â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€^âŠ¤
    â€£ 8 Lipschitz æ€§è´¨çš„ ğ»_{ğ‘¥,ğ‘¥} â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿° LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")ï¼Œå¼•ç†
    [8.11](#S8.Thmtheorem11 "å¼•ç† 8.11\. â€£ 8.9 è®¡ç®—ï¼šæ­¥éª¤ 5 çŸ©é˜µå‡½æ•° -2â¢ğ›¾_ğ‘—â‚€â¢(ğ‘¥)â‹…ğ‘“â¢(ğ‘¥)_ğ‘—â‚€â¢(ğ‘“â¢(ğ‘¥)_ğ‘—â‚€âˆ˜ğ‘£)^âŠ¤
    â€£ 8 Lipschitz æ€§è´¨çš„ ğ»_{ğ‘¥,ğ‘¥} â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿° LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")ï¼Œå¼•ç†
    [8.12](#S8.Thmtheorem12 "å¼•ç† 8.12\. â€£ 8.10 è®¡ç®—ï¼šæ­¥éª¤ 6 çŸ©é˜µå‡½æ•° -ğ‘â¢(ğ‘¥,:)_{ğ‘—â‚€,ğ‘–â‚€}â‹…ğ‘“â¢(ğ‘¥)_ğ‘—â‚€â¢(ğ‘“â¢(ğ‘¥)_ğ‘—â‚€âˆ˜ğ‘£)^âŠ¤
    â€£ 8 Lipschitz æ€§è´¨çš„ ğ»_{ğ‘¥,ğ‘¥} â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿° LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")ï¼Œå¼•ç†
    [8.13](#S8.Thmtheorem13 "å¼•ç† 8.13\. â€£ 8.11 è®¡ç®—ï¼šæ­¥éª¤ 7 çŸ©é˜µå‡½æ•° 2â¢ğ›¾_ğ‘—â‚€â¢(ğ‘¥)â¢ğ‘â¢(ğ‘¥,:)_{ğ‘—â‚€,ğ‘–â‚€}â‹…ğ‘“â¢(ğ‘¥)_ğ‘—â‚€â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€^âŠ¤
    â€£ 8 Lipschitz æ€§è´¨çš„ ğ»_{ğ‘¥,ğ‘¥} â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿° LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")ï¼Œå¼•ç†
    [8.14](#S8.Thmtheorem14 "å¼•ç† 8.14\. â€£ 8.12 è®¡ç®—ï¼šæ­¥éª¤ 8 çŸ©é˜µå‡½æ•° ğ›¾_ğ‘—â‚€â¢(ğ‘¥)Â²â‹…ğ‘“â¢(ğ‘¥)_ğ‘—â‚€â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€^âŠ¤
    â€£ 8 Lipschitz æ€§è´¨çš„ ğ»_{ğ‘¥,ğ‘¥} â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿° LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")ï¼Œä»¥åŠå¼•ç†
    [8.15](#S8.Thmtheorem15 "å¼•ç† 8.15\. â€£ 8.13 è®¡ç®—ï¼šæ­¥éª¤ 9 çŸ©é˜µå‡½æ•° (ğ‘“â¢(ğ‘¥)_ğ‘—â‚€âˆ˜ğ‘£)â‹…(ğ‘“â¢(ğ‘¥)_ğ‘—â‚€âˆ˜ğ‘£)^âŠ¤
    â€£ 8 Lipschitz æ€§è´¨çš„ ğ»_{ğ‘¥,ğ‘¥} â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿° LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")ã€‚âˆ
- en: '8.3 A Core Tool: Upper Bound for Several Basic Functions'
  id: totrans-890
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3 æ ¸å¿ƒå·¥å…·ï¼šå‡ ä¸ªåŸºæœ¬å‡½æ•°çš„ä¸Šç•Œ
- en: In this section, we analyze the upper bound of several basic functions.
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬åˆ†æå‡ ä¸ªåŸºæœ¬å‡½æ•°çš„ä¸Šç•Œã€‚
- en: Lemma 8.3  ([[55](#bib.bib55), [72](#bib.bib72)]).
  id: totrans-892
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 8.3  ([[55](#bib.bib55), [72](#bib.bib72)])ã€‚
- en: Provided that the subsequent requirements are satisfied
  id: totrans-893
  prefs: []
  type: TYPE_NORMAL
  zh: åªè¦æ»¡è¶³ä»¥ä¸‹è¦æ±‚
- en: â€¢
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $\operatorname{\mathsf{A}}\in\mathbb{R}^{n^{2}\times d^{2}}$
  id: totrans-895
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾$\operatorname{\mathsf{A}}\in\mathbb{R}^{n^{2}\times d^{2}}$
- en: â€¢
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $x\in\mathbb{R}^{d^{2}}$
  id: totrans-897
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾$x\in\mathbb{R}^{d^{2}}$
- en: â€¢
  id: totrans-898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'We define $u(x)$ as Definition[4.8](#S4.Thmtheorem8 "Definition 4.8\. â€£ 4.3
    Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time")'
  id: totrans-899
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®šä¹‰$u(x)$è§å®šä¹‰[4.8](#S4.Thmtheorem8 "Definition 4.8\. â€£ 4.3 å…³äºğ‘‹çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£
    å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’ŒSVMæŠ€å·§é‡æ–°è¡¨è¿°LLMä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")
- en: â€¢
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $\beta$
  id: totrans-901
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾$\beta$
- en: Then we have
  id: totrans-902
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\beta\geq\exp(-R^{2}).$ |  |'
  id: totrans-903
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\beta\geq\exp(-R^{2}).$ |  |'
- en: Lemma 8.4  (Basic Functions Upper Bound).
  id: totrans-904
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 8.4  (åŸºæœ¬å‡½æ•°ä¸Šç•Œ)ã€‚
- en: If the following conditions hold,
  id: totrans-905
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼Œ
- en: â€¢
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $u(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.8](#S4.Thmtheorem8
    "Definition 4.8\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-907
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾$u(x)_{j_{0}}\in\mathbb{R}^{n}$çš„å®šä¹‰è§å®šä¹‰[4.8](#S4.Thmtheorem8 "Definition 4.8\.
    â€£ 4.3 å…³äºğ‘‹çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’ŒSVMæŠ€å·§é‡æ–°è¡¨è¿°LLMä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")
- en: â€¢
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-909
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾$\alpha(x)_{j_{0}}\in\mathbb{R}$çš„å®šä¹‰è§å®šä¹‰[4.9](#S4.Thmtheorem9 "Definition 4.9\.
    â€£ 4.3 å…³äºğ‘‹çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’ŒSVMæŠ€å·§é‡æ–°è¡¨è¿°LLMä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")
- en: â€¢
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-911
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾$f(x)_{j_{0}}\in\mathbb{R}^{n}$çš„å®šä¹‰è§å®šä¹‰[4.10](#S4.Thmtheorem10 "Definition 4.10\.
    â€£ 4.3 å…³äºğ‘‹çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’ŒSVMæŠ€å·§é‡æ–°è¡¨è¿°LLMä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")
- en: â€¢
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4
    Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-913
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾$c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$çš„å®šä¹‰è§å®šä¹‰[4.12](#S4.Thmtheorem12 "Definition
    4.12\. â€£ 4.5 å…³äºğ‘‹å’Œğ‘Œçš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’ŒSVMæŠ€å·§é‡æ–°è¡¨è¿°LLMä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")
- en: â€¢
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  id: totrans-915
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾$\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
- en: â€¢
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $\beta$
  id: totrans-917
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾$\beta$
- en: â€¢
  id: totrans-918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$
  id: totrans-919
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$
- en: â€¢
  id: totrans-920
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq R$
  id: totrans-921
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq R$
- en: â€¢
  id: totrans-922
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|x\|_{2}\leq R$
  id: totrans-923
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|x\|_{2}\leq R$
- en: â€¢
  id: totrans-924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $|b_{j_{0},i_{0}}|\leq R$
  id: totrans-925
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $|b_{j_{0},i_{0}}|\leq R$
- en: â€¢
  id: totrans-926
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $R\geq 4$
  id: totrans-927
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾$R\geq 4$
- en: â€¢
  id: totrans-928
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|v\|_{2}\leq R^{2}$
  id: totrans-929
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|v\|_{2}\leq R^{2}$
- en: 'Then we have: for all $x\in\mathbb{R}^{d^{2}}$'
  id: totrans-930
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¾—åˆ°ï¼šå¯¹äºæ‰€æœ‰$x\in\mathbb{R}^{d^{2}}$
- en: â€¢
  id: totrans-931
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 1\. $\|u(x)_{j_{0}}\|_{2}\leq\sqrt{n}\cdot\exp(R^{2})$
  id: totrans-932
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬1éƒ¨åˆ†ï¼$\|u(x)_{j_{0}}\|_{2}\leq\sqrt{n}\cdot\exp(R^{2})$
- en: â€¢
  id: totrans-933
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 2\. $|\alpha(x)_{j_{0}}|\leq n\exp(R^{2})$
  id: totrans-934
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬2éƒ¨åˆ†ï¼$|\alpha(x)_{j_{0}}|\leq n\exp(R^{2})$
- en: â€¢
  id: totrans-935
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 3\. $|\alpha(x)_{j_{0}}|^{-1}\leq\exp(R^{2})$
  id: totrans-936
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬3éƒ¨åˆ†ï¼$|\alpha(x)_{j_{0}}|^{-1}\leq\exp(R^{2})$
- en: â€¢
  id: totrans-937
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 4\. $\|f(x)_{j_{0}}\|_{2}\leq 1$
  id: totrans-938
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬4éƒ¨åˆ†ï¼$\|f(x)_{j_{0}}\|_{2}\leq 1$
- en: â€¢
  id: totrans-939
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 5\. $|\gamma(x)_{j_{0}}|\leq R^{2}$
  id: totrans-940
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬5éƒ¨åˆ†ï¼$|\gamma(x)_{j_{0}}|\leq R^{2}$
- en: â€¢
  id: totrans-941
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 6\. $|c(x,:)_{j_{0},i_{0}}|\leq 2R^{2}$
  id: totrans-942
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬6éƒ¨åˆ†ï¼$|c(x,:)_{j_{0},i_{0}}|\leq 2R^{2}$
- en: Proof.
  id: totrans-943
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: We present our proof as follows.
  id: totrans-944
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„è¯æ˜å¦‚ä¸‹ã€‚
- en: Proof of Part 1. We have
  id: totrans-945
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬1éƒ¨åˆ†çš„è¯æ˜ã€‚æˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;u(x)_{j_{0}}\&#124;_{2}=$ |  |'
  id: totrans-946
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\|u(x)_{j_{0}}\|_{2}=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-947
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-948
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-949
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step follows from DefinitionÂ [4.8](#S4.Thmtheorem8 "Definition
    4.8\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), the second step is based on FactÂ [4.2](#S4.Thmtheorem2
    "Fact 4.2\. â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), the third step follows from FactÂ [4.2](#S4.Thmtheorem2
    "Fact 4.2\. â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), and the fourth step is because of $\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq
    R$ (see from the Lemma statement).'
  id: totrans-950
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ç¬¬ä¸€æ­¥æ¥è‡ªå®šä¹‰[4.8](#S4.Thmtheorem8 "Definition 4.8\. â€£ 4.3 Helpful Definitions
    With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time")ï¼Œç¬¬äºŒæ­¥åŸºäºäº‹å®[4.2](#S4.Thmtheorem2 "Fact 4.2\. â€£ 4.1 Basic Facts
    â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")ï¼Œç¬¬ä¸‰æ­¥æ¥è‡ªäº‹å®[4.2](#S4.Thmtheorem2 "Fact 4.2\. â€£ 4.1 Basic Facts â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")ï¼Œç¬¬å››æ­¥æ˜¯å› ä¸º$\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq
    R$ï¼ˆè§å¼•ç†å£°æ˜ï¼‰ã€‚'
- en: Proof of Part 2. We have
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬2éƒ¨åˆ†çš„è¯æ˜ã€‚æˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle&#124;\alpha(x)_{j_{0}}&#124;=$ |  |'
  id: totrans-952
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\| \alpha(x)_{j_{0}} \|=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-953
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-954
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-955
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step is due to DefinitionÂ [4.9](#S4.Thmtheorem9 "Definition
    4.9\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), the second is based on FactÂ [4.2](#S4.Thmtheorem2
    "Fact 4.2\. â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), the third step follows from Part 1. and the forth
    step follows from simple algebra.'
  id: totrans-956
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ç¬¬ä¸€æ­¥æ˜¯æ ¹æ®å®šä¹‰[4.9](#S4.Thmtheorem9 "Definition 4.9\. â€£ 4.3 Helpful Definitions
    With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time")ï¼Œç¬¬äºŒæ­¥åŸºäºäº‹å®[4.2](#S4.Thmtheorem2 "Fact 4.2\. â€£ 4.1 Basic Facts
    â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")ï¼Œç¬¬ä¸‰æ­¥æ¥è‡ªç¬¬1éƒ¨åˆ†ï¼Œç¬¬å››æ­¥æ¥è‡ªç®€å•çš„ä»£æ•°ã€‚'
- en: Proof of Part 3.
  id: totrans-957
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬3éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: We have
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle&#124;\alpha^{-1}(x)_{j_{0}}&#124;=$ |  |'
  id: totrans-959
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\| \alpha^{-1}(x)_{j_{0}} \|=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-960
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-961
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is because of DefinitionÂ [4.9](#S4.Thmtheorem9 "Definition
    4.9\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), the second step follows from the
    definition of $\beta$ and the third step is due to LemmaÂ [8.3](#S8.Thmtheorem3
    "Lemma 8.3 ([55, 72]). â€£ 8.3 A Core Tool: Upper Bound for Several Basic Functions
    â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time").'
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ç¬¬ä¸€æ­¥æ˜¯å› ä¸ºå®šä¹‰[4.9](#S4.Thmtheorem9 "Definition 4.9\. â€£ 4.3 Helpful Definitions
    With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time")ï¼Œç¬¬äºŒæ­¥æ¥è‡ª$\beta$çš„å®šä¹‰ï¼Œç¬¬ä¸‰æ­¥æ˜¯ç”±äºå¼•ç†[8.3](#S8.Thmtheorem3 "Lemma 8.3
    ([55, 72]). â€£ 8.3 A Core Tool: Upper Bound for Several Basic Functions â€£ 8 Lipschitz
    Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")ã€‚'
- en: Proof of Part 4. We have
  id: totrans-963
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬4éƒ¨åˆ†çš„è¯æ˜ã€‚æˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;f(x)_{j_{0}}\&#124;_{2}\leq$ |  |'
  id: totrans-964
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\| f(x)_{j_{0}} \|_{2} \leq$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-965
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step follows from FactÂ [4.2](#S4.Thmtheorem2 "Fact 4.2\. â€£
    4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), the second step is due to DefinitionÂ [4.10](#S4.Thmtheorem10
    "Definition 4.10\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ç¬¬ä¸€æ­¥æ¥è‡ªäºäº‹å® [4.2](#S4.Thmtheorem2 "Fact 4.2. â€£ 4.1 Basic Facts â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")ï¼Œç¬¬äºŒæ­¥ç”±äºå®šä¹‰
    [4.10](#S4.Thmtheorem10 "Definition 4.10. â€£ 4.3 Helpful Definitions With Respect
    to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")'
- en: Proof of Part 5. We have
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ 5 éƒ¨åˆ†çš„è¯æ˜ã€‚æˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle&#124;\gamma(x)_{j_{0}}&#124;=$ |  |'
  id: totrans-968
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle&#124;\gamma(x)_{j_{0}}&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-969
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-970
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-971
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: where the first step follows from the definition of $\gamma(x)_{j_{0}}$ norm
    of $v$ (from the Lemma statement), and the last step follows from simple algebra.
  id: totrans-972
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ç¬¬ä¸€æ­¥ç”± $v$ çš„ $\gamma(x)_{j_{0}}$ èŒƒæ•°å®šä¹‰å¾—å‡ºï¼ˆæ¥è‡ªå¼•ç†é™ˆè¿°ï¼‰ï¼Œæœ€åä¸€æ­¥ç”±ç®€å•çš„ä»£æ•°å¾—å‡ºã€‚
- en: Proof of Part 6. We have
  id: totrans-973
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ 6 éƒ¨åˆ†çš„è¯æ˜ã€‚æˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle&#124;c(x,:)_{j_{0},i_{0}}&#124;=$ |  |'
  id: totrans-974
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle&#124;c(x,:)_{j_{0},i_{0}}&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-975
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-976
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-977
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-978
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is based on DefinitionÂ [4.12](#S4.Thmtheorem12 "Definition
    4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the second
    step is because of the definition of $\gamma_{j_{0}}(x)$ (see from the Lemma statement),
    and the last step follows from $R\geq 1$. âˆ'
  id: totrans-979
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ç¬¬ä¸€æ­¥åŸºäºå®šä¹‰ [4.12](#S4.Thmtheorem12 "Definition 4.12. â€£ 4.5 Helpful Definitions
    With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")ï¼Œç¬¬äºŒæ­¥æ˜¯ç”±äº $\gamma_{j_{0}}(x)$ çš„å®šä¹‰ï¼ˆè§å¼•ç†é™ˆè¿°ï¼‰ï¼Œæœ€åä¸€æ­¥ç”±äº $R\geq
    1$ã€‚âˆ'
- en: '8.4 A Core Tool: Lipschitz Property for Several Basic Functions'
  id: totrans-980
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4 ä¸€ä¸ªæ ¸å¿ƒå·¥å…·ï¼šå¤šä¸ªåŸºæœ¬å‡½æ•°çš„åˆ©æ™®å¸ŒèŒ¨æ€§è´¨
- en: In this section, we analyze the Lipschitz property of several basic functions.
  id: totrans-981
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†å‡ ä¸ªåŸºæœ¬å‡½æ•°çš„åˆ©æ™®å¸ŒèŒ¨æ€§è´¨ã€‚
- en: Lemma 8.5  (Basic Functions Lipschitz Property).
  id: totrans-982
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 8.5ï¼ˆåŸºæœ¬å‡½æ•°çš„åˆ©æ™®å¸ŒèŒ¨æ€§è´¨ï¼‰ã€‚
- en: If the following conditions hold,
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼Œ
- en: â€¢
  id: totrans-984
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|v\|\leq R^{2}$
  id: totrans-985
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|v\|\leq R^{2}$
- en: â€¢
  id: totrans-986
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq R$
  id: totrans-987
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq R$
- en: â€¢
  id: totrans-988
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $\beta$
  id: totrans-989
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $\beta$
- en: â€¢
  id: totrans-990
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $\beta^{-1}\leq\exp(R^{2})$
  id: totrans-991
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $\beta^{-1}\leq\exp(R^{2})$
- en: â€¢
  id: totrans-992
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $R\geq 4$
  id: totrans-993
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $R\geq 4$
- en: 'Then, we have: for all $x,\widetilde{x}\in\mathbb{R}^{d^{2}}$'
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œæˆ‘ä»¬æœ‰ï¼šå¯¹æ‰€æœ‰ $x,\widetilde{x}\in\mathbb{R}^{d^{2}}$
- en: â€¢
  id: totrans-995
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 1\. $1$2
  id: totrans-996
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ 1 éƒ¨åˆ†ã€‚$1$2
- en: â€¢
  id: totrans-997
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 2\. $|\alpha(x)^{-1}-\alpha^{-1}(\widetilde{x})|\leq n\exp(4R^{2})\cdot\|x-\widetilde{x}\|_{2}$
  id: totrans-998
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ 2 éƒ¨åˆ†ã€‚$|\alpha(x)^{-1}-\alpha^{-1}(\widetilde{x})|\leq n\exp(4R^{2})\cdot\|x-\widetilde{x}\|_{2}$
- en: â€¢
  id: totrans-999
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 3\. $1$2
  id: totrans-1000
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ 3 éƒ¨åˆ†ã€‚$1$2
- en: â€¢
  id: totrans-1001
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 4\. $1$2
  id: totrans-1002
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ 4 éƒ¨åˆ†ã€‚$1$2
- en: â€¢
  id: totrans-1003
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 5\. $1$2
  id: totrans-1004
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ 5 éƒ¨åˆ†ã€‚$1$2
- en: Proof.
  id: totrans-1005
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: Proof of Part 1.
  id: totrans-1006
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ 1 éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: '|  | $\displaystyle\&#124;u(x)_{j_{0}}-u(\widetilde{x})_{j_{0}}\&#124;_{2}=$
    |  |'
  id: totrans-1007
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;u(x)_{j_{0}}-u(\widetilde{x})_{j_{0}}\&#124;_{2}=$
    |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1008
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1009
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1010
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1011
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1012
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is due to DefinitionÂ [4.8](#S4.Thmtheorem8 "Definition
    4.8\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), the second step is because of
    FactÂ [4.2](#S4.Thmtheorem2 "Fact 4.2\. â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time"), the third step
    is based on FactÂ [4.2](#S4.Thmtheorem2 "Fact 4.2\. â€£ 4.1 Basic Facts â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the fourth
    step follows from FactÂ [4.3](#S4.Thmtheorem3 "Fact 4.3\. â€£ 4.1 Basic Facts â€£ 4
    Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")
    , fifth step is due to $\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq R$.'
  id: totrans-1013
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ç¬¬ä¸€æ­¥ç”±äºå®šä¹‰ [4.8](#S4.Thmtheorem8 "Definition 4.8\. â€£ 4.3 Helpful Definitions
    With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time")ï¼Œç¬¬äºŒæ­¥å› ä¸ºäº‹å® [4.2](#S4.Thmtheorem2 "Fact 4.2\. â€£ 4.1 Basic Facts
    â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")ï¼Œç¬¬ä¸‰æ­¥åŸºäºäº‹å® [4.2](#S4.Thmtheorem2 "Fact 4.2\. â€£ 4.1 Basic Facts â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")ï¼Œç¬¬å››æ­¥è·Ÿéšäº‹å®
    [4.3](#S4.Thmtheorem3 "Fact 4.3\. â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")ï¼Œç¬¬äº”æ­¥ç”±äº $\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq
    R$ã€‚'
- en: Proof of Part 2 We have
  id: totrans-1014
  prefs: []
  type: TYPE_NORMAL
  zh: Part 2 çš„è¯æ˜æˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle&#124;\alpha(x)^{-1}_{j_{0}}-\alpha(\widetilde{x})^{-1}_{j_{0}}&#124;\leq$
    |  |'
  id: totrans-1015
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle&#124;\alpha(x)^{-1}_{j_{0}}-\alpha(\widetilde{x})^{-1}_{j_{0}}&#124;\leq$
    |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1016
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1017
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1018
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1019
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1020
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is due to simple algebra, the second step is due to $\beta\geq\langle
    u(x)_{j_{0}},{\bf 1}_{n}\rangle$ (see DefinitionÂ [4.9](#S4.Thmtheorem9 "Definition
    4.9\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")), the fourth step is based on FactÂ [4.1](#S4.Thmtheorem1
    "Fact 4.1\. â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") and FactÂ [4.2](#S4.Thmtheorem2 "Fact 4.2\. â€£ 4.1
    Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), the fifth step is because of Part 1, and the sixth step follows from .'
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ç¬¬ä¸€æ­¥ç”±äºç®€å•ä»£æ•°ï¼Œç¬¬äºŒæ­¥ç”±äº $\beta\geq\langle u(x)_{j_{0}},{\bf 1}_{n}\rangle$ï¼ˆè§å®šä¹‰ [4.9](#S4.Thmtheorem9
    "Definition 4.9\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")ï¼‰ï¼Œç¬¬å››æ­¥åŸºäºäº‹å®
    [4.1](#S4.Thmtheorem1 "Fact 4.1\. â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") å’Œäº‹å® [4.2](#S4.Thmtheorem2 "Fact
    4.2\. â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")ï¼Œç¬¬äº”æ­¥å› ä¸º Part 1ï¼Œç¬¬å…­æ­¥è·Ÿéš ã€‚'
- en: Proof of Part 3. We have
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
  zh: Part 3 çš„è¯æ˜æˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;f(x)_{j_{0}}-f(\widetilde{x})_{j_{0}}\&#124;_{2}=$
    |  |'
  id: totrans-1023
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;f(x)_{j_{0}}-f(\widetilde{x})_{j_{0}}\&#124;_{2}=$
    |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1024
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-1025
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1026
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is due to DefinitionÂ [4.10](#S4.Thmtheorem10 "Definition
    4.10\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), the second step is based on triangle
    inequality, the third step follows from FactÂ [4.2](#S4.Thmtheorem2 "Fact 4.2\.
    â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), the fourth follows from combination of Part 1, Part 2 and
    LemmaÂ [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound). â€£ 8.3 A
    Core Tool: Upper Bound for Several Basic Functions â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥}
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-1027
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ç¬¬ä¸€æ­¥åŸºäºå®šä¹‰Â [4.10](#S4.Thmtheorem10 "å®šä¹‰ 4.10. â€£ 4.3 å…³äº ğ‘‹ çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§é‡æ–°æ„å»ºå•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")ï¼Œç¬¬äºŒæ­¥åŸºäºä¸‰è§’ä¸ç­‰å¼ï¼Œç¬¬ä¸‰æ­¥ä¾æ®äº‹å®Â [4.2](#S4.Thmtheorem2
    "äº‹å® 4.2. â€£ 4.1 åŸºæœ¬äº‹å® â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§é‡æ–°æ„å»ºå•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")ï¼Œç¬¬å››æ­¥åˆ™æºäºç¬¬
    1 éƒ¨åˆ†ã€ç¬¬ 2 éƒ¨åˆ†å’Œå¼•ç†Â [8.4](#S8.Thmtheorem4 "å¼•ç† 8.4 (åŸºæœ¬å‡½æ•°ä¸Šç•Œ). â€£ 8.3 æ ¸å¿ƒå·¥å…·ï¼šè‹¥å¹²åŸºæœ¬å‡½æ•°çš„ä¸Šç•Œ â€£
    8 ğ»_{ğ‘¥,ğ‘¥} çš„ Lipschitz æ€§è´¨ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§é‡æ–°æ„å»ºå•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£") çš„ç»„åˆã€‚
- en: Proof of Part 4. We have
  id: totrans-1028
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ 4 éƒ¨åˆ†çš„è¯æ˜ã€‚æˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle&#124;\gamma_{j_{0}}(x)-\gamma_{j_{0}}(\widetilde{x})&#124;=$
    |  |'
  id: totrans-1029
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle&#124;\gamma_{j_{0}}(x)-\gamma_{j_{0}}(\widetilde{x})&#124;=$
    |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1030
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1031
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1032
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: where the first step is based on the definition of $\gamma_{j_{0}}(x)$ and $R\geq
    4$.
  id: totrans-1033
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ç¬¬ä¸€æ­¥åŸºäº $\gamma_{j_{0}}(x)$ çš„å®šä¹‰å’Œ $R\geq 4$ã€‚
- en: Proof of Part 5. We have
  id: totrans-1034
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ 5 éƒ¨åˆ†çš„è¯æ˜ã€‚æˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle&#124;c(x,:)_{j_{0},i_{0}}-c(\widetilde{x},:)_{j_{0},i_{0}}&#124;=$
    |  |'
  id: totrans-1035
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle&#124;c(x,:)_{j_{0},i_{0}}-c(\widetilde{x},:)_{j_{0},i_{0}}&#124;=$
    |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1036
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1037
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step follows from DefinitionÂ [4.12](#S4.Thmtheorem12 "Definition
    4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the second
    step is based on the definition of $\gamma_{j_{0}}(x)$ and the last step follows
    from Part 4. âˆ'
  id: totrans-1038
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ç¬¬ä¸€æ­¥åŸºäºå®šä¹‰Â [4.12](#S4.Thmtheorem12 "å®šä¹‰ 4.12. â€£ 4.5 å…³äº ğ‘‹ å’Œ ğ‘Œ çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§é‡æ–°æ„å»ºå•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")ï¼Œç¬¬äºŒæ­¥åŸºäº
    $\gamma_{j_{0}}(x)$ çš„å®šä¹‰ï¼Œæœ€åä¸€æ­¥æ¥è‡ªç¬¬ 4 éƒ¨åˆ†ã€‚âˆ
- en: For convenient, we define
  id: totrans-1039
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ–¹ä¾¿ï¼Œæˆ‘ä»¬å®šä¹‰
- en: Definition 8.6.
  id: totrans-1040
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å®šä¹‰ 8.6ã€‚
- en: We define $R_{0}$ as follows
  id: totrans-1041
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®šä¹‰ $R_{0}$ å¦‚ä¸‹
- en: '|  | $\displaystyle R_{0}:=n^{1.5}\exp(10R^{2}).$ |  |'
  id: totrans-1042
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle R_{0}:=n^{1.5}\exp(10R^{2}).$ |  |'
- en: '8.5 Calculation: Step 1 Lipschitz for Matrix Function $c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ
    v)$'
  id: totrans-1043
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5 è®¡ç®—ï¼šæ­¥éª¤ 1 Lipschitz å¯¹äºçŸ©é˜µå‡½æ•° $c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ
    v)$
- en: In this section, we introduce our calculation of Lipschitz for $c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ
    v)$.
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»å¯¹ $c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ v)$
    çš„ Lipschitz è®¡ç®—ã€‚
- en: Lemma 8.7.
  id: totrans-1045
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 8.7ã€‚
- en: If the following conditions
  id: totrans-1046
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶
- en: â€¢
  id: totrans-1047
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $G_{1}(x)=c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ
    v)$
  id: totrans-1048
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $G_{1}(x)=c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ v)$
- en: â€¢
  id: totrans-1049
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $R_{0}$ be defined as Definition[8.6](#S8.Thmtheorem6 "Definition 8.6\.
    â€£ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions â€£ 8 Lipschitz
    Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")'
  id: totrans-1050
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $R_{0}$ å®šä¹‰å¦‚å®šä¹‰ [8.6](#S8.Thmtheorem6 "å®šä¹‰ 8.6. â€£ 8.4 æ ¸å¿ƒå·¥å…·ï¼šè‹¥å¹²åŸºæœ¬å‡½æ•°çš„ Lipschitz
    æ€§è´¨ â€£ 8 ğ»_{ğ‘¥,ğ‘¥} çš„ Lipschitz æ€§è´¨ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§é‡æ–°æ„å»ºå•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")
- en: â€¢
  id: totrans-1051
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $\operatorname{\mathsf{A}}\in\mathbb{R}^{n^{2}\times d^{2}}$ be defined
    as Definition[4.8](#S4.Thmtheorem8 "Definition 4.8\. â€£ 4.3 Helpful Definitions
    With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time")'
  id: totrans-1052
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ä»¤ $\operatorname{\mathsf{A}}\in\mathbb{R}^{n^{2}\times d^{2}}$ å®šä¹‰ä¸ºå®šä¹‰[4.8](#S4.Thmtheorem8
    "Definition 4.8\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
- en: â€¢
  id: totrans-1053
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1054
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ä»¤ $\alpha(x)_{j_{0}}\in\mathbb{R}$ å®šä¹‰ä¸ºå®šä¹‰[4.9](#S4.Thmtheorem9 "Definition 4.9\.
    â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")'
- en: â€¢
  id: totrans-1055
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1056
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ä»¤ $f(x)_{j_{0}}\in\mathbb{R}^{n}$ å®šä¹‰ä¸ºå®šä¹‰[4.10](#S4.Thmtheorem10 "Definition
    4.10\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")'
- en: â€¢
  id: totrans-1057
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4
    Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1058
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ä»¤ $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ å®šä¹‰ä¸ºå®šä¹‰[4.12](#S4.Thmtheorem12 "Definition
    4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
- en: â€¢
  id: totrans-1059
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  id: totrans-1060
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
- en: â€¢
  id: totrans-1061
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  id: totrans-1062
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
- en: â€¢
  id: totrans-1063
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $R\geq 4$
  id: totrans-1064
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $R\geq 4$
- en: Then, we have
  id: totrans-1065
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;G_{1}(x)-G_{1}(\widetilde{x})\&#124;\leq 10R^{4}\cdot
    R_{0}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
  id: totrans-1066
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{1}(x)-G_{1}(\widetilde{x})\&#124;\leq 10R^{4}\cdot
    R_{0}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
- en: Proof.
  id: totrans-1067
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: We define
  id: totrans-1068
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®šä¹‰
- en: '|  | $\displaystyle G_{1,1}=$ |  |'
  id: totrans-1069
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{1,1}=$ |  |'
- en: '|  | $\displaystyle G_{1,2}=$ |  |'
  id: totrans-1070
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{1,2}=$ |  |'
- en: we have
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;G_{1,1}\&#124;=$ |  |'
  id: totrans-1072
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{1,1}\&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1073
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1074
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1075
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is based on definitionÂ $G_{1,1}$, the second step is due
    to FactÂ [4.3](#S4.Thmtheorem3 "Fact 4.3\. â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£
    A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on
    Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the third
    step follows from LemmaÂ [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper
    Bound). â€£ 8.3 A Core Tool: Upper Bound for Several Basic Functions â€£ 8 Lipschitz
    Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), and the fourth step is because of LemmaÂ [8.5](#S8.Thmtheorem5 "Lemma 8.5
    (Basic Functions Lipschitz Property). â€£ 8.4 A Core Tool: Lipschitz Property for
    Several Basic Functions â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1076
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ï¼Œç¬¬ä¸€æ­¥åŸºäºå®šä¹‰Â $G_{1,1}$ï¼Œç¬¬äºŒæ­¥åŸºäºäº‹å®Â [4.3](#S4.Thmtheorem3 "Fact 4.3\. â€£ 4.1 Basic
    Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")ï¼Œç¬¬ä¸‰æ­¥æ¥æºäºå¼•ç†Â [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound).
    â€£ 8.3 A Core Tool: Upper Bound for Several Basic Functions â€£ 8 Lipschitz Property
    of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")ï¼Œç¬¬å››æ­¥åˆ™æ˜¯ç”±äºå¼•ç†Â [8.5](#S8.Thmtheorem5
    "Lemma 8.5 (Basic Functions Lipschitz Property). â€£ 8.4 A Core Tool: Lipschitz
    Property for Several Basic Functions â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time")ã€‚'
- en: Additionally, we have
  id: totrans-1077
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;G_{1,2}\&#124;=$ |  |'
  id: totrans-1078
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{1,2}\&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1079
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1080
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1081
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is because of definition of $G_{1,2}$, the second step
    is due to FactÂ [4.3](#S4.Thmtheorem3 "Fact 4.3\. â€£ 4.1 Basic Facts â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the third
    step follows from LemmaÂ [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper
    Bound). â€£ 8.3 A Core Tool: Upper Bound for Several Basic Functions â€£ 8 Lipschitz
    Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), and the fourth step is because of LemmaÂ [8.5](#S8.Thmtheorem5 "Lemma 8.5
    (Basic Functions Lipschitz Property). â€£ 8.4 A Core Tool: Lipschitz Property for
    Several Basic Functions â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1082
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ç¬¬ä¸€æ­¥æ˜¯ç”±äº $G_{1,2}$ çš„å®šä¹‰ï¼Œç¬¬äºŒæ­¥æ˜¯ç”±äºäº‹å® [4.3](#S4.Thmtheorem3 "Fact 4.3\. â€£ 4.1 Basic
    Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")ï¼Œç¬¬ä¸‰æ­¥æ¥è‡ªå¼•ç† [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound).
    â€£ 8.3 A Core Tool: Upper Bound for Several Basic Functions â€£ 8 Lipschitz Property
    of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")ï¼Œç¬¬å››æ­¥æ˜¯ç”±äºå¼•ç†
    [8.5](#S8.Thmtheorem5 "Lemma 8.5 (Basic Functions Lipschitz Property). â€£ 8.4 A
    Core Tool: Lipschitz Property for Several Basic Functions â€£ 8 Lipschitz Property
    of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")ã€‚'
- en: Combining the above two equations, we complete the proof. âˆ
  id: totrans-1083
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“åˆä¸Šè¿°ä¸¤ä¸ªæ–¹ç¨‹ï¼Œæˆ‘ä»¬å®Œæˆäº†è¯æ˜ã€‚âˆ
- en: '8.6 Calculation: Step 2 Lipschitz for Matrix Function $-\gamma_{j_{0}}(x)\cdot
    c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ v)$'
  id: totrans-1084
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.6 è®¡ç®—ï¼šç¬¬2æ­¥ çŸ©é˜µå‡½æ•° $-\gamma_{j_{0}}(x)\cdot c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ
    v)$ çš„ Lipschitz å¸¸æ•°
- en: In this section, we introduce our calculation of Lipschitz for $-\gamma_{j_{0}}(x)\cdot
    c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ v)$.
  id: totrans-1085
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»‹ç» $-\gamma_{j_{0}}(x)\cdot c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ
    v)$ çš„ Lipschitz å¸¸æ•°è®¡ç®—ã€‚
- en: Lemma 8.8.
  id: totrans-1086
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 8.8ã€‚
- en: If the following conditions hold
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶
- en: â€¢
  id: totrans-1088
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $1$2
  id: totrans-1089
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $1$2
- en: â€¢
  id: totrans-1090
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1091
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'è®¾ $\alpha(x)_{j_{0}}\in\mathbb{R}$ å¦‚å®šä¹‰[4.9](#S4.Thmtheorem9 "Definition 4.9\.
    â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") æ‰€å®šä¹‰'
- en: â€¢
  id: totrans-1092
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1093
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'è®¾ $f(x)_{j_{0}}\in\mathbb{R}^{n}$ å¦‚å®šä¹‰[4.10](#S4.Thmtheorem10 "Definition 4.10\.
    â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") æ‰€å®šä¹‰'
- en: â€¢
  id: totrans-1094
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4
    Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1095
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'è®¾ $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ å¦‚å®šä¹‰[4.12](#S4.Thmtheorem12 "Definition
    4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time") æ‰€å®šä¹‰'
- en: â€¢
  id: totrans-1096
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  id: totrans-1097
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
- en: â€¢
  id: totrans-1098
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $R\geq 4$
  id: totrans-1099
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $R\geq 4$
- en: Then, we have
  id: totrans-1100
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;G_{2}(x)-G_{2}(\widetilde{x})\&#124;\leq 10R^{4}\cdot
    R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
  id: totrans-1101
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{2}(x)-G_{2}(\widetilde{x})\&#124;\leq 10R^{4}\cdot
    R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
- en: Proof.
  id: totrans-1102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: We define
  id: totrans-1103
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®šä¹‰
- en: '|  | $\displaystyle G_{2,1}=$ |  |'
  id: totrans-1104
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{2,1}=$ |  |'
- en: '|  | $\displaystyle G_{2,2}=$ |  |'
  id: totrans-1105
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{2,2}=$ |  |'
- en: '|  | $\displaystyle G_{2,3}=$ |  |'
  id: totrans-1106
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{2,3}=$ |  |'
- en: We have
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;G_{2,1}\&#124;=$ |  |'
  id: totrans-1108
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{2,1}\&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1109
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1110
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1111
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is because of definition of $G_{2,1}$, the second step
    is due to FactÂ [4.3](#S4.Thmtheorem3 "Fact 4.3\. â€£ 4.1 Basic Facts â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the third
    step follows from LemmaÂ [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper
    Bound). â€£ 8.3 A Core Tool: Upper Bound for Several Basic Functions â€£ 8 Lipschitz
    Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), and the fourth step is because of LemmaÂ [8.5](#S8.Thmtheorem5 "Lemma 8.5
    (Basic Functions Lipschitz Property). â€£ 8.4 A Core Tool: Lipschitz Property for
    Several Basic Functions â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1112
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ç¬¬ä¸€æ­¥æ˜¯å› ä¸º $G_{2,1}$ çš„å®šä¹‰ï¼Œç¬¬äºŒæ­¥æ˜¯ç”±äºäº‹å®Â [4.3](#S4.Thmtheorem3 "Fact 4.3\. â€£ 4.1 Basic
    Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")ï¼Œç¬¬ä¸‰æ­¥æ˜¯æ ¹æ®å¼•ç†Â [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound).
    â€£ 8.3 A Core Tool: Upper Bound for Several Basic Functions â€£ 8 Lipschitz Property
    of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")ï¼Œç¬¬å››æ­¥æ˜¯å› ä¸ºå¼•ç†Â [8.5](#S8.Thmtheorem5
    "Lemma 8.5 (Basic Functions Lipschitz Property). â€£ 8.4 A Core Tool: Lipschitz
    Property for Several Basic Functions â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time")ã€‚'
- en: Additionally, we have
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;G_{2,2}\&#124;=$ |  |'
  id: totrans-1114
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{2,2}\&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1115
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1116
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1117
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is because of definition of $G_{2,2}$, the second step
    is due to FactÂ [4.3](#S4.Thmtheorem3 "Fact 4.3\. â€£ 4.1 Basic Facts â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the third
    step follows from LemmaÂ [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper
    Bound). â€£ 8.3 A Core Tool: Upper Bound for Several Basic Functions â€£ 8 Lipschitz
    Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), and the fourth step is because of LemmaÂ [8.5](#S8.Thmtheorem5 "Lemma 8.5
    (Basic Functions Lipschitz Property). â€£ 8.4 A Core Tool: Lipschitz Property for
    Several Basic Functions â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1118
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ç¬¬ä¸€æ­¥æ˜¯å› ä¸º $G_{2,2}$ çš„å®šä¹‰ï¼Œç¬¬äºŒæ­¥æ˜¯ç”±äºäº‹å®Â [4.3](#S4.Thmtheorem3 "Fact 4.3\. â€£ 4.1 Basic
    Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")ï¼Œç¬¬ä¸‰æ­¥æ˜¯æ ¹æ®å¼•ç†Â [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound).
    â€£ 8.3 A Core Tool: Upper Bound for Several Basic Functions â€£ 8 Lipschitz Property
    of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")ï¼Œç¬¬å››æ­¥æ˜¯å› ä¸ºå¼•ç†Â [8.5](#S8.Thmtheorem5
    "Lemma 8.5 (Basic Functions Lipschitz Property). â€£ 8.4 A Core Tool: Lipschitz
    Property for Several Basic Functions â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time")ã€‚'
- en: Additionally, we have
  id: totrans-1119
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;G_{2,3}\&#124;=$ |  |'
  id: totrans-1120
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{2,3}\&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1121
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1122
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is because of definition of $G_{2,3}$, the second step
    is due to FactÂ [4.3](#S4.Thmtheorem3 "Fact 4.3\. â€£ 4.1 Basic Facts â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the third
    step follows from LemmaÂ [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper
    Bound). â€£ 8.3 A Core Tool: Upper Bound for Several Basic Functions â€£ 8 Lipschitz
    Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") and LemmaÂ [8.5](#S8.Thmtheorem5 "Lemma 8.5 (Basic Functions Lipschitz Property).
    â€£ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions â€£ 8 Lipschitz
    Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  id: totrans-1123
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ç¬¬ä¸€æ­¥æ˜¯ç”±äº $G_{2,3}$ çš„å®šä¹‰ï¼Œç¬¬äºŒæ­¥æ˜¯ç”±äºäº‹å® [4.3](#S4.Thmtheorem3 "äº‹å® 4.3\. â€£ 4.1 åŸºæœ¬äº‹å® â€£
    4 åˆæ­¥ â€£ ä¸€ç§å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§é‡æ–°è¡¨è¿° LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³å®ƒ")ï¼Œç¬¬ä¸‰æ­¥æ¥è‡ªå¼•ç† [8.4](#S8.Thmtheorem4
    "å¼•ç† 8.4ï¼ˆåŸºæœ¬å‡½æ•°ä¸Šç•Œï¼‰ã€‚ â€£ 8.3 æ ¸å¿ƒå·¥å…·ï¼šå‡ ä¸ªåŸºæœ¬å‡½æ•°çš„ä¸Šç•Œ â€£ 8 Lipschitz å±æ€§ ğ»_{ğ‘¥,ğ‘¥} â€£ ä¸€ç§å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§é‡æ–°è¡¨è¿°
    LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³å®ƒ") å’Œå¼•ç† [8.5](#S8.Thmtheorem5 "å¼•ç† 8.5ï¼ˆåŸºæœ¬å‡½æ•° Lipschitz å±æ€§ï¼‰ã€‚
    â€£ 8.4 æ ¸å¿ƒå·¥å…·ï¼šå‡ ä¸ªåŸºæœ¬å‡½æ•°çš„ Lipschitz å±æ€§ â€£ 8 Lipschitz å±æ€§ ğ»_{ğ‘¥,ğ‘¥} â€£ ä¸€ç§å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§é‡æ–°è¡¨è¿°
    LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³å®ƒ")ã€‚
- en: Combining all the above equations finish the proof. âˆ
  id: totrans-1124
  prefs: []
  type: TYPE_NORMAL
  zh: å°†ä¸Šè¿°æ‰€æœ‰æ–¹ç¨‹ç»“åˆèµ·æ¥å®Œæˆè¯æ˜ã€‚âˆ
- en: '8.7 Calculation: Step 3 Lipschitz for Matrix Function $-2\gamma_{j_{0}}(x)\cdot(f(x)_{j_{0}}\circ
    v)f(x)_{j_{0}}^{\top}$'
  id: totrans-1125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7 è®¡ç®—ï¼šæ­¥éª¤ 3 Lipschitz å¯¹çŸ©é˜µå‡½æ•° $-2\gamma_{j_{0}}(x)\cdot(f(x)_{j_{0}}\circ v)f(x)_{j_{0}}^{\top}$
- en: In this section, we introduce our calculation of Lipschitz for $-2\gamma_{j_{0}}(x)\cdot(f(x)_{j_{0}}\circ
    v)f(x)_{j_{0}}^{\top}$.
  id: totrans-1126
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†å¯¹ $-2\gamma_{j_{0}}(x)\cdot(f(x)_{j_{0}}\circ v)f(x)_{j_{0}}^{\top}$
    çš„ Lipschitz è®¡ç®—ã€‚
- en: Lemma 8.9.
  id: totrans-1127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 8.9ã€‚
- en: If the following conditions hold
  id: totrans-1128
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶
- en: â€¢
  id: totrans-1129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $1$2.
  id: totrans-1130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $1$2ã€‚
- en: â€¢
  id: totrans-1131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $R_{0}$ be defined in Definition[8.6](#S8.Thmtheorem6 "Definition 8.6\.
    â€£ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions â€£ 8 Lipschitz
    Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  id: totrans-1132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $R_{0}$ æ ¹æ®å®šä¹‰[8.6](#S8.Thmtheorem6 "å®šä¹‰ 8.6\. â€£ 8.4 æ ¸å¿ƒå·¥å…·ï¼šå‡ ä¸ªåŸºæœ¬å‡½æ•°çš„ Lipschitz å±æ€§
    â€£ 8 Lipschitz å±æ€§ ğ»_{ğ‘¥,ğ‘¥} â€£ ä¸€ç§å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§é‡æ–°è¡¨è¿° LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³å®ƒ")
    å®šä¹‰ã€‚
- en: â€¢
  id: totrans-1133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $\alpha(x)_{j_{0}}\in\mathbb{R}$ æ ¹æ®å®šä¹‰[4.9](#S4.Thmtheorem9 "å®šä¹‰ 4.9\. â€£ 4.3
    å…³äº ğ‘‹ çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ ä¸€ç§å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§é‡æ–°è¡¨è¿° LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³å®ƒ") å®šä¹‰ã€‚
- en: â€¢
  id: totrans-1135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾$f(x)_{j_{0}}\in\mathbb{R}^{n}$ æ ¹æ®å®šä¹‰[4.10](#S4.Thmtheorem10 "å®šä¹‰ 4.10\. â€£ 4.3
    å…³äº ğ‘‹ çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ ä¸€ç§å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§é‡æ–°è¡¨è¿° LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³å®ƒ")
- en: â€¢
  id: totrans-1137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4
    Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ æ ¹æ®å®šä¹‰[4.12](#S4.Thmtheorem12 "å®šä¹‰ 4.12\.
    â€£ 4.5 å…³äº ğ‘‹ å’Œ ğ‘Œ çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ ä¸€ç§å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§é‡æ–°è¡¨è¿° LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³å®ƒ")
- en: â€¢
  id: totrans-1139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  id: totrans-1140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
- en: â€¢
  id: totrans-1141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  id: totrans-1142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
- en: â€¢
  id: totrans-1143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $R\geq 4$
  id: totrans-1144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $R\geq 4$
- en: Then, we have
  id: totrans-1145
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;G_{3}(x)-G_{3}(\widetilde{x})\&#124;\leq 10R^{4}\cdot
    R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
  id: totrans-1146
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{3}(x)-G_{3}(\widetilde{x})\&#124;\leq 10R^{4}\cdot
    R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
- en: Proof.
  id: totrans-1147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: We define
  id: totrans-1148
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®šä¹‰
- en: '|  | $\displaystyle G_{3,1}=$ |  |'
  id: totrans-1149
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{3,1}=$ |  |'
- en: '|  | $\displaystyle G_{3,2}=$ |  |'
  id: totrans-1150
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{3,2}=$ |  |'
- en: '|  | $\displaystyle G_{3,3}=$ |  |'
  id: totrans-1151
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{3,3}=$ |  |'
- en: For $G_{3,1}$, we have
  id: totrans-1152
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº $G_{3,1}$ï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;G_{3,1}\&#124;\leq$ |  |'
  id: totrans-1153
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\|G_{3,1}\|\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1154
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is based on FactÂ [4.3](#S4.Thmtheorem3 "Fact 4.3\. â€£ 4.1
    Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") and the second step is due to LemmaÂ [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic
    Functions Upper Bound). â€£ 8.3 A Core Tool: Upper Bound for Several Basic Functions
    â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time") and LemmaÂ [8.5](#S8.Thmtheorem5 "Lemma 8.5 (Basic Functions
    Lipschitz Property). â€£ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions
    â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time").'
  id: totrans-1155
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ç¬¬ä¸€æ­¥åŸºäºäº‹å®[4.3](#S4.Thmtheorem3 "äº‹å® 4.3. â€£ 4.1 åŸºæœ¬äº‹å® â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’ŒSVMæŠ€å·§é‡æ–°è¡¨è¿°LLMä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")ï¼Œç¬¬äºŒæ­¥ç”±äºå¼•ç†[8.4](#S8.Thmtheorem4
    "å¼•ç† 8.4ï¼ˆåŸºæœ¬å‡½æ•°ä¸Šç•Œï¼‰ã€‚ â€£ 8.3 æ ¸å¿ƒå·¥å…·ï¼šè‹¥å¹²åŸºæœ¬å‡½æ•°çš„ä¸Šç•Œ â€£ 8 Lipschitzå±æ€§ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’ŒSVMæŠ€å·§é‡æ–°è¡¨è¿°LLMä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")å’Œå¼•ç†[8.5](#S8.Thmtheorem5
    "å¼•ç† 8.5ï¼ˆåŸºæœ¬å‡½æ•°Lipschitzå±æ€§ï¼‰ã€‚ â€£ 8.4 æ ¸å¿ƒå·¥å…·ï¼šè‹¥å¹²åŸºæœ¬å‡½æ•°çš„Lipschitzå±æ€§ â€£ 8 Lipschitzå±æ€§ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’ŒSVMæŠ€å·§é‡æ–°è¡¨è¿°LLMä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")ã€‚
- en: Similarly, we have
  id: totrans-1156
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ ·ï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;G_{3,2}\&#124;\leq 2R_{0}\cdot R^{4}\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  id: totrans-1157
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\|G_{3,2}\|\leq 2R_{0}\cdot R^{4}\|x-\widetilde{x}\|_{2}$
    |  |'
- en: and
  id: totrans-1158
  prefs: []
  type: TYPE_NORMAL
  zh: å’Œ
- en: '|  | $\displaystyle\&#124;G_{3,3}\&#124;\leq 2R_{0}\cdot R^{4}\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  id: totrans-1159
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\|G_{3,3}\|\leq 2R_{0}\cdot R^{4}\|x-\widetilde{x}\|_{2}$
    |  |'
- en: âˆ
  id: totrans-1160
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ
- en: '8.8 Calculation: Step 4 Lipschitz for Matrix Function $-c(x,:)_{j_{0},i_{0}}\cdot(f(x)_{j_{0}}\circ
    v)f(x)_{j_{0}}^{\top}$'
  id: totrans-1161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.8 è®¡ç®—ï¼šæ­¥éª¤ 4 å¯¹çŸ©é˜µå‡½æ•°$-c(x,:)_{j_{0},i_{0}}\cdot(f(x)_{j_{0}}\circ v)f(x)_{j_{0}}^{\top}$çš„Lipschitzè®¡ç®—
- en: In this section, we introduce our calculation of Lipschitz for $-c(x,:)_{j_{0},i_{0}}\cdot(f(x)_{j_{0}}\circ
    v)f(x)_{j_{0}}^{\top}$.
  id: totrans-1162
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†å¯¹$-c(x,:)_{j_{0},i_{0}}\cdot(f(x)_{j_{0}}\circ v)f(x)_{j_{0}}^{\top}$çš„Lipschitzè®¡ç®—ã€‚
- en: Lemma 8.10.
  id: totrans-1163
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 8.10ã€‚
- en: If the following conditions hold
  id: totrans-1164
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶
- en: â€¢
  id: totrans-1165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $\alpha(x)_{j_{0}}\in\mathbb{R}$ å¦‚å®šä¹‰[4.9](#S4.Thmtheorem9 "å®šä¹‰ 4.9. â€£ 4.3 å…³äºğ‘‹çš„æœ‰ç”¨å®šä¹‰
    â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’ŒSVMæŠ€å·§é‡æ–°è¡¨è¿°LLMä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")
- en: â€¢
  id: totrans-1167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $f(x)_{j_{0}}\in\mathbb{R}^{n}$ å¦‚å®šä¹‰[4.10](#S4.Thmtheorem10 "å®šä¹‰ 4.10. â€£ 4.3
    å…³äºğ‘‹çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’ŒSVMæŠ€å·§é‡æ–°è¡¨è¿°LLMä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")
- en: â€¢
  id: totrans-1169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4
    Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ å¦‚å®šä¹‰[4.12](#S4.Thmtheorem12 "å®šä¹‰ 4.12. â€£
    4.5 å…³äºğ‘‹å’Œğ‘Œçš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’ŒSVMæŠ€å·§é‡æ–°è¡¨è¿°LLMä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")
- en: â€¢
  id: totrans-1171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  id: totrans-1172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
- en: â€¢
  id: totrans-1173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  id: totrans-1174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
- en: â€¢
  id: totrans-1175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $R\geq 4$
  id: totrans-1176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $R\geq 4$
- en: â€¢
  id: totrans-1177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $1$2
  id: totrans-1178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $1$2
- en: Then, we have
  id: totrans-1179
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;G_{4}(x)-G_{4}(\widetilde{x})\&#124;\leq 10R^{4}\cdot
    R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
  id: totrans-1180
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\|G_{4}(x)-G_{4}(\widetilde{x})\|\leq 10R^{4}\cdot R_{0}\|x-\widetilde{x}\|_{2}$
    |  |'
- en: Proof.
  id: totrans-1181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: We define
  id: totrans-1182
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®šä¹‰
- en: '|  | $\displaystyle G_{4,1}=$ |  |'
  id: totrans-1183
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{4,1}=$ |  |'
- en: '|  | $\displaystyle G_{4,2}=$ |  |'
  id: totrans-1184
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{4,2}=$ |  |'
- en: '|  | $\displaystyle G_{4,3}=$ |  |'
  id: totrans-1185
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{4,3}=$ |  |'
- en: For $G_{4,1}$, we have
  id: totrans-1186
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº $G_{4,1}$ï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;G_{4,1}\&#124;\leq R^{2}\cdot R_{0}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  id: totrans-1187
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{4,1}\&#124;\leq R^{2}\cdot R_{0}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
- en: For $G_{4,2}$, we have
  id: totrans-1188
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº $G_{4,2}$ï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;G_{4,2}\&#124;\leq 2R^{4}\cdot R_{0}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  id: totrans-1189
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{4,2}\&#124;\leq 2R^{4}\cdot R_{0}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
- en: For $G_{4,3}$, we have
  id: totrans-1190
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº $G_{4,3}$ï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;G_{4,3}\&#124;\leq 2R^{4}\cdot R_{0}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  id: totrans-1191
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{4,3}\&#124;\leq 2R^{4}\cdot R_{0}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
- en: âˆ
  id: totrans-1192
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ
- en: '8.9 Calculation: Step 5 Lipschitz for Matrix Function $-2\gamma_{j_{0}}(x)\cdot
    f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top}$'
  id: totrans-1193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.9 è®¡ç®—ï¼šæ­¥éª¤ 5 å¯¹çŸ©é˜µå‡½æ•° $-2\gamma_{j_{0}}(x)\cdot f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top}$
    çš„ Lipschitz è®¡ç®—
- en: In this section, we introduce our calculation of Lipschitz for $-2\gamma_{j_{0}}(x)\cdot
    f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top}$.
  id: totrans-1194
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬ä»‹ç»äº†å¯¹ $-2\gamma_{j_{0}}(x)\cdot f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top}$
    çš„ Lipschitz è®¡ç®—ã€‚
- en: Lemma 8.11.
  id: totrans-1195
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 8.11ã€‚
- en: If the following conditions hold
  id: totrans-1196
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶
- en: â€¢
  id: totrans-1197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $R_{0}$ be defined as Definition[8.6](#S8.Thmtheorem6 "Definition 8.6\.
    â€£ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions â€£ 8 Lipschitz
    Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")'
  id: totrans-1198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $R_{0}$ æŒ‰ç…§å®šä¹‰[8.6](#S8.Thmtheorem6 "å®šä¹‰ 8.6\. â€£ 8.4 ä¸€ä¸ªæ ¸å¿ƒå·¥å…·ï¼šå¤šä¸ªåŸºæœ¬å‡½æ•°çš„ Lipschitz
    å±æ€§ â€£ 8 Lipschitz å±æ€§ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§å¯¹ LLM ä¸­å•å±‚æ³¨æ„åŠ›è¿›è¡Œé‡æ–°è¡¨è¿°ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³") å®šä¹‰ã€‚
- en: â€¢
  id: totrans-1199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $\alpha(x)_{j_{0}}\in\mathbb{R}$ æŒ‰ç…§å®šä¹‰[4.9](#S4.Thmtheorem9 "å®šä¹‰ 4.9\. â€£ 4.3
    ä¸ ğ‘‹ ç›¸å…³çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§å¯¹ LLM ä¸­å•å±‚æ³¨æ„åŠ›è¿›è¡Œé‡æ–°è¡¨è¿°ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³") å®šä¹‰ã€‚
- en: â€¢
  id: totrans-1201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $f(x)_{j_{0}}\in\mathbb{R}^{n}$ æŒ‰ç…§å®šä¹‰[4.10](#S4.Thmtheorem10 "å®šä¹‰ 4.10\. â€£ 4.3
    ä¸ ğ‘‹ ç›¸å…³çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§å¯¹ LLM ä¸­å•å±‚æ³¨æ„åŠ›è¿›è¡Œé‡æ–°è¡¨è¿°ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³") å®šä¹‰ã€‚
- en: â€¢
  id: totrans-1203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4
    Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ æŒ‰ç…§å®šä¹‰[4.12](#S4.Thmtheorem12 "å®šä¹‰ 4.12\.
    â€£ 4.5 ä¸ ğ‘‹ å’Œ ğ‘Œ ç›¸å…³çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§å¯¹ LLM ä¸­å•å±‚æ³¨æ„åŠ›è¿›è¡Œé‡æ–°è¡¨è¿°ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")
    å®šä¹‰ã€‚
- en: â€¢
  id: totrans-1205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  id: totrans-1206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
- en: â€¢
  id: totrans-1207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  id: totrans-1208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
- en: â€¢
  id: totrans-1209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $R\geq 4$
  id: totrans-1210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $R\geq 4$
- en: â€¢
  id: totrans-1211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $1$2
  id: totrans-1212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $1$2
- en: Then, we have
  id: totrans-1213
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;G_{5}(x)-G_{5}(\widetilde{x})\&#124;\leq 10R^{4}\cdot
    R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
  id: totrans-1214
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{5}(x)-G_{5}(\widetilde{x})\&#124;\leq 10R^{4}\cdot
    R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
- en: Proof.
  id: totrans-1215
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: 'This proof is similar to the proof of LemmaÂ [8.9](#S8.Thmtheorem9 "Lemma 8.9\.
    â€£ 8.7 Calculation: Step 3 Lipschitz for Matrix Function -2â¢ğ›¾_ğ‘—â‚€â¢(ğ‘¥)â‹…(ğ‘“â¢(ğ‘¥)_ğ‘—â‚€âˆ˜ğ‘£)â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€^âŠ¤
    â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), so we omit it here. âˆ'
  id: totrans-1216
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¯æ˜ç±»ä¼¼äºå¼•ç† [8.9](#S8.Thmtheorem9 "å¼•ç† 8.9\. â€£ 8.7 è®¡ç®—ï¼šæ­¥éª¤ 3 å¯¹çŸ©é˜µå‡½æ•° -2â¢ğ›¾_ğ‘—â‚€â¢(ğ‘¥)â‹…(ğ‘“â¢(ğ‘¥)_ğ‘—â‚€âˆ˜ğ‘£)â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€^âŠ¤
    â€£ 8 Lipschitz å±æ€§ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§å¯¹ LLM ä¸­å•å±‚æ³¨æ„åŠ›è¿›è¡Œé‡æ–°è¡¨è¿°ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³") çš„è¯æ˜ï¼Œå› æ­¤åœ¨è¿™é‡Œçœç•¥ã€‚âˆ
- en: '8.10 Calculation: Step 6 Lipschitz for Matrix Function $-c(x,:)_{j_{0},i_{0}}\cdot
    f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top}$'
  id: totrans-1217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.10 è®¡ç®—ï¼šæ­¥éª¤ 6 å¯¹çŸ©é˜µå‡½æ•° $-c(x,:)_{j_{0},i_{0}}\cdot f(x)_{j_{0}}(f(x)_{j_{0}}\circ
    v)^{\top}$ çš„ Lipschitz è®¡ç®—
- en: In this section, we introduce our calculation of Lipschitz for $-c(x,:)_{j_{0},i_{0}}\cdot
    f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top}$.
  id: totrans-1218
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬ä»‹ç»äº†å¯¹ $-c(x,:)_{j_{0},i_{0}}\cdot f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top}$
    çš„ Lipschitz è®¡ç®—ã€‚
- en: Lemma 8.12.
  id: totrans-1219
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 8.12ã€‚
- en: If the following conditions hold
  id: totrans-1220
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶
- en: â€¢
  id: totrans-1221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $\alpha(x)_{j_{0}}\in\mathbb{R}$ å®šä¹‰ä¸ºå®šä¹‰[4.9](#S4.Thmtheorem9 "å®šä¹‰ 4.9\. â€£ 4.3
    æœ‰å…³ ğ‘‹ çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿° LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶ä»¥çŸ©é˜µä¹˜æ³•æ—¶é—´æ±‚è§£")
- en: â€¢
  id: totrans-1223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $f(x)_{j_{0}}\in\mathbb{R}^{n}$ å®šä¹‰ä¸ºå®šä¹‰[4.10](#S4.Thmtheorem10 "å®šä¹‰ 4.10\. â€£
    4.3 æœ‰å…³ ğ‘‹ çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿° LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶ä»¥çŸ©é˜µä¹˜æ³•æ—¶é—´æ±‚è§£")
- en: â€¢
  id: totrans-1225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4
    Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ å®šä¹‰ä¸ºå®šä¹‰[4.12](#S4.Thmtheorem12 "å®šä¹‰ 4.12\.
    â€£ 4.5 æœ‰å…³ ğ‘‹ å’Œ ğ‘Œ çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿° LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶ä»¥çŸ©é˜µä¹˜æ³•æ—¶é—´æ±‚è§£")
- en: â€¢
  id: totrans-1227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  id: totrans-1228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
- en: â€¢
  id: totrans-1229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  id: totrans-1230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
- en: â€¢
  id: totrans-1231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $R\geq 4$
  id: totrans-1232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $R\geq 4$
- en: â€¢
  id: totrans-1233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $1$2
  id: totrans-1234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $1$2
- en: Then, we have
  id: totrans-1235
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;G_{5}(x)-G_{5}(\widetilde{x})\&#124;\leq 10R^{4}\cdot
    R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
  id: totrans-1236
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{5}(x)-G_{5}(\widetilde{x})\&#124;\leq 10R^{4}\cdot
    R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
- en: Proof.
  id: totrans-1237
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: 'This proof is similar to the proof of LemmaÂ [8.10](#S8.Thmtheorem10 "Lemma
    8.10\. â€£ 8.8 Calculation: Step 4 Lipschitz for Matrix Function -ğ‘â¢(ğ‘¥,:)_{ğ‘—â‚€,ğ‘–â‚€}â‹…(ğ‘“â¢(ğ‘¥)_ğ‘—â‚€âˆ˜ğ‘£)â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€^âŠ¤
    â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), so we omit it here. âˆ'
  id: totrans-1238
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¯æ˜ç±»ä¼¼äºå¼•ç†Â [8.10](#S8.Thmtheorem10 "å¼•ç† 8.10\. â€£ 8.8 è®¡ç®—ï¼šæ­¥éª¤ 4 Lipschitz å¯¹çŸ©é˜µå‡½æ•° -ğ‘â¢(ğ‘¥,:)_{ğ‘—â‚€,ğ‘–â‚€}â‹…(ğ‘“â¢(ğ‘¥)_ğ‘—â‚€âˆ˜ğ‘£)â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€^âŠ¤
    â€£ 8 Lipschitz æ€§è´¨ ğ»_{ğ‘¥,ğ‘¥} â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿° LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶ä»¥çŸ©é˜µä¹˜æ³•æ—¶é—´æ±‚è§£"), å› æ­¤æˆ‘ä»¬åœ¨æ­¤çœç•¥ã€‚
    âˆ
- en: '8.11 Calculation: Step 7 Lipschitz for Matrix Function $2\gamma_{j_{0}}(x)c(x,:)_{j_{0},i_{0}}\cdot
    f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$'
  id: totrans-1239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.11 è®¡ç®—ï¼šæ­¥éª¤ 7 Lipschitz å¯¹çŸ©é˜µå‡½æ•° $2\gamma_{j_{0}}(x)c(x,:)_{j_{0},i_{0}}\cdot f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
- en: In this section, we introduce our calculation of Lipschitz for $2\gamma_{j_{0}}(x)c(x,:)_{j_{0},i_{0}}\cdot
    f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$.
  id: totrans-1240
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»å¯¹ $2\gamma_{j_{0}}(x)c(x,:)_{j_{0},i_{0}}\cdot f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
    çš„ Lipschitz è®¡ç®—ã€‚
- en: Lemma 8.13.
  id: totrans-1241
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 8.13ã€‚
- en: If the following conditions hold
  id: totrans-1242
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶
- en: â€¢
  id: totrans-1243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $\alpha(x)_{j_{0}}\in\mathbb{R}$ å®šä¹‰ä¸ºå®šä¹‰[4.9](#S4.Thmtheorem9 "å®šä¹‰ 4.9\. â€£ 4.3
    æœ‰å…³ ğ‘‹ çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿° LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶ä»¥çŸ©é˜µä¹˜æ³•æ—¶é—´æ±‚è§£")
- en: â€¢
  id: totrans-1245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $f(x)_{j_{0}}\in\mathbb{R}^{n}$ å®šä¹‰ä¸ºå®šä¹‰[4.10](#S4.Thmtheorem10 "å®šä¹‰ 4.10\. â€£
    4.3 æœ‰å…³ ğ‘‹ çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿° LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶ä»¥çŸ©é˜µä¹˜æ³•æ—¶é—´æ±‚è§£")
- en: â€¢
  id: totrans-1247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4
    Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ å®šä¹‰ä¸ºå®šä¹‰[4.12](#S4.Thmtheorem12 "å®šä¹‰ 4.12\.
    â€£ 4.5 æœ‰å…³ ğ‘‹ å’Œ ğ‘Œ çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿° LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶ä»¥çŸ©é˜µä¹˜æ³•æ—¶é—´æ±‚è§£")
- en: â€¢
  id: totrans-1249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  id: totrans-1250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
- en: â€¢
  id: totrans-1251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  id: totrans-1252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
- en: â€¢
  id: totrans-1253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $R\geq 4$
  id: totrans-1254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $R\geq 4$
- en: â€¢
  id: totrans-1255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $1$2
  id: totrans-1256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $1$2
- en: Then, we have
  id: totrans-1257
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;G_{7}(x)-G_{7}(\widetilde{x})\&#124;\leq 10R^{4}R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  id: totrans-1258
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{7}(x)-G_{7}(\widetilde{x})\&#124;\leq 10R^{4}R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
- en: Proof.
  id: totrans-1259
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: We define
  id: totrans-1260
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®šä¹‰
- en: '|  | $\displaystyle G_{7,1}=$ |  |'
  id: totrans-1261
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{7,1}=$ |  |'
- en: '|  | $\displaystyle G_{7,2}=$ |  |'
  id: totrans-1262
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{7,2}=$ |  |'
- en: '|  | $\displaystyle G_{7,3}=$ |  |'
  id: totrans-1263
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{7,3}=$ |  |'
- en: '|  | $\displaystyle G_{7,4}=$ |  |'
  id: totrans-1264
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{7,4}=$ |  |'
- en: For $G_{7,1}$, we have
  id: totrans-1265
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº $G_{7,1}$ï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;G_{7,1}\&#124;=$ |  |'
  id: totrans-1266
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{7,1}\&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1267
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1268
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1269
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is due to the definition of $G_{7,1}$, the second step
    is because of FactÂ [4.3](#S4.Thmtheorem3 "Fact 4.3\. â€£ 4.1 Basic Facts â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the third
    step is based on Part 4 of LemmaÂ [8.5](#S8.Thmtheorem5 "Lemma 8.5 (Basic Functions
    Lipschitz Property). â€£ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions
    â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time") and FactÂ [4.3](#S4.Thmtheorem3 "Fact 4.3\. â€£ 4.1 Basic Facts
    â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), and the last step comes from Part 4 and Part 6 of LemmaÂ [8.4](#S8.Thmtheorem4
    "Lemma 8.4 (Basic Functions Upper Bound). â€£ 8.3 A Core Tool: Upper Bound for Several
    Basic Functions â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time").'
  id: totrans-1270
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ç¬¬ä¸€æ­¥æºäº $G_{7,1}$ çš„å®šä¹‰ï¼Œç¬¬äºŒæ­¥æ˜¯ç”±äºäº‹å® [4.3](#S4.Thmtheorem3 "Fact 4.3\. â€£ 4.1 Basic
    Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")ï¼Œç¬¬ä¸‰æ­¥åŸºäºå¼•ç† [8.5](#S8.Thmtheorem5 "Lemma 8.5 (Basic Functions Lipschitz Property).
    â€£ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions â€£ 8 Lipschitz
    Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") ç¬¬ 4 éƒ¨åˆ†å’Œäº‹å® [4.3](#S4.Thmtheorem3 "Fact 4.3\. â€£ 4.1 Basic Facts â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")ï¼Œæœ€åä¸€æ­¥æ¥è‡ªå¼•ç†
    [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound). â€£ 8.3 A Core Tool:
    Upper Bound for Several Basic Functions â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A
    Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time") ç¬¬ 4 éƒ¨åˆ†å’Œç¬¬ 6 éƒ¨åˆ†ã€‚'
- en: Similarly, for $G_{7,2}$, we have
  id: totrans-1271
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼åœ°ï¼Œå¯¹äº $G_{7,2}$ï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;G_{7,2}\&#124;\leq 2R_{0}\cdot R^{2}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  id: totrans-1272
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{7,2}\&#124;\leq 2R_{0}\cdot R^{2}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
- en: For $G_{7,3}$, we have
  id: totrans-1273
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº $G_{7,3}$ï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;G_{7,3}\&#124;\leq 2R_{0}\cdot 2R^{4}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  id: totrans-1274
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{7,3}\&#124;\leq 2R_{0}\cdot 2R^{4}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
- en: For $G_{7,4}$, we have
  id: totrans-1275
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº $G_{7,4}$ï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;G_{7,4}\&#124;\leq 2R_{0}\cdot 2R^{4}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  id: totrans-1276
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{7,4}\&#124;\leq 2R_{0}\cdot 2R^{4}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
- en: âˆ
  id: totrans-1277
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ
- en: '8.12 Calculation: Step 8 Lipschitz for Matrix Function $\gamma_{j_{0}}(x)^{2}\cdot
    f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$'
  id: totrans-1278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.12 è®¡ç®—ï¼šæ­¥éª¤ 8 çŸ©é˜µå‡½æ•° $\gamma_{j_{0}}(x)^{2}\cdot f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
    çš„ Lipschitz
- en: In this section, we introduce our calculation of Lipschitz for $\gamma_{j_{0}}(x)^{2}\cdot
    f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$.
  id: totrans-1279
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬ä»‹ç»äº†å¯¹ $\gamma_{j_{0}}(x)^{2}\cdot f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$ çš„
    Lipschitz è®¡ç®—ã€‚
- en: Lemma 8.14.
  id: totrans-1280
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 8.14ã€‚
- en: If the following conditions hold
  id: totrans-1281
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä»¥ä¸‹æ¡ä»¶æˆç«‹
- en: â€¢
  id: totrans-1282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'è®¾ $\alpha(x)_{j_{0}}\in\mathbb{R}$ å®šä¹‰å¦‚å®šä¹‰[4.9](#S4.Thmtheorem9 "Definition 4.9\.
    â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")'
- en: â€¢
  id: totrans-1284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $f(x)_{j_{0}}\in\mathbb{R}^{n}$ å¦‚å®šä¹‰[4.10](#S4.Thmtheorem10 "å®šä¹‰ 4.10\. â€£ 4.3
    å…³äº ğ‘‹ çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿° LLM çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")
- en: â€¢
  id: totrans-1286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4
    Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ å¦‚å®šä¹‰[4.12](#S4.Thmtheorem12 "å®šä¹‰ 4.12\.
    â€£ 4.5 å…³äº ğ‘‹ å’Œ ğ‘Œ çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿° LLM çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")
- en: â€¢
  id: totrans-1288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  id: totrans-1289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
- en: â€¢
  id: totrans-1290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  id: totrans-1291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$ï¼Œ$\|x\|_{2}\leq R$ï¼Œ$\|v\|_{2}\leq R^{2}$
- en: â€¢
  id: totrans-1292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $R\geq 4$
  id: totrans-1293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $R\geq 4$
- en: â€¢
  id: totrans-1294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $G_{8,1}=\gamma_{j_{0}}(x)^{2}\cdot f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
  id: totrans-1295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $G_{8,1}=\gamma_{j_{0}}(x)^{2}\cdot f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
- en: Then, we have
  id: totrans-1296
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¾—åˆ°
- en: '|  | $\displaystyle\&#124;G_{8}(x)-G_{8}(\widetilde{x})\&#124;\leq 10R^{4}R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  id: totrans-1297
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{8}(x)-G_{8}(\widetilde{x})\&#124;\leq 10R^{4}R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
- en: Proof.
  id: totrans-1298
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: We define
  id: totrans-1299
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®šä¹‰
- en: '|  | $\displaystyle G_{8,1}=$ |  |'
  id: totrans-1300
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{8,1}=$ |  |'
- en: '|  | $\displaystyle G_{8,2}=$ |  |'
  id: totrans-1301
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{8,2}=$ |  |'
- en: '|  | $\displaystyle G_{8,3}=$ |  |'
  id: totrans-1302
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{8,3}=$ |  |'
- en: '|  | $\displaystyle G_{8,4}=$ |  |'
  id: totrans-1303
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{8,4}=$ |  |'
- en: We can show that
  id: totrans-1304
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è¯æ˜
- en: '|  | $\displaystyle\max_{i\in[4]}\&#124;G_{8,i}\&#124;\leq R^{4}\cdot R_{0}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  id: totrans-1305
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{i\in[4]}\&#124;G_{8,i}\&#124;\leq R^{4}\cdot R_{0}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
- en: âˆ
  id: totrans-1306
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ
- en: '8.13 Calculation: Step 9 Lipschitz for Matrix Function $(f(x)_{j_{0}}\circ
    v)\cdot(f(x)_{j_{0}}\circ v)^{\top}$'
  id: totrans-1307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.13 è®¡ç®—ï¼šç¬¬ 9 æ­¥ Lipschitz å¯¹çŸ©é˜µå‡½æ•° $(f(x)_{j_{0}}\circ v)\cdot(f(x)_{j_{0}}\circ
    v)^{\top}$
- en: In this section, we introduce our calculation of Lipschitz for $(f(x)_{j_{0}}\circ
    v)\cdot(f(x)_{j_{0}}\circ v)^{\top}$.
  id: totrans-1308
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»å¯¹ $(f(x)_{j_{0}}\circ v)\cdot(f(x)_{j_{0}}\circ v)^{\top}$ çš„ Lipschitz
    è®¡ç®—ã€‚
- en: Lemma 8.15.
  id: totrans-1309
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 8.15ã€‚
- en: If the following conditions hold
  id: totrans-1310
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶
- en: â€¢
  id: totrans-1311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $\alpha(x)_{j_{0}}\in\mathbb{R}$ å¦‚å®šä¹‰[4.9](#S4.Thmtheorem9 "å®šä¹‰ 4.9\. â€£ 4.3
    å…³äº ğ‘‹ çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿° LLM çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")
- en: â€¢
  id: totrans-1313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $f(x)_{j_{0}}\in\mathbb{R}^{n}$ å¦‚å®šä¹‰[4.10](#S4.Thmtheorem10 "å®šä¹‰ 4.10\. â€£ 4.3
    å…³äº ğ‘‹ çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿° LLM çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")
- en: â€¢
  id: totrans-1315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4
    Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ å¦‚å®šä¹‰[4.12](#S4.Thmtheorem12 "å®šä¹‰ 4.12\.
    â€£ 4.5 å…³äº ğ‘‹ å’Œ ğ‘Œ çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿° LLM çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")
- en: â€¢
  id: totrans-1317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  id: totrans-1318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
- en: â€¢
  id: totrans-1319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  id: totrans-1320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$ï¼Œ$\|x\|_{2}\leq R$ï¼Œ$\|v\|_{2}\leq R^{2}$
- en: â€¢
  id: totrans-1321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $R\geq 4$
  id: totrans-1322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $R\geq 4$
- en: â€¢
  id: totrans-1323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $G_{9}(x)=(f(x)_{j_{0}}\circ v)\cdot(f(x)_{j_{0}}\circ v)^{\top}$
  id: totrans-1324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $G_{9}(x)=(f(x)_{j_{0}}\circ v)\cdot(f(x)_{j_{0}}\circ v)^{\top}$
- en: Then, we have
  id: totrans-1325
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¾—åˆ°
- en: '|  | $\displaystyle\&#124;G_{9}(x)-G_{9}(\widetilde{x})\&#124;\leq 10R^{4}R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  id: totrans-1326
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{9}(x)-G_{9}(\widetilde{x})\&#124;\leq 10R^{4}R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
- en: Proof.
  id: totrans-1327
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: We define
  id: totrans-1328
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®šä¹‰
- en: '|  | $\displaystyle G_{9,1}=$ |  |'
  id: totrans-1329
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{9,1}=$ |  |'
- en: '|  | $\displaystyle G_{9,2}=$ |  |'
  id: totrans-1330
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{9,2}=$ |  |'
- en: We can show that
  id: totrans-1331
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è¯æ˜
- en: '|  | $\displaystyle\max_{i\in[2]}\&#124;G_{9,i}\&#124;\leq R^{4}\cdot R_{0}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  id: totrans-1332
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{i\in[2]}\&#124;G_{9,i}\&#124;\leq R^{4}\cdot R_{0}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
- en: âˆ
  id: totrans-1333
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ
- en: 9 Hessian for $X$ Is PSD
  id: totrans-1334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 Hessian å¯¹ $X$ æ˜¯ PSD
- en: 'In SectionÂ [9.1](#S9.SS1 "9.1 Main Result â€£ 9 Hessian for ğ‘‹ Is PSD â€£ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time"), we present the
    main result of PSD bound for Hessian. In SectionÂ [9.2](#S9.SS2 "9.2 PSD Bound
    â€£ 9 Hessian for ğ‘‹ Is PSD â€£ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), we show the PSD bound for $B(x)$. Throughout this section, we will use
    the symbol $H$ for the sake of simplicity.'
  id: totrans-1335
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬[9.1](#S9.SS1 "9.1 ä¸»è¦ç»“æœ â€£ 9 Hessian å¯¹ ğ‘‹ æ˜¯ PSD â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§é‡æ–°æ„å»ºå•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´ä¸­è§£å†³å®ƒ")èŠ‚ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†Hessiançš„PSDç•Œé™ã€‚åœ¨ç¬¬[9.2](#S9.SS2
    "9.2 PSD ç•Œé™ â€£ 9 Hessian å¯¹ ğ‘‹ æ˜¯ PSD â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§é‡æ–°æ„å»ºå•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´ä¸­è§£å†³å®ƒ")èŠ‚ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†$B(x)$çš„PSDç•Œé™ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œä¸ºäº†ç®€ä¾¿èµ·è§ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç¬¦å·$H$ã€‚
- en: 9.1 Main Result
  id: totrans-1336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1 ä¸»è¦ç»“æœ
- en: In this section, we introduce the main result of the PSD bound for Hessian.
  id: totrans-1337
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Hessiançš„PSDç•Œé™çš„ä¸»è¦ç»“æœã€‚
- en: Lemma 9.1.
  id: totrans-1338
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 9.1ã€‚
- en: If the following conditions hold
  id: totrans-1339
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶
- en: â€¢
  id: totrans-1340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $j_{0}\in[n]$
  id: totrans-1341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾$j_{0}\in[n]$
- en: â€¢
  id: totrans-1342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $i_{0}\in[d]$
  id: totrans-1343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾$i_{0}\in[d]$
- en: â€¢
  id: totrans-1344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $H_{j_{0},i_{0}}=\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}x\mathrm{d}x}\in\mathbb{R}^{d^{2}\times
    d^{2}}$
  id: totrans-1345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾$H_{j_{0},i_{0}}=\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}x\mathrm{d}x}\in\mathbb{R}^{d^{2}\times
    d^{2}}$
- en: â€¢
  id: totrans-1346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $B_{j_{0},i_{0}}(x)\in\mathbb{R}^{n\times n}$ be defined as Definition[7.3](#S7.Thmtheorem3
    "Definition 7.3\. â€£ 7.3 Defining ğµâ¢(ğ‘¥) â€£ 7 Hessian for ğ‘‹ â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1347
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾$B_{j_{0},i_{0}}(x)\in\mathbb{R}^{n\times n}$çš„å®šä¹‰è§å®šä¹‰[7.3](#S7.Thmtheorem3 "å®šä¹‰
    7.3\. â€£ 7.3 å®šä¹‰ ğµâ¢(ğ‘¥) â€£ 7 Hessian å¯¹ ğ‘‹ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œæ”¯æŒå‘é‡æœºæŠ€å·§é‡æ–°æ„å»ºå•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´ä¸­è§£å†³å®ƒ")ã€‚
- en: â€“
  id: totrans-1348
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€“
- en: Therefore, $1$2
  id: totrans-1349
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œ$1$2
- en: â€¢
  id: totrans-1350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $\max_{j_{0}\in[n]}\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq R$
  id: totrans-1351
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾$\max_{j_{0}\in[n]}\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq R$
- en: â€¢
  id: totrans-1352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $\sigma_{\min}$.
  id: totrans-1353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾$\sigma_{\min}$ã€‚
- en: â€¢
  id: totrans-1354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $H=\sum_{j_{0}=1}^{n}\sum_{i_{0}=1}^{d}H_{j_{0},i_{0}}$
  id: totrans-1355
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾$H=\sum_{j_{0}=1}^{n}\sum_{i_{0}=1}^{d}H_{j_{0},i_{0}}$
- en: â€¢
  id: totrans-1356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $H_{\operatorname{reg},j_{0},i_{0}}=\operatorname{\mathsf{A}}_{j_{0}}^{\top}(B_{j_{0},i_{0}}(x)+W^{2})\operatorname{\mathsf{A}}_{j_{0}}$
    is a positive diagonal matrix.
  id: totrans-1357
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾$H_{\operatorname{reg},j_{0},i_{0}}=\operatorname{\mathsf{A}}_{j_{0}}^{\top}(B_{j_{0},i_{0}}(x)+W^{2})\operatorname{\mathsf{A}}_{j_{0}}$æ˜¯ä¸€ä¸ªæ­£å¯¹è§’çŸ©é˜µã€‚
- en: â€¢
  id: totrans-1358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $H_{\operatorname{reg}}=\sum_{j_{0}=1}^{n}\sum_{i_{0}=1}^{d}H_{\operatorname{reg},j_{0},i_{0}}$
  id: totrans-1359
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾$H_{\operatorname{reg}}=\sum_{j_{0}=1}^{n}\sum_{i_{0}=1}^{d}H_{\operatorname{reg},j_{0},i_{0}}$
- en: â€¢
  id: totrans-1360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $C_{0}:=30R^{8}$ (be a local parameter in this lemma)
  id: totrans-1361
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾$C_{0}:=30R^{8}$ï¼ˆåœ¨æœ¬å¼•ç†ä¸­çš„å±€éƒ¨å‚æ•°ï¼‰
- en: â€¢
  id: totrans-1362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let  (denote
    the strongly convex parameter for hessian)
  id: totrans-1363
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ï¼ˆè¡¨ç¤ºHessiançš„å¼ºå‡¸å‚æ•°ï¼‰
- en: Then, we have
  id: totrans-1364
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬æœ‰
- en: â€¢
  id: totrans-1365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 1. For each $j_{0}\in[n]$
  id: totrans-1366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬1éƒ¨åˆ†ã€‚å¯¹äºæ¯ä¸ª$j_{0}\in[n]$
- en: '|  | $\displaystyle-C_{0}I_{n}\preceq B_{j_{0},i_{0}}(x)\preceq C_{0}I_{n}$
    |  |'
  id: totrans-1367
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle-C_{0}I_{n}\preceq B_{j_{0},i_{0}}(x)\preceq C_{0}I_{n}$
    |  |'
- en: â€¢
  id: totrans-1368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 2. For each $j_{0}\in[n]$
  id: totrans-1369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬2éƒ¨åˆ†ã€‚å¯¹äºæ¯ä¸ª$j_{0}\in[n]$
- en: '|  | $\displaystyle\&#124;H_{j_{0},i_{0}}(x)\&#124;\leq C_{0}R^{2}.$ |  |'
  id: totrans-1370
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;H_{j_{0},i_{0}}(x)\&#124;\leq C_{0}R^{2}.$ |  |'
- en: â€¢
  id: totrans-1371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 3. For each $j_{0}\in[n]$, if $\min_{j_{1}\in[n]}w_{j_{1},j_{1}}\geq\frac{l}{\sigma_{\min}(\operatorname{\mathsf{A}}_{j_{0}})^{2}}+C_{0}$,
    then we have
  id: totrans-1372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬3éƒ¨åˆ†ã€‚å¯¹äºæ¯ä¸ª$j_{0}\in[n]$ï¼Œå¦‚æœ$\min_{j_{1}\in[n]}w_{j_{1},j_{1}}\geq\frac{l}{\sigma_{\min}(\operatorname{\mathsf{A}}_{j_{0}})^{2}}+C_{0}$ï¼Œåˆ™æˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle H_{\operatorname{reg},j_{0},i_{0}}(x)\succeq l\cdot I_{d^{2}}$
    |  |'
  id: totrans-1373
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle H_{\operatorname{reg},j_{0},i_{0}}(x)\succeq l\cdot I_{d^{2}}$
    |  |'
- en: â€¢
  id: totrans-1374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 4. For each $j_{0}\in[n]$, if $\min_{j_{1}\in[n]}w_{j_{1},j_{1}}\geq\frac{l}{\sigma_{\min}(\operatorname{\mathsf{A}}_{j_{0}})^{2}}+100\cdot
    C_{0}$, then we have
  id: totrans-1375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬4éƒ¨åˆ†ã€‚å¯¹äºæ¯ä¸ª$j_{0}\in[n]$ï¼Œå¦‚æœ$\min_{j_{1}\in[n]}w_{j_{1},j_{1}}\geq\frac{l}{\sigma_{\min}(\operatorname{\mathsf{A}}_{j_{0}})^{2}}+100\cdot
    C_{0}$ï¼Œåˆ™æˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle 1.1\cdot(B(x)_{j_{0},i_{0}}+W^{2})\succeq W^{2}\succeq
    0.9\cdot(B(x)_{j_{0},i_{0}}+W^{2})$ |  |'
  id: totrans-1376
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle 1.1\cdot(B(x)_{j_{0},i_{0}}+W^{2})\succeq W^{2}\succeq
    0.9\cdot(B(x)_{j_{0},i_{0}}+W^{2})$ |  |'
- en: and
  id: totrans-1377
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å’Œ
- en: '|  | $\displaystyle 1.1H_{j_{0},i_{0}}\succeq H_{\operatorname{reg},j_{0},i_{0}}\succeq
    0.9H_{j_{0},i_{0}}$ |  |'
  id: totrans-1378
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle 1.1H_{j_{0},i_{0}}\succeq H_{\operatorname{reg},j_{0},i_{0}}\succeq
    0.9H_{j_{0},i_{0}}$ |  |'
- en: â€¢
  id: totrans-1379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 5. For each $j_{0}\in[n]$, if $\min_{j_{1}\in[n]}w_{j_{1},j_{1}}\geq\frac{l}{nd\sigma_{\min}(\operatorname{\mathsf{A}}_{\min})^{2}}+C_{0}$,
    then we have
  id: totrans-1380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ 5 éƒ¨åˆ†ã€‚å¯¹äºæ¯ä¸ª $j_{0}\in[n]$ï¼Œå¦‚æœ $\min_{j_{1}\in[n]}w_{j_{1},j_{1}}\geq\frac{l}{nd\sigma_{\min}(\operatorname{\mathsf{A}}_{\min})^{2}}+C_{0}$ï¼Œé‚£ä¹ˆæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle H_{\operatorname{reg}}(x)\succeq l\cdot I_{d^{2}}$ |  |'
  id: totrans-1381
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle H_{\operatorname{reg}}(x)\succeq l\cdot I_{d^{2}}$ |  |'
- en: â€¢
  id: totrans-1382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 6. For each $j_{0}\in[n]$, if $\min_{j_{1}\in[n]}w_{j_{1},j_{1}}\geq\frac{l}{nd\sigma_{\min}(\operatorname{\mathsf{A}}_{\min})^{2}}+100\cdot
    C_{0}$, then we have
  id: totrans-1383
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ 6 éƒ¨åˆ†ã€‚å¯¹äºæ¯ä¸ª $j_{0}\in[n]$ï¼Œå¦‚æœ $\min_{j_{1}\in[n]}w_{j_{1},j_{1}}\geq\frac{l}{nd\sigma_{\min}(\operatorname{\mathsf{A}}_{\min})^{2}}+100\cdot
    C_{0}$ï¼Œé‚£ä¹ˆæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle 1.1H\succeq H_{\operatorname{reg}}\succeq 0.9H$ |  |'
  id: totrans-1384
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle 1.1H\succeq H_{\operatorname{reg}}\succeq 0.9H$ |  |'
- en: Proof.
  id: totrans-1385
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: Proof of Part 1.
  id: totrans-1386
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ 1 éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: 'It directly follows from LemmaÂ [9.2](#S9.Thmtheorem2 "Lemma 9.2\. â€£ 9.2 PSD
    Bound â€£ 9 Hessian for ğ‘‹ Is PSD â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time").'
  id: totrans-1387
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´æ¥æ¥è‡ªäºå¼•ç† [9.2](#S9.Thmtheorem2 "å¼•ç† 9.2. â€£ 9.2 PSD ç•Œé™ â€£ 9 Hessian å¯¹äº ğ‘‹ æ˜¯ PSD â€£
    åŸºäºå¼ é‡å’Œ SVM æŠ€å·§çš„ LLM å•å±‚æ³¨æ„åŠ›çš„å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")ã€‚
- en: Proof of Part 2. We have
  id: totrans-1388
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ 2 éƒ¨åˆ†çš„è¯æ˜ã€‚æˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;H_{j_{0},i_{0}}\&#124;=$ |  |'
  id: totrans-1389
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;H_{j_{0},i_{0}}\&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1390
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1391
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1392
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: where the first step follows from the $H_{j_{0},i_{0}}=\operatorname{\mathsf{A}}_{j_{0}}^{\top}B_{j_{0},i_{0}}(x)\operatorname{\mathsf{A}}_{j_{0}}$,
    and the last step follow from Part 1.
  id: totrans-1393
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ç¬¬ä¸€æ­¥æ¥è‡ª $H_{j_{0},i_{0}}=\operatorname{\mathsf{A}}_{j_{0}}^{\top}B_{j_{0},i_{0}}(x)\operatorname{\mathsf{A}}_{j_{0}}$ï¼Œæœ€åä¸€æ­¥æ¥è‡ªç¬¬
    1 éƒ¨åˆ†ã€‚
- en: Proof of Part 3.
  id: totrans-1394
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ 3 éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: The proof is similar to [[55](#bib.bib55)].
  id: totrans-1395
  prefs: []
  type: TYPE_NORMAL
  zh: è¯æ˜ä¸[[55](#bib.bib55)]ç±»ä¼¼ã€‚
- en: Proof of Part 4.
  id: totrans-1396
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ 4 éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: The proof is similar to [[55](#bib.bib55)].
  id: totrans-1397
  prefs: []
  type: TYPE_NORMAL
  zh: è¯æ˜ä¸[[55](#bib.bib55)]ç±»ä¼¼ã€‚
- en: Proof of Part 5 and Part 6. It is because we can write $H$ terms $H_{j_{0},i_{0}}$,
    $i_{0}\in[d]$. âˆ
  id: totrans-1398
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ 5 éƒ¨åˆ†å’Œç¬¬ 6 éƒ¨åˆ†çš„è¯æ˜ã€‚å› ä¸ºæˆ‘ä»¬å¯ä»¥å†™ $H$ é¡¹ $H_{j_{0},i_{0}}$ï¼Œ$i_{0}\in[d]$ã€‚âˆ
- en: 9.2 PSD Bound
  id: totrans-1399
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2 PSD ç•Œé™
- en: In this section, we analyze the PSD bound for each of the $B_{\operatorname{rank}}$.
  id: totrans-1400
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†æ¯ä¸ª $B_{\operatorname{rank}}$ çš„ PSD ç•Œé™ã€‚
- en: Lemma 9.2.
  id: totrans-1401
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 9.2ã€‚
- en: If the following condition holds
  id: totrans-1402
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä»¥ä¸‹æ¡ä»¶æˆç«‹
- en: â€¢
  id: totrans-1403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $1$2
  id: totrans-1404
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2
- en: â€¢
  id: totrans-1405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $1$2
  id: totrans-1406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2
- en: â€¢
  id: totrans-1407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $1$2
  id: totrans-1408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2
- en: â€¢
  id: totrans-1409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $B_{\operatorname{rank}}^{3}:=(f(x)_{j_{0}}\circ v)\cdot(f(x)_{j_{0}}\circ v)^{\top}$
  id: totrans-1410
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $B_{\operatorname{rank}}^{3}:=(f(x)_{j_{0}}\circ v)\cdot(f(x)_{j_{0}}\circ v)^{\top}$
- en: â€¢
  id: totrans-1411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $|\gamma(x)_{j_{0}}|\leq R^{2}$
  id: totrans-1412
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $|\gamma(x)_{j_{0}}|\leq R^{2}$
- en: â€¢
  id: totrans-1413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $|c(x,:)_{j_{0},i_{0}}|\leq 2R^{2}$
  id: totrans-1414
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $|c(x,:)_{j_{0},i_{0}}|\leq 2R^{2}$
- en: â€¢
  id: totrans-1415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|v\|_{2}\leq R^{2}$
  id: totrans-1416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|v\|_{2}\leq R^{2}$
- en: Then, we have
  id: totrans-1417
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬æœ‰
- en: â€¢
  id: totrans-1418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 1.
  id: totrans-1419
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ 1 éƒ¨åˆ†ã€‚
- en: '|  | $\displaystyle-8R^{6}\cdot I_{n}\preceq B_{\operatorname{diag}}^{1}\preceq
    8R^{6}\cdot I_{n}$ |  |'
  id: totrans-1420
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle-8R^{6}\cdot I_{n}\preceq B_{\operatorname{diag}}^{1}\preceq
    8R^{6}\cdot I_{n}$ |  |'
- en: â€¢
  id: totrans-1421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 2.
  id: totrans-1422
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ 2 éƒ¨åˆ†ã€‚
- en: '|  | $\displaystyle-16R^{8}\cdot I_{n}\preceq B_{\operatorname{rank}}^{1}\preceq
    16R^{8}\cdot I_{n}$ |  |'
  id: totrans-1423
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle-16R^{8}\cdot I_{n}\preceq B_{\operatorname{rank}}^{1}\preceq
    16R^{8}\cdot I_{n}$ |  |'
- en: â€¢
  id: totrans-1424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 3.
  id: totrans-1425
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ 3 éƒ¨åˆ†ã€‚
- en: '|  | $\displaystyle-8R^{4}\cdot I_{n}\preceq B_{\operatorname{rank}}^{2}\preceq
    8R^{4}\cdot I_{n}$ |  |'
  id: totrans-1426
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle-8R^{4}\cdot I_{n}\preceq B_{\operatorname{rank}}^{2}\preceq
    8R^{4}\cdot I_{n}$ |  |'
- en: â€¢
  id: totrans-1427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 4.
  id: totrans-1428
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ 4 éƒ¨åˆ†ã€‚
- en: '|  | $\displaystyle 0\cdot I_{n}\preceq B_{\operatorname{rank}}^{3}\preceq
    8R^{4}\cdot I_{n}$ |  |'
  id: totrans-1429
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle 0\cdot I_{n}\preceq B_{\operatorname{rank}}^{3}\preceq
    8R^{4}\cdot I_{n}$ |  |'
- en: Proof.
  id: totrans-1430
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: Proof of Part 1.
  id: totrans-1431
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ 1 éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: '|  | $\displaystyle B_{\operatorname{diag}}^{1}=$ |  |'
  id: totrans-1432
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle B_{\operatorname{diag}}^{1}=$ |  |'
- en: '|  | $\displaystyle\preceq$ |  |'
  id: totrans-1433
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\preceq$ |  |'
- en: '|  | $\displaystyle\preceq$ |  |'
  id: totrans-1434
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\preceq$ |  |'
- en: where the first step follows from the definition of $B_{\operatorname{diag}}^{1}$,
    and $\|v\|_{2}\leq R^{2}$.
  id: totrans-1435
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ç¬¬ä¸€æ­¥æºè‡ª $B_{\operatorname{diag}}^{1}$ çš„å®šä¹‰ï¼Œå¹¶ä¸” $\|v\|_{2}\leq R^{2}$ã€‚
- en: Proof of Part 2.
  id: totrans-1436
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ 2 éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: '|  | $\displaystyle B_{\operatorname{rank}}^{1}=$ |  |'
  id: totrans-1437
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle B_{\operatorname{rank}}^{1}=$ |  |'
- en: '|  | $\displaystyle\succeq$ |  |'
  id: totrans-1438
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\succeq$ |  |'
- en: '|  | $\displaystyle\succeq$ |  |'
  id: totrans-1439
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\succeq$ |  |'
- en: '|  | $\displaystyle\succeq$ |  |'
  id: totrans-1440
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\succeq$ |  |'
- en: '|  | $\displaystyle\succeq$ |  |'
  id: totrans-1441
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\succeq$ |  |'
- en: 'where the first step follows from the definition of $B_{\operatorname{rank}}^{1}$
    and FactÂ [4.4](#S4.Thmtheorem4 "Fact 4.4\. â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£
    A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on
    Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the fourth
    step follows from FactÂ [4.1](#S4.Thmtheorem1 "Fact 4.1\. â€£ 4.1 Basic Facts â€£ 4
    Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    and last step follows from LemmaÂ [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions
    Upper Bound). â€£ 8.3 A Core Tool: Upper Bound for Several Basic Functions â€£ 8 Lipschitz
    Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") and $\|v\|_{2}\leq R^{2}$.'
  id: totrans-1442
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ç¬¬ä¸€æ­¥ç”± $B_{\operatorname{rank}}^{1}$ çš„å®šä¹‰å’Œäº‹å® [4.4](#S4.Thmtheorem4 "Fact 4.4\.
    â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time") å¾—å‡ºï¼Œç¬¬å››æ­¥ç”±äº‹å® [4.1](#S4.Thmtheorem1 "Fact 4.1\. â€£ 4.1 Basic
    Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") å¾—å‡ºï¼Œæœ€åä¸€æ­¥ç”±å¼•ç† [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound).
    â€£ 8.3 A Core Tool: Upper Bound for Several Basic Functions â€£ 8 Lipschitz Property
    of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")
    å’Œ $\|v\|_{2}\leq R^{2}$ å¾—å‡ºã€‚'
- en: Proof of Part 3.
  id: totrans-1443
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: '|  | $\displaystyle B_{\operatorname{rank}}^{2}=$ |  |'
  id: totrans-1444
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle B_{\operatorname{rank}}^{2}=$ |  |'
- en: '|  | $\displaystyle\preceq$ |  |'
  id: totrans-1445
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\preceq$ |  |'
- en: '|  | $\displaystyle\preceq$ |  |'
  id: totrans-1446
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\preceq$ |  |'
- en: 'where the first step follows from definition of $B_{\operatorname{rank}}^{2}$
    and LemmaÂ [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound). â€£ 8.3
    A Core Tool: Upper Bound for Several Basic Functions â€£ 8 Lipschitz Property of
    ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-1447
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¬¬ä¸€éƒ¨åˆ†çš„è¯æ˜ç”± $B_{\operatorname{rank}}^{2}$ çš„å®šä¹‰å’Œå¼•ç† [8.4](#S8.Thmtheorem4 "Lemma
    8.4 (Basic Functions Upper Bound). â€£ 8.3 A Core Tool: Upper Bound for Several
    Basic Functions â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time") å¾—å‡ºã€‚'
- en: Proof of Part 4.
  id: totrans-1448
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬å››éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: '|  | $\displaystyle B_{\operatorname{rank}}^{3}=$ |  |'
  id: totrans-1449
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle B_{\operatorname{rank}}^{3}=$ |  |'
- en: '|  | $\displaystyle\preceq$ |  |'
  id: totrans-1450
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\preceq$ |  |'
- en: '|  | $\displaystyle\preceq$ |  |'
  id: totrans-1451
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\preceq$ |  |'
- en: '|  | $\displaystyle\preceq$ |  |'
  id: totrans-1452
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\preceq$ |  |'
- en: 'where the first step follows from definition of $B_{\operatorname{rank}}^{3}$
    and LemmaÂ [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound). â€£ 8.3
    A Core Tool: Upper Bound for Several Basic Functions â€£ 8 Lipschitz Property of
    ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-1453
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¬¬ä¸€éƒ¨åˆ†çš„è¯æ˜ç”± $B_{\operatorname{rank}}^{3}$ çš„å®šä¹‰å’Œå¼•ç† [8.4](#S8.Thmtheorem4 "Lemma
    8.4 (Basic Functions Upper Bound). â€£ 8.3 A Core Tool: Upper Bound for Several
    Basic Functions â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time") å¾—å‡ºã€‚'
- en: âˆ
  id: totrans-1454
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ
- en: 10 Hessian for $Y$
  id: totrans-1455
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10 HessiançŸ©é˜µå¯¹äº $Y$
- en: 'In SectionÂ [10.1](#S10.SS1 "10.1 Hessian Property â€£ 10 Hessian for ğ‘Œ â€£ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time"), we present the
    hessian property with respect to $Y$ for one $j_{0},i_{0}$.'
  id: totrans-1456
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨ç¬¬ [10.1](#S10.SS1 "10.1 Hessian Property â€£ 10 Hessian for ğ‘Œ â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") èŠ‚ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç›¸å¯¹äº $Y$ çš„HessiançŸ©é˜µæ€§è´¨ï¼Œå¯¹äºä¸€ä¸ª
    $j_{0},i_{0}$ã€‚'
- en: 10.1 Hessian Property
  id: totrans-1457
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1 HessiançŸ©é˜µæ€§è´¨
- en: In this section, we analyze the Hessian properties.
  id: totrans-1458
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬åˆ†æHessiançŸ©é˜µçš„æ€§è´¨ã€‚
- en: Lemma 10.1.
  id: totrans-1459
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 10.1ã€‚
- en: If the following conditions hold
  id: totrans-1460
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä»¥ä¸‹æ¡ä»¶æˆç«‹
- en: â€¢
  id: totrans-1461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $B_{j_{0}}(x)=f(x)_{j_{0}}f(x)_{j_{0}}^{\top}\in\mathbb{R}^{n\times n}$
    (because of Lemma[10.2](#S10.Thmtheorem2 "Lemma 10.2\. â€£ 10.2 Hessian for One
    ğ‘—â‚€,ğ‘–â‚€ â€£ 10 Hessian for ğ‘Œ â€£ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"))'
  id: totrans-1462
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'è®¾ $B_{j_{0}}(x)=f(x)_{j_{0}}f(x)_{j_{0}}^{\top}\in\mathbb{R}^{n\times n}$ï¼ˆå› ä¸ºå¼•ç†
    [10.2](#S10.Thmtheorem2 "Lemma 10.2\. â€£ 10.2 Hessian for One ğ‘—â‚€,ğ‘–â‚€ â€£ 10 Hessian
    for ğ‘Œ â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")ï¼‰'
- en: â€¢
  id: totrans-1463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $B(x)=\sum_{j_{0}=1}^{n}B_{j_{0}}(x)$
  id: totrans-1464
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $B(x)=\sum_{j_{0}=1}^{n}B_{j_{0}}(x)$
- en: â€¢
  id: totrans-1465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $1$2
  id: totrans-1466
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $1$2
- en: â€¢
  id: totrans-1467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $H_{i_{0}}\in\mathbb{R}^{d\times d}$
  id: totrans-1468
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $H_{i_{0}}\in\mathbb{R}^{d\times d}$
- en: â€¢
  id: totrans-1469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $H_{\operatorname{reg},i_{0}}=A_{3}^{\top}(B(x)+W^{2})A_{3}$ is a positive
    diagonal matrix
  id: totrans-1470
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $H_{\operatorname{reg},i_{0}}=A_{3}^{\top}(B(x)+W^{2})A_{3}$ æ˜¯ä¸€ä¸ªæ­£å¯¹è§’çŸ©é˜µ
- en: â€¢
  id: totrans-1471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $H(y)\in\mathbb{R}^{d^{2}\times d^{2}}$ be $$H(y)=\begin{bmatrix}H_{1}&amp;0&amp;\cdots&amp;0\\
  id: totrans-1472
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $H(y)\in\mathbb{R}^{d^{2}\times d^{2}}$ ä¸º $$H(y)=\begin{bmatrix}H_{1}&amp;0&amp;\cdots&amp;0\\
- en: 0&amp;H_{2}&amp;\cdots&amp;0\\
  id: totrans-1473
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 0&amp;H_{2}&amp;\cdots&amp;0\\
- en: \vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
  id: totrans-1474
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: \vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
- en: 0&amp;0&amp;\cdots&amp;H_{d}\end{bmatrix}$$
  id: totrans-1475
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 0&amp;0&amp;\cdots&amp;H_{d}\end{bmatrix}$$
- en: Then, we have
  id: totrans-1476
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬æœ‰
- en: â€¢
  id: totrans-1477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 1.
  id: totrans-1478
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬1éƒ¨åˆ†ã€‚
- en: '|  | $\displaystyle 0\preceq B_{j_{0}}(x)\preceq I_{n}$ |  |'
  id: totrans-1479
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle 0\preceq B_{j_{0}}(x)\preceq I_{n}$ |  |'
- en: â€¢
  id: totrans-1480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 2.
  id: totrans-1481
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬2éƒ¨åˆ†ã€‚
- en: '|  | $\displaystyle 0\preceq B(x)\preceq n\cdot I_{n}$ |  |'
  id: totrans-1482
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle 0\preceq B(x)\preceq n\cdot I_{n}$ |  |'
- en: â€¢
  id: totrans-1483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 3. If $\min_{j_{1}\in[n]}w_{j_{1},j_{1}}^{2}\geq\frac{l}{\sigma_{\min}(A_{3})^{2}}$
  id: totrans-1484
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬3éƒ¨åˆ†ã€‚å¦‚æœ $\min_{j_{1}\in[n]}w_{j_{1},j_{1}}^{2}\geq\frac{l}{\sigma_{\min}(A_{3})^{2}}$
- en: '|  | $\displaystyle H_{\operatorname{reg},i_{0}}\succeq l\cdot I_{d},~{}~{}~{}H(y)\succeq
    l\cdot I_{d^{2}}$ |  |'
  id: totrans-1485
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle H_{\operatorname{reg},i_{0}}\succeq l\cdot I_{d},~{}~{}~{}H(y)\succeq
    l\cdot I_{d^{2}}$ |  |'
- en: â€¢
  id: totrans-1486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 4. If $\min_{j_{1}\in[n]}w_{j_{1},j_{1}}^{2}\geq\frac{l}{\sigma_{\min}(A_{3})^{2}}+100n$
  id: totrans-1487
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬4éƒ¨åˆ†ã€‚å¦‚æœ $\min_{j_{1}\in[n]}w_{j_{1},j_{1}}^{2}\geq\frac{l}{\sigma_{\min}(A_{3})^{2}}+100n$
- en: '|  | $\displaystyle 0.9(W^{2}+B(x))\preceq W^{2}\preceq 1.1(W^{2}+B(x))$ |  |'
  id: totrans-1488
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle 0.9(W^{2}+B(x))\preceq W^{2}\preceq 1.1(W^{2}+B(x))$ |  |'
- en: â€¢
  id: totrans-1489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 5. Lipschitz, Due to $H(y)$, then
  id: totrans-1490
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬5éƒ¨åˆ†ã€‚Lipschitzï¼Œç”±äº $H(y)$ï¼Œåˆ™
- en: '|  | $\displaystyle\&#124;H(y)-H(\widetilde{y})\&#124;\leq\&#124;y-\widetilde{y}\&#124;_{2}$
    |  |'
  id: totrans-1491
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;H(y)-H(\widetilde{y})\&#124;\leq\&#124;y-\widetilde{y}\&#124;_{2}$
    |  |'
- en: Proof.
  id: totrans-1492
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: 'For hessian closed-form, we can obtain them from LemmaÂ [10.2](#S10.Thmtheorem2
    "Lemma 10.2\. â€£ 10.2 Hessian for One ğ‘—â‚€,ğ‘–â‚€ â€£ 10 Hessian for ğ‘Œ â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1493
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¯¹äº Hessian çš„é—­å¼å½¢å¼ï¼Œæˆ‘ä»¬å¯ä»¥ä»å¼•ç†Â [10.2](#S10.Thmtheorem2 "å¼•ç† 10.2\. â€£ 10.2 Hessian
    for One ğ‘—â‚€,ğ‘–â‚€ â€£ 10 Hessian for ğ‘Œ â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time") ä¸­è·å¾—ã€‚'
- en: The proofs are straightforward, so we omit the details here. âˆ
  id: totrans-1494
  prefs: []
  type: TYPE_NORMAL
  zh: è¯æ˜è¿‡ç¨‹å¾ˆç®€å•ï¼Œå› æ­¤åœ¨æ­¤çœç•¥è¯¦ç»†æ­¥éª¤ã€‚âˆ
- en: 10.2 Hessian for One $j_{0},i_{0}$
  id: totrans-1495
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2 ä¸€ä¸ª $j_{0},i_{0}$ çš„ Hessian
- en: In this section, we analyze the Hessian for the matrix $Y$.
  id: totrans-1496
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬åˆ†æçŸ©é˜µ $Y$ çš„ Hessianã€‚
- en: Lemma 10.2.
  id: totrans-1497
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 10.2ã€‚
- en: If the following conditions hold
  id: totrans-1498
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä»¥ä¸‹æ¡ä»¶æˆç«‹
- en: â€¢
  id: totrans-1499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: We define a temporary notation here $v:=f(x)_{j_{0}}$ in the statement. Note
    that $v$ could have different meaning in other sections.)
  id: totrans-1500
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨æ­¤å®šä¹‰ä¸€ä¸ªä¸´æ—¶ç¬¦å· $v:=f(x)_{j_{0}}$ã€‚æ³¨æ„ï¼Œ$v$ åœ¨å…¶ä»–éƒ¨åˆ†å¯èƒ½æœ‰ä¸åŒçš„å«ä¹‰ã€‚
- en: â€¢
  id: totrans-1501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $f(x)_{j_{0}}$ be defined as Definition[4.10](#S4.Thmtheorem10 "Definition
    4.10\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1502
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ä»¤ $f(x)_{j_{0}}$ å®šä¹‰ä¸ºå®šä¹‰[4.10](#S4.Thmtheorem10 "å®šä¹‰ 4.10\. â€£ 4.3 Helpful Definitions
    With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time")ã€‚'
- en: â€¢
  id: totrans-1503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $c(x,:)_{j_{0},i_{0}}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4
    Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-1504
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ä»¤ $c(x,:)_{j_{0},i_{0}}$ å®šä¹‰ä¸ºå®šä¹‰[4.12](#S4.Thmtheorem12 "å®šä¹‰ 4.12\. â€£ 4.5 Helpful
    Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")ã€‚'
- en: â€¢
  id: totrans-1505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $h(y)_{i_{0}}$ be defined as Definition[4.11](#S4.Thmtheorem11 "Definition
    4.11\. â€£ 4.4 A Helpful Definition With Respect to ğ‘Œ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1506
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ä»¤ $h(y)_{i_{0}}$ å®šä¹‰ä¸ºå®šä¹‰[4.11](#S4.Thmtheorem11 "å®šä¹‰ 4.11\. â€£ 4.4 A Helpful Definition
    With Respect to ğ‘Œ â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time")ã€‚'
- en: â€¢
  id: totrans-1507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $L_{j_{0},i_{0}}$ be defined as Definition[4.10](#S4.Thmtheorem10 "Definition
    4.10\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1508
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ä»¤ $L_{j_{0},i_{0}}$ å®šä¹‰ä¸ºå®šä¹‰[4.10](#S4.Thmtheorem10 "å®šä¹‰ 4.10\. â€£ 4.3 Helpful Definitions
    With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time")ã€‚'
- en: Then, we have
  id: totrans-1509
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬æœ‰
- en: â€¢
  id: totrans-1510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 1. For $i_{1}=i_{2}$, the diagonal case
  id: totrans-1511
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬1éƒ¨åˆ†ã€‚å¯¹äº $i_{1}=i_{2}$ï¼Œå¯¹è§’çº¿æƒ…å†µ
- en: '|  | $1$2 |  |'
  id: totrans-1512
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: â€¢
  id: totrans-1513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 2. For $i_{1}\neq i_{2}$, the off-diagonal case
  id: totrans-1514
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬2éƒ¨åˆ†ã€‚å¯¹äº $i_{1}\neq i_{2}$ï¼Œéå¯¹è§’çº¿æƒ…å†µ
- en: '|  | $1$2 |  |'
  id: totrans-1515
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: â€¢
  id: totrans-1516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 3. The $\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0}}\mathrm{d}y_{i_{0}}}\in\mathbb{R}^{d\times
    d}$
  id: totrans-1517
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰éƒ¨åˆ†. $\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0}}\mathrm{d}y_{i_{0}}}\in\mathbb{R}^{d\times
    d}$
- en: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0}}\mathrm{d}y_{i_{0}}}=A_{3}^{\top}vv^{\top}A_{3}$
    |  |'
  id: totrans-1518
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0}}\mathrm{d}y_{i_{0}}}=A_{3}^{\top}vv^{\top}A_{3}$
    |  |'
- en: Proof.
  id: totrans-1519
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: Proof of Part 1.
  id: totrans-1520
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0},i_{1}}\mathrm{d}y_{i_{0},i_{1}}}=$
    |  |'
  id: totrans-1521
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0},i_{1}}\mathrm{d}y_{i_{0},i_{1}}}=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-1522
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-1523
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-1524
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step follows from simple algebra, the second step follows from
    LemmaÂ [5.2](#S5.Thmtheorem2 "Lemma 5.2\. â€£ 5.2 Gradient With Respect to ğ‘¦ â€£ 5
    Gradient â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    the third step follows from LemmaÂ [5.2](#S5.Thmtheorem2 "Lemma 5.2\. â€£ 5.2 Gradient
    With Respect to ğ‘¦ â€£ 5 Gradient â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), and the last step follows from FactÂ [4.1](#S4.Thmtheorem1
    "Fact 4.1\. â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  id: totrans-1525
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ç¬¬ä¸€æ­¥ç”±ç®€å•ä»£æ•°å¾—å‡ºï¼Œç¬¬äºŒæ­¥ç”±å¼•ç† [5.2](#S5.Thmtheorem2 "å¼•ç† 5.2\. â€£ 5.2 å…³äº ğ‘¦ çš„æ¢¯åº¦ â€£ 5 æ¢¯åº¦ â€£
    å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ„ LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³å®ƒ") å¾—å‡ºï¼Œç¬¬ä¸‰æ­¥ç”±å¼•ç† [5.2](#S5.Thmtheorem2
    "å¼•ç† 5.2\. â€£ 5.2 å…³äº ğ‘¦ çš„æ¢¯åº¦ â€£ 5 æ¢¯åº¦ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ„ LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³å®ƒ")
    å¾—å‡ºï¼Œæœ€åä¸€æ­¥ç”±äº‹å® [4.1](#S4.Thmtheorem1 "äº‹å® 4.1\. â€£ 4.1 åŸºæœ¬äº‹å® â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM
    æŠ€å·§é‡æ„ LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³å®ƒ") å¾—å‡ºã€‚
- en: Proof of Part 2.
  id: totrans-1526
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒéƒ¨åˆ†çš„è¯æ˜ã€‚
- en: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0},i_{2}}\mathrm{d}y_{i_{0},i_{1}}}=$
    |  |'
  id: totrans-1527
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0},i_{2}}\mathrm{d}y_{i_{0},i_{1}}}=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-1528
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-1529
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-1530
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step follows from simple algebra, the second step follows from
    LemmaÂ [5.2](#S5.Thmtheorem2 "Lemma 5.2\. â€£ 5.2 Gradient With Respect to ğ‘¦ â€£ 5
    Gradient â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    the third step follows from LemmaÂ [5.2](#S5.Thmtheorem2 "Lemma 5.2\. â€£ 5.2 Gradient
    With Respect to ğ‘¦ â€£ 5 Gradient â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), and the last step follows from FactÂ [4.1](#S4.Thmtheorem1
    "Fact 4.1\. â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  id: totrans-1531
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ç¬¬ä¸€æ­¥ç”±ç®€å•ä»£æ•°å¾—å‡ºï¼Œç¬¬äºŒæ­¥ç”±å¼•ç† [5.2](#S5.Thmtheorem2 "å¼•ç† 5.2\. â€£ 5.2 å…³äº ğ‘¦ çš„æ¢¯åº¦ â€£ 5 æ¢¯åº¦ â€£
    å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ„ LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³å®ƒ") å¾—å‡ºï¼Œç¬¬ä¸‰æ­¥ç”±å¼•ç† [5.2](#S5.Thmtheorem2
    "å¼•ç† 5.2\. â€£ 5.2 å…³äº ğ‘¦ çš„æ¢¯åº¦ â€£ 5 æ¢¯åº¦ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ„ LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³å®ƒ")
    å¾—å‡ºï¼Œæœ€åä¸€æ­¥ç”±äº‹å® [4.1](#S4.Thmtheorem1 "äº‹å® 4.1\. â€£ 4.1 åŸºæœ¬äº‹å® â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM
    æŠ€å·§é‡æ„ LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³å®ƒ") å¾—å‡ºã€‚
- en: Proof of Part 3.
  id: totrans-1532
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: It follows by combining above two parts directly. âˆ
  id: totrans-1533
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ç›´æ¥ç»“åˆä»¥ä¸Šä¸¤éƒ¨åˆ†å¾—å‡ºã€‚ âˆ
- en: 11 Hessian for $X$
  id: totrans-1534
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11 å¯¹ $X$ çš„ Hessian
- en: 'In SectionÂ [11.1](#S11.SS1 "11.1 Computing Hessian â€£ 11 Hessian for ğ‘‹ and ğ‘Œ
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), we compute
    the Hessian matrix with respect to both $X$. In SectionÂ [11.2](#S11.SS2 "11.2
    A Helpful Lemma â€£ 11 Hessian for ğ‘‹ and ğ‘Œ â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), we present several helpful lemmas for the following
    proof. In SectionÂ [11.2](#S11.SS2 "11.2 A Helpful Lemma â€£ 11 Hessian for ğ‘‹ and
    ğ‘Œ â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), we create
    $B(x)$ for the further analysis.'
  id: totrans-1535
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç« èŠ‚ [11.1](#S11.SS1 "11.1 è®¡ç®— Hessian â€£ 11 ğ‘‹ å’Œ ğ‘Œ çš„ Hessian â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM
    æŠ€å·§é‡æ„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³") ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—äº†ç›¸å¯¹äº $X$ çš„ Hessian çŸ©é˜µã€‚åœ¨ç« èŠ‚ [11.2](#S11.SS2 "11.2
    ä¸€ä¸ªæœ‰ç”¨çš„å¼•ç† â€£ 11 ğ‘‹ å’Œ ğ‘Œ çš„ Hessian â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³") ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å‡ ä¸ªå¯¹åç»­è¯æ˜æœ‰å¸®åŠ©çš„å¼•ç†ã€‚åœ¨ç« èŠ‚
    [11.2](#S11.SS2 "11.2 ä¸€ä¸ªæœ‰ç”¨çš„å¼•ç† â€£ 11 ğ‘‹ å’Œ ğ‘Œ çš„ Hessian â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")
    ä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ç”¨äºè¿›ä¸€æ­¥åˆ†æçš„ $B(x)$ã€‚
- en: 11.1 Computing Hessian
  id: totrans-1536
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1 è®¡ç®— Hessian
- en: In this section, we compute the Hessian matrix for $X$.
  id: totrans-1537
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—äº† $X$ çš„ Hessian çŸ©é˜µã€‚
- en: Lemma 11.1.
  id: totrans-1538
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 11.1ã€‚
- en: If the following conditions hold
  id: totrans-1539
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶
- en: â€¢
  id: totrans-1540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $f(x)_{j_{0}}$ be defined as Definition[4.10](#S4.Thmtheorem10 "Definition
    4.10\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1541
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $f(x)_{j_{0}}$ çš„å®šä¹‰å‚è§å®šä¹‰ [4.10](#S4.Thmtheorem10 "å®šä¹‰ 4.10 â€£ 4.3 å…³äº ğ‘‹ çš„æœ‰ç”¨å®šä¹‰ â€£
    4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")ã€‚
- en: â€¢
  id: totrans-1542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $c(x,y)_{j_{0},i_{0}}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4
    Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-1543
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $c(x,y)_{j_{0},i_{0}}$ çš„å®šä¹‰å‚è§å®šä¹‰ [4.12](#S4.Thmtheorem12 "å®šä¹‰ 4.12 â€£ 4.5 å…³äº ğ‘‹
    å’Œ ğ‘Œ çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")ã€‚
- en: â€¢
  id: totrans-1544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $h(y)_{i_{0}}$ be defined as Definition[4.11](#S4.Thmtheorem11 "Definition
    4.11\. â€£ 4.4 A Helpful Definition With Respect to ğ‘Œ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1545
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $h(y)_{i_{0}}$ çš„å®šä¹‰å‚è§å®šä¹‰ [4.11](#S4.Thmtheorem11 "å®šä¹‰ 4.11 â€£ 4.4 å…³äº ğ‘Œ çš„æœ‰ç”¨å®šä¹‰ â€£
    4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")ã€‚
- en: â€¢
  id: totrans-1546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $L_{j_{0},i_{0}}$ be defined as Definition[4.7](#S4.Thmtheorem7 "Definition
    4.7\. â€£ 4.2 General Definitions â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  id: totrans-1547
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $L_{j_{0},i_{0}}$ çš„å®šä¹‰å‚è§å®šä¹‰ [4.7](#S4.Thmtheorem7 "å®šä¹‰ 4.7 â€£ 4.2 ä¸€èˆ¬å®šä¹‰ â€£ 4 åˆæ­¥
    â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")ã€‚
- en: Then, we have
  id: totrans-1548
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œæˆ‘ä»¬æœ‰
- en: â€¢
  id: totrans-1549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 1.
  id: totrans-1550
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€éƒ¨åˆ†ã€‚
- en: '|  | $\displaystyle\frac{\mathrm{d}}{\mathrm{d}y_{i_{0},i_{1}}}(\frac{\mathrm{d}}{\mathrm{d}x_{i}}L_{j_{0},i_{0}})=$
    |  |'
  id: totrans-1551
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}}{\mathrm{d}y_{i_{0},i_{1}}}(\frac{\mathrm{d}}{\mathrm{d}x_{i}}L_{j_{0},i_{0}})=$
    |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1552
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1553
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: Proof.
  id: totrans-1554
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: We can show
  id: totrans-1555
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è¯æ˜
- en: '|  |  | $\displaystyle~{}\frac{\mathrm{d}}{\mathrm{d}y_{i_{0},i_{1}}}(\frac{\mathrm{d}}{\mathrm{d}x_{i}}L_{j_{0},i_{0}})$
    |  |'
  id: totrans-1556
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}\frac{\mathrm{d}}{\mathrm{d}y_{i_{0},i_{1}}}(\frac{\mathrm{d}}{\mathrm{d}x_{i}}L_{j_{0},i_{0}})$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-1557
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-1558
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1559
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-1560
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1561
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-1562
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1563
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1564
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: 'where the first step is due to Part 6 of LemmaÂ [5.1](#S5.Thmtheorem1 "Lemma
    5.1 (Gradient with respect to ğ‘¥). â€£ 5.1 Gradient for ğ‘¥ â€£ 5 Gradient â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), the second step comes from the
    product rule of derivative, the third step is based on LemmaÂ [10.2](#S10.Thmtheorem2
    "Lemma 10.2\. â€£ 10.2 Hessian for One ğ‘—â‚€,ğ‘–â‚€ â€£ 10 Hessian for ğ‘Œ â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), and the last step follows from
    simple algebra.'
  id: totrans-1565
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ç¬¬ä¸€æ­¥åŸºäºå¼•ç† [5.1](#S5.Thmtheorem1 "å¼•ç† 5.1ï¼ˆå…³äº ğ‘¥ çš„æ¢¯åº¦ï¼‰. â€£ 5.1 å…³äº ğ‘¥ çš„æ¢¯åº¦ â€£ 5 æ¢¯åº¦ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ
    SVM æŠ€å·§çš„å•å±‚æ³¨æ„åŠ›é‡æ„åŠå…¶çŸ©é˜µä¹˜æ³•æ—¶é—´æ±‚è§£") ç¬¬6éƒ¨åˆ†ï¼Œç¬¬äºŒæ­¥æ¥è‡ªå¯¼æ•°çš„ä¹˜ç§¯æ³•åˆ™ï¼Œç¬¬ä¸‰æ­¥åŸºäºå¼•ç† [10.2](#S10.Thmtheorem2 "å¼•ç†
    10.2\. â€£ 10.2 å…³äºä¸€ä¸ª ğ‘—â‚€,ğ‘–â‚€ çš„ Hessian â€£ 10 å…³äº ğ‘Œ çš„ Hessian â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§çš„å•å±‚æ³¨æ„åŠ›é‡æ„åŠå…¶çŸ©é˜µä¹˜æ³•æ—¶é—´æ±‚è§£")ï¼Œæœ€åä¸€æ­¥åˆ™æ˜¯ç®€å•çš„ä»£æ•°è¿ç®—ã€‚
- en: Thus, we complete the proof. âˆ
  id: totrans-1566
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬å®Œæˆäº†è¯æ˜ã€‚ âˆ
- en: 11.2 A Helpful Lemma
  id: totrans-1567
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2 ä¸€ä¸ªæœ‰ç”¨çš„å¼•ç†
- en: In this section, we provide a helpful Lemma.
  id: totrans-1568
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæœ‰ç”¨çš„å¼•ç†ã€‚
- en: Lemma 11.2.
  id: totrans-1569
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 11.2ã€‚
- en: If the following conditions hold
  id: totrans-1570
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶
- en: â€¢
  id: totrans-1571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $f(x)_{j_{0}}$ be defined in Definition[4.10](#S4.Thmtheorem10 "Definition
    4.10\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1572
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $f(x)_{j_{0}}$ æŒ‰ç…§å®šä¹‰[4.10](#S4.Thmtheorem10 "å®šä¹‰ 4.10\. â€£ 4.3 å…³äº ğ‘‹ çš„æœ‰ç”¨å®šä¹‰ â€£ 4
    åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§çš„å•å±‚æ³¨æ„åŠ›é‡æ„åŠå…¶çŸ©é˜µä¹˜æ³•æ—¶é—´æ±‚è§£") å®šä¹‰ã€‚
- en: â€¢
  id: totrans-1573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $\operatorname{\mathsf{A}}\in\mathbb{R}^{n^{2}\times d^{2}}$ be defined
    in Definition[4.8](#S4.Thmtheorem8 "Definition 4.8\. â€£ 4.3 Helpful Definitions
    With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time").'
  id: totrans-1574
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $\operatorname{\mathsf{A}}\in\mathbb{R}^{n^{2}\times d^{2}}$ æŒ‰ç…§å®šä¹‰[4.8](#S4.Thmtheorem8
    "å®šä¹‰ 4.8\. â€£ 4.3 å…³äº ğ‘‹ çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§çš„å•å±‚æ³¨æ„åŠ›é‡æ„åŠå…¶çŸ©é˜µä¹˜æ³•æ—¶é—´æ±‚è§£") å®šä¹‰ã€‚
- en: â€¢
  id: totrans-1575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $c(x,y)_{j_{0},i_{0}}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4
    Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-1576
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $c(x,y)_{j_{0},i_{0}}$ æŒ‰ç…§å®šä¹‰[4.12](#S4.Thmtheorem12 "å®šä¹‰ 4.12\. â€£ 4.5 æœ‰å…³ ğ‘‹ å’Œ
    ğ‘Œ çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§çš„å•å±‚æ³¨æ„åŠ›é‡æ„åŠå…¶çŸ©é˜µä¹˜æ³•æ—¶é—´æ±‚è§£") å®šä¹‰ã€‚
- en: â€¢
  id: totrans-1577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $h(y)_{i_{0}}$ be defined as Definition[4.11](#S4.Thmtheorem11 "Definition
    4.11\. â€£ 4.4 A Helpful Definition With Respect to ğ‘Œ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1578
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $h(y)_{i_{0}}$ æŒ‰ç…§å®šä¹‰[4.11](#S4.Thmtheorem11 "å®šä¹‰ 4.11\. â€£ 4.4 å…³äº ğ‘Œ çš„æœ‰ç”¨å®šä¹‰ â€£ 4
    åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§çš„å•å±‚æ³¨æ„åŠ›é‡æ„åŠå…¶çŸ©é˜µä¹˜æ³•æ—¶é—´æ±‚è§£") å®šä¹‰ã€‚
- en: â€¢
  id: totrans-1579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $L_{j_{0},i_{0}}$ be defined as Definition[4.7](#S4.Thmtheorem7 "Definition
    4.7\. â€£ 4.2 General Definitions â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  id: totrans-1580
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $L_{j_{0},i_{0}}$ æŒ‰ç…§å®šä¹‰[4.7](#S4.Thmtheorem7 "å®šä¹‰ 4.7\. â€£ 4.2 ä¸€èˆ¬å®šä¹‰ â€£ 4 åˆæ­¥ â€£
    å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§çš„å•å±‚æ³¨æ„åŠ›é‡æ„åŠå…¶çŸ©é˜µä¹˜æ³•æ—¶é—´æ±‚è§£") å®šä¹‰ã€‚
- en: Then, we have
  id: totrans-1581
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¾—åˆ°
- en: â€¢
  id: totrans-1582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 1.
  id: totrans-1583
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬1éƒ¨åˆ†ã€‚
- en: '|  | $1$2 |  |'
  id: totrans-1584
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: â€¢
  id: totrans-1585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 2.
  id: totrans-1586
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬2éƒ¨åˆ†ã€‚
- en: '|  | $1$2 |  |'
  id: totrans-1587
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: â€¢
  id: totrans-1588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 3.
  id: totrans-1589
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬3éƒ¨åˆ†ã€‚
- en: '|  | $1$2 |  |'
  id: totrans-1590
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: â€¢
  id: totrans-1591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 4.
  id: totrans-1592
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬4éƒ¨åˆ†ã€‚
- en: '|  | $1$2 |  |'
  id: totrans-1593
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Proof.
  id: totrans-1594
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: Proof of Part 1.
  id: totrans-1595
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬1éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: '|  | $\displaystyle\langle f(x)_{j_{0}},A_{3,*,i_{1}}\rangle\cdot\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},h(y)_{i_{0}}\rangle=$
    |  |'
  id: totrans-1596
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\langle f(x)_{j_{0}},A_{3,*,i_{1}}\rangle\cdot\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},h(y)_{i_{0}}\rangle=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-1597
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step follows from FactÂ [4.1](#S4.Thmtheorem1 "Fact 4.1\. â€£
    4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), and the second step follows from FactÂ [4.1](#S4.Thmtheorem1
    "Fact 4.1\. â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  id: totrans-1598
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ç¬¬ä¸€æ­¥æ¥è‡ªäºäº‹å® [4.1](#S4.Thmtheorem1 "Fact 4.1\. â€£ 4.1 Basic Facts â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")ï¼Œç¬¬äºŒæ­¥æ¥è‡ªäºäº‹å®
    [4.1](#S4.Thmtheorem1 "Fact 4.1\. â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")ã€‚'
- en: Proof of Part 2.
  id: totrans-1599
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒéƒ¨åˆ†çš„è¯æ˜ã€‚
- en: '|  | $\displaystyle\langle f(x)_{j_{0}},A_{3,*,i_{1}}\rangle\cdot\langle f(x)_{j_{0}},h(y)_{i_{0}}\rangle\cdot\langle
    f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle=$ |  |'
  id: totrans-1600
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\langle f(x)_{j_{0}},A_{3,*,i_{1}}\rangle\cdot\langle f(x)_{j_{0}},h(y)_{i_{0}}\rangle\cdot\langle
    f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle=$ |  |'
- en: 'where the first step follows from FactÂ [4.1](#S4.Thmtheorem1 "Fact 4.1\. â€£
    4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time").'
  id: totrans-1601
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ç¬¬ä¸€æ­¥æ¥è‡ªäºäº‹å® [4.1](#S4.Thmtheorem1 "Fact 4.1\. â€£ 4.1 Basic Facts â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")ã€‚'
- en: Proof of Part 3.
  id: totrans-1602
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: '|  | $\displaystyle\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},A_{3,*,i_{1}}\rangle=$
    |  |'
  id: totrans-1603
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},A_{3,*,i_{1}}\rangle=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-1604
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-1605
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first, second, and last step follows from FactÂ [4.1](#S4.Thmtheorem1
    "Fact 4.1\. â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  id: totrans-1606
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ç¬¬ä¸€æ­¥ã€ç¬¬äºŒæ­¥å’Œæœ€åä¸€æ­¥æ¥è‡ªäºäº‹å® [4.1](#S4.Thmtheorem1 "Fact 4.1\. â€£ 4.1 Basic Facts â€£ 4
    Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")ã€‚'
- en: Proof of Part 4.
  id: totrans-1607
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬å››éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: '|  | $1$2 |  |'
  id: totrans-1608
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 'where the first step follows from FactÂ [4.1](#S4.Thmtheorem1 "Fact 4.1\. â€£
    4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time").'
  id: totrans-1609
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ç¬¬ä¸€æ­¥æ¥è‡ªäºäº‹å® [4.1](#S4.Thmtheorem1 "Fact 4.1\. â€£ 4.1 Basic Facts â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")ã€‚'
- en: âˆ
  id: totrans-1610
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ
- en: 11.3 Creating $B(x,y)$
  id: totrans-1611
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3 åˆ›å»º $B(x,y)$
- en: In this section, we give a formal definition of $B(x,y)$.
  id: totrans-1612
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ç»™å‡º $B(x,y)$ çš„æ­£å¼å®šä¹‰ã€‚
- en: Definition 11.3.
  id: totrans-1613
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å®šä¹‰ 11.3ã€‚
- en: We define $B(x,y)$
  id: totrans-1614
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®šä¹‰ $B(x,y)$
- en: '|  | $\displaystyle B(x,y)=B_{\operatorname{diag}}^{1}+B_{\operatorname{rank}}^{1}+B_{\operatorname{rank}}^{2}+B_{\operatorname{rank}}^{1}$
    |  |'
  id: totrans-1615
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle B(x,y)=B_{\operatorname{diag}}^{1}+B_{\operatorname{rank}}^{1}+B_{\operatorname{rank}}^{2}+B_{\operatorname{rank}}^{1}$
    |  |'
- en: where
  id: totrans-1616
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­
- en: â€¢
  id: totrans-1617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $B_{\operatorname{rank}}^{1}(x,y)=(f(x)_{j_{0}}\circ h(y)_{i_{0}})f(x)_{j_{0}}^{\top}$
  id: totrans-1618
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $B_{\operatorname{rank}}^{1}(x,y)=(f(x)_{j_{0}}\circ h(y)_{i_{0}})f(x)_{j_{0}}^{\top}$
- en: â€¢
  id: totrans-1619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $1$2
  id: totrans-1620
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2
- en: â€¢
  id: totrans-1621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $B_{\operatorname{diag}}^{1}(x,y)=-c(x,y)_{j_{0},i_{0}}\operatorname{diag}(f(x)_{j_{0}})$
  id: totrans-1622
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $B_{\operatorname{diag}}^{1}(x,y)=-c(x,y)_{j_{0},i_{0}}\operatorname{diag}(f(x)_{j_{0}})$
- en: â€¢
  id: totrans-1623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $B_{\operatorname{rank}}^{3}(x,y)=c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
  id: totrans-1624
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $B_{\operatorname{rank}}^{3}(x,y)=c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
- en: Lemma 11.4.
  id: totrans-1625
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 11.4ã€‚
- en: If the following conditions
  id: totrans-1626
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶
- en: â€¢
  id: totrans-1627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $B(x,y)$ be defined as Definition[11.3](#S11.Thmtheorem3 "Definition 11.3\.
    â€£ 11.3 Creating ğµâ¢(ğ‘¥,ğ‘¦) â€£ 11 Hessian for ğ‘‹ and ğ‘Œ â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  id: totrans-1628
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'è®© $B(x,y)$ è¢«å®šä¹‰ä¸ºå®šä¹‰ [11.3](#S11.Thmtheorem3 "Definition 11.3\. â€£ 11.3 Creating
    ğµâ¢(ğ‘¥,ğ‘¦) â€£ 11 Hessian for ğ‘‹ and ğ‘Œ â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time")ã€‚'
- en: Then, we have
  id: totrans-1629
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬æœ‰
- en: â€¢
  id: totrans-1630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 1.
  id: totrans-1631
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€éƒ¨åˆ†ã€‚
- en: '|  | $1$2 |  |'
  id: totrans-1632
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: â€¢
  id: totrans-1633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 2.  $i_{1}\neq i_{0}$
  id: totrans-1634
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬äºŒéƒ¨åˆ†ã€‚ $i_{1}\neq i_{0}$
- en: '|  | $1$2 |  |'
  id: totrans-1635
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Proof.
  id: totrans-1636
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: Proof of Part 1. We have
  id: totrans-1637
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€éƒ¨åˆ†çš„è¯æ˜ã€‚æˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0},i_{2}}\mathrm{d}x_{i}}=$
    |  |'
  id: totrans-1638
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0},i_{2}}\mathrm{d}x_{i}}=$
    |  |'
- en: 'where the first step follows from combining LemmaÂ [11.1](#S11.Thmtheorem1 "Lemma
    11.1\. â€£ 11.1 Computing Hessian â€£ 11 Hessian for ğ‘‹ and ğ‘Œ â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") and LemmaÂ [11.2](#S11.Thmtheorem2
    "Lemma 11.2\. â€£ 11.2 A Helpful Lemma â€£ 11 Hessian for ğ‘‹ and ğ‘Œ â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1639
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ç¬¬ä¸€æ­¥æ¥è‡ªäºç»“åˆå¼•ç†[11.1](#S11.Thmtheorem1 "å¼•ç† 11.1. â€£ 11.1 è®¡ç®— Hessian â€£ 11 Hessian
    for ğ‘‹ å’Œ ğ‘Œ â€£ åŸºäºå¼ é‡å’Œ SVM æŠ€å·§çš„ LLM å•å±‚æ³¨æ„åŠ›çš„å¿«é€Ÿä¼˜åŒ–è§†å›¾ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")å’Œå¼•ç†[11.2](#S11.Thmtheorem2
    "å¼•ç† 11.2. â€£ 11.2 ä¸€ä¸ªæœ‰ç”¨çš„å¼•ç† â€£ 11 Hessian for ğ‘‹ å’Œ ğ‘Œ â€£ åŸºäºå¼ é‡å’Œ SVM æŠ€å·§çš„ LLM å•å±‚æ³¨æ„åŠ›çš„å¿«é€Ÿä¼˜åŒ–è§†å›¾ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")ã€‚
- en: Then, we can have
  id: totrans-1640
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°
- en: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0}}\mathrm{d}x}=\operatorname{\mathsf{A}}_{j_{0}}^{\top}B(x,y)A_{3}$
    |  |'
  id: totrans-1641
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0}}\mathrm{d}x}=\operatorname{\mathsf{A}}_{j_{0}}^{\top}B(x,y)A_{3}$
    |  |'
- en: Proof of Part 2. We have
  id: totrans-1642
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒéƒ¨åˆ†çš„è¯æ˜å¦‚ä¸‹ã€‚
- en: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}\mathrm{d}x_{i}}=$
    |  |'
  id: totrans-1643
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}\mathrm{d}x_{i}}=$
    |  |'
- en: 'where the first step follows from combining LemmaÂ [11.1](#S11.Thmtheorem1 "Lemma
    11.1\. â€£ 11.1 Computing Hessian â€£ 11 Hessian for ğ‘‹ and ğ‘Œ â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") and LemmaÂ [11.2](#S11.Thmtheorem2
    "Lemma 11.2\. â€£ 11.2 A Helpful Lemma â€£ 11 Hessian for ğ‘‹ and ğ‘Œ â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1644
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ç¬¬ä¸€æ­¥æ¥è‡ªäºç»“åˆå¼•ç†[11.1](#S11.Thmtheorem1 "å¼•ç† 11.1. â€£ 11.1 è®¡ç®— Hessian â€£ 11 Hessian
    for ğ‘‹ å’Œ ğ‘Œ â€£ åŸºäºå¼ é‡å’Œ SVM æŠ€å·§çš„ LLM å•å±‚æ³¨æ„åŠ›çš„å¿«é€Ÿä¼˜åŒ–è§†å›¾ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")å’Œå¼•ç†[11.2](#S11.Thmtheorem2
    "å¼•ç† 11.2. â€£ 11.2 ä¸€ä¸ªæœ‰ç”¨çš„å¼•ç† â€£ 11 Hessian for ğ‘‹ å’Œ ğ‘Œ â€£ åŸºäºå¼ é‡å’Œ SVM æŠ€å·§çš„ LLM å•å±‚æ³¨æ„åŠ›çš„å¿«é€Ÿä¼˜åŒ–è§†å›¾ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")ã€‚
- en: Then, we can have
  id: totrans-1645
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°
- en: '|  | $1$2 |  |'
  id: totrans-1646
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: âˆ
  id: totrans-1647
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ
- en: 12 Lipschitz for Hessian of $x,y$
  id: totrans-1648
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12 å¯¹ $x, y$ çš„ Hessian çš„ Lipschitz æ¡ä»¶
- en: 'In SectionÂ [12.1](#S12.SS1 "12.1 Main Results â€£ 12 Lipschitz for Hessian of
    ğ‘¥,ğ‘¦ â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), we present
    the main results of the Lipschitz property of $H_{x,y}$. In SectionÂ [12.6](#S12.SS6
    "12.6 Calculation: Step 2 Lipschitz for Matrix Function -âŸ¨ğ‘“â¢(ğ‘¥)_ğ‘—â‚€,â„â¢(ğ‘¦)_ğ‘–â‚€âŸ©â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€^âŠ¤
    â€£ 12 Lipschitz for Hessian of ğ‘¥,ğ‘¦ â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), we analyze the second step of Lipschitz function $-\langle
    f(x)_{j_{0}},h(y)_{i_{0}}\rangle f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$. In SectionÂ [12.8](#S12.SS8
    "12.8 Calculation: Step 4 Lipschitz for Matrix Function ğ‘â¢(ğ‘¥,ğ‘¦)_{ğ‘—â‚€,ğ‘–â‚€}â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€^âŠ¤
    â€£ 12 Lipschitz for Hessian of ğ‘¥,ğ‘¦ â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), we analyze the fourth step of Lipschitz function $c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$.'
  id: totrans-1649
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨ç¬¬[12.1èŠ‚](#S12.SS1 "12.1 ä¸»è¦ç»“æœ â€£ 12 å¯¹ ğ‘¥,ğ‘¦ çš„ Hessian çš„ Lipschitz æ¡ä»¶ â€£ åŸºäºå¼ é‡å’Œ SVM
    æŠ€å·§çš„ LLM å•å±‚æ³¨æ„åŠ›çš„å¿«é€Ÿä¼˜åŒ–è§†å›¾ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº† $H_{x,y}$ çš„ Lipschitz æ€§è´¨çš„ä¸»è¦ç»“æœã€‚åœ¨ç¬¬[12.6èŠ‚](#S12.SS6
    "12.6 è®¡ç®—: ç¬¬ 2 æ­¥å¯¹çŸ©é˜µå‡½æ•°çš„ Lipschitz â€£ âŸ¨ğ‘“â¢(ğ‘¥)_ğ‘—â‚€,â„â¢(ğ‘¦)_ğ‘–â‚€âŸ©â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€^âŠ¤ â€£ 12
    å¯¹ ğ‘¥,ğ‘¦ çš„ Hessian çš„ Lipschitz æ¡ä»¶ â€£ åŸºäºå¼ é‡å’Œ SVM æŠ€å·§çš„ LLM å•å±‚æ³¨æ„åŠ›çš„å¿«é€Ÿä¼˜åŒ–è§†å›¾ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†
    Lipschitz å‡½æ•° $-\langle f(x)_{j_{0}},h(y)_{i_{0}}\rangle f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
    çš„ç¬¬äºŒæ­¥ã€‚åœ¨ç¬¬[12.8èŠ‚](#S12.SS8 "12.8 è®¡ç®—: ç¬¬ 4 æ­¥å¯¹çŸ©é˜µå‡½æ•°çš„ Lipschitz â€£ ğ‘â¢(ğ‘¥,ğ‘¦)_{ğ‘—â‚€,ğ‘–â‚€}â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€^âŠ¤
    â€£ 12 å¯¹ ğ‘¥,ğ‘¦ çš„ Hessian çš„ Lipschitz æ¡ä»¶ â€£ åŸºäºå¼ é‡å’Œ SVM æŠ€å·§çš„ LLM å•å±‚æ³¨æ„åŠ›çš„å¿«é€Ÿä¼˜åŒ–è§†å›¾ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†
    Lipschitz å‡½æ•° $c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$ çš„ç¬¬å››æ­¥ã€‚'
- en: 12.1 Main Results
  id: totrans-1650
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1 ä¸»è¦ç»“æœ
- en: 'In this section, we present the main result of SectionÂ [12](#S12 "12 Lipschitz
    for Hessian of ğ‘¥,ğ‘¦ â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  id: totrans-1651
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç¬¬[12èŠ‚](#S12 "12 å¯¹ ğ‘¥,ğ‘¦ çš„ Hessian çš„ Lipschitz æ¡ä»¶ â€£ åŸºäºå¼ é‡å’Œ SVM æŠ€å·§çš„ LLM
    å•å±‚æ³¨æ„åŠ›çš„å¿«é€Ÿä¼˜åŒ–è§†å›¾ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")çš„ä¸»è¦ç»“æœã€‚
- en: Lemma 12.1.
  id: totrans-1652
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 12.1ã€‚
- en: If the following conditions hold
  id: totrans-1653
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶
- en: â€¢
  id: totrans-1654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\max_{j_{0}\in[n]}\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq R$
  id: totrans-1655
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\max_{j_{0}\in[n]}\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq R$
- en: â€¢
  id: totrans-1656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $H(x,y)_{j_{0},i_{0}}\in\mathbb{R}^{d^{2}\times d}$
  id: totrans-1657
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $H(x,y)_{j_{0},i_{0}}\in\mathbb{R}^{d^{2}\times d}$
- en: â€¢
  id: totrans-1658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}x\mathrm{d}y_{i_{1}}}={\bf 0}_{d^{2}\times
    d}$
  id: totrans-1659
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}x\mathrm{d}y_{i_{1}}}={\bf 0}_{d^{2}\times
    d}$
- en: â€¢
  id: totrans-1660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $H(x,y)\in\mathbb{R}^{d^{2}\times d^{2}}$ be
  id: totrans-1661
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $H(x,y)\in\mathbb{R}^{d^{2}\times d^{2}}$ ä¸º
- en: '|  | $1$2 |  |'
  id: totrans-1662
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Then we have
  id: totrans-1663
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬æœ‰
- en: â€¢
  id: totrans-1664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 1\. For $j_{0}\in[d],i_{0}\in[n]$
  id: totrans-1665
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: éƒ¨åˆ† 1\. å¯¹äº $j_{0}\in[d],i_{0}\in[n]$
- en: '|  | $1$2 |  |'
  id: totrans-1666
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: â€¢
  id: totrans-1667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 2.
  id: totrans-1668
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: éƒ¨åˆ† 2ã€‚
- en: '|  | $1$2 |  |'
  id: totrans-1669
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Proof.
  id: totrans-1670
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: 'Proof of Part 1. It follows from LemmaÂ [12.2](#S12.Thmtheorem2 "Lemma 12.2\.
    â€£ 12.2 Summary of Four Steps on Lipschitz for Matrix Functions â€£ 12 Lipschitz
    for Hessian of ğ‘¥,ğ‘¦ â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  id: totrans-1671
  prefs: []
  type: TYPE_NORMAL
  zh: éƒ¨åˆ† 1 çš„è¯æ˜ã€‚å®ƒæ¥è‡ªäºå¼•ç† [12.2](#S12.Thmtheorem2 "å¼•ç† 12.2\. â€£ 12.2 çŸ©é˜µå‡½æ•° Lipschitz è¿ç»­æ€§çš„å››ä¸ªæ­¥éª¤æ€»ç»“
    â€£ 12 ğ‘¥,ğ‘¦ çš„ Hessian çš„ Lipschitz è¿ç»­æ€§ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°æ„é€  LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")ã€‚
- en: Proof of Part 2. We can show that
  id: totrans-1672
  prefs: []
  type: TYPE_NORMAL
  zh: éƒ¨åˆ† 2 çš„è¯æ˜ã€‚æˆ‘ä»¬å¯ä»¥å±•ç¤º
- en: '|  | $1$2 |  |'
  id: totrans-1673
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where the first step follows from that we can write $H$ terms $H_{j_{0},i_{0}}$,
    $i_{0}\in[d]$. âˆ
  id: totrans-1674
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆçš„æ­¥éª¤æ¥è‡ªäºæˆ‘ä»¬å¯ä»¥å†™å‡º $H$ é¡¹ $H_{j_{0},i_{0}}$ï¼Œ$i_{0}\in[d]$ã€‚âˆ
- en: 12.2 Summary of Four Steps on Lipschitz for Matrix Functions
  id: totrans-1675
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2 çŸ©é˜µå‡½æ•° Lipschitz è¿ç»­æ€§çš„å››ä¸ªæ­¥éª¤æ€»ç»“
- en: In this section, we summarize the four steps for analyzing the Lipschitz for
    different matrix functions.
  id: totrans-1676
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æ€»ç»“äº†åˆ†æä¸åŒçŸ©é˜µå‡½æ•° Lipschitz è¿ç»­æ€§çš„å››ä¸ªæ­¥éª¤ã€‚
- en: Lemma 12.2.
  id: totrans-1677
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 12.2ã€‚
- en: If the following conditions hold
  id: totrans-1678
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶
- en: â€¢
  id: totrans-1679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $G_{1}(x,y)=(f(x)_{j_{0}}\circ h(y)_{i_{0}})f(x)_{j_{0}}^{\top}$
  id: totrans-1680
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $G_{1}(x,y)=(f(x)_{j_{0}}\circ h(y)_{i_{0}})f(x)_{j_{0}}^{\top}$
- en: â€¢
  id: totrans-1681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $1$2
  id: totrans-1682
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2
- en: â€¢
  id: totrans-1683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $G_{3}(x,y)=-c(x,y)_{j_{0},i_{0}}\operatorname{diag}(f(x)_{j_{0}})$
  id: totrans-1684
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $G_{3}(x,y)=-c(x,y)_{j_{0},i_{0}}\operatorname{diag}(f(x)_{j_{0}})$
- en: â€¢
  id: totrans-1685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $G_{4}(x,y)=c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
  id: totrans-1686
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $G_{4}(x,y)=c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
- en: Then, we have
  id: totrans-1687
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $1$2 |  |'
  id: totrans-1688
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Proof.
  id: totrans-1689
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: 'The proof follows from LemmaÂ [12.5](#S12.Thmtheorem5 "Lemma 12.5\. â€£ 12.5 Calculation:
    Step 1 Lipschitz for Matrix Function (ğ‘“â¢(ğ‘¥)_ğ‘—â‚€âˆ˜â„â¢(ğ‘¦)_ğ‘–â‚€)â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€^âŠ¤ â€£ 12 Lipschitz
    for Hessian of ğ‘¥,ğ‘¦ â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), LemmaÂ [12.6](#S12.Thmtheorem6 "Lemma 12.6\. â€£ 12.6 Calculation: Step 2
    Lipschitz for Matrix Function -âŸ¨ğ‘“â¢(ğ‘¥)_ğ‘—â‚€,â„â¢(ğ‘¦)_ğ‘–â‚€âŸ©â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€^âŠ¤ â€£ 12 Lipschitz
    for Hessian of ğ‘¥,ğ‘¦ â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), LemmaÂ [12.7](#S12.Thmtheorem7 "Lemma 12.7\. â€£ 12.7 Calculation: Step 3
    Lipschitz for Matrix Function -ğ‘â¢(ğ‘¥,ğ‘¦)_{ğ‘—â‚€,ğ‘–â‚€}â¢diag(ğ‘“â¢(ğ‘¥)_ğ‘—â‚€) â€£ 12 Lipschitz for
    Hessian of ğ‘¥,ğ‘¦ â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), and LemmaÂ [12.8](#S12.Thmtheorem8 "Lemma 12.8\. â€£ 12.8 Calculation: Step
    4 Lipschitz for Matrix Function ğ‘â¢(ğ‘¥,ğ‘¦)_{ğ‘—â‚€,ğ‘–â‚€}â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€^âŠ¤ â€£ 12 Lipschitz
    for Hessian of ğ‘¥,ğ‘¦ â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"). âˆ'
  id: totrans-1690
  prefs: []
  type: TYPE_NORMAL
  zh: è¯æ˜æ¥è‡ªäºå¼•ç† [12.5](#S12.Thmtheorem5 "å¼•ç† 12.5\. â€£ 12.5 è®¡ç®—ï¼šæ­¥éª¤ 1 çŸ©é˜µå‡½æ•°çš„ Lipschitz è¿ç»­æ€§
    (ğ‘“â¢(ğ‘¥)_ğ‘—â‚€âˆ˜â„â¢(ğ‘¦)_ğ‘–â‚€)â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€^âŠ¤ â€£ 12 ğ‘¥,ğ‘¦ çš„ Hessian çš„ Lipschitz è¿ç»­æ€§ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ
    SVM æŠ€å·§é‡æ–°æ„é€  LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")ã€å¼•ç† [12.6](#S12.Thmtheorem6 "å¼•ç† 12.6\. â€£ 12.6
    è®¡ç®—ï¼šæ­¥éª¤ 2 çŸ©é˜µå‡½æ•°çš„ Lipschitz è¿ç»­æ€§ -âŸ¨ğ‘“â¢(ğ‘¥)_ğ‘—â‚€,â„â¢(ğ‘¦)_ğ‘–â‚€âŸ©â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€^âŠ¤ â€£ 12 ğ‘¥,ğ‘¦
    çš„ Hessian çš„ Lipschitz è¿ç»­æ€§ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°æ„é€  LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")ã€å¼•ç†
    [12.7](#S12.Thmtheorem7 "å¼•ç† 12.7\. â€£ 12.7 è®¡ç®—ï¼šæ­¥éª¤ 3 çŸ©é˜µå‡½æ•°çš„ Lipschitz è¿ç»­æ€§ -ğ‘â¢(ğ‘¥,ğ‘¦)_{ğ‘—â‚€,ğ‘–â‚€}â¢diag(ğ‘“â¢(ğ‘¥)_ğ‘—â‚€)
    â€£ 12 ğ‘¥,ğ‘¦ çš„ Hessian çš„ Lipschitz è¿ç»­æ€§ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°æ„é€  LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")
    å’Œå¼•ç† [12.8](#S12.Thmtheorem8 "å¼•ç† 12.8\. â€£ 12.8 è®¡ç®—ï¼šæ­¥éª¤ 4 çŸ©é˜µå‡½æ•°çš„ Lipschitz è¿ç»­æ€§ ğ‘â¢(ğ‘¥,ğ‘¦)_{ğ‘—â‚€,ğ‘–â‚€}â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€â¢ğ‘“â¢(ğ‘¥)_ğ‘—â‚€^âŠ¤
    â€£ 12 ğ‘¥,ğ‘¦ çš„ Hessian çš„ Lipschitz è¿ç»­æ€§ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°æ„é€  LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")ã€‚âˆ
- en: '12.3 A Core Tool: Upper Bound for Several Basic Functions'
  id: totrans-1691
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3 æ ¸å¿ƒå·¥å…·ï¼šå‡ ä¸ªåŸºæœ¬å‡½æ•°çš„ä¸Šç•Œ
- en: In this section, we give an upper bound for each of the basic functions.
  id: totrans-1692
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ç»™å‡ºäº†æ¯ä¸ªåŸºæœ¬å‡½æ•°çš„ä¸Šç•Œã€‚
- en: Lemma 12.3.
  id: totrans-1693
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 12.3ã€‚
- en: If the following conditions hold
  id: totrans-1694
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶
- en: â€¢
  id: totrans-1695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $f(y)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-1696
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'è®¾ $f(y)_{j_{0}}\in\mathbb{R}^{n}$ å¦‚å®šä¹‰[4.10](#S4.Thmtheorem10 "Definition 4.10\.
    â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") æ‰€å®šä¹‰ã€‚'
- en: â€¢
  id: totrans-1697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $h(y)_{i_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.11](#S4.Thmtheorem11
    "Definition 4.11\. â€£ 4.4 A Helpful Definition With Respect to ğ‘Œ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-1698
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'è®¾ $h(y)_{i_{0}}\in\mathbb{R}^{n}$ å¦‚å®šä¹‰[4.11](#S4.Thmtheorem11 "Definition 4.11\.
    â€£ 4.4 A Helpful Definition With Respect to ğ‘Œ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") æ‰€å®šä¹‰ã€‚'
- en: â€¢
  id: totrans-1699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4
    Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-1700
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'è®¾ $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ å¦‚å®šä¹‰[4.12](#S4.Thmtheorem12 "Definition
    4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time") æ‰€å®šä¹‰ã€‚'
- en: â€¢
  id: totrans-1701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $R\geq 4$
  id: totrans-1702
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $R\geq 4$
- en: â€¢
  id: totrans-1703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|A_{3}\|\leq R$
  id: totrans-1704
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|A_{3}\|\leq R$
- en: â€¢
  id: totrans-1705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|y_{i_{0}}\|\leq R$
  id: totrans-1706
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|y_{i_{0}}\|\leq R$
- en: â€¢
  id: totrans-1707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|b_{j_{0},i_{0}}\|_{2}\leq R$
  id: totrans-1708
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|b_{j_{0},i_{0}}\|_{2}\leq R$
- en: Then, we have
  id: totrans-1709
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¾—åˆ°
- en: â€¢
  id: totrans-1710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 1\. $\|h(y)_{i_{0}}\|_{2}\leq R^{2}$
  id: totrans-1711
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬1éƒ¨åˆ†\. $\|h(y)_{i_{0}}\|_{2}\leq R^{2}$
- en: â€¢
  id: totrans-1712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 2\. $|c(x,y)_{j_{0},i_{0}}|\leq 2R^{2}$
  id: totrans-1713
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬2éƒ¨åˆ†\. $|c(x,y)_{j_{0},i_{0}}|\leq 2R^{2}$
- en: Proof.
  id: totrans-1714
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: Proof of Part 1.
  id: totrans-1715
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬1éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: '|  | $\displaystyle\&#124;h(y)_{i_{0}}\&#124;_{2}=$ |  |'
  id: totrans-1716
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;h(y)_{i_{0}}\&#124;_{2}=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1717
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1718
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is due to DefinitionÂ [4.11](#S4.Thmtheorem11 "Definition
    4.11\. â€£ 4.4 A Helpful Definition With Respect to ğ‘Œ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), the second step is based on FactÂ [4.3](#S4.Thmtheorem3
    "Fact 4.3\. â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") and the third step is because of LemmaÂ [8.4](#S8.Thmtheorem4
    "Lemma 8.4 (Basic Functions Upper Bound). â€£ 8.3 A Core Tool: Upper Bound for Several
    Basic Functions â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time").'
  id: totrans-1719
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ç¬¬ä¸€æ­¥ç”±äºå®šä¹‰Â [4.11](#S4.Thmtheorem11 "Definition 4.11\. â€£ 4.4 A Helpful Definition
    With Respect to ğ‘Œ â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time")ï¼Œç¬¬äºŒæ­¥åŸºäºäº‹å®Â [4.3](#S4.Thmtheorem3 "Fact 4.3\. â€£ 4.1 Basic Facts
    â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")ï¼Œç¬¬ä¸‰æ­¥ç”±äºå¼•ç†Â [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound).
    â€£ 8.3 A Core Tool: Upper Bound for Several Basic Functions â€£ 8 Lipschitz Property
    of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")ã€‚'
- en: Proof of Part 2.
  id: totrans-1720
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬2éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: '|  | $\displaystyle&#124;c(x,y)_{j_{0},i_{0}}&#124;=$ |  |'
  id: totrans-1721
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle&#124;c(x,y)_{j_{0},i_{0}}&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1722
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1723
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1724
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is because of DefinitionÂ [4.12](#S4.Thmtheorem12 "Definition
    4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the second
    step is based on triangle inequality and Cauchyâ€“Schwarz inequality, the third
    step is due to LemmaÂ [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound).
    â€£ 8.3 A Core Tool: Upper Bound for Several Basic Functions â€£ 8 Lipschitz Property
    of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    and the last step follows from $R\geq 4$. âˆ'
  id: totrans-1725
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ç¬¬ä¸€æ­¥æ˜¯æ ¹æ®å®šä¹‰[4.12](#S4.Thmtheorem12 "Definition 4.12\. â€£ 4.5 Helpful Definitions
    With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")ï¼Œç¬¬äºŒæ­¥åŸºäºä¸‰è§’ä¸ç­‰å¼å’ŒæŸ¯è¥¿â€“æ–½ç“¦å…¹ä¸ç­‰å¼ï¼Œç¬¬ä¸‰æ­¥æ˜¯ç”±äºå¼•ç†[8.4](#S8.Thmtheorem4
    "Lemma 8.4 (Basic Functions Upper Bound). â€£ 8.3 A Core Tool: Upper Bound for Several
    Basic Functions â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time")ï¼Œæœ€åä¸€æ­¥è·Ÿéšäº $R\geq 4$ã€‚âˆ'
- en: '12.4 A Core Tool: Lipschitz Property for Several Basic Functions'
  id: totrans-1726
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.4 æ ¸å¿ƒå·¥å…·ï¼šå‡ ç§åŸºæœ¬å‡½æ•°çš„ Lipschitz æ€§è´¨
- en: In this section, we introduce the Lipschitz property for several basic functions.
  id: totrans-1727
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†å‡ ç§åŸºæœ¬å‡½æ•°çš„ Lipschitz æ€§è´¨ã€‚
- en: Lemma 12.4.
  id: totrans-1728
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 12.4ã€‚
- en: If the following conditions hold
  id: totrans-1729
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä»¥ä¸‹æ¡ä»¶æˆç«‹
- en: â€¢
  id: totrans-1730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $f(y)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-1731
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ä»¤ $f(y)_{j_{0}}\in\mathbb{R}^{n}$ å®šä¹‰ä¸ºå®šä¹‰[4.10](#S4.Thmtheorem10 "Definition
    4.10\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")ã€‚'
- en: â€¢
  id: totrans-1732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $h(y)_{i_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.11](#S4.Thmtheorem11
    "Definition 4.11\. â€£ 4.4 A Helpful Definition With Respect to ğ‘Œ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-1733
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ä»¤ $h(y)_{i_{0}}\in\mathbb{R}^{n}$ å®šä¹‰ä¸ºå®šä¹‰[4.11](#S4.Thmtheorem11 "Definition
    4.11\. â€£ 4.4 A Helpful Definition With Respect to ğ‘Œ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")ã€‚'
- en: â€¢
  id: totrans-1734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4
    Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-1735
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ä»¤ $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ å®šä¹‰ä¸ºå®šä¹‰[4.12](#S4.Thmtheorem12 "Definition
    4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")ã€‚'
- en: â€¢
  id: totrans-1736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $R\geq 4$
  id: totrans-1737
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $R\geq 4$
- en: â€¢
  id: totrans-1738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|A_{3}\|\leq R$
  id: totrans-1739
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|A_{3}\|\leq R$
- en: â€¢
  id: totrans-1740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|y_{i_{0}}\|\leq R$
  id: totrans-1741
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|y_{i_{0}}\|\leq R$
- en: â€¢
  id: totrans-1742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|b_{j_{0},i_{0}}\|_{2}\leq R$
  id: totrans-1743
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|b_{j_{0},i_{0}}\|_{2}\leq R$
- en: â€¢
  id: totrans-1744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $R_{0}$ be defined as Definition[8.6](#S8.Thmtheorem6 "Definition 8.6\.
    â€£ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions â€£ 8 Lipschitz
    Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  id: totrans-1745
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ä»¤ $R_{0}$ å®šä¹‰ä¸ºå®šä¹‰[8.6](#S8.Thmtheorem6 "Definition 8.6\. â€£ 8.4 A Core Tool: Lipschitz
    Property for Several Basic Functions â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time")ã€‚'
- en: Then, we have
  id: totrans-1746
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬æœ‰
- en: â€¢
  id: totrans-1747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 1\. $\|h(y)_{i_{0}}-h(\widetilde{y})_{i_{0}}\|_{2}\leq R\|y-\widetilde{y}\|_{2}$
  id: totrans-1748
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€éƒ¨åˆ†ã€‚$\|h(y)_{i_{0}}-h(\widetilde{y})_{i_{0}}\|_{2}\leq R\|y-\widetilde{y}\|_{2}$
- en: â€¢
  id: totrans-1749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 2. $1$2
  id: totrans-1750
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬äºŒéƒ¨åˆ†ã€‚$1$2
- en: â€¢
  id: totrans-1751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 3. $|c(x,y)_{j_{0},i_{0}}-c(x,\widetilde{y})_{j_{0},i_{0}})|\leq R\|y-\widetilde{y}\|_{2}$
  id: totrans-1752
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰éƒ¨åˆ†ã€‚$|c(x,y)_{j_{0},i_{0}}-c(x,\widetilde{y})_{j_{0},i_{0}}|\leq R\|y-\widetilde{y}\|_{2}$
- en: Proof.
  id: totrans-1753
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: Proof of Part 1.
  id: totrans-1754
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: '|  | $\displaystyle\&#124;h(y)_{i_{0}}-h(\widetilde{y})_{i_{0}}\&#124;_{2}=$
    |  |'
  id: totrans-1755
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;h(y)_{i_{0}}-h(\widetilde{y})_{i_{0}}\&#124;_{2}=$
    |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1756
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1757
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step follows from DefinitionÂ [4.11](#S4.Thmtheorem11 "Definition
    4.11\. â€£ 4.4 A Helpful Definition With Respect to ğ‘Œ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), the second step is based on FactÂ [4.3](#S4.Thmtheorem3
    "Fact 4.3\. â€£ 4.1 Basic Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), and the third step is due to LemmaÂ [8.4](#S8.Thmtheorem4
    "Lemma 8.4 (Basic Functions Upper Bound). â€£ 8.3 A Core Tool: Upper Bound for Several
    Basic Functions â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time").'
  id: totrans-1758
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ç¬¬ä¸€æ­¥æ˜¯æ ¹æ®å®šä¹‰Â [4.11](#S4.Thmtheorem11 "Definition 4.11\. â€£ 4.4 A Helpful Definition
    With Respect to ğ‘Œ â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time")ï¼Œç¬¬äºŒæ­¥åŸºäºäº‹å®Â [4.3](#S4.Thmtheorem3 "Fact 4.3\. â€£ 4.1 Basic Facts
    â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")ï¼Œç¬¬ä¸‰æ­¥ç”±äºå¼•ç†Â [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound).
    â€£ 8.3 A Core Tool: Upper Bound for Several Basic Functions â€£ 8 Lipschitz Property
    of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")ã€‚'
- en: Proof of Part 2.
  id: totrans-1759
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒéƒ¨åˆ†çš„è¯æ˜ã€‚
- en: '|  | $\displaystyle&#124;c(x,y)_{j_{0},i_{0}}-c(\widetilde{x},y_{j_{0},i_{0}})&#124;=$
    |  |'
  id: totrans-1760
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle&#124;c(x,y)_{j_{0},i_{0}}-c(\widetilde{x},y_{j_{0},i_{0}})&#124;=$
    |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1761
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1762
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is due to DefinitionÂ [4.12](#S4.Thmtheorem12 "Definition
    4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the second
    step follows from Cauchyâ€“Schwarz inequality, and the third step is because of
    Part 1 of LemmaÂ [12.3](#S12.SS3 "12.3 A Core Tool: Upper Bound for Several Basic
    Functions â€£ 12 Lipschitz for Hessian of ğ‘¥,ğ‘¦ â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") and Part 3 of LemmaÂ [8.5](#S8.Thmtheorem5 "Lemma
    8.5 (Basic Functions Lipschitz Property). â€£ 8.4 A Core Tool: Lipschitz Property
    for Several Basic Functions â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1763
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ç¬¬ä¸€æ­¥æ˜¯æ ¹æ®å®šä¹‰Â [4.12](#S4.Thmtheorem12 "Definition 4.12\. â€£ 4.5 Helpful Definitions
    With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")ï¼Œç¬¬äºŒæ­¥æ¥è‡ªäºæŸ¯è¥¿â€“æ–½ç“¦å…¹ä¸ç­‰å¼ï¼Œç¬¬ä¸‰æ­¥ç”±äºå¼•ç†Â [12.3](#S12.SS3 "12.3 A Core
    Tool: Upper Bound for Several Basic Functions â€£ 12 Lipschitz for Hessian of ğ‘¥,ğ‘¦
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")çš„ç¬¬ä¸€éƒ¨åˆ†å’Œå¼•ç†Â [8.5](#S8.Thmtheorem5
    "Lemma 8.5 (Basic Functions Lipschitz Property). â€£ 8.4 A Core Tool: Lipschitz
    Property for Several Basic Functions â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time")çš„ç¬¬ä¸‰éƒ¨åˆ†ã€‚'
- en: Proof of Part 3.
  id: totrans-1764
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: '|  | $\displaystyle&#124;c(x,y)_{j_{0},i_{0}}-c(x,\widetilde{y})_{j_{0},i_{0}})&#124;=$
    |  |'
  id: totrans-1765
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle&#124;c(x,y)_{j_{0},i_{0}}-c(x,\widetilde{y})_{j_{0},i_{0}})&#124;=$
    |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1766
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1767
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step follows from DefinitionÂ [4.12](#S4.Thmtheorem12 "Definition
    4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the second
    step is due to Cauchyâ€“Schwarz inequality and the third step is because of Part
    4 of LemmaÂ [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound). â€£ 8.3
    A Core Tool: Upper Bound for Several Basic Functions â€£ 8 Lipschitz Property of
    ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")
    and Part 1 of this Lemma. âˆ'
  id: totrans-1768
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: '12.5 Calculation: Step 1 Lipschitz for Matrix Function $(f(x)_{j_{0}}\circ
    h(y)_{i_{0}})f(x)_{j_{0}}^{\top}$'
  id: totrans-1769
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.5 è®¡ç®—ï¼šæ­¥éª¤ 1 çŸ©é˜µå‡½æ•°çš„ Lipschitz æ€§è´¨ $(f(x)_{j_{0}}\circ h(y)_{i_{0}})f(x)_{j_{0}}^{\top}$
- en: In this section, we calculate the Lipschitz for $(f(x)_{j_{0}}\circ h(y)_{i_{0}})f(x)_{j_{0}}^{\top}$.
  id: totrans-1770
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®¡ç®— $(f(x)_{j_{0}}\circ h(y)_{i_{0}})f(x)_{j_{0}}^{\top}$ çš„ Lipschitz
    æ€§è´¨ã€‚
- en: Lemma 12.5.
  id: totrans-1771
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 12.5ã€‚
- en: If the following conditions
  id: totrans-1772
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶
- en: â€¢
  id: totrans-1773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $G_{1}(x,y)=(f(x)_{j_{0}}\circ h(y)_{i_{0}})f(x)_{j_{0}}^{\top}$
  id: totrans-1774
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $G_{1}(x,y)=(f(x)_{j_{0}}\circ h(y)_{i_{0}})f(x)_{j_{0}}^{\top}$
- en: â€¢
  id: totrans-1775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $R_{0}$ be defined in Definition[8.6](#S8.Thmtheorem6 "Definition 8.6\.
    â€£ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions â€£ 8 Lipschitz
    Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  id: totrans-1776
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $R_{0}$ å®šä¹‰è§å®šä¹‰[8.6](#S8.Thmtheorem6 "å®šä¹‰ 8.6\. â€£ 8.4 åŸºæœ¬å‡½æ•°çš„ Lipschitz æ€§è´¨ â€£ 8
    Lipschitz æ€§è´¨ ğ»_{ğ‘¥,ğ‘¥} â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿° LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")ã€‚
- en: â€¢
  id: totrans-1777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1778
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $\alpha(x)_{j_{0}}\in\mathbb{R}$ å®šä¹‰è§å®šä¹‰[4.9](#S4.Thmtheorem9 "å®šä¹‰ 4.9\. â€£ 4.3
    å…³äº ğ‘‹ çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿° LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")
- en: â€¢
  id: totrans-1779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1780
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $f(x)_{j_{0}}\in\mathbb{R}^{n}$ å®šä¹‰è§å®šä¹‰[4.10](#S4.Thmtheorem10 "å®šä¹‰ 4.10\. â€£
    4.3 å…³äº ğ‘‹ çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿° LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")
- en: â€¢
  id: totrans-1781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4
    Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1782
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ å®šä¹‰è§å®šä¹‰[4.12](#S4.Thmtheorem12 "å®šä¹‰ 4.12\.
    â€£ 4.5 å…³äº ğ‘‹ å’Œ ğ‘Œ çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿° LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")
- en: â€¢
  id: totrans-1783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  id: totrans-1784
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
- en: â€¢
  id: totrans-1785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  id: totrans-1786
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
- en: â€¢
  id: totrans-1787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $R\geq 4$
  id: totrans-1788
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $R\geq 4$
- en: Then, we have
  id: totrans-1789
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $1$2 |  |'
  id: totrans-1790
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Proof.
  id: totrans-1791
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: We define
  id: totrans-1792
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®šä¹‰
- en: '|  | $\displaystyle G_{1,1}=$ |  |'
  id: totrans-1793
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{1,1}=$ |  |'
- en: '|  | $\displaystyle G_{1,2}=$ |  |'
  id: totrans-1794
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{1,2}=$ |  |'
- en: '|  | $\displaystyle G_{1,3}=$ |  |'
  id: totrans-1795
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{1,3}=$ |  |'
- en: 'where the first step follows from definition of $G_{1,1}$, the second step
    is based on FactÂ [4.2](#S4.Thmtheorem2 "Fact 4.2\. â€£ 4.1 Basic Facts â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time") and the
    third step is due to LemmaÂ [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper
    Bound). â€£ 8.3 A Core Tool: Upper Bound for Several Basic Functions â€£ 8 Lipschitz
    Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  id: totrans-1796
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ç¬¬ä¸€æ­¥ä¾æ® $G_{1,1}$ çš„å®šä¹‰ï¼Œç¬¬äºŒæ­¥åŸºäºäº‹å® [4.2](#S4.Thmtheorem2 "äº‹å® 4.2\. â€£ 4.1 åŸºæœ¬äº‹å® â€£ 4
    åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿° LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")ï¼Œç¬¬ä¸‰æ­¥ç”±å¼•ç† [8.4](#S8.Thmtheorem4
    "å¼•ç† 8.4ï¼ˆåŸºæœ¬å‡½æ•°ä¸Šç•Œï¼‰ã€‚ â€£ 8.3 æ ¸å¿ƒå·¥å…·ï¼šå‡ ä¸ªåŸºæœ¬å‡½æ•°çš„ä¸Šç•Œ â€£ 8 Lipschitz æ€§è´¨ ğ»_{ğ‘¥,ğ‘¥} â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ
    SVM æŠ€å·§é‡æ–°è¡¨è¿° LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£") æ¨å¯¼è€Œæ¥ã€‚
- en: We have
  id: totrans-1797
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;G_{1,1}\&#124;=$ |  |'
  id: totrans-1798
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{1,1}\&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1799
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1800
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step follows from definition of $G_{1,1}$, the second step
    is due to FactÂ [4.3](#S4.Thmtheorem3 "Fact 4.3\. â€£ 4.1 Basic Facts â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), and the
    third step is based on combining LemmaÂ [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic
    Functions Upper Bound). â€£ 8.3 A Core Tool: Upper Bound for Several Basic Functions
    â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), LemmaÂ [8.5](#S8.Thmtheorem5 "Lemma 8.5 (Basic Functions
    Lipschitz Property). â€£ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions
    â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), and LemmaÂ [12.3](#S12.SS3 "12.3 A Core Tool: Upper Bound
    for Several Basic Functions â€£ 12 Lipschitz for Hessian of ğ‘¥,ğ‘¦ â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1801
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¬¬ä¸€æ­¥éµå¾ª$G_{1,1}$çš„å®šä¹‰ï¼Œç¬¬äºŒæ­¥ç”±äºäº‹å®[4.3](#S4.Thmtheorem3 "Fact 4.3\. â€£ 4.1 Basic Facts
    â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")ï¼Œç¬¬ä¸‰æ­¥åŸºäºç»“åˆå¼•ç†[8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound).
    â€£ 8.3 A Core Tool: Upper Bound for Several Basic Functions â€£ 8 Lipschitz Property
    of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")ã€å¼•ç†[8.5](#S8.Thmtheorem5
    "Lemma 8.5 (Basic Functions Lipschitz Property). â€£ 8.4 A Core Tool: Lipschitz
    Property for Several Basic Functions â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time") å’Œå¼•ç†[12.3](#S12.SS3
    "12.3 A Core Tool: Upper Bound for Several Basic Functions â€£ 12 Lipschitz for
    Hessian of ğ‘¥,ğ‘¦ â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")ã€‚'
- en: Also, we have
  id: totrans-1802
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;G_{1,2}\&#124;=$ |  |'
  id: totrans-1803
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{1,2}\&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1804
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1805
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is based on definition of $G_{1,2}$, the second step is
    because of FactÂ [4.3](#S4.Thmtheorem3 "Fact 4.3\. â€£ 4.1 Basic Facts â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), and the
    third step follows from LemmaÂ [12.4](#S12.Thmtheorem4 "Lemma 12.4\. â€£ 12.4 A Core
    Tool: Lipschitz Property for Several Basic Functions â€£ 12 Lipschitz for Hessian
    of ğ‘¥,ğ‘¦ â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-1806
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¬¬ä¸€æ­¥åŸºäº$G_{1,2}$çš„å®šä¹‰ï¼Œç¬¬äºŒæ­¥ç”±äºäº‹å®[4.3](#S4.Thmtheorem3 "Fact 4.3\. â€£ 4.1 Basic Facts
    â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")ï¼Œç¬¬ä¸‰æ­¥éµå¾ªå¼•ç†[12.4](#S12.Thmtheorem4 "Lemma 12.4\. â€£ 12.4 A Core Tool: Lipschitz
    Property for Several Basic Functions â€£ 12 Lipschitz for Hessian of ğ‘¥,ğ‘¦ â€£ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time")ã€‚'
- en: Additionally,
  id: totrans-1807
  prefs: []
  type: TYPE_NORMAL
  zh: å¦å¤–ï¼Œ
- en: '|  | $\displaystyle\&#124;G_{1,3}\&#124;=$ |  |'
  id: totrans-1808
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{1,3}\&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1809
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1810
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step follows from the definition of $G_{1,3}$, the second step
    follows from FactÂ [4.3](#S4.Thmtheorem3 "Fact 4.3\. â€£ 4.1 Basic Facts â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), and the
    third step is because of LemmaÂ [8.5](#S8.Thmtheorem5 "Lemma 8.5 (Basic Functions
    Lipschitz Property). â€£ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions
    â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time").'
  id: totrans-1811
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¬¬ä¸€æ­¥éµå¾ª$G_{1,3}$çš„å®šä¹‰ï¼Œç¬¬äºŒæ­¥åŸºäºäº‹å®[4.3](#S4.Thmtheorem3 "Fact 4.3\. â€£ 4.1 Basic Facts
    â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")ï¼Œç¬¬ä¸‰æ­¥ç”±äºå¼•ç†[8.5](#S8.Thmtheorem5 "Lemma 8.5 (Basic Functions Lipschitz Property).
    â€£ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions â€£ 8 Lipschitz
    Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")ã€‚'
- en: Combining all the above equations we complete the proof. âˆ
  id: totrans-1812
  prefs: []
  type: TYPE_NORMAL
  zh: ç»¼åˆä»¥ä¸Šæ‰€æœ‰æ–¹ç¨‹ï¼Œæˆ‘ä»¬å®Œæˆäº†è¯æ˜ã€‚âˆ
- en: '12.6 Calculation: Step 2 Lipschitz for Matrix Function $-\langle f(x)_{j_{0}},h(y)_{i_{0}}\rangle
    f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$'
  id: totrans-1813
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.6 è®¡ç®—ï¼šæ­¥éª¤ 2 å¯¹çŸ©é˜µå‡½æ•°çš„ Lipschitz å¸¸æ•° $-\langle f(x)_{j_{0}},h(y)_{i_{0}}\rangle
    f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
- en: In this section, we calculate the Lipschitz for $-\langle f(x)_{j_{0}},h(y)_{i_{0}}\rangle
    f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$.
  id: totrans-1814
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®¡ç®— $-\langle f(x)_{j_{0}},h(y)_{i_{0}}\rangle f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
    çš„ Lipschitz å¸¸æ•°ã€‚
- en: Lemma 12.6.
  id: totrans-1815
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 12.6ã€‚
- en: If the following conditions
  id: totrans-1816
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä»¥ä¸‹æ¡ä»¶
- en: â€¢
  id: totrans-1817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1818
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $\alpha(x)_{j_{0}}\in\mathbb{R}$ å®šä¹‰ä¸ºå®šä¹‰ [4.9](#S4.Thmtheorem9 "Definition 4.9\.
    â€£ 4.3 å¯¹ $ğ‘‹$ çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ„ LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")
- en: â€¢
  id: totrans-1819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1820
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $f(x)_{j_{0}}\in\mathbb{R}^{n}$ å®šä¹‰ä¸ºå®šä¹‰ [4.10](#S4.Thmtheorem10 "Definition
    4.10\. â€£ 4.3 å¯¹ $ğ‘‹$ çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ„ LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")
- en: â€¢
  id: totrans-1821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4
    Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1822
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ å®šä¹‰ä¸ºå®šä¹‰ [4.12](#S4.Thmtheorem12 "Definition
    4.12\. â€£ 4.5 å¯¹ $ğ‘‹$ å’Œ $ğ‘Œ$ çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ„ LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")
- en: â€¢
  id: totrans-1823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  id: totrans-1824
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
- en: â€¢
  id: totrans-1825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  id: totrans-1826
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
- en: â€¢
  id: totrans-1827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $R\geq 4$
  id: totrans-1828
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $R\geq 4$
- en: â€¢
  id: totrans-1829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $1$2
  id: totrans-1830
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $1$2
- en: Then, we have
  id: totrans-1831
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $1$2 |  |'
  id: totrans-1832
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Proof.
  id: totrans-1833
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: We define
  id: totrans-1834
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®šä¹‰
- en: '|  | $\displaystyle G_{2,1}=$ |  |'
  id: totrans-1835
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{2,1}=$ |  |'
- en: '|  | $\displaystyle G_{2,2}=$ |  |'
  id: totrans-1836
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{2,2}=$ |  |'
- en: '|  | $\displaystyle G_{2,3}=$ |  |'
  id: totrans-1837
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{2,3}=$ |  |'
- en: '|  | $\displaystyle G_{2,4}=$ |  |'
  id: totrans-1838
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{2,4}=$ |  |'
- en: We have
  id: totrans-1839
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;G_{2,1}\&#124;=$ |  |'
  id: totrans-1840
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{2,1}\&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1841
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1842
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is based on the definition of $G_{2,1}$, the second step
    follows from FactÂ [4.1](#S4.Thmtheorem1 "Fact 4.1\. â€£ 4.1 Basic Facts â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), and the
    third step is because of LemmaÂ [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions
    Upper Bound). â€£ 8.3 A Core Tool: Upper Bound for Several Basic Functions â€£ 8 Lipschitz
    Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  id: totrans-1843
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ç¬¬ä¸€æ­¥åŸºäº $G_{2,1}$ çš„å®šä¹‰ï¼Œç¬¬äºŒæ­¥æ¥è‡ªäºäº‹å® [4.1](#S4.Thmtheorem1 "Fact 4.1\. â€£ 4.1 åŸºæœ¬äº‹å®
    â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ„ LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")ï¼Œç¬¬ä¸‰æ­¥ç”±äºå¼•ç† [8.4](#S8.Thmtheorem4
    "Lemma 8.4 (åŸºæœ¬å‡½æ•°ä¸Šç•Œ). â€£ 8.3 æ ¸å¿ƒå·¥å…·ï¼šå‡ ä¸ªåŸºæœ¬å‡½æ•°çš„ä¸Šç•Œ â€£ 8 Lipschitz å±æ€§ ğ»_{ğ‘¥,ğ‘¥} â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ
    SVM æŠ€å·§é‡æ„ LLM ä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")ã€‚
- en: and
  id: totrans-1844
  prefs: []
  type: TYPE_NORMAL
  zh: å’Œ
- en: '|  | $\displaystyle\&#124;G_{2,2}\&#124;=$ |  |'
  id: totrans-1845
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{2,2}\&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1846
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1847
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is due to the definition of $G_{2,1}$, the second step
    is based on FactÂ [4.1](#S4.Thmtheorem1 "Fact 4.1\. â€£ 4.1 Basic Facts â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), and the
    third step follows from LemmaÂ [12.4](#S12.Thmtheorem4 "Lemma 12.4\. â€£ 12.4 A Core
    Tool: Lipschitz Property for Several Basic Functions â€£ 12 Lipschitz for Hessian
    of ğ‘¥,ğ‘¦ â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-1848
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ç¬¬ä¸€æ­¥æ˜¯åŸºäº $G_{2,1}$ çš„å®šä¹‰ï¼Œç¬¬äºŒæ­¥åŸºäºäº‹å®Â [4.1](#S4.Thmtheorem1 "Fact 4.1\. â€£ 4.1 Basic
    Facts â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")ï¼Œç¬¬ä¸‰æ­¥åˆ™æ¥è‡ªå¼•ç†Â [12.4](#S12.Thmtheorem4 "Lemma 12.4\. â€£ 12.4 A Core Tool: Lipschitz
    Property for Several Basic Functions â€£ 12 Lipschitz for Hessian of ğ‘¥,ğ‘¦ â€£ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time")ã€‚'
- en: Similarly, we have
  id: totrans-1849
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ ·åœ°ï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;G_{2,3}\&#124;\leq$ |  |'
  id: totrans-1850
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{2,3}\&#124;\leq$ |  |'
- en: '|  | $\displaystyle\&#124;G_{2,4}\&#124;\leq$ |  |'
  id: totrans-1851
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{2,4}\&#124;\leq$ |  |'
- en: Combining all the above equations we complete the proof. âˆ
  id: totrans-1852
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“åˆä¸Šè¿°æ‰€æœ‰æ–¹ç¨‹ï¼Œæˆ‘ä»¬å®Œæˆäº†è¯æ˜ã€‚âˆ
- en: '12.7 Calculation: Step 3 Lipschitz for Matrix Function $-c(x,y)_{j_{0},i_{0}}\operatorname{diag}(f(x)_{j_{0}})$'
  id: totrans-1853
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.7 è®¡ç®—ï¼šæ­¥éª¤ 3 Lipschitz å¯¹çŸ©é˜µå‡½æ•° $-c(x,y)_{j_{0},i_{0}}\operatorname{diag}(f(x)_{j_{0}})$
- en: In this section, we calculate the Lipschitz for $-c(x,y)_{j_{0},i_{0}}\operatorname{diag}(f(x)_{j_{0}})$.
  id: totrans-1854
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®¡ç®— $-c(x,y)_{j_{0},i_{0}}\operatorname{diag}(f(x)_{j_{0}})$ çš„ Lipschitz
    å¸¸æ•°ã€‚
- en: Lemma 12.7.
  id: totrans-1855
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 12.7ã€‚
- en: If the following conditions
  id: totrans-1856
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä»¥ä¸‹æ¡ä»¶
- en: â€¢
  id: totrans-1857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1858
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'è®¾ $\alpha(x)_{j_{0}}\in\mathbb{R}$ å®šä¹‰ä¸ºå®šä¹‰[4.9](#S4.Thmtheorem9 "Definition 4.9\.
    â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")'
- en: â€¢
  id: totrans-1859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1860
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'è®¾ $f(x)_{j_{0}}\in\mathbb{R}^{n}$ å®šä¹‰ä¸ºå®šä¹‰[4.10](#S4.Thmtheorem10 "Definition
    4.10\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")'
- en: â€¢
  id: totrans-1861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4
    Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1862
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'è®¾ $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ å®šä¹‰ä¸ºå®šä¹‰[4.12](#S4.Thmtheorem12 "Definition
    4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
- en: â€¢
  id: totrans-1863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  id: totrans-1864
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
- en: â€¢
  id: totrans-1865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  id: totrans-1866
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$ï¼Œ$\|x\|_{2}\leq R$ï¼Œ$\|v\|_{2}\leq R^{2}$
- en: â€¢
  id: totrans-1867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $R\geq 4$
  id: totrans-1868
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $R\geq 4$
- en: â€¢
  id: totrans-1869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $R_{0}$ be defined as Definition[8.6](#S8.Thmtheorem6 "Definition 8.6\.
    â€£ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions â€£ 8 Lipschitz
    Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  id: totrans-1870
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'è®¾ $R_{0}$ å®šä¹‰ä¸ºå®šä¹‰[8.6](#S8.Thmtheorem6 "Definition 8.6\. â€£ 8.4 A Core Tool: Lipschitz
    Property for Several Basic Functions â€£ 8 Lipschitz Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time")ã€‚'
- en: â€¢
  id: totrans-1871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $G_{3}(x,y)=-c(x,y)_{j_{0},i_{0}}\operatorname{diag}(f(x)_{j_{0}})$
  id: totrans-1872
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $G_{3}(x,y)=-c(x,y)_{j_{0},i_{0}}\operatorname{diag}(f(x)_{j_{0}})$
- en: Then, we have
  id: totrans-1873
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $1$2 |  |'
  id: totrans-1874
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Proof.
  id: totrans-1875
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: We define
  id: totrans-1876
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®šä¹‰
- en: '|  | $\displaystyle G_{3,1}=$ |  |'
  id: totrans-1877
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{3,1}=$ |  |'
- en: '|  | $\displaystyle G_{3,2}=$ |  |'
  id: totrans-1878
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{3,2}=$ |  |'
- en: '|  | $\displaystyle G_{3,3}=$ |  |'
  id: totrans-1879
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{3,3}=$ |  |'
- en: For $G_{3,1}$, we have
  id: totrans-1880
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº $G_{3,1}$ï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;G_{3,1}\&#124;=$ |  |'
  id: totrans-1881
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{3,1}\&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1882
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1883
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step follows from definition of $G_{3,1}$, the second step
    is based on FactÂ [4.2](#S4.Thmtheorem2 "Fact 4.2\. â€£ 4.1 Basic Facts â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time") and the
    third step is because of LemmaÂ [12.4](#S12.Thmtheorem4 "Lemma 12.4\. â€£ 12.4 A
    Core Tool: Lipschitz Property for Several Basic Functions â€£ 12 Lipschitz for Hessian
    of ğ‘¥,ğ‘¦ â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-1884
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œç¬¬ä¸€æ­¥éµå¾ª$G_{3,1}$çš„å®šä¹‰ï¼Œç¬¬äºŒæ­¥åŸºäºäº‹å®[4.2](#S4.Thmtheorem2 "äº‹å® 4.2\. â€£ 4.1 åŸºæœ¬äº‹å® â€£ 4 åˆæ­¥
    â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’ŒSVMæŠ€å·§é‡æ„LLMä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")ï¼Œç¬¬ä¸‰æ­¥ç”±äºå¼•ç†[12.4](#S12.Thmtheorem4 "å¼•ç†
    12.4\. â€£ 12.4 æ ¸å¿ƒå·¥å…·ï¼šå‡ ç§åŸºæœ¬å‡½æ•°çš„Lipschitzæ€§è´¨ â€£ 12 å¯¹ğ‘¥,ğ‘¦çš„Hessiançš„Lipschitz â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’ŒSVMæŠ€å·§é‡æ„LLMä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")ã€‚
- en: Similarly, we have
  id: totrans-1885
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ ·åœ°ï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;G_{3,2}\&#124;\leq$ |  |'
  id: totrans-1886
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{3,2}\&#124;\leq$ |  |'
- en: '|  | $\displaystyle\&#124;G_{3,3}\&#124;\leq$ |  |'
  id: totrans-1887
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{3,3}\&#124;\leq$ |  |'
- en: Combining all the above equations we complete the proof. âˆ
  id: totrans-1888
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“åˆä»¥ä¸Šæ‰€æœ‰æ–¹ç¨‹ï¼Œæˆ‘ä»¬å®Œæˆäº†è¯æ˜ã€‚ âˆ
- en: '12.8 Calculation: Step 4 Lipschitz for Matrix Function $c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$'
  id: totrans-1889
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.8 è®¡ç®—ï¼šæ­¥éª¤ 4 çŸ©é˜µå‡½æ•° $c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$ çš„Lipschitzå¸¸æ•°
- en: In this section, we calculate the Lipschitz for $c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$.
  id: totrans-1890
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—$c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$çš„Lipschitzå¸¸æ•°ã€‚
- en: Lemma 12.8.
  id: totrans-1891
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 12.8ã€‚
- en: If the following conditions
  id: totrans-1892
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä»¥ä¸‹æ¡ä»¶
- en: â€¢
  id: totrans-1893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1894
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤$\alpha(x)_{j_{0}}\in\mathbb{R}$ï¼Œå…¶å®šä¹‰è§å®šä¹‰[4.9](#S4.Thmtheorem9 "å®šä¹‰ 4.9\. â€£ 4.3
    å…³äºğ‘‹çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’ŒSVMæŠ€å·§é‡æ„LLMä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")
- en: â€¢
  id: totrans-1895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. â€£ 4.3 Helpful Definitions With Respect to ğ‘‹ â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1896
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤$f(x)_{j_{0}}\in\mathbb{R}^{n}$ï¼Œå…¶å®šä¹‰è§å®šä¹‰[4.10](#S4.Thmtheorem10 "å®šä¹‰ 4.10\. â€£
    4.3 å…³äºğ‘‹çš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’ŒSVMæŠ€å·§é‡æ„LLMä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")
- en: â€¢
  id: totrans-1897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. â€£ 4.5 Helpful Definitions With Respect to Both ğ‘‹ and ğ‘Œ â€£ 4
    Preliminary â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1898
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤$c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ï¼Œå…¶å®šä¹‰è§å®šä¹‰[4.12](#S4.Thmtheorem12 "å®šä¹‰ 4.12\.
    â€£ 4.5 å…³äºğ‘‹å’Œğ‘Œçš„æœ‰ç”¨å®šä¹‰ â€£ 4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’ŒSVMæŠ€å·§é‡æ„LLMä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")
- en: â€¢
  id: totrans-1899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  id: totrans-1900
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤$\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
- en: â€¢
  id: totrans-1901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  id: totrans-1902
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$ï¼Œ$\|x\|_{2}\leq R$ï¼Œ$\|v\|_{2}\leq R^{2}$
- en: â€¢
  id: totrans-1903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $R\geq 4$
  id: totrans-1904
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤$R\geq 4$
- en: â€¢
  id: totrans-1905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $R_{0}$ be defined in Definition[8.6](#S8.Thmtheorem6 "Definition 8.6\.
    â€£ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions â€£ 8 Lipschitz
    Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  id: totrans-1906
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤$R_{0}$ï¼Œå…¶å®šä¹‰è§å®šä¹‰[8.6](#S8.Thmtheorem6 "å®šä¹‰ 8.6\. â€£ 8.4 æ ¸å¿ƒå·¥å…·ï¼šå‡ ç§åŸºæœ¬å‡½æ•°çš„Lipschitzæ€§è´¨
    â€£ 8 ğ»_{ğ‘¥,ğ‘¥}çš„Lipschitzæ€§è´¨ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’ŒSVMæŠ€å·§é‡æ„LLMä¸­çš„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")ã€‚
- en: â€¢
  id: totrans-1907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $G_{4}(x,y)=c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
  id: totrans-1908
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤$G_{4}(x,y)=c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
- en: Then, we have
  id: totrans-1909
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $1$2 |  |'
  id: totrans-1910
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Proof.
  id: totrans-1911
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: We define
  id: totrans-1912
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®šä¹‰
- en: '|  | $\displaystyle G_{4,1}=$ |  |'
  id: totrans-1913
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{4,1}=$ |  |'
- en: '|  | $\displaystyle G_{4,2}=$ |  |'
  id: totrans-1914
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{4,2}=$ |  |'
- en: '|  | $\displaystyle G_{4,3}=$ |  |'
  id: totrans-1915
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{4,3}=$ |  |'
- en: '|  | $\displaystyle G_{4,4}=$ |  |'
  id: totrans-1916
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{4,4}=$ |  |'
- en: For $G_{4,1}$, we have
  id: totrans-1917
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº$G_{4,1}$ï¼Œæˆ‘ä»¬æœ‰
- en: '|  | $\displaystyle\&#124;G_{4,1}\&#124;=$ |  |'
  id: totrans-1918
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{4,1}\&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1919
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1920
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is due to definition of $G_{4,1}$, the second step is
    because of FactÂ [4.2](#S4.Thmtheorem2 "Fact 4.2\. â€£ 4.1 Basic Facts â€£ 4 Preliminary
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time") and the
    third step follows from LemmaÂ [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions
    Upper Bound). â€£ 8.3 A Core Tool: Upper Bound for Several Basic Functions â€£ 8 Lipschitz
    Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") and LemmaÂ [8.5](#S8.Thmtheorem5 "Lemma 8.5 (Basic Functions Lipschitz Property).
    â€£ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions â€£ 8 Lipschitz
    Property of ğ»_{ğ‘¥,ğ‘¥} â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  id: totrans-1921
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ç¬¬ä¸€æ­¥æ˜¯ç”±äº $G_{4,1}$ çš„å®šä¹‰ï¼Œç¬¬äºŒæ­¥æ˜¯å› ä¸ºäº‹å® [4.2](#S4.Thmtheorem2 "äº‹å® 4.2\. â€£ 4.1 åŸºæœ¬äº‹å® â€£
    4 åˆæ­¥ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§çš„å•å±‚æ³¨æ„åŠ›é‡æ„åŠåœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")ï¼Œç¬¬ä¸‰æ­¥æ¥è‡ªå¼•ç† [8.4](#S8.Thmtheorem4
    "å¼•ç† 8.4 (åŸºæœ¬å‡½æ•°çš„ä¸Šç•Œ). â€£ 8.3 æ ¸å¿ƒå·¥å…·ï¼šå‡ ä¸ªåŸºæœ¬å‡½æ•°çš„ä¸Šç•Œ â€£ 8 ğ»_{ğ‘¥,ğ‘¥} çš„ Lipschitz æ€§è´¨ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ
    SVM æŠ€å·§çš„å•å±‚æ³¨æ„åŠ›é‡æ„åŠåœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³") å’Œå¼•ç† [8.5](#S8.Thmtheorem5 "å¼•ç† 8.5 (åŸºæœ¬å‡½æ•°çš„ Lipschitz
    æ€§è´¨). â€£ 8.4 æ ¸å¿ƒå·¥å…·ï¼šå‡ ä¸ªåŸºæœ¬å‡½æ•°çš„ Lipschitz æ€§è´¨ â€£ 8 ğ»_{ğ‘¥,ğ‘¥} çš„ Lipschitz æ€§è´¨ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ
    SVM æŠ€å·§çš„å•å±‚æ³¨æ„åŠ›é‡æ„åŠåœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")ã€‚
- en: Similarly, we have
  id: totrans-1922
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼åœ°ï¼Œæˆ‘ä»¬å¾—åˆ°
- en: '|  | $\displaystyle\&#124;G_{4,2}\&#124;\leq$ |  |'
  id: totrans-1923
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{4,2}\&#124;\leq$ |  |'
- en: '|  | $\displaystyle\&#124;G_{4,3}\&#124;\leq$ |  |'
  id: totrans-1924
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{4,3}\&#124;\leq$ |  |'
- en: '|  | $\displaystyle\&#124;G_{4,4}\&#124;\leq$ |  |'
  id: totrans-1925
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{4,4}\&#124;\leq$ |  |'
- en: Combining all the above equations we complete the proof. âˆ
  id: totrans-1926
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“åˆä»¥ä¸Šæ‰€æœ‰æ–¹ç¨‹ï¼Œæˆ‘ä»¬å®Œæˆäº†è¯æ˜ã€‚âˆ
- en: 12.9 PSD Upper Bound for Hessian $x,y$
  id: totrans-1927
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.9 å…³äº Hessian $x,y$ çš„ PSD ä¸Šç•Œ
- en: In this section, we analyze the PSD upper bound for Hessian.
  id: totrans-1928
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº† Hessian çš„ PSD ä¸Šç•Œã€‚
- en: Lemma 12.9.
  id: totrans-1929
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 12.9ã€‚
- en: If the following conditions hold
  id: totrans-1930
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä»¥ä¸‹æ¡ä»¶æˆç«‹
- en: â€¢
  id: totrans-1931
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\max_{j_{0}\in[n]}\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq R$
  id: totrans-1932
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\max_{j_{0}\in[n]}\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq R$
- en: â€¢
  id: totrans-1933
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $H(x,y)_{j_{0},i_{0}}\in\mathbb{R}^{d^{2}\times d}$
  id: totrans-1934
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $H(x,y)_{j_{0},i_{0}}\in\mathbb{R}^{d^{2}\times d}$
- en: â€¢
  id: totrans-1935
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}x\mathrm{d}y_{i_{1}}}={\bf 0}_{d^{2}\times
    d}$
  id: totrans-1936
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}x\mathrm{d}y_{i_{1}}}={\bf 0}_{d^{2}\times
    d}$
- en: â€¢
  id: totrans-1937
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $H(x,y)\in\mathbb{R}^{d^{2}\times d^{2}}$ be
  id: totrans-1938
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $H(x,y)\in\mathbb{R}^{d^{2}\times d^{2}}$ ä¸º
- en: '|  | $1$2 |  |'
  id: totrans-1939
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Then we have
  id: totrans-1940
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¾—åˆ°
- en: â€¢
  id: totrans-1941
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 1\. For $j_{0}\in[d],i_{0}\in[n]$
  id: totrans-1942
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬1éƒ¨åˆ†ã€‚å¯¹äº $j_{0}\in[d],i_{0}\in[n]$
- en: '|  | $\displaystyle\&#124;H(x,y)_{j_{0},i_{0}}\&#124;\leq 10R^{2}$ |  |'
  id: totrans-1943
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;H(x,y)_{j_{0},i_{0}}\&#124;\leq 10R^{2}$ |  |'
- en: â€¢
  id: totrans-1944
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 2.
  id: totrans-1945
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬2éƒ¨åˆ†ã€‚
- en: '|  | $\displaystyle\&#124;H(x,y)\&#124;\leq nd\cdot 10R^{2}$ |  |'
  id: totrans-1946
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;H(x,y)\&#124;\leq nd\cdot 10R^{2}$ |  |'
- en: Proof.
  id: totrans-1947
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: 'Proof of Part 1. It follows from LemmaÂ [12.10](#S12.Thmtheorem10 "Lemma 12.10\.
    â€£ 12.10 Upper Bound on Hessian Spectral Norms â€£ 12 Lipschitz for Hessian of ğ‘¥,ğ‘¦
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-1948
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬1éƒ¨åˆ†çš„è¯æ˜ã€‚å®ƒæ¥è‡ªå¼•ç† [12.10](#S12.Thmtheorem10 "å¼•ç† 12.10\. â€£ 12.10 å…³äº Hessian è°±èŒƒæ•°çš„ä¸Šç•Œ
    â€£ 12 å¯¹äº ğ‘¥,ğ‘¦ çš„ Hessian çš„ Lipschitz â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§çš„å•å±‚æ³¨æ„åŠ›é‡æ„åŠåœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")ã€‚
- en: Proof of Part 2. We can show that
  id: totrans-1949
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬2éƒ¨åˆ†çš„è¯æ˜ã€‚æˆ‘ä»¬å¯ä»¥å±•ç¤º
- en: '|  | $\displaystyle\&#124;H(x,y)\&#124;=$ |  |'
  id: totrans-1950
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;H(x,y)\&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1951
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: where the first step is due to the assumption of $H(x,y)$, and the second step
    comes from Part 1. âˆ
  id: totrans-1952
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ç¬¬ä¸€æ­¥æ˜¯ç”±äº $H(x,y)$ çš„å‡è®¾ï¼Œç¬¬äºŒæ­¥æ¥è‡ªç¬¬1éƒ¨åˆ†ã€‚âˆ
- en: 12.10 Upper Bound on Hessian Spectral Norms
  id: totrans-1953
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.10 å…³äº Hessian è°±èŒƒæ•°çš„ä¸Šç•Œ
- en: In this section, we find the upper bound for the Hessian spectral norms.
  id: totrans-1954
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æ‰¾åˆ°äº† Hessian è°±èŒƒæ•°çš„ä¸Šç•Œã€‚
- en: Lemma 12.10.
  id: totrans-1955
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 12.10ã€‚
- en: If the following conditions hold
  id: totrans-1956
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä»¥ä¸‹æ¡ä»¶æˆç«‹
- en: â€¢
  id: totrans-1957
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $G_{1}(x,y)=(f(x)_{j_{0}}\circ h(y)_{i_{0}})f(x)_{j_{0}}^{\top}$
  id: totrans-1958
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $G_{1}(x,y)=(f(x)_{j_{0}}\circ h(y)_{i_{0}})f(x)_{j_{0}}^{\top}$
- en: â€¢
  id: totrans-1959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $1$2
  id: totrans-1960
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2
- en: â€¢
  id: totrans-1961
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $G_{3}(x,y)=-c(x,y)_{j_{0},i_{0}}\operatorname{diag}(f(x)_{j_{0}})$
  id: totrans-1962
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $G_{3}(x,y)=-c(x,y)_{j_{0},i_{0}}\operatorname{diag}(f(x)_{j_{0}})$
- en: â€¢
  id: totrans-1963
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $G_{4}(x,y)=c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
  id: totrans-1964
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $G_{4}(x,y)=c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
- en: Then, we have
  id: totrans-1965
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¾—åˆ°
- en: â€¢
  id: totrans-1966
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 1\. $\|G_{1}(x,y)\|\leq R^{2}$
  id: totrans-1967
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬1éƒ¨åˆ†ã€‚$\|G_{1}(x,y)\|\leq R^{2}$
- en: â€¢
  id: totrans-1968
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 2\. $\|G_{2}(x,y)\|\leq R^{2}$
  id: totrans-1969
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬2éƒ¨åˆ†ã€‚$\|G_{2}(x,y)\|\leq R^{2}$
- en: â€¢
  id: totrans-1970
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 3\. $\|G_{3}(x,y)\|\leq 2R^{2}$
  id: totrans-1971
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬3éƒ¨åˆ†ã€‚$\|G_{3}(x,y)\|\leq 2R^{2}$
- en: â€¢
  id: totrans-1972
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 4\. $\|G_{4}(x,y)\|\leq 2R^{2}$
  id: totrans-1973
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬4éƒ¨åˆ†ã€‚$\|G_{4}(x,y)\|\leq 2R^{2}$
- en: â€¢
  id: totrans-1974
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 5.
  id: totrans-1975
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬5éƒ¨åˆ†ã€‚
- en: '|  | $\displaystyle\sum_{k=1}^{4}\&#124;G_{k}(x,y)\&#124;\leq 10R^{2}$ |  |'
  id: totrans-1976
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\sum_{k=1}^{4}\&#124;G_{k}(x,y)\&#124;\leq 10R^{2}$ |  |'
- en: Proof.
  id: totrans-1977
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: The proof is straightforward by using upper bound on each term âˆ
  id: totrans-1978
  prefs: []
  type: TYPE_NORMAL
  zh: è¯æ˜é€šè¿‡å¯¹æ¯é¡¹ä½¿ç”¨ä¸Šç•Œæ¥å¾—å‡ºï¼Œâˆ
- en: 13 Generating a Spectral Sparsifier via TensorSketch
  id: totrans-1979
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13 é€šè¿‡ TensorSketch ç”Ÿæˆè°±ç¨€ç–åŒ–å™¨
- en: 'Tensor type sketching has been widely used in problems [[165](#bib.bib165),
    [58](#bib.bib58), [52](#bib.bib52), [6](#bib.bib6), [163](#bib.bib163), [173](#bib.bib173),
    [166](#bib.bib166), [202](#bib.bib202), [170](#bib.bib170)]. SectionÂ [13.1](#S13.SS1
    "13.1 Oblivious Subspace Embedding â€£ 13 Generating a Spectral Sparsifier via TensorSketch
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time") presents
    the definition of oblivious subspace embedding. In SectionÂ [13.2](#S13.SS2 "13.2
    TensorSRHT â€£ 13 Generating a Spectral Sparsifier via TensorSketch â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), we give an overview of $\mathsf{TensorSRHT}$.
    In SectionÂ [13.4](#S13.SS4 "13.4 Fast Approximation for Hessian via Sketching
    â€£ 13 Generating a Spectral Sparsifier via TensorSketch â€£ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time"), we introduce the fast approximation
    for hessian via sketching.'
  id: totrans-1980
  prefs: []
  type: TYPE_NORMAL
  zh: Tensor ç±»å‹çš„è‰å›¾åœ¨é—®é¢˜ä¸­è¢«å¹¿æ³›ä½¿ç”¨ [[165](#bib.bib165), [58](#bib.bib58), [52](#bib.bib52),
    [6](#bib.bib6), [163](#bib.bib163), [173](#bib.bib173), [166](#bib.bib166), [202](#bib.bib202),
    [170](#bib.bib170)]ã€‚ç¬¬ [13.1](#S13.SS1 "13.1 æ— å…³å­ç©ºé—´åµŒå…¥ â€£ 13 é€šè¿‡ TensorSketch ç”Ÿæˆè°±ç¨€ç–åŒ–å™¨
    â€£ å¿«é€Ÿä¼˜åŒ–è§†å›¾ï¼šåŸºäº Tensor å’Œ SVM æŠ€å·§çš„å•å±‚æ³¨æ„åŠ›é‡æ–°æ„é€ ï¼Œå¹¶ä»¥çŸ©é˜µä¹˜æ³•æ—¶é—´è§£å†³") èŠ‚ä»‹ç»äº†æ— å…³å­ç©ºé—´åµŒå…¥çš„å®šä¹‰ã€‚åœ¨ç¬¬ [13.2](#S13.SS2
    "13.2 TensorSRHT â€£ 13 é€šè¿‡ TensorSketch ç”Ÿæˆè°±ç¨€ç–åŒ–å™¨ â€£ å¿«é€Ÿä¼˜åŒ–è§†å›¾ï¼šåŸºäº Tensor å’Œ SVM æŠ€å·§çš„å•å±‚æ³¨æ„åŠ›é‡æ–°æ„é€ ï¼Œå¹¶ä»¥çŸ©é˜µä¹˜æ³•æ—¶é—´è§£å†³")
    èŠ‚ä¸­ï¼Œæˆ‘ä»¬æ¦‚è¿°äº† $\mathsf{TensorSRHT}$ã€‚åœ¨ç¬¬ [13.4](#S13.SS4 "13.4 é€šè¿‡è‰å›¾è¿›è¡Œ Hessian çš„å¿«é€Ÿè¿‘ä¼¼ â€£
    13 é€šè¿‡ TensorSketch ç”Ÿæˆè°±ç¨€ç–åŒ–å™¨ â€£ å¿«é€Ÿä¼˜åŒ–è§†å›¾ï¼šåŸºäº Tensor å’Œ SVM æŠ€å·§çš„å•å±‚æ³¨æ„åŠ›é‡æ–°æ„é€ ï¼Œå¹¶ä»¥çŸ©é˜µä¹˜æ³•æ—¶é—´è§£å†³")
    èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†é€šè¿‡è‰å›¾è¿›è¡Œ Hessian çš„å¿«é€Ÿè¿‘ä¼¼ã€‚
- en: 13.1 Oblivious Subspace Embedding
  id: totrans-1981
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1 æ— å…³å­ç©ºé—´åµŒå…¥
- en: We define oblivious subspace embedding,
  id: totrans-1982
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®šä¹‰äº†æ— å…³å­ç©ºé—´åµŒå…¥ï¼Œ
- en: Definition 13.1  (Oblivious subspace embedding, [[156](#bib.bib156)]).
  id: totrans-1983
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å®šä¹‰ 13.1  (æ— å…³å­ç©ºé—´åµŒå…¥ï¼Œ[[156](#bib.bib156)])ã€‚
- en: We define $(\epsilon,\delta,d,n)$ is a distribution on $m\times n$, where $m$,
    and $\delta$, for any fixed $n\times d$, a matrix $S$ has the property that the
    singular values of $SU$.
  id: totrans-1984
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®šä¹‰ $(\epsilon,\delta,d,n)$ æ˜¯ $m\times n$ çš„ä¸€ä¸ªåˆ†å¸ƒï¼Œå…¶ä¸­ $m$ å’Œ $\delta$ï¼Œå¯¹äºä»»ä½•å›ºå®šçš„ $n\times
    d$ï¼ŒçŸ©é˜µ $S$ å…·æœ‰ $SU$ çš„å¥‡å¼‚å€¼æ€§è´¨ã€‚
- en: 13.2 TensorSRHT
  id: totrans-1985
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2 TensorSRHT
- en: We define a well-known sketching matrix family called TensorSRHT [[104](#bib.bib104),
    [6](#bib.bib6)]. It has been used in many optimization literature [[163](#bib.bib163),
    [173](#bib.bib173), [166](#bib.bib166)].
  id: totrans-1986
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®šä¹‰äº†ä¸€ç§ç§°ä¸º TensorSRHT çš„è‘—åè‰å›¾çŸ©é˜µæ— [[104](#bib.bib104), [6](#bib.bib6)]ã€‚å®ƒå·²è¢«å¹¿æ³›ç”¨äºè®¸å¤šä¼˜åŒ–æ–‡çŒ®
    [[163](#bib.bib163), [173](#bib.bib173), [166](#bib.bib166)]ã€‚
- en: Definition 13.2  (Tensor subsampled randomized Hadamard transform (TensorSRHT)
    [[6](#bib.bib6), [163](#bib.bib163)]).
  id: totrans-1987
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å®šä¹‰ 13.2  (Tensor å­æ ·æœ¬éšæœº Hadamard å˜æ¢ (TensorSRHT) [[6](#bib.bib6), [163](#bib.bib163)])ã€‚
- en: The $\mathsf{TensorSRHT}$ is defined as
  id: totrans-1988
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathsf{TensorSRHT}$ å®šä¹‰ä¸º
- en: '|  | $\displaystyle S:=\frac{1}{\sqrt{m}}P\cdot(HD_{1}\otimes HD_{2}),$ |  |'
  id: totrans-1989
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle S:=\frac{1}{\sqrt{m}}P\cdot(HD_{1}\otimes HD_{2}),$ |  |'
- en: where each row of $P\in\{0,1\}^{m\times n^{2}}$ at a random coordinate and one
    can view $P$ is a $n\times n$, $D_{2}$ independent diagonal matrices with diagonals
    that are each independently set to be a Rademacher random variable (uniform in
    $\{-1,1\}$).
  id: totrans-1990
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $P\in\{0,1\}^{m\times n^{2}}$ çš„æ¯ä¸€è¡Œåœ¨éšæœºåæ ‡ä¸‹ï¼Œ$P$ å¯ä»¥è§†ä½œæ˜¯ä¸€ä¸ª $n\times n$ çš„çŸ©é˜µï¼Œ$D_{2}$
    æ˜¯ç‹¬ç«‹çš„å¯¹è§’çŸ©é˜µï¼Œå…¶å¯¹è§’çº¿ä¸Šçš„å…ƒç´ ç‹¬ç«‹åœ°è®¾ç½®ä¸º Rademacher éšæœºå˜é‡ï¼ˆåœ¨ $\{-1,1\}$ ä¸­å‡åŒ€åˆ†å¸ƒï¼‰ã€‚
- en: It is known [[6](#bib.bib6)] that TensorSRHT matrices imply the OSE.
  id: totrans-1991
  prefs: []
  type: TYPE_NORMAL
  zh: å·²çŸ¥[[6](#bib.bib6)] TensorSRHT çŸ©é˜µæš—ç¤º OSEã€‚
- en: Lemma 13.3  ([[6](#bib.bib6), [163](#bib.bib163)] , see for example, Lemma 2.12
    inÂ [[163](#bib.bib163)]).
  id: totrans-1992
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 13.3  ([[6](#bib.bib6), [163](#bib.bib163)] ï¼Œä¾‹å¦‚ï¼Œè§[[163](#bib.bib163)]ä¸­çš„å¼•ç†
    2.12)ã€‚
- en: 'Let $S$ be a TensorSRHT matrix defined in DefinitionÂ [13.2](#S13.Thmtheorem2
    "Definition 13.2 (Tensor subsampled randomized Hadamard transform (TensorSRHT)
    [6, 163]). â€£ 13.2 TensorSRHT â€£ 13 Generating a Spectral Sparsifier via TensorSketch
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"). If'
  id: totrans-1993
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ $S$ æ˜¯åœ¨å®šä¹‰ [13.2](#S13.Thmtheorem2 "å®šä¹‰ 13.2 (å¼ é‡å­é‡‡æ ·éšæœº Hadamard å˜æ¢ (TensorSRHT)
    [6, 163])ã€‚ â€£ 13.2 TensorSRHT â€£ 13 é€šè¿‡ TensorSketch ç”Ÿæˆè°±ç¨€ç–åŒ–å™¨ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿°å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")
    ä¸­å®šä¹‰çš„ TensorSRHT çŸ©é˜µã€‚å¦‚æœ
- en: '|  | $\displaystyle m=O(\epsilon^{-2}d^{2}\log^{3}(nd/\epsilon\delta)),$ |  |'
  id: totrans-1994
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle m=O(\epsilon^{-2}d^{2}\log^{3}(nd/\epsilon\delta)),$ |  |'
- en: then $S$-OSE for degree-$2$ tensors.
  id: totrans-1995
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆ $S$-OSE å¯¹äºåº¦æ•°-$2$ å¼ é‡ã€‚
- en: Further for matrices $A_{1},A_{2}\in\mathbb{R}^{n\times d}$ can be computed
    in $\widetilde{O}(nd+md^{2})$ time.
  id: totrans-1996
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºçŸ©é˜µ $A_{1},A_{2}\in\mathbb{R}^{n\times d}$ å¯ä»¥åœ¨ $\widetilde{O}(nd+md^{2})$ æ—¶é—´å†…è®¡ç®—ã€‚
- en: 13.3 TensorSparse
  id: totrans-1997
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.3 TensorSparse
- en: '[[166](#bib.bib166)] define TensorSparse by compose Sparse embedding [[133](#bib.bib133),
    [43](#bib.bib43)] with tensor operation [[136](#bib.bib136)].'
  id: totrans-1998
  prefs: []
  type: TYPE_NORMAL
  zh: '[[166](#bib.bib166)] é€šè¿‡å°†ç¨€ç–åµŒå…¥ [[133](#bib.bib133), [43](#bib.bib43)] ä¸å¼ é‡æ“ä½œ [[136](#bib.bib136)]
    ç»„åˆæ¥å®šä¹‰ TensorSparseã€‚'
- en: Definition 13.4  (TensorSparse, see DefinitionÂ 7.6 in [[166](#bib.bib166)]).
  id: totrans-1999
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å®šä¹‰ 13.4  ï¼ˆTensorSparseï¼Œè§å®šä¹‰ 7.6 åœ¨ [[166](#bib.bib166)]ï¼‰ã€‚
- en: 'Let $h_{1},h_{2}:[n]\times[s]\rightarrow[m/s]$-wise independent hash functions
    and let $\sigma_{1},\sigma_{2}:[n]\times[s]\rightarrow\{\pm 1\}$-wise independent
    random sign functions. Then, the degree two tensor sparse transform, $S:\mathbb{R}^{n}\times\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$
    is given as:'
  id: totrans-2000
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ $h_{1},h_{2}:[n]\times[s]\rightarrow[m/s]$-wise ç‹¬ç«‹å“ˆå¸Œå‡½æ•°ï¼Œ$\sigma_{1},\sigma_{2}:[n]\times[s]\rightarrow\{\pm
    1\}$-wise ç‹¬ç«‹éšæœºç¬¦å·å‡½æ•°ã€‚ç„¶åï¼Œåº¦æ•°äºŒå¼ é‡ç¨€ç–å˜æ¢ $S:\mathbb{R}^{n}\times\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$
    ç”±ä»¥ä¸‹å…¬å¼ç»™å‡ºï¼š
- en: '|  | $\displaystyle R_{r,(i,j)}=$ |  |'
  id: totrans-2001
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle R_{r,(i,j)}=$ |  |'
- en: Lemma 13.5  (Theorem 7.10 in [[166](#bib.bib166)]).
  id: totrans-2002
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 13.5  ï¼ˆå®šç† 7.10 åœ¨ [[166](#bib.bib166)]ï¼‰ã€‚
- en: 'Let $\epsilon\in(0,1)$ be success probability. Let $S\in\mathbb{R}^{m\times
    n^{2}}$ matrix (Def.Â [13.4](#S13.Thmtheorem4 "Definition 13.4 (TensorSparse, see
    Definition 7.6 in [166]). â€£ 13.3 TensorSparse â€£ 13 Generating a Spectral Sparsifier
    via TensorSketch â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")). Suppose $m=\Omega(\epsilon^{-2}d^{2}\log(n/\delta))$, then TensorSparse
    provides $(\epsilon,\delta,d^{2},n^{2})$-OSE.'
  id: totrans-2003
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ $\epsilon\in(0,1)$ ä¸ºæˆåŠŸæ¦‚ç‡ã€‚è®¾ $S\in\mathbb{R}^{m\times n^{2}}$ çŸ©é˜µï¼ˆå®šä¹‰ [13.4](#S13.Thmtheorem4
    "å®šä¹‰ 13.4 (TensorSparseï¼Œè§å®šä¹‰ 7.6 åœ¨ [166] ä¸­)ã€‚ â€£ 13.3 TensorSparse â€£ 13 é€šè¿‡ TensorSketch
    ç”Ÿæˆè°±ç¨€ç–åŒ–å™¨ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ–°è¡¨è¿°å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³")ï¼‰ã€‚å‡è®¾ $m=\Omega(\epsilon^{-2}d^{2}\log(n/\delta))$ï¼Œåˆ™
    TensorSparse æä¾› $(\epsilon,\delta,d^{2},n^{2})$-OSEã€‚
- en: Further for matrices $A_{1},A_{2}\in\mathbb{R}^{n\times d}$ can be computed
    in $O((\operatorname{nnz}(A_{1})+\operatorname{nnz}(A_{2}))s+md^{2})$ time
  id: totrans-2004
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºçŸ©é˜µ $A_{1},A_{2}\in\mathbb{R}^{n\times d}$ï¼Œå¯ä»¥åœ¨ $O((\operatorname{nnz}(A_{1})+\operatorname{nnz}(A_{2}))s+md^{2})$
    æ—¶é—´å†…è®¡ç®—ã€‚
- en: 13.4 Fast Approximation for Hessian via Sketching
  id: totrans-2005
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.4 é€šè¿‡è‰å›¾è¿›è¡Œ Hessian çš„å¿«é€Ÿè¿‘ä¼¼
- en: In this section, we present the fast approximation for hessian via sketching.
  id: totrans-2006
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»é€šè¿‡è‰å›¾è¿›è¡Œ Hessian çš„å¿«é€Ÿè¿‘ä¼¼ã€‚
- en: Lemma 13.6.
  id: totrans-2007
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 13.6ã€‚
- en: If the following conditions hold
  id: totrans-2008
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶
- en: â€¢
  id: totrans-2009
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $A_{1}\in\mathbb{R}^{n\times d}$
  id: totrans-2010
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $A_{1}\in\mathbb{R}^{n\times d}$
- en: â€¢
  id: totrans-2011
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $\operatorname{\mathsf{A}}=(A_{1}\otimes A_{2})\in\mathbb{R}^{n^{2}\times
    d^{2}}$
  id: totrans-2012
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $\operatorname{\mathsf{A}}=(A_{1}\otimes A_{2})\in\mathbb{R}^{n^{2}\times
    d^{2}}$
- en: â€¢
  id: totrans-2013
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $W\in\mathbb{R}^{n\times n}$ denote a positive diagonal matrix
  id: totrans-2014
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $W\in\mathbb{R}^{n\times n}$ è¡¨ç¤ºä¸€ä¸ªæ­£å¯¹è§’çŸ©é˜µ
- en: â€¢
  id: totrans-2015
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $\overline{A}_{1}=WA_{1}$
  id: totrans-2016
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $\overline{A}_{1}=WA_{1}$
- en: â€¢
  id: totrans-2017
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $\overline{\operatorname{\mathsf{A}}}=(\overline{A}_{1}\otimes A_{2})\in\mathbb{R}^{n^{2}\times
    d^{2}}$
  id: totrans-2018
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $\overline{\operatorname{\mathsf{A}}}=(\overline{A}_{1}\otimes A_{2})\in\mathbb{R}^{n^{2}\times
    d^{2}}$
- en: Then, we have
  id: totrans-2019
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¾—åˆ°
- en: â€¢
  id: totrans-2020
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 1.
  id: totrans-2021
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€éƒ¨åˆ†ã€‚
- en: '|  | $\displaystyle\operatorname{\mathsf{A}}^{\top}(W^{2}\otimes I_{n})\operatorname{\mathsf{A}}=\overline{\operatorname{\mathsf{A}}}^{\top}\overline{\operatorname{\mathsf{A}}}$
    |  |'
  id: totrans-2022
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\operatorname{\mathsf{A}}^{\top}(W^{2}\otimes I_{n})\operatorname{\mathsf{A}}=\overline{\operatorname{\mathsf{A}}}^{\top}\overline{\operatorname{\mathsf{A}}}$
    |  |'
- en: â€¢
  id: totrans-2023
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 2. For any constant $\epsilon\in(0,0.1)$ time to compute $S\overline{\operatorname{\mathsf{A}}}$
    such that
  id: totrans-2024
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬äºŒéƒ¨åˆ†ã€‚å¯¹äºä»»ä½•å¸¸æ•° $\epsilon\in(0,0.1)$ï¼Œè®¡ç®— $S\overline{\operatorname{\mathsf{A}}}$
    çš„æ—¶é—´ä½¿å¾—
- en: '|  | $\displaystyle(1-\epsilon)\cdot\overline{\operatorname{\mathsf{A}}}^{\top}\overline{\operatorname{\mathsf{A}}}\preceq\overline{\operatorname{\mathsf{A}}}^{\top}S^{\top}S\overline{\operatorname{\mathsf{A}}}\preceq(1+\epsilon)\cdot\overline{\operatorname{\mathsf{A}}}^{\top}\overline{\operatorname{\mathsf{A}}}$
    |  |'
  id: totrans-2025
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle(1-\epsilon)\cdot\overline{\operatorname{\mathsf{A}}}^{\top}\overline{\operatorname{\mathsf{A}}}\preceq\overline{\operatorname{\mathsf{A}}}^{\top}S^{\top}S\overline{\operatorname{\mathsf{A}}}\preceq(1+\epsilon)\cdot\overline{\operatorname{\mathsf{A}}}^{\top}\overline{\operatorname{\mathsf{A}}}$
    |  |'
- en: holds with probability $1-\delta$.
  id: totrans-2026
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¥æ¦‚ç‡$1-\delta$æˆç«‹ã€‚
- en: â€¢
  id: totrans-2027
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Part 3. For any $\epsilon\in(0,0.1)$ time to compute $S\overline{\operatorname{\mathsf{A}}}$
    such that
  id: totrans-2028
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰éƒ¨åˆ†ã€‚å¯¹äºä»»ä½• $\epsilon\in(0,0.1)$ï¼Œè®¡ç®— $S\overline{\operatorname{\mathsf{A}}}$ çš„æ—¶é—´ï¼Œä½¿å¾—
- en: '|  | $\displaystyle(1-\epsilon)\cdot\overline{\operatorname{\mathsf{A}}}^{\top}\overline{\operatorname{\mathsf{A}}}\preceq\overline{\operatorname{\mathsf{A}}}^{\top}S^{\top}S\overline{\operatorname{\mathsf{A}}}\preceq(1+\epsilon)\cdot\overline{\operatorname{\mathsf{A}}}^{\top}\overline{\operatorname{\mathsf{A}}}$
    |  |'
  id: totrans-2029
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle(1-\epsilon)\cdot\overline{\operatorname{\mathsf{A}}}^{\top}\overline{\operatorname{\mathsf{A}}}\preceq\overline{\operatorname{\mathsf{A}}}^{\top}S^{\top}S\overline{\operatorname{\mathsf{A}}}\preceq(1+\epsilon)\cdot\overline{\operatorname{\mathsf{A}}}^{\top}\overline{\operatorname{\mathsf{A}}}$
    |  |'
- en: holds with probability $1-\delta$.
  id: totrans-2030
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¥æ¦‚ç‡$1-\delta$æˆç«‹ã€‚
- en: Proof.
  id: totrans-2031
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è¯æ˜ã€‚
- en: Proof of Part 1.
  id: totrans-2032
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: We can show
  id: totrans-2033
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å±•ç¤º
- en: '|  | $\displaystyle\operatorname{\mathsf{A}}^{\top}(W^{2}\otimes I_{n})\operatorname{\mathsf{A}}=$
    |  |'
  id: totrans-2034
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\operatorname{\mathsf{A}}^{\top}(W^{2}\otimes I_{n})\operatorname{\mathsf{A}}=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-2035
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-2036
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-2037
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: where the first step follows from $(W^{2}\otimes I)=(W\otimes I_{n})\cdot(W\otimes
    I_{n})$ operation and $W$, the third step follows from the definition of $\overline{A}_{1}$.
  id: totrans-2038
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ç¬¬ä¸€æ­¥æ¥è‡ª$(W^{2}\otimes I)=(W\otimes I_{n})\cdot(W\otimes I_{n})$ æ“ä½œå’Œ $W$ï¼Œç¬¬ä¸‰æ­¥æ¥è‡ª
    $\overline{A}_{1}$ çš„å®šä¹‰ã€‚
- en: Proof of Part 2.
  id: totrans-2039
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒéƒ¨åˆ†çš„è¯æ˜ã€‚
- en: 'It follows from using LemmaÂ [13.3](#S13.Thmtheorem3 "Lemma 13.3 ([6, 163] ,
    see for example, Lemma 2.12 in [163]). â€£ 13.2 TensorSRHT â€£ 13 Generating a Spectral
    Sparsifier via TensorSketch â€£ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  id: totrans-2040
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯ä»¥ä»å¼•ç†Â [13.3](#S13.Thmtheorem3 "å¼•ç† 13.3 ([6, 163]ï¼Œä¾‹å¦‚ï¼Œè§[163]ä¸­çš„å¼•ç† 2.12)ã€‚ â€£ 13.2
    TensorSRHT â€£ 13 é€šè¿‡ TensorSketch ç”Ÿæˆè°±ç¨€ç–å™¨ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäº Tensor å’Œ SVM æŠ€å·§çš„å•å±‚æ³¨æ„åŠ›çš„é‡æ–°è¡¨è¿°ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³å®ƒã€‚")
    å¾—å‡ºã€‚
- en: Proof of Part 3.
  id: totrans-2041
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰éƒ¨åˆ†çš„è¯æ˜ã€‚
- en: 'It follows from using LemmaÂ [13.5](#S13.Thmtheorem5 "Lemma 13.5 (Theorem 7.10
    in [166]). â€£ 13.3 TensorSparse â€£ 13 Generating a Spectral Sparsifier via TensorSketch
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-2042
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯ä»¥ä»å¼•ç†Â [13.5](#S13.Thmtheorem5 "å¼•ç† 13.5 (å®šç† 7.10 åœ¨ [166] ä¸­)ã€‚ â€£ 13.3 TensorSparse
    â€£ 13 é€šè¿‡ TensorSketch ç”Ÿæˆè°±ç¨€ç–å™¨ â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäº Tensor å’Œ SVM æŠ€å·§çš„å•å±‚æ³¨æ„åŠ›çš„é‡æ–°è¡¨è¿°ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³å®ƒã€‚")
    å¾—å‡ºã€‚
- en: âˆ
  id: totrans-2043
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ
- en: '14 Analysis Of AlgorithmÂ [1](#alg1 "Algorithm 1 â€£ 6 Hessian â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")'
  id: totrans-2044
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14 ç®—æ³•åˆ†æÂ [1](#alg1 "ç®—æ³• 1 â€£ 6 Hessian â€£ å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäº Tensor å’Œ SVM æŠ€å·§çš„å•å±‚æ³¨æ„åŠ›çš„é‡æ–°è¡¨è¿°ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…è§£å†³å®ƒ")
- en: 'We introduce the concept of a $(l,M)$-good function in SectionÂ [14.1](#S14.SS1
    "14.1 (ğ‘™,ğ‘€)-Good Loss Function â€£ 14 Analysis Of Algorithm 1 â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") and discuss the notion of a well-initialized
    point. Subsequently, we will present our approximation and update rule methods
    in SectionÂ [14.2](#S14.SS2 "14.2 Convergence â€£ 14 Analysis Of Algorithm 1 â€£ A
    Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time"). In light of the
    optimization problem introduced in DefinitionÂ [1.2](#S1.Thmtheorem2 "Definition
    1.2 (Attention optimization). â€£ 1 Introduction â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), we put forward AlgorithmÂ [1](#alg1 "Algorithm 1
    â€£ 6 Hessian â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    and in this section, we establish the correctness and convergence of the algorithm.'
  id: totrans-2045
  prefs: []
  type: TYPE_NORMAL
  zh: 'æˆ‘ä»¬åœ¨ç¬¬[14.1](#S14.SS1 "14.1 (ğ‘™,ğ‘€)-Good Loss Function â€£ 14 Analysis Of Algorithm
    1 â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")èŠ‚ä»‹ç»äº† $(l,M)$-è‰¯å¥½å‡½æ•°çš„æ¦‚å¿µï¼Œå¹¶è®¨è®ºäº†è‰¯å¥½åˆå§‹åŒ–ç‚¹çš„æ¦‚å¿µã€‚éšåï¼Œæˆ‘ä»¬å°†åœ¨ç¬¬[14.2](#S14.SS2
    "14.2 Convergence â€£ 14 Analysis Of Algorithm 1 â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")èŠ‚ä¸­ä»‹ç»æˆ‘ä»¬çš„è¿‘ä¼¼å’Œæ›´æ–°è§„åˆ™æ–¹æ³•ã€‚é‰´äºå®šä¹‰[1.2](#S1.Thmtheorem2 "Definition
    1.2 (Attention optimization). â€£ 1 Introduction â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")ä¸­å¼•å…¥çš„ä¼˜åŒ–é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç®—æ³•[1](#alg1 "Algorithm 1 â€£ 6 Hessian
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")ï¼Œåœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å»ºç«‹äº†è¯¥ç®—æ³•çš„æ­£ç¡®æ€§å’Œæ”¶æ•›æ€§ã€‚'
- en: 14.1 $(l,M)$-Good Loss Function
  id: totrans-2046
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1 $(l,M)$-è‰¯å¥½æŸå¤±å‡½æ•°
- en: 'We will now introduce the definition of a $(l,M)$-Good Loss Function. Next,
    letâ€™s revisit the optimization problem defined in DefinitionÂ [4.7](#S4.Thmtheorem7
    "Definition 4.7\. â€£ 4.2 General Definitions â€£ 4 Preliminary â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") as follows:'
  id: totrans-2047
  prefs: []
  type: TYPE_NORMAL
  zh: 'æˆ‘ä»¬ç°åœ¨ä»‹ç» $(l,M)$-è‰¯å¥½æŸå¤±å‡½æ•°çš„å®šä¹‰ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å›é¡¾åœ¨å®šä¹‰[4.7](#S4.Thmtheorem7 "Definition 4.7\.
    â€£ 4.2 General Definitions â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")ä¸­å®šä¹‰çš„ä¼˜åŒ–é—®é¢˜ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š'
- en: '|  | $1$2 |  |'
  id: totrans-2048
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: We will now demonstrate that our optimization function possesses the following
    properties.
  id: totrans-2049
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å±•ç¤ºæˆ‘ä»¬çš„ä¼˜åŒ–å‡½æ•°å…·æœ‰ä»¥ä¸‹ç‰¹æ€§ã€‚
- en: Definition 14.1  ($(l,M)$-good Loss function).
  id: totrans-2050
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å®šä¹‰ 14.1 ($(l,M)$-è‰¯å¥½æŸå¤±å‡½æ•°)ã€‚
- en: For a function $L:\mathbb{R}^{d}\rightarrow\mathbb{R}$, if the following conditions
    hold,
  id: totrans-2051
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä¸€ä¸ªå‡½æ•° $L:\mathbb{R}^{d}\rightarrow\mathbb{R}$ï¼Œå¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼Œ
- en: â€¢
  id: totrans-2052
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Hessian is $M$ such that
  id: totrans-2053
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Hessian æ˜¯ $M$ï¼Œæ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼š
- en: '|  | $1$2 |  |'
  id: totrans-2054
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: â€¢
  id: totrans-2055
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $l$ as a positive scalar. If there exists a vector $x^{*}\in\mathbb{R}^{d^{2}}$
    such that the following holds
  id: totrans-2056
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $l$ ä¸ºä¸€ä¸ªæ­£æ ‡é‡ã€‚å¦‚æœå­˜åœ¨ä¸€ä¸ªå‘é‡ $x^{*}\in\mathbb{R}^{d^{2}}$ï¼Œä½¿å¾—ä»¥ä¸‹æ¡ä»¶æˆç«‹ï¼š
- en: â€“
  id: totrans-2057
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€“
- en: $\nabla L(x^{*},y^{*})={\bf 0}_{d}$.
  id: totrans-2058
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\nabla L(x^{*},y^{*})={\bf 0}_{d}$ã€‚
- en: â€“
  id: totrans-2059
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€“
- en: $\nabla^{2}L(x^{*},y^{*})\succeq l\cdot I_{2d^{2}}$.
  id: totrans-2060
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\nabla^{2}L(x^{*},y^{*})\succeq l\cdot I_{2d^{2}}$ã€‚
- en: â€¢
  id: totrans-2061
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Good Initialization Point. Let $x_{0}$ denote the initialization point. If $r_{0}:=(\|x_{0}-x_{*}\|_{2}+\|y_{0}-y_{*}\|_{2})$
    satisfies
  id: totrans-2062
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è‰¯å¥½çš„åˆå§‹åŒ–ç‚¹ã€‚è®¾ $x_{0}$ ä¸ºåˆå§‹åŒ–ç‚¹ã€‚å¦‚æœ $r_{0}:=(\|x_{0}-x_{*}\|_{2}+\|y_{0}-y_{*}\|_{2})$
    æ»¡è¶³
- en: '|  | $\displaystyle r_{0}M\leq 0.1l.$ |  |'
  id: totrans-2063
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle r_{0}M\leq 0.1l.$ |  |'
- en: we say $L$-good
  id: totrans-2064
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç§° $L$ ä¸º $L$-è‰¯å¥½
- en: 'Drawing upon LemmaÂ [6.1](#S6.Thmtheorem1 "Lemma 6.1\. â€£ 6 Hessian â€£ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time") and LemmaÂ [12.1](#S12.Thmtheorem1
    "Lemma 12.1\. â€£ 12.1 Main Results â€£ 12 Lipschitz for Hessian of ğ‘¥,ğ‘¦ â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), we can establish that our loss
    function (See DefinitionÂ [4.7](#S4.Thmtheorem7 "Definition 4.7\. â€£ 4.2 General
    Definitions â€£ 4 Preliminary â€£ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")) satisfies the aforementioned assumption.'
  id: totrans-2065
  prefs: []
  type: TYPE_NORMAL
  zh: å€Ÿé‰´å¼•ç†[6.1](#S6.Thmtheorem1 "å¼•ç† 6.1\. â€£ 6 Hessian â€£ ä¸€ç§å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")å’Œå¼•ç†[12.1](#S12.Thmtheorem1
    "å¼•ç† 12.1\. â€£ 12.1 ä¸»è¦ç»“æœ â€£ 12 Lipschitz å¯¹ ğ‘¥,ğ‘¦ çš„ Hessian â€£ ä¸€ç§å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")ï¼Œæˆ‘ä»¬å¯ä»¥ç¡®å®šæˆ‘ä»¬çš„æŸå¤±å‡½æ•°ï¼ˆè§å®šä¹‰[4.7](#S4.Thmtheorem7
    "å®šä¹‰ 4.7\. â€£ 4.2 ä¸€èˆ¬å®šä¹‰ â€£ 4 åˆæ­¥ â€£ ä¸€ç§å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")ï¼‰æ»¡è¶³ä¸Šè¿°å‡è®¾ã€‚
- en: 14.2 Convergence
  id: totrans-2066
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.2 æ”¶æ•›æ€§
- en: 'After introducing the approximation method â€™Sparsifier via TensorSketchâ€™ in
    SectionÂ [13](#S13 "13 Generating a Spectral Sparsifier via TensorSketch â€£ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time"), we will now proceed
    to introduce the update method employed in AlgorithmÂ [1](#alg1 "Algorithm 1 â€£
    6 Hessian â€£ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").
    In this section, we demonstrate the concept of approximate update and present
    an induction hypothesis.'
  id: totrans-2067
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬[13](#S13 "13 é€šè¿‡ TensorSketch ç”Ÿæˆè°±ç¨€ç–åŒ–å™¨ â€£ ä¸€ç§å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")èŠ‚ä»‹ç»äº†è¿‘ä¼¼æ–¹æ³•â€˜é€šè¿‡
    TensorSketch ç”Ÿæˆç¨€ç–åŒ–å™¨â€™åï¼Œæˆ‘ä»¬å°†ä»‹ç»åœ¨ç®—æ³•[1](#alg1 "ç®—æ³• 1 â€£ 6 Hessian â€£ ä¸€ç§å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM
    æŠ€å·§é‡æ„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")ä¸­ä½¿ç”¨çš„æ›´æ–°æ–¹æ³•ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æ¼”ç¤ºäº†è¿‘ä¼¼æ›´æ–°çš„æ¦‚å¿µï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªå½’çº³å‡è®¾ã€‚
- en: Definition 14.2  (Approximate Update).
  id: totrans-2068
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å®šä¹‰ 14.2ï¼ˆè¿‘ä¼¼æ›´æ–°ï¼‰ã€‚
- en: The following process is considered by us
  id: totrans-2069
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è€ƒè™‘ä»¥ä¸‹è¿‡ç¨‹
- en: '|  | $$\displaystyle\begin{bmatrix}x(t+1)\\ y(t+1)\end{bmatrix}\leftarrow\begin{bmatrix}x(t)\\'
  id: totrans-2070
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\displaystyle\begin{bmatrix}x(t+1)\\ y(t+1)\end{bmatrix}\leftarrow\begin{bmatrix}x(t)\\'
- en: y(t)\end{bmatrix}-\begin{bmatrix}g(x(t))\\
  id: totrans-2071
  prefs: []
  type: TYPE_NORMAL
  zh: y(t)\end{bmatrix}-\begin{bmatrix}g(x(t))\\
- en: g(y(t))\end{bmatrix}\widetilde{H}^{-1}$$ |  |
  id: totrans-2072
  prefs: []
  type: TYPE_NORMAL
  zh: $g(y(t))\end{bmatrix}\widetilde{H}^{-1}$$ |  |
- en: A tool from previous work is presented by us now.
  id: totrans-2073
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨ä»‹ç»ä¸€ä¸ªæ¥è‡ªå…ˆå‰å·¥ä½œçš„å·¥å…·ã€‚
- en: Lemma 14.3  (Iterative shrinking, a variation of Lemma 6.9 on page 32 of [[118](#bib.bib118)]).
  id: totrans-2074
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 14.3ï¼ˆè¿­ä»£æ”¶ç¼©ï¼Œè§ [[118](#bib.bib118)] ç¬¬ 32 é¡µçš„å¼•ç† 6.9 çš„å˜ä½“ï¼‰ã€‚
- en: If the following conditions hold
  id: totrans-2075
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶
- en: â€¢
  id: totrans-2076
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Loss Function $L$-good (see DefinitionÂ [14.1](#S14.Thmtheorem1 "Definition
    14.1 ((ğ‘™,ğ‘€)-good Loss function). â€£ 14.1 (ğ‘™,ğ‘€)-Good Loss Function â€£ 14 Analysis
    Of Algorithm 1 â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")).'
  id: totrans-2077
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æŸå¤±å‡½æ•° $L$-è‰¯å¥½ï¼ˆè§å®šä¹‰[14.1](#S14.Thmtheorem1 "å®šä¹‰ 14.1ï¼ˆ(ğ‘™,ğ‘€)-è‰¯å¥½æŸå¤±å‡½æ•°ï¼‰ã€‚ â€£ 14.1 (ğ‘™,ğ‘€)-è‰¯å¥½æŸå¤±å‡½æ•°
    â€£ 14 ç®—æ³• 1 çš„åˆ†æ â€£ ä¸€ç§å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")ï¼‰ã€‚
- en: â€¢
  id: totrans-2078
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $\epsilon\in(0,0.1)$ (see Lemma[13.6](#S13.Thmtheorem6 "Lemma 13.6\. â€£
    13.4 Fast Approximation for Hessian via Sketching â€£ 13 Generating a Spectral Sparsifier
    via TensorSketch â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")).'
  id: totrans-2079
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $\epsilon\in(0,0.1)$ï¼ˆè§å¼•ç†[13.6](#S13.Thmtheorem6 "å¼•ç† 13.6\. â€£ 13.4 é€šè¿‡ç»˜å›¾å¿«é€Ÿè¿‘ä¼¼
    Hessian â€£ 13 é€šè¿‡ TensorSketch ç”Ÿæˆè°±ç¨€ç–åŒ–å™¨ â€£ ä¸€ç§å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")ï¼‰ã€‚
- en: â€¢
  id: totrans-2080
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $x^{*},y^{*}$ be defined in Definition[14.2](#S14.Thmtheorem2 "Definition
    14.2 (Approximate Update). â€£ 14.2 Convergence â€£ 14 Analysis Of Algorithm 1 â€£ A
    Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-2081
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $x^{*},y^{*}$ å¦‚å®šä¹‰[14.2](#S14.Thmtheorem2 "å®šä¹‰ 14.2ï¼ˆè¿‘ä¼¼æ›´æ–°ï¼‰ã€‚ â€£ 14.2 æ”¶æ•›æ€§ â€£ 14 ç®—æ³•
    1 çš„åˆ†æ â€£ ä¸€ç§å¿«é€Ÿä¼˜åŒ–è§†è§’ï¼šåŸºäºå¼ é‡å’Œ SVM æŠ€å·§é‡æ„å•å±‚æ³¨æ„åŠ›ï¼Œå¹¶åœ¨çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£")ä¸­å®šä¹‰ã€‚
- en: â€¢
  id: totrans-2082
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $r_{t}:=\|x_{t}-x^{*}\|_{2}+\|y_{t}-y^{*}\|_{2}$.
  id: totrans-2083
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $r_{t}:=\|x_{t}-x^{*}\|_{2}+\|y_{t}-y^{*}\|_{2}$ã€‚
- en: â€¢
  id: totrans-2084
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $\overline{r}_{t}:=M\cdot r_{t}$
  id: totrans-2085
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¤ $\overline{r}_{t}:=M\cdot r_{t}$
- en: It follows that
  id: totrans-2086
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤
- en: '|  | $\displaystyle r_{t+1}\leq 2\cdot(\epsilon_{0}+\overline{r}_{t}/(l-\overline{r}_{t}))\cdot
    r_{t}.$ |  |'
  id: totrans-2087
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle r_{t+1}\leq 2\cdot(\epsilon_{0}+\overline{r}_{t}/(l-\overline{r}_{t}))\cdot
    r_{t}.$ |  |'
- en: 'In this context, where $T$ denotes the total number of iterations in the algorithm,
    we require the following lemma based on the induction hypothesis to apply LemmaÂ [14.3](#S14.Thmtheorem3
    "Lemma 14.3 (Iterative shrinking, a variation of Lemma 6.9 on page 32 of [118]).
    â€£ 14.2 Convergence â€£ 14 Analysis Of Algorithm 1 â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"). This lemma is a well-established concept in the
    literature, and for further details, you can refer to [[118](#bib.bib118)].'
  id: totrans-2088
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­ï¼Œ$T$ è¡¨ç¤ºç®—æ³•ä¸­çš„æ€»è¿­ä»£æ¬¡æ•°ï¼Œæˆ‘ä»¬éœ€è¦åŸºäºå½’çº³å‡è®¾çš„ä»¥ä¸‹å¼•ç†æ¥åº”ç”¨å¼•ç† [14.3](#S14.Thmtheorem3 "Lemma
    14.3 (Iterative shrinking, a variation of Lemma 6.9 on page 32 of [118]). â€£ 14.2
    Convergence â€£ 14 Analysis Of Algorithm 1 â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")ã€‚è¿™ä¸ªå¼•ç†æ˜¯æ–‡çŒ®ä¸­çš„ä¸€ä¸ªæˆç†Ÿæ¦‚å¿µï¼Œæ›´å¤šç»†èŠ‚è¯·å‚è€ƒ [[118](#bib.bib118)]ã€‚'
- en: Lemma 14.4  (Induction hypothesis, Lemma 6.10 on page 34 of [[118](#bib.bib118)]).
  id: totrans-2089
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å¼•ç† 14.4  ï¼ˆå½’çº³å‡è®¾ï¼Œå¼•ç† 6.10 åœ¨ [[118](#bib.bib118)] çš„ç¬¬ 34 é¡µï¼‰ã€‚
- en: If the following condition hold
  id: totrans-2090
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶
- en: â€¢
  id: totrans-2091
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: '$\epsilon=0.01$ (see Lemma[13.6](#S13.Thmtheorem6 "Lemma 13.6\. â€£ 13.4 Fast
    Approximation for Hessian via Sketching â€£ 13 Generating a Spectral Sparsifier
    via TensorSketch â€£ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"))'
  id: totrans-2092
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$\epsilon=0.01$ï¼ˆè§å¼•ç†[13.6](#S13.Thmtheorem6 "Lemma 13.6\. â€£ 13.4 Fast Approximation
    for Hessian via Sketching â€£ 13 Generating a Spectral Sparsifier via TensorSketch
    â€£ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")ï¼‰'
- en: â€¢
  id: totrans-2093
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $x^{*},y^{*}$ be defined in Definition[14.2](#S14.Thmtheorem2 "Definition
    14.2 (Approximate Update). â€£ 14.2 Convergence â€£ 14 Analysis Of Algorithm 1 â€£ A
    Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-2094
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'è®¾ $x^{*},y^{*}$ åœ¨å®šä¹‰[14.2](#S14.Thmtheorem2 "Definition 14.2 (Approximate Update).
    â€£ 14.2 Convergence â€£ 14 Analysis Of Algorithm 1 â€£ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") ä¸­å®šä¹‰ã€‚'
- en: â€¢
  id: totrans-2095
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Let $r_{t}:=\|x_{t}-x^{*}\|_{2}+\|y_{t}-y^{*}\|_{2}$.
  id: totrans-2096
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®¾ $r_{t}:=\|x_{t}-x^{*}\|_{2}+\|y_{t}-y^{*}\|_{2}$ã€‚
- en: â€¢
  id: totrans-2097
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: For each $i\in[T]$, for all $i\in[t]$
  id: totrans-2098
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ª $i\in[T]$ï¼Œå¯¹æ‰€æœ‰ $i\in[t]$
- en: â€¢
  id: totrans-2099
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Let $l$ be Defined in Definition[14.1](#S14.Thmtheorem1 "Definition 14.1 ((ğ‘™,ğ‘€)-good
    Loss function). â€£ 14.1 (ğ‘™,ğ‘€)-Good Loss Function â€£ 14 Analysis Of Algorithm 1 â€£
    A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on
    Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-2100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'è®¾ $l$ åœ¨å®šä¹‰[14.1](#S14.Thmtheorem1 "Definition 14.1 ((ğ‘™,ğ‘€)-good Loss function).
    â€£ 14.1 (ğ‘™,ğ‘€)-Good Loss Function â€£ 14 Analysis Of Algorithm 1 â€£ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") ä¸­å®šä¹‰'
- en: â€¢
  id: totrans-2101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $M\cdot r_{i}\leq 0.1l$.
  id: totrans-2102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $M\cdot r_{i}\leq 0.1l$ã€‚
- en: It follows that
  id: totrans-2103
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±æ­¤å¾—å‡º
- en: â€¢
  id: totrans-2104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $r_{t+1}\leq 0.4r_{t}$
  id: totrans-2105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $r_{t+1}\leq 0.4r_{t}$
- en: â€¢
  id: totrans-2106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $M\cdot r_{t+1}\leq 0.1l$
  id: totrans-2107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $M\cdot r_{t+1}\leq 0.1l$
- en: References
  id: totrans-2108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: 'AAA^+ [23] Zaid Alyafeai, MagedÂ S Alshaibani, Badr AlKhamissi, Hamzah Luqman,
    Ebrahim Alareqi, and Ali Fadel. Taqyim: Evaluating arabic nlp tasks using chatgpt
    models. arXiv preprint arXiv:2306.16322, 2023.'
  id: totrans-2109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'AAA^+ [23] Zaid Alyafeai, Maged S Alshaibani, Badr AlKhamissi, Hamzah Luqman,
    Ebrahim Alareqi, å’Œ Ali Fadel. Taqyim: ä½¿ç”¨ chatgpt æ¨¡å‹è¯„ä¼°é˜¿æ‹‰ä¼¯è¯­ NLP ä»»åŠ¡ã€‚arXiv é¢„å°æœ¬ arXiv:2306.16322,
    2023ã€‚'
- en: AC [06] Nir Ailon and Bernard Chazelle. Approximate nearest neighbors and the
    fast johnson-lindenstrauss transform. In Proceedings of the thirty-eighth annual
    ACM symposium on Theory of computing, pages 557â€“563, 2006.
  id: totrans-2110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AC [06] Nir Ailon å’Œ Bernard Chazelle. è¿‘ä¼¼æœ€è¿‘é‚»å’Œå¿«é€Ÿçš„ johnson-lindenstrauss å˜æ¢ã€‚å‘è¡¨äºç¬¬ä¸‰åå…«å±Š
    ACM ç†è®ºè®¡ç®—æœºå­¦å¹´ä¼šè®®è®ºæ–‡é›†ï¼Œç¬¬ 557â€“563 é¡µï¼Œ2006 å¹´ã€‚
- en: '[3] Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained
    analysis of optimization and generalization for overparameterized two-layer neural
    networks. In International Conference on Machine Learning, pages 322â€“332\. PMLR,
    2019.'
  id: totrans-2111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, å’Œ Ruosong Wang. å¯¹è¿‡å‚æ•°åŒ–ä¸¤å±‚ç¥ç»ç½‘ç»œçš„ä¼˜åŒ–å’Œæ³›åŒ–è¿›è¡Œç»†ç²’åº¦åˆ†æã€‚å‘è¡¨äºå›½é™…æœºå™¨å­¦ä¹ ä¼šè®®ï¼Œç¬¬
    322â€“332 é¡µã€‚PMLRï¼Œ2019ã€‚'
- en: '[4] Sanjeev Arora, SimonÂ S Du, Wei Hu, Zhiyuan Li, RussÂ R Salakhutdinov, and
    Ruosong Wang. On exact computation with an infinitely wide neural net. Advances
    in neural information processing systems, 32, 2019.'
  id: totrans-2112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, å’Œ
    Ruosong Wang. å…³äºæ— é™å®½ç¥ç»ç½‘ç»œçš„ç²¾ç¡®è®¡ç®—ã€‚ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ï¼Œ32ï¼Œ2019ã€‚'
- en: 'AHO^+ [23] Kabir Ahuja, Rishav Hada, Millicent Ochieng, Prachi Jain, Harshita
    Diddee, Samuel Maina, Tanuja Ganu, Sameer Segal, Maxamed Axmed, Kalika Bali, etÂ al.
    Mega: Multilingual evaluation of generative ai. arXiv preprint arXiv:2303.12528,
    2023.'
  id: totrans-2113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AHO^+ [23] Kabir Ahuja, Rishav Hada, Millicent Ochieng, Prachi Jain, Harshita
    Diddee, Samuel Maina, Tanuja Ganu, Sameer Segal, Maxamed Axmed, Kalika Bali ç­‰äººã€‚Megaï¼šç”Ÿæˆ
    AI çš„å¤šè¯­è¨€è¯„ä¼°ã€‚arXiv é¢„å°æœ¬ arXiv:2303.12528ï¼Œ2023ã€‚
- en: AKK^+ [20] ThomasÂ D Ahle, Michael Kapralov, JakobÂ BT Knudsen, Rasmus Pagh, Ameya
    Velingker, DavidÂ P Woodruff, and Amir Zandieh. Oblivious sketching of high-degree
    polynomial kernels. In Proceedings of the Fourteenth Annual ACM-SIAM Symposium
    on Discrete Algorithms, pages 141â€“160\. SIAM, 2020.
  id: totrans-2114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AKK^+ [20] Thomas D Ahle, Michael Kapralov, Jakob BT Knudsen, Rasmus Pagh, Ameya
    Velingker, David P Woodruff å’Œ Amir Zandieh. é«˜åº¦å¤šé¡¹å¼æ ¸çš„æ— æ„è¯†è‰å›¾ã€‚å‘è¡¨äºã€Šç¬¬åå››å±Š ACM-SIAM ç¦»æ•£ç®—æ³•å¹´ä¼šè®ºæ–‡é›†ã€‹ï¼Œç¬¬
    141â€“160 é¡µã€‚SIAMï¼Œ2020ã€‚
- en: ALH [21] Navid Azizan, Sahin Lale, and Babak Hassibi. Stochastic mirror descent
    on overparameterized nonlinear models. IEEE Transactions on Neural Networks and
    Learning Systems, 33(12):7717â€“7727, 2021.
  id: totrans-2115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ALH [21] Navid Azizan, Sahin Lale å’Œ Babak Hassibi. é’ˆå¯¹è¿‡å‚æ•°åŒ–éçº¿æ€§æ¨¡å‹çš„éšæœºé•œé¢ä¸‹é™ã€‚å‘è¡¨äºã€ŠIEEE
    ç¥ç»ç½‘ç»œä¸å­¦ä¹ ç³»ç»Ÿæ±‡åˆŠã€‹ï¼Œ33(12):7717â€“7727ï¼Œ2021ã€‚
- en: ALS^+ [18] Alexandr Andoni, Chengyu Lin, Ying Sheng, Peilin Zhong, and Ruiqi
    Zhong. Subspace embedding and linear regression with orlicz norm. In International
    Conference on Machine Learning, pages 224â€“233\. PMLR, 2018.
  id: totrans-2116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ALS^+ [18] Alexandr Andoni, Chengyu Lin, Ying Sheng, Peilin Zhong, å’Œ Ruiqi Zhong.
    å­ç©ºé—´åµŒå…¥ä¸ Orlicz èŒƒæ•°ä¸‹çš„çº¿æ€§å›å½’ã€‚å‘è¡¨äºã€Šå›½é™…æœºå™¨å­¦ä¹ ä¼šè®®ã€‹ï¼Œç¬¬ 224â€“233 é¡µã€‚PMLRï¼Œ2018ã€‚
- en: 'ALS^+ [22] Josh Alman, Jiehao Liang, Zhao Song, Ruizhe Zhang, and Danyang Zhuo.
    Bypass exponential time preprocessing: Fast neural network training via weight-data
    correlation preprocessing. arXiv preprint arXiv:2211.14227, 2022.'
  id: totrans-2117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ALS^+ [22] Josh Alman, Jiehao Liang, Zhao Song, Ruizhe Zhang å’Œ Danyang Zhuo.
    ç»•è¿‡æŒ‡æ•°æ—¶é—´é¢„å¤„ç†ï¼šé€šè¿‡æƒé‡-æ•°æ®ç›¸å…³é¢„å¤„ç†åŠ é€Ÿç¥ç»ç½‘ç»œè®­ç»ƒã€‚arXiv é¢„å°æœ¬ arXiv:2211.14227ï¼Œ2022ã€‚
- en: AMC^+ [23] Ahmed Abdelali, Hamdy Mubarak, ShammurÂ Absar Chowdhury, Maram Hasanain,
    Basel Mousi, Sabri Boughorbel, YassineÂ El Kheir, Daniel Izham, Fahim Dalvi, Majd
    Hawasly, etÂ al. Benchmarking arabic ai with large language models. arXiv preprint
    arXiv:2305.14982, 2023.
  id: totrans-2118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AMC^+ [23] Ahmed Abdelali, Hamdy Mubarak, Shammur Absar Chowdhury, Maram Hasanain,
    Basel Mousi, Sabri Boughorbel, Yassine El Kheir, Daniel Izham, Fahim Dalvi, Majd
    Hawasly ç­‰äººã€‚ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œé˜¿æ‹‰ä¼¯è¯­ AI åŸºå‡†æµ‹è¯•ã€‚arXiv é¢„å°æœ¬ arXiv:2305.14982ï¼Œ2023ã€‚
- en: '[11] Josh Alman and Zhao Song. Fast attention requires bounded entries. arXiv
    preprint arXiv:2302.13214, 2023.'
  id: totrans-2119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Josh Alman å’Œ Zhao Song. å¿«é€Ÿæ³¨æ„åŠ›éœ€è¦æœ‰ç•Œæ¡ç›®ã€‚arXiv é¢„å°æœ¬ arXiv:2302.13214ï¼Œ2023ã€‚'
- en: '[12] Daman Arora, HimanshuÂ Gaurav Singh, etÂ al. Have llms advanced enough?
    a challenging problem solving benchmark for large language models. arXiv preprint
    arXiv:2305.15074, 2023.'
  id: totrans-2120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Daman Arora, Himanshu Gaurav Singh ç­‰äººã€‚å¤§å‹è¯­è¨€æ¨¡å‹æ˜¯å¦è¶³å¤Ÿå…ˆè¿›ï¼Ÿé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‘æˆ˜æ€§é—®é¢˜è§£å†³åŸºå‡†ã€‚arXiv
    é¢„å°æœ¬ arXiv:2305.15074ï¼Œ2023ã€‚'
- en: '[13] Ehsan Amid and ManfredÂ K Warmuth. Winnowing with gradient descent. In
    Conference on Learning Theory, pages 163â€“182\. PMLR, 2020.'
  id: totrans-2121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Ehsan Amid å’Œ Manfred K Warmuth. ä½¿ç”¨æ¢¯åº¦ä¸‹é™çš„ç­›é€‰ã€‚å‘è¡¨äºã€Šå­¦ä¹ ç†è®ºä¼šè®®ã€‹ï¼Œç¬¬ 163â€“182 é¡µã€‚PMLRï¼Œ2020ã€‚'
- en: '[14] Ehsan Amid and ManfredÂ KK Warmuth. Reparameterizing mirror descent as
    gradient descent. Advances in Neural Information Processing Systems, 33:8430â€“8439,
    2020.'
  id: totrans-2122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Ehsan Amid å’Œ Manfred KK Warmuth. å°†é•œé¢ä¸‹é™é‡æ–°å‚æ•°åŒ–ä¸ºæ¢¯åº¦ä¸‹é™ã€‚å‘è¡¨äºã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹ï¼Œ33:8430â€“8439ï¼Œ2020ã€‚'
- en: AW [21] Josh Alman and VirginiaÂ Vassilevska Williams. A refined laser method
    and faster matrix multiplication. In Proceedings of the 2021 ACM-SIAM Symposium
    on Discrete Algorithms (SODA), pages 522â€“539\. SIAM, 2021.
  id: totrans-2123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AW [21] Josh Alman å’Œ Virginia Vassilevska Williams. ç²¾ç»†åŒ–çš„æ¿€å…‰æ–¹æ³•ä¸æ›´å¿«çš„çŸ©é˜µä¹˜æ³•ã€‚å‘è¡¨äºã€Š2021
    å¹´ ACM-SIAM ç¦»æ•£ç®—æ³•ç ”è®¨ä¼šï¼ˆSODAï¼‰ã€‹è®ºæ–‡é›†ï¼Œç¬¬ 522â€“539 é¡µã€‚SIAMï¼Œ2021ã€‚
- en: '[16] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for
    deep learning via over-parameterization. In International conference on machine
    learning, pages 242â€“252\. PMLR, 2019.'
  id: totrans-2124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Zeyuan Allen-Zhu, Yuanzhi Li å’Œ Zhao Song. é€šè¿‡è¿‡å‚æ•°åŒ–çš„æ·±åº¦å­¦ä¹ æ”¶æ•›ç†è®ºã€‚å‘è¡¨äºã€Šå›½é™…æœºå™¨å­¦ä¹ ä¼šè®®ã€‹ï¼Œç¬¬
    242â€“252 é¡µã€‚PMLRï¼Œ2019ã€‚'
- en: '[17] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of
    training recurrent neural networks. Advances in neural information processing
    systems, 32, 2019.'
  id: totrans-2125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Zeyuan Allen-Zhu, Yuanzhi Li å’Œ Zhao Song. å¾ªç¯ç¥ç»ç½‘ç»œè®­ç»ƒçš„æ”¶æ•›é€Ÿåº¦ã€‚å‘è¡¨äºã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹ï¼Œ32ï¼Œ2019ã€‚'
- en: BCB [14] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine
    translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473,
    2014.
  id: totrans-2126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BCB [14] Dzmitry Bahdanau, Kyunghyun Cho å’Œ Yoshua Bengio. é€šè¿‡å…±åŒå­¦ä¹ å¯¹é½å’Œç¿»è¯‘è¿›è¡Œç¥ç»æœºå™¨ç¿»è¯‘ã€‚arXiv
    é¢„å°æœ¬ arXiv:1409.0473ï¼Œ2014ã€‚
- en: 'BCE^+ [23] SÃ©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke,
    Eric Horvitz, Ece Kamar, Peter Lee, YinÂ Tat Lee, Yuanzhi Li, Scott Lundberg, etÂ al.
    Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv
    preprint arXiv:2303.12712, 2023.'
  id: totrans-2127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BCE^+ [23] SÃ©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke,
    Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundbergï¼Œç­‰ã€‚äººå·¥é€šç”¨æ™ºèƒ½çš„ç«èŠ±ï¼šå…³äºgpt-4çš„æ—©æœŸå®éªŒã€‚arXiv
    é¢„å°æœ¬ arXiv:2303.12712ï¼Œ2023å¹´ã€‚
- en: BCL^+ [23] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su,
    Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, etÂ al. A multitask,
    multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and
    interactivity. arXiv preprint arXiv:2302.04023, 2023.
  id: totrans-2128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BCL^+ [23] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su,
    Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chungï¼Œç­‰ã€‚å¯¹ChatGPTåœ¨æ¨ç†ã€å¹»è§‰å’Œäº’åŠ¨æ€§çš„å¤šä»»åŠ¡ã€å¤šè¯­è¨€ã€å¤šæ¨¡æ€è¯„ä¼°ã€‚arXiv
    é¢„å°æœ¬ arXiv:2302.04023ï¼Œ2023å¹´ã€‚
- en: BCS [97] Peter BÃ¼rgisser, Michael Clausen, and MohammadÂ A Shokrollahi. Algebraic
    complexity theory, volume 315. Springer Science & Business Media, 1997.
  id: totrans-2129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BCS [97] Peter BÃ¼rgisser, Michael Clausen, å’Œ Mohammad A Shokrollahiã€‚ä»£æ•°å¤æ‚æ€§ç†è®ºï¼Œç¬¬315å·ã€‚Springer
    Science & Business Mediaï¼Œ1997å¹´ã€‚
- en: BGVV [20] Guy Blanc, Neha Gupta, Gregory Valiant, and Paul Valiant. Implicit
    regularization for deep neural networks driven by an ornstein-uhlenbeck like process.
    In Conference on learning theory, pages 483â€“513\. PMLR, 2020.
  id: totrans-2130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BGVV [20] Guy Blanc, Neha Gupta, Gregory Valiant, å’Œ Paul Valiantã€‚ç”±å¥¥æ©æ–¯å¦-ä¹Œä¼¦è´å…‹è¿‡ç¨‹é©±åŠ¨çš„æ·±åº¦ç¥ç»ç½‘ç»œçš„éšå¼æ­£åˆ™åŒ–ã€‚è½½äºå­¦ä¹ ç†è®ºä¼šè®®è®ºæ–‡é›†ï¼Œç¬¬483â€“513é¡µã€‚PMLRï¼Œ2020å¹´ã€‚
- en: 'BHS^+ [23] Ning Bian, Xianpei Han, LeÂ Sun, Hongyu Lin, Yaojie Lu, and Ben He.
    Chatgpt is a knowledgeable but inexperienced solver: An investigation of commonsense
    problem in large language models. arXiv preprint arXiv:2303.16421, 2023.'
  id: totrans-2131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BHS^+ [23] Ning Bian, Xianpei Han, Le Sun, Hongyu Lin, Yaojie Lu, å’Œ Ben Heã€‚Chatgptæ˜¯ä¸€ä¸ªçŸ¥è¯†ä¸°å¯Œä½†ç»éªŒä¸è¶³çš„æ±‚è§£è€…ï¼šå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¸¸è¯†é—®é¢˜çš„è°ƒæŸ¥ã€‚arXiv
    é¢„å°æœ¬ arXiv:2303.16421ï¼Œ2023å¹´ã€‚
- en: BMR^+ [20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, JaredÂ D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    etÂ al. Language models are few-shot learners. Advances in neural information processing
    systems, 33:1877â€“1901, 2020.
  id: totrans-2132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BMR^+ [20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askellï¼Œç­‰ã€‚è¯­è¨€æ¨¡å‹æ˜¯å°‘æ ·æœ¬å­¦ä¹ è€…ã€‚ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ï¼Œ33:1877â€“1901ï¼Œ2020å¹´ã€‚
- en: BPSW [20] Jan vanÂ den Brand, Binghui Peng, Zhao Song, and Omri Weinstein. Training
    (overparametrized) neural networks in near-linear time. arXiv preprint arXiv:2006.11648,
    2020.
  id: totrans-2133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BPSW [20] Jan van den Brand, Binghui Peng, Zhao Song, å’Œ Omri Weinsteinã€‚åœ¨è¿‘çº¿æ€§æ—¶é—´å†…è®­ç»ƒï¼ˆè¿‡å‚æ•°åŒ–çš„ï¼‰ç¥ç»ç½‘ç»œã€‚arXiv
    é¢„å°æœ¬ arXiv:2006.11648ï¼Œ2020å¹´ã€‚
- en: Bra [20] Jan vanÂ den Brand. A deterministic linear program solver in current
    matrix multiplication time. In Proceedings of the Fourteenth Annual ACM-SIAM Symposium
    on Discrete Algorithms (SODA), pages 259â€“278\. SIAM, 2020.
  id: totrans-2134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bra [20] Jan van den Brandã€‚åœ¨å½“å‰çŸ©é˜µä¹˜æ³•æ—¶é—´å†…çš„ç¡®å®šæ€§çº¿æ€§ç¨‹åºæ±‚è§£å™¨ã€‚è½½äºç¬¬åå››å±ŠACM-SIAMç¦»æ•£ç®—æ³•å¹´ä¼šè®ºæ–‡é›†ï¼ˆSODAï¼‰ï¼Œç¬¬259â€“278é¡µã€‚SIAMï¼Œ2020å¹´ã€‚
- en: BS [23] Jan denÂ van Brand and Zhao Song. A $\sqrt{n}$ passes streaming algorithm
    for solving bipartite matching exactly. Manuscript, 2023.
  id: totrans-2135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BS [23] Jan den van Brand å’Œ Zhao Songã€‚ä¸€ä¸ª$\sqrt{n}$æ¬¡æµå¼ç®—æ³•ç”¨äºç²¾ç¡®æ±‚è§£äºŒåˆ†åŒ¹é…ã€‚æ‰‹ç¨¿ï¼Œ2023å¹´ã€‚
- en: BSY [23] Song Bian, Zhao Song, and Junze Yin. Federated empirical risk minimization
    via second-order method. arXiv preprint arXiv:2305.17482, 2023.
  id: totrans-2136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BSY [23] Song Bian, Zhao Song, å’Œ Junze Yinã€‚é€šè¿‡äºŒé˜¶æ–¹æ³•è¿›è¡Œè”é‚¦ç»éªŒé£é™©æœ€å°åŒ–ã€‚arXiv é¢„å°æœ¬ arXiv:2305.17482ï¼Œ2023å¹´ã€‚
- en: BSZ [23] Jan vanÂ den Brand, Zhao Song, and Tianyi Zhou. Algorithm and hardness
    for dynamic attention maintenance in large language models. arXiv preprint arXiv:2304.02207,
    2023.
  id: totrans-2137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BSZ [23] Jan van den Brand, Zhao Song, å’Œ Tianyi Zhouã€‚å¤§å‹è¯­è¨€æ¨¡å‹ä¸­åŠ¨æ€æ³¨æ„åŠ›ç»´æŠ¤çš„ç®—æ³•ä¸éš¾åº¦ã€‚arXiv
    é¢„å°æœ¬ arXiv:2304.02207ï¼Œ2023å¹´ã€‚
- en: BW [14] Christos Boutsidis and DavidÂ P Woodruff. Optimal cur matrix decompositions.
    In Proceedings of the forty-sixth annual ACM symposium on Theory of computing
    (STOC), pages 353â€“362, 2014.
  id: totrans-2138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BW [14] Christos Boutsidis å’Œ David P Woodruffã€‚æœ€ä½³CURçŸ©é˜µåˆ†è§£ã€‚è½½äºç¬¬46å±ŠACMè®¡ç®—ç†è®ºå¹´ä¼šè®ºæ–‡é›†ï¼ˆSTOCï¼‰ï¼Œç¬¬353â€“362é¡µï¼Œ2014å¹´ã€‚
- en: BWZ [16] Christos Boutsidis, DavidÂ P Woodruff, and Peilin Zhong. Optimal principal
    component analysis in distributed and streaming models. In Proceedings of the
    forty-eighth annual ACM symposium on Theory of Computing, pages 236â€“249, 2016.
  id: totrans-2139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BWZ [16] Christos Boutsidis, David P Woodruff, å’Œ Peilin Zhongã€‚åœ¨åˆ†å¸ƒå¼å’Œæµæ¨¡å‹ä¸­è¿›è¡Œæœ€ä½³ä¸»æˆåˆ†åˆ†æã€‚è½½äºç¬¬48å±ŠACMè®¡ç®—ç†è®ºå¹´ä¼šè®ºæ–‡é›†ï¼Œç¬¬236â€“249é¡µï¼Œ2016å¹´ã€‚
- en: BYKS [22] Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering
    latent knowledge in language models without supervision. arXiv preprint arXiv:2212.03827,
    2022.
  id: totrans-2140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BYKS [22] Collin Burns, Haotian Ye, Dan Klein, å’Œ Jacob Steinhardtã€‚æ— ç›‘ç£ä¸‹å‘ç°è¯­è¨€æ¨¡å‹ä¸­çš„æ½œåœ¨çŸ¥è¯†ã€‚arXiv
    é¢„å°æœ¬ arXiv:2212.03827ï¼Œ2022å¹´ã€‚
- en: 'CGH^+ [19] Tianle Cai, Ruiqi Gao, Jikai Hou, Siyu Chen, Dong Wang, DiÂ He, Zhihua
    Zhang, and Liwei Wang. Gram-gauss-newton method: Learning overparameterized neural
    networks for regression problems. arXiv preprint arXiv:1905.11675, 2019.'
  id: totrans-2141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CGH^+ [19] Tianle Cai, Ruiqi Gao, Jikai Hou, Siyu Chen, Dong Wang, Di He, Zhihua
    Zhang å’Œ Liwei Wang. Gram-Gauss-Newton æ–¹æ³•ï¼šå­¦ä¹ è¿‡å‚æ•°åŒ–ç¥ç»ç½‘ç»œä»¥è§£å†³å›å½’é—®é¢˜ã€‚arXiv é¢„å°æœ¬ arXiv:1905.11675,
    2019ã€‚
- en: CGLZ [20] Matthias Christandl, FranÃ§oisÂ Le Gall, Vladimir Lysikov, and Jeroen
    Zuiddam. Barriers for rectangular matrix multiplication. In arXiv preprint, 2020.
  id: totrans-2142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CGLZ [20] Matthias Christandl, FranÃ§ois Le Gall, Vladimir Lysikov å’Œ Jeroen Zuiddam.
    çŸ©å½¢çŸ©é˜µä¹˜æ³•çš„éšœç¢ã€‚å‘è¡¨äº arXiv é¢„å°æœ¬ï¼Œ2020ã€‚
- en: Cha [22] ChatGPT. Optimizing language models for dialogue. OpenAI Blog, November
    2022.
  id: totrans-2143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cha [22] ChatGPT. ä¼˜åŒ–å¯¹è¯çš„è¯­è¨€æ¨¡å‹ã€‚OpenAI åšå®¢ï¼Œ2022å¹´11æœˆã€‚
- en: 'CHBP [23] YewÂ Ken Chia, Pengfei Hong, Lidong Bing, and Soujanya Poria. Instructeval:
    Towards holistic evaluation of instruction-tuned large language models. arXiv
    preprint arXiv:2306.04757, 2023.'
  id: totrans-2144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CHBP [23] Yew Ken Chia, Pengfei Hong, Lidong Bing å’Œ Soujanya Poria. Instructevalï¼šæœç€æ•´ä½“è¯„ä¼°æŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿ˆè¿›ã€‚arXiv
    é¢„å°æœ¬ arXiv:2306.04757, 2023ã€‚
- en: 'CL [01] Chih-Chung Chang and Chih-Jen Lin. Training v-support vector classifiers:
    theory and algorithms. Neural computation, 13(9):2119â€“2147, 2001.'
  id: totrans-2145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CL [01] Chih-Chung Chang å’Œ Chih-Jen Lin. è®­ç»ƒ v-æ”¯æŒå‘é‡åˆ†ç±»å™¨ï¼šç†è®ºä¸ç®—æ³•ã€‚ç¥ç»è®¡ç®—ï¼Œ13(9):2119â€“2147,
    2001ã€‚
- en: 'CLBBJ [23] Joseph Chervenak, Harry Lieman, Miranda Blanco-Breindel, and Sangita
    Jindal. The promise and peril of using a large language model to obtain clinical
    information: Chatgpt performs strongly as a fertility counseling tool with limitations.
    Fertility and Sterility, 2023.'
  id: totrans-2146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CLBBJ [23] Joseph Chervenak, Harry Lieman, Miranda Blanco-Breindel å’Œ Sangita
    Jindal. ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è·å–ä¸´åºŠä¿¡æ¯çš„æ‰¿è¯ºä¸é£é™©ï¼šChatGPT ä½œä¸ºç”Ÿè‚²å’¨è¯¢å·¥å…·è¡¨ç°å‡ºè‰²ä½†æœ‰é™ã€‚ç”Ÿè‚²ä¸ä¸å­•æœŸåˆŠï¼Œ2023ã€‚
- en: CLMY [21] HanQin Cai, Yuchen Lou, Daniel McKenzie, and Wotao Yin. A zeroth-order
    block coordinate descent algorithm for huge-scale black-box optimization. In International
    Conference on Machine Learning, pages 1193â€“1203\. PMLR, 2021.
  id: totrans-2147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CLMY [21] HanQin Cai, Yuchen Lou, Daniel McKenzie å’Œ Wotao Yin. ç”¨äºå¤§è§„æ¨¡é»‘ç®±ä¼˜åŒ–çš„é›¶é˜¶å—åæ ‡ä¸‹é™ç®—æ³•ã€‚å‘è¡¨äºå›½é™…æœºå™¨å­¦ä¹ ä¼šè®®ï¼Œé¡µé¢
    1193â€“1203\. PMLR, 2021ã€‚
- en: 'CLP^+ [21] Beidi Chen, Zichang Liu, Binghui Peng, Zhaozhuo Xu, JonathanÂ Lingjie
    Li, Tri Dao, Zhao Song, Anshumali Shrivastava, and Christopher Re. Mongoose: A
    learnable lsh framework for efficient neural network training. In International
    Conference on Learning Representations, 2021.'
  id: totrans-2148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CLP^+ [21] Beidi Chen, Zichang Liu, Binghui Peng, Zhaozhuo Xu, Jonathan Lingjie
    Li, Tri Dao, Zhao Song, Anshumali Shrivastava å’Œ Christopher Re. Mongooseï¼šä¸€ä¸ªå¯å­¦ä¹ çš„
    LSH æ¡†æ¶ï¼Œç”¨äºé«˜æ•ˆçš„ç¥ç»ç½‘ç»œè®­ç»ƒã€‚å‘è¡¨äºå›½é™…å­¦ä¹ è¡¨å¾ä¼šè®®ï¼Œ2021ã€‚
- en: CLS [19] MichaelÂ B Cohen, YinÂ Tat Lee, and Zhao Song. Solving linear programs
    in the current matrix multiplication time. In STOC, 2019.
  id: totrans-2149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CLS [19] Michael B Cohen, Yin Tat Lee å’Œ Zhao Song. åœ¨å½“å‰çŸ©é˜µä¹˜æ³•æ—¶é—´å†…æ±‚è§£çº¿æ€§ç¨‹åºã€‚å‘è¡¨äº STOC,
    2019ã€‚
- en: CNP [23] CayqueÂ Monteiro CastroÂ Nascimento and AndrÃ©Â Silva Pimentel. Do large
    language models understand chemistry? a conversation with chatgpt. Journal of
    Chemical Information and Modeling, 63(6):1649â€“1655, 2023.
  id: totrans-2150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNP [23] Cayque Monteiro Castro Nascimento å’Œ AndrÃ© Silva Pimentel. å¤§å‹è¯­è¨€æ¨¡å‹æ˜¯å¦ç†è§£åŒ–å­¦ï¼Ÿä¸
    ChatGPT çš„å¯¹è¯ã€‚åŒ–å­¦ä¿¡æ¯ä¸å»ºæ¨¡æœŸåˆŠï¼Œ63(6):1649â€“1655, 2023ã€‚
- en: Coh [16] MichaelÂ B Cohen. Nearly tight oblivious subspace embeddings by trace
    inequalities. In Proceedings of the twenty-seventh annual ACM-SIAM symposium on
    Discrete algorithms, pages 278â€“287\. SIAM, 2016.
  id: totrans-2151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Coh [16] Michael B Cohen. é€šè¿‡è¿¹ä¸ç­‰å¼è·å¾—å‡ ä¹ç´§å‡‘çš„ç›²ç›®å­ç©ºé—´åµŒå…¥ã€‚å‘è¡¨äºç¬¬äºŒåä¸ƒå±Š ACM-SIAM ç¦»æ•£ç®—æ³•å¹´ä¼šï¼Œé¡µé¢ 278â€“287\.
    SIAM, 2016ã€‚
- en: Cop [82] Don Coppersmith. Rapid multiplication of rectangular matrices. SIAM
    Journal on Computing, 11(3):467â€“471, 1982.
  id: totrans-2152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cop [82] Don Coppersmith. çŸ©å½¢çŸ©é˜µçš„å¿«é€Ÿä¹˜æ³•ã€‚SIAM è®¡ç®—æœŸåˆŠï¼Œ11(3):467â€“471, 1982ã€‚
- en: CPK^+ [23] Minje Choi, Jiaxin Pei, Sagar Kumar, Chang Shu, and David Jurgens.
    Do llms understand social knowledge? evaluating the sociability of large language
    models with socket benchmark. arXiv preprint arXiv:2305.14938, 2023.
  id: totrans-2153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPK^+ [23] Minje Choi, Jiaxin Pei, Sagar Kumar, Chang Shu å’Œ David Jurgens. å¤§å‹è¯­è¨€æ¨¡å‹æ˜¯å¦ç†è§£ç¤¾ä¼šçŸ¥è¯†ï¼Ÿé€šè¿‡
    Socket åŸºå‡†è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„ç¤¾äº¤èƒ½åŠ›ã€‚arXiv é¢„å°æœ¬ arXiv:2305.14938, 2023ã€‚
- en: CW [13] KennethÂ L Clarkson and DavidÂ P Woodruff. Low-rank approximation and
    regression in input sparsity time. In STOC, 2013.
  id: totrans-2154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CW [13] Kenneth L Clarkson å’Œ David P Woodruff. è¾“å…¥ç¨€ç–æ—¶é—´ä¸­çš„ä½ç§©è¿‘ä¼¼ä¸å›å½’ã€‚å‘è¡¨äº STOC, 2013ã€‚
- en: 'CWJ^+ [23] YiÂ Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and Ruifeng Xu. Exploring
    the use of large language models for reference-free text quality evaluation: A
    preliminary empirical study. arXiv preprint arXiv:2304.00723, 2023.'
  id: totrans-2155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CWJ^+ [23] Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi å’Œ Ruifeng Xu. æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ— å‚è€ƒæ–‡æœ¬è´¨é‡è¯„ä¼°ä¸­çš„åº”ç”¨ï¼šåˆæ­¥å®è¯ç ”ç©¶ã€‚arXiv
    é¢„å°æœ¬ arXiv:2304.00723, 2023ã€‚
- en: 'CZL^+ [23] Yong Cao, LiÂ Zhou, Seolhwa Lee, Laura Cabello, Min Chen, and Daniel
    Hershcovich. Assessing cross-cultural alignment between chatgpt and human societies:
    An empirical study. arXiv preprint arXiv:2303.17466, 2023.'
  id: totrans-2156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CZL^+ [23] æ›¹å‹‡ã€å‘¨åŠ›ã€æç‘¶åã€åŠ³æ‹‰Â·å¡è´æ´›ã€é™ˆæ•å’Œä¸¹å°¼å°”Â·èµ«ä»€ç§‘ç»´å¥‡ã€‚è¯„ä¼° ChatGPT å’Œäººç±»ç¤¾ä¼šä¹‹é—´çš„è·¨æ–‡åŒ–å¯¹é½ï¼šä¸€é¡¹å®è¯ç ”ç©¶ã€‚arXiv
    é¢„å°æœ¬ arXiv:2303.17466ï¼Œ2023å¹´ã€‚
- en: 'DCLT [18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
    Bert: Pre-training of deep bidirectional transformers for language understanding.
    arXiv preprint arXiv:1810.04805, 2018.'
  id: totrans-2157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DCLT [18] é›…å„å¸ƒÂ·å¾·å¤«æ—ã€å¼ æ•ä¼Ÿã€æè‚¯é¡¿å’Œå…‹é‡Œæ–¯è’‚å¨œÂ·æ‰˜å¡”è¯ºå¨ƒã€‚BERTï¼šç”¨äºè¯­è¨€ç†è§£çš„æ·±åº¦åŒå‘å˜æ¢å™¨çš„é¢„è®­ç»ƒã€‚arXiv é¢„å°æœ¬ arXiv:1810.04805ï¼Œ2018å¹´ã€‚
- en: DDH^+ [21] Damai Dai, LiÂ Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu
    Wei. Knowledge neurons in pretrained transformers. arXiv preprint arXiv:2104.08696,
    2021.
  id: totrans-2158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DDH^+ [21] æˆ´è¾¾è¿ˆã€è‘£ä¸½ã€éƒé›…å¦‚ã€éš‹å¿—èŠ³ã€å¸¸å®å®å’Œé­ä¼å¦‚ã€‚é¢„è®­ç»ƒå˜æ¢å™¨ä¸­çš„çŸ¥è¯†ç¥ç»å…ƒã€‚arXiv é¢„å°æœ¬ arXiv:2104.08696ï¼Œ2021å¹´ã€‚
- en: DGG [23] Aniket Deroy, Kripabandhu Ghosh, and Saptarshi Ghosh. How ready are
    pre-trained abstractive models and llms for legal case judgement summarization?
    arXiv preprint arXiv:2306.01248, 2023.
  id: totrans-2159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DGG [23] é˜¿å°¼å‡¯ç‰¹Â·å¾·ç½—ä¼Šã€å…‹é‡Œå¸•ç­å¾·èƒ¡Â·æˆˆä»€å’Œè¨æ™®å¡”å°”å¸ŒÂ·æˆˆä»€ã€‚é¢„è®­ç»ƒæŠ½è±¡æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ³•å¾‹æ¡ˆä»¶åˆ¤å†³æ€»ç»“ä¸­çš„å‡†å¤‡ç¨‹åº¦å¦‚ä½•ï¼ŸarXiv é¢„å°æœ¬
    arXiv:2306.01248ï¼Œ2023å¹´ã€‚
- en: DJS^+ [19] Huaian Diao, Rajesh Jayaram, Zhao Song, Wen Sun, and David Woodruff.
    Optimal sketching for kronecker product regression and low rank approximation.
    Advances in neural information processing systems, 32, 2019.
  id: totrans-2160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DJS^+ [19] è¿ªæ˜‚Â·åã€æ‹‰æ°ä»€Â·è´¾äºšæ‹‰å§†ã€å®‹é’Šã€å­™é›¯å’Œå¤§å«Â·ä¼å¾·é²å¤«ã€‚å…‹ç½—å†…å…‹ä¹˜ç§¯å›å½’å’Œä½ç§©é€¼è¿‘çš„æœ€ä¼˜è‰å›¾ã€‚ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ï¼Œ32ï¼Œ2019å¹´ã€‚
- en: 'DL [23] Xuan-Quy Dao and Ngoc-Bich Le. Investigating the effectiveness of chatgpt
    in mathematical reasoning and problem solving: Evidence from the vietnamese national
    high school graduation examination. arXiv preprint arXiv:2306.06331, 2023.'
  id: totrans-2161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DL [23] é˜®ä¹…å¥‡å’Œé»ç‰ç¢§ã€‚ç ”ç©¶ ChatGPT åœ¨æ•°å­¦æ¨ç†å’Œé—®é¢˜è§£å†³ä¸­çš„æœ‰æ•ˆæ€§ï¼šæ¥è‡ªè¶Šå—å›½å®¶é«˜ä¸­æ¯•ä¸šè€ƒè¯•çš„è¯æ®ã€‚arXiv é¢„å°æœ¬ arXiv:2306.06331ï¼Œ2023å¹´ã€‚
- en: DLMS [23] Yichuan Deng, Zhihang Li, Sridhar Mahadevan, and Zhao Song. Zero-th
    order algorithm for softmax attention optimization. arXiv preprint arXiv:2307.08352,
    2023.
  id: totrans-2162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DLMS [23] é‚“ä¸€å·ã€ææ™ºèˆªã€æ–¯é‡Œè¾¾Â·é©¬å“ˆå¾·ä¸‡å’Œå®‹é’Šã€‚ç”¨äºè½¯æœ€å¤§æ³¨æ„åŠ›ä¼˜åŒ–çš„é›¶é˜¶ç®—æ³•ã€‚arXiv é¢„å°æœ¬ arXiv:2307.08352ï¼Œ2023å¹´ã€‚
- en: DLS [23] Yichuan Deng, Zhihang Li, and Zhao Song. Attention scheme inspired
    softmax regression. arXiv preprint arXiv:2304.10411, 2023.
  id: totrans-2163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DLS [23] é‚“ä¸€å·ã€ææ™ºèˆªå’Œå®‹é’Šã€‚å—å¯å‘çš„æ³¨æ„åŠ›æœºåˆ¶è½¯æœ€å¤§å›å½’ã€‚arXiv é¢„å°æœ¬ arXiv:2304.10411ï¼Œ2023å¹´ã€‚
- en: DML [21] Alex Damian, Tengyu Ma, and JasonÂ D Lee. Label noise sgd provably prefers
    flat global minimizers. Advances in Neural Information Processing Systems, 34:27449â€“27461,
    2021.
  id: totrans-2164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DML [21] äºšå†å…‹æ–¯Â·è¾¾ç±³å®‰ã€æ»•æ˜±é©¬å’Œæ°æ£®Â·DÂ·æã€‚æ ‡ç­¾å™ªå£° SGD å¯è¯æ˜åå¥½å¹³å¦çš„å…¨å±€æœ€å°å€¼ã€‚ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ï¼Œ34:27449â€“27461ï¼Œ2021å¹´ã€‚
- en: DMS [23] Yichuan Deng, Sridhar Mahadevan, and Zhao Song. Randomized and deterministic
    attention sparsification algorithms for over-parameterized feature dimension.
    arXiv preprint arXiv:2304.04397, 2023.
  id: totrans-2165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DMS [23] é‚“ä¸€å·ã€æ–¯é‡Œè¾¾Â·é©¬å“ˆå¾·ä¸‡å’Œå®‹é’Šã€‚ç”¨äºè¿‡å‚æ•°ç‰¹å¾ç»´åº¦çš„éšæœºå’Œç¡®å®šæ€§æ³¨æ„åŠ›ç¨€ç–åŒ–ç®—æ³•ã€‚arXiv é¢„å°æœ¬ arXiv:2304.04397ï¼Œ2023å¹´ã€‚
- en: DSSW [18] Huaian Diao, Zhao Song, Wen Sun, and David Woodruff. Sketching for
    kronecker product regression and p-splines. In International Conference on Artificial
    Intelligence and Statistics, pages 1299â€“1308\. PMLR, 2018.
  id: totrans-2166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DSSW [18] è¿ªæ˜‚Â·åã€å®‹é’Šã€å­™é›¯å’Œå¤§å«Â·ä¼å¾·é²å¤«ã€‚ç”¨äºå…‹ç½—å†…å…‹ä¹˜ç§¯å›å½’å’Œ p-splines çš„è‰å›¾ã€‚åœ¨å›½é™…äººå·¥æ™ºèƒ½ä¸ç»Ÿè®¡ä¼šè®®ä¸Šï¼Œ1299â€“1308é¡µã€‚PMLRï¼Œ2018å¹´ã€‚
- en: DSWY [22] Yichuan Deng, Zhao Song, Yitan Wang, and Yuanyuan Yang. A nearly optimal
    size coreset algorithm with nearly linear time. arXiv preprint arXiv:2210.08361,
    2022.
  id: totrans-2167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DSWY [22] é‚“ä¸€å·ã€å®‹é’Šã€ç‹æ€¡å¦å’Œæ¨åª›åª›ã€‚å‡ ä¹æœ€ä¼˜å¤§å°çš„æ ¸å¿ƒé›†ç®—æ³•ï¼Œæ—¶é—´å¤æ‚åº¦æ¥è¿‘çº¿æ€§ã€‚arXiv é¢„å°æœ¬ arXiv:2210.08361ï¼Œ2022å¹´ã€‚
- en: DSY [23] Yichuan Deng, Zhao Song, and Junze Yin. Faster robust tensor power
    method for arbitrary order. arXiv preprint arXiv:2306.00406, 2023.
  id: totrans-2168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DSY [23] é‚“ä¸€å·ã€å®‹é’Šå’Œå°¹å†›æ³½ã€‚ç”¨äºä»»æ„é˜¶çš„æ›´å¿«é²æ£’å¼ é‡å¹‚æ–¹æ³•ã€‚arXiv é¢„å°æœ¬ arXiv:2306.00406ï¼Œ2023å¹´ã€‚
- en: DSZZ [23] Yichuan Deng, Zhao Song, Lichen Zhang, and Ruizhe Zhang. Efficient
    algorithm for solving hyperbolic programs. arXiv preprint arXiv:2306.07587, 2023.
  id: totrans-2169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DSZZ [23] é‚“ä¸€å·ã€å®‹é’Šã€å¼ ç«‹è‡£å’Œå¼ ç‘å“²ã€‚æ±‚è§£åŒæ›²ç¨‹åºçš„é«˜æ•ˆç®—æ³•ã€‚arXiv é¢„å°æœ¬ arXiv:2306.07587ï¼Œ2023å¹´ã€‚
- en: DWZ [23] Ran Duan, Hongxun Wu, and Renfei Zhou. Faster matrix multiplication
    via asymmetric hashing. In FOCS, 2023.
  id: totrans-2170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DWZ [23] æ®µå†‰ã€å´å®å‹‹å’Œå‘¨ä»»é£ã€‚é€šè¿‡éå¯¹ç§°å“ˆå¸ŒåŠ é€ŸçŸ©é˜µä¹˜æ³•ã€‚åœ¨ FOCSï¼Œ2023å¹´ã€‚
- en: DZPS [18] SimonÂ S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient
    descent provably optimizes over-parameterized neural networks. arXiv preprint
    arXiv:1810.02054, 2018.
  id: totrans-2171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DZPS [18] è¥¿è’™Â·SÂ·æœã€ç¿Ÿç†™å®‡ã€å·´çº³å·´æ–¯Â·æ³¢ç§‘æ–¯å’Œé˜¿å°”è’‚Â·è¾›æ ¼ã€‚æ¢¯åº¦ä¸‹é™å¯è¯æ˜ä¼˜åŒ–è¿‡å‚æ•°ç¥ç»ç½‘ç»œã€‚arXiv é¢„å°æœ¬ arXiv:1810.02054ï¼Œ2018å¹´ã€‚
- en: EMZ [21] Hossein Esfandiari, Vahab Mirrokni, and Peilin Zhong. Almost linear
    time density level set estimation via dbscan. In Proceedings of the AAAI Conference
    on Artificial Intelligence, volumeÂ 35, pages 7349â€“7357, 2021.
  id: totrans-2172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EMZ [21] Hossein Esfandiari, Vahab Mirrokni, å’Œ Peilin Zhong. é€šè¿‡ DBSCAN è¿›è¡Œè¿‘ä¼¼çº¿æ€§æ—¶é—´å¯†åº¦æ°´å¹³é›†ä¼°è®¡.
    è§äº AAAI äººå·¥æ™ºèƒ½ä¼šè®®è®ºæ–‡é›†, ç¬¬ 35 å·, é¡µç  7349â€“7357, 2021.
- en: Fer [23] Emilio Ferrara. Should chatgpt be biased? challenges and risks of bias
    in large language models. arXiv preprint arXiv:2304.03738, 2023.
  id: totrans-2173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fer [23] Emilio Ferrara. ChatGPT æ˜¯å¦åº”æœ‰åè§ï¼Ÿå¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„åè§æŒ‘æˆ˜ä¸é£é™©. arXiv é¢„å°æœ¬ arXiv:2304.03738,
    2023.
- en: Fra [23] MichaelÂ C Frank. Baby steps in evaluating the capacities of large language
    models. Nature Reviews Psychology, pages 1â€“2, 2023.
  id: totrans-2174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fra [23] Michael C Frank. è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹èƒ½åŠ›çš„åˆæ­¥æ­¥éª¤. ã€Šè‡ªç„¶è¯„è®ºå¿ƒç†å­¦ã€‹, é¡µç  1â€“2, 2023.
- en: GGL^+ [23] Taicheng Guo, Kehan Guo, Zhengwen Liang, Zhichun Guo, NiteshÂ V Chawla,
    Olaf Wiest, Xiangliang Zhang, etÂ al. What indeed can gpt models do in chemistry?
    a comprehensive benchmark on eight tasks. arXiv preprint arXiv:2305.18365, 2023.
  id: totrans-2175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GGL^+ [23] Taicheng Guo, Kehan Guo, Zhengwen Liang, Zhichun Guo, Nitesh V Chawla,
    Olaf Wiest, Xiangliang Zhang, ç­‰. GPT æ¨¡å‹åœ¨åŒ–å­¦ä¸­ç©¶ç«Ÿèƒ½åšä»€ä¹ˆï¼Ÿå…«é¡¹ä»»åŠ¡çš„ç»¼åˆåŸºå‡†æµ‹è¯•. arXiv é¢„å°æœ¬ arXiv:2305.18365,
    2023.
- en: GLSS [18] Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing
    implicit bias in terms of optimization geometry. In International Conference on
    Machine Learning, pages 1832â€“1841\. PMLR, 2018.
  id: totrans-2176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GLSS [18] Suriya Gunasekar, Jason Lee, Daniel Soudry, å’Œ Nathan Srebro. ä»ä¼˜åŒ–å‡ ä½•çš„è§’åº¦æè¿°éšå¼åè§.
    è§äºå›½é™…æœºå™¨å­¦ä¹ ä¼šè®®, é¡µç  1832â€“1841. PMLR, 2018.
- en: GMS [23] Yeqi Gao, Sridhar Mahadevan, and Zhao Song. An over-parameterized exponential
    regression. arXiv preprint arXiv:2303.16504, 2023.
  id: totrans-2177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GMS [23] Yeqi Gao, Sridhar Mahadevan, å’Œ Zhao Song. ä¸€ç§è¿‡å‚æ•°åŒ–çš„æŒ‡æ•°å›å½’. arXiv é¢„å°æœ¬ arXiv:2303.16504,
    2023.
- en: GQSW [22] Yeqi Gao, Lianke Qin, Zhao Song, and Yitan Wang. A sublinear adversarial
    training algorithm. arXiv preprint arXiv:2208.05395, 2022.
  id: totrans-2178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GQSW [22] Yeqi Gao, Lianke Qin, Zhao Song, å’Œ Yitan Wang. ä¸€ç§æ¬¡çº¿æ€§çš„å¯¹æŠ—è®­ç»ƒç®—æ³•. arXiv
    é¢„å°æœ¬ arXiv:2208.05395, 2022.
- en: GS [22] Yuzhou Gu and Zhao Song. A faster small treewidth sdp solver. arXiv
    preprint arXiv:2211.06033, 2022.
  id: totrans-2179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GS [22] Yuzhou Gu å’Œ Zhao Song. æ›´å¿«çš„å°æ ‘å®½ SDP æ±‚è§£å™¨. arXiv é¢„å°æœ¬ arXiv:2211.06033, 2022.
- en: 'GSX [23] Yeqi Gao, Zhao Song, and Shenghao Xie. In-context learning for attention
    scheme: from single softmax regression to multiple softmax regression via a tensor
    trick. arXiv preprint arXiv:2307.02419, 2023.'
  id: totrans-2180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GSX [23] Yeqi Gao, Zhao Song, å’Œ Shenghao Xie. æ³¨æ„åŠ›æœºåˆ¶çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼šä»å•ä¸€ softmax å›å½’åˆ°é€šè¿‡å¼ é‡æŠ€å·§çš„å¤šé‡
    softmax å›å½’. arXiv é¢„å°æœ¬ arXiv:2307.02419, 2023.
- en: '[73] Yeqi Gao, Zhao Song, and Junze Yin. Gradientcoin: A peer-to-peer decentralized
    large language models. arXiv preprint arXiv:2308.10502, 2023.'
  id: totrans-2181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Yeqi Gao, Zhao Song, å’Œ Junze Yin. Gradientcoin: ä¸€ä¸ªç‚¹å¯¹ç‚¹å»ä¸­å¿ƒåŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹. arXiv
    é¢„å°æœ¬ arXiv:2308.10502, 2023.'
- en: '[74] Yeqi Gao, Zhao Song, and Junze Yin. An iterative algorithm for rescaled
    hyperbolic functions regression. arXiv preprint arXiv:2305.00660, 2023.'
  id: totrans-2182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Yeqi Gao, Zhao Song, å’Œ Junze Yin. ä¸€ç§ç”¨äºç¼©æ”¾åŒæ›²å‡½æ•°å›å½’çš„è¿­ä»£ç®—æ³•. arXiv é¢„å°æœ¬ arXiv:2305.00660,
    2023.'
- en: '[75] Yeqi Gao, Zhao Song, Xin Yang, and Ruizhe Zhang. Fast quantum algorithm
    for attention computation. arXiv preprint arXiv:2307.08045, 2023.'
  id: totrans-2183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Yeqi Gao, Zhao Song, Xin Yang, å’Œ Ruizhe Zhang. å¿«é€Ÿé‡å­ç®—æ³•ç”¨äºæ³¨æ„åŠ›è®¡ç®—. arXiv é¢„å°æœ¬
    arXiv:2307.08045, 2023.'
- en: '[76] Yuzhou Gu, Zhao Song, Junze Yin, and Lichen Zhang. Low rank matrix completion
    via robust alternating minimization in nearly linear time. arXiv preprint arXiv:2302.11068,
    2023.'
  id: totrans-2184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Yuzhou Gu, Zhao Song, Junze Yin, å’Œ Lichen Zhang. é€šè¿‡å¼ºå¥çš„äº¤æ›¿æœ€å°åŒ–åœ¨è¿‘çº¿æ€§æ—¶é—´å†…å®Œæˆä½ç§©çŸ©é˜µ.
    arXiv é¢„å°æœ¬ arXiv:2302.11068, 2023.'
- en: GSZ [23] Yuzhou Gu, Zhao Song, and Lichen Zhang. A nearly-linear time algorithm
    for structured support vector machines. arXiv preprint arXiv:2307.07735, 2023.
  id: totrans-2185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GSZ [23] Yuzhou Gu, Zhao Song, å’Œ Lichen Zhang. ä¸€ç§ç”¨äºç»“æ„æ”¯æŒå‘é‡æœºçš„è¿‘çº¿æ€§æ—¶é—´ç®—æ³•. arXiv é¢„å°æœ¬
    arXiv:2307.07735, 2023.
- en: GU [18] FranÃ§oisÂ Le Gall and Florent Urrutia. Improved rectangular matrix multiplication
    using powers of the coppersmith-winograd tensor. In Proceedings of the Twenty-Ninth
    Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), 2018.
  id: totrans-2186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GU [18] FranÃ§ois Le Gall å’Œ Florent Urrutia. ä½¿ç”¨ Coppersmith-Winograd å¼ é‡çš„å¹‚æ”¹è¿›çš„çŸ©å½¢çŸ©é˜µä¹˜æ³•.
    è§äºç¬¬äºŒåä¹å±Šå¹´åº¦ ACM-SIAM ç¦»æ•£ç®—æ³•ç ”è®¨ä¼š (SODA) è®ºæ–‡é›†, 2018.
- en: HBKG [23] Peter Hase, Mohit Bansal, Been Kim, and Asma Ghandeharioun. Does localization
    inform editing? surprising differences in causality-based localization vs. knowledge
    editing in language models. arXiv preprint arXiv:2301.04213, 2023.
  id: totrans-2187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HBKG [23] Peter Hase, Mohit Bansal, Been Kim, å’Œ Asma Ghandeharioun. æœ¬åœ°åŒ–æ˜¯å¦æœ‰åŠ©äºç¼–è¾‘ï¼Ÿåœ¨è¯­è¨€æ¨¡å‹ä¸­åŸºäºå› æœå…³ç³»çš„æœ¬åœ°åŒ–ä¸çŸ¥è¯†ç¼–è¾‘ä¹‹é—´çš„æƒŠäººå·®å¼‚.
    arXiv é¢„å°æœ¬ arXiv:2301.04213, 2023.
- en: HF [23] Thilo Hagendorff and Sarah Fabi. Human-like intuitive behavior and reasoning
    biases emerged in language modelsâ€“and disappeared in gpt-4. arXiv preprint arXiv:2306.07622,
    2023.
  id: totrans-2188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HF [23] Thilo Hagendorff å’Œ Sarah Fabi. äººç±»èˆ¬çš„ç›´è§‚è¡Œä¸ºå’Œæ¨ç†åå·®åœ¨è¯­è¨€æ¨¡å‹ä¸­å‡ºç°ï¼Œå¹¶åœ¨ GPT-4 ä¸­æ¶ˆå¤±. arXiv
    é¢„å°æœ¬ arXiv:2306.07622, 2023.
- en: 'HJS^+ [22] Baihe Huang, Shunhua Jiang, Zhao Song, Runzhou Tao, and Ruizhe Zhang.
    Solving sdp faster: A robust ipm framework and efficient implementation. In 2022
    IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS), pages 233â€“244\.
    IEEE, 2022.'
  id: totrans-2189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HJS^+ [22] ç™½é¹¤Â·é»„ï¼Œè’‹é¡ºåï¼Œèµµæ¾ï¼Œæ¶¦æ´²Â·é™¶ï¼Œå’Œç‘å“²Â·å¼ ã€‚ã€Šæ›´å¿«åœ°è§£å†³SDPï¼šä¸€ä¸ªå¼ºå¥çš„IPMæ¡†æ¶å’Œé«˜æ•ˆå®ç°ã€‹ã€‚åœ¨2022å¹´IEEEç¬¬63å±Šè®¡ç®—æœºç§‘å­¦åŸºç¡€å¹´ä¼šï¼ˆFOCSï¼‰ï¼Œé¡µç 233â€“244ã€‚IEEEï¼Œ2022å¹´ã€‚
- en: 'HLSY [21] Baihe Huang, Xiaoxiao Li, Zhao Song, and Xin Yang. Fl-ntk: A neural
    tangent kernel-based framework for federated learning analysis. In International
    Conference on Machine Learning, pages 4423â€“4434\. PMLR, 2021.'
  id: totrans-2190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HLSY [21] ç™½é¹¤Â·é»„ï¼Œè‚–è‚–Â·æï¼Œèµµæ¾ï¼Œå’Œè¾›Â·æ¨ã€‚ã€ŠFL-NTKï¼šä¸€ä¸ªåŸºäºç¥ç»åˆ‡çº¿æ ¸çš„è”é‚¦å­¦ä¹ åˆ†ææ¡†æ¶ã€‹ã€‚åœ¨å›½é™…æœºå™¨å­¦ä¹ ä¼šè®®è®ºæ–‡é›†ï¼Œé¡µç 4423â€“4434ã€‚PMLRï¼Œ2021å¹´ã€‚
- en: HLZ [23] Sophie Huiberts, YinÂ Tat Lee, and Xinzhi Zhang. Upper and lower bounds
    on the smoothed complexity of the simplex method. In Proceedings of the 55th Annual
    ACM Symposium on Theory of Computing, pages 1904â€“1917, 2023.
  id: totrans-2191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HLZ [23] ç´¢è²Â·éœä¼Šä¼¯èŒ¨ï¼Œå°¹Â·å¡”ç‰¹Â·æï¼Œå’Œè¾›æ™ºÂ·å¼ ã€‚ã€Šå•çº¯å½¢æ³•å¹³æ»‘å¤æ‚åº¦çš„ä¸Šä¸‹ç•Œã€‹ã€‚åœ¨ç¬¬55å±ŠACMç†è®ºè®¡ç®—ç ”è®¨ä¼šè®ºæ–‡é›†ï¼Œé¡µç 1904â€“1917ï¼Œ2023å¹´ã€‚
- en: 'HM [19] John Hewitt and ChristopherÂ D Manning. A structural probe for finding
    syntax in word representations. In Proceedings of the 2019 Conference of the North
    American Chapter of the Association for Computational Linguistics: Human Language
    Technologies, Volume 1 (Long and Short Papers), pages 4129â€“4138, 2019.'
  id: totrans-2192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HM [19] çº¦ç¿°Â·ä¼‘ä¼Šç‰¹å’Œå…‹é‡Œæ–¯æ‰˜å¼—Â·DÂ·æ›¼å®ã€‚ã€Šå¯»æ‰¾è¯è¡¨ç¤ºä¸­çš„å¥æ³•ç»“æ„çš„ç»“æ„æ¢æµ‹å™¨ã€‹ã€‚åœ¨2019å¹´åŒ—ç¾è®¡ç®—è¯­è¨€å­¦åä¼šäººç±»è¯­è¨€æŠ€æœ¯ä¼šè®®è®ºæ–‡é›†ç¬¬1å·ï¼ˆé•¿ç¯‡å’ŒçŸ­ç¯‡è®ºæ–‡ï¼‰ï¼Œé¡µç 4129â€“4138ï¼Œ2019å¹´ã€‚
- en: 'HWLM [21] JeffÂ Z HaoChen, Colin Wei, Jason Lee, and Tengyu Ma. Shape matters:
    Understanding the implicit bias of the noise covariance. In Conference on Learning
    Theory, pages 2315â€“2357\. PMLR, 2021.'
  id: totrans-2193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HWLM [21] æ°å¤«Â·ZÂ·éƒæ™¨ï¼Œç§‘æ—Â·éŸ¦ï¼Œæ°æ£®Â·æï¼Œå’Œæ»•å®‡Â·é©¬ã€‚ã€Šå½¢çŠ¶å¾ˆé‡è¦ï¼šç†è§£å™ªå£°åæ–¹å·®çš„éšå«åå·®ã€‹ã€‚åœ¨å­¦ä¹ ç†è®ºä¼šè®®ï¼Œé¡µç 2315â€“2357ã€‚PMLRï¼Œ2021å¹´ã€‚
- en: JDST [20] Ziwei Ji, Miroslav Dudik, RobertÂ E Schapire, and Matus Telgarsky.
    Gradient descent follows the regularization path for general losses. In Conference
    on Learning Theory, pages 2109â€“2136\. PMLR, 2020.
  id: totrans-2194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JDST [20] çºªå­ä¼Ÿï¼Œç±³ç½—æ–¯æ‹‰å¤«Â·æœè¿ªå…‹ï¼Œç½—ä¼¯ç‰¹Â·EÂ·æ²™çš®å°”ï¼Œå’Œé©¬å›¾æ–¯Â·ç‰¹å°”åŠ å°”æ–¯åŸºã€‚ã€Šæ¢¯åº¦ä¸‹é™éµå¾ªä¸€èˆ¬æŸå¤±çš„æ­£åˆ™åŒ–è·¯å¾„ã€‹ã€‚åœ¨å­¦ä¹ ç†è®ºä¼šè®®ï¼Œé¡µç 2109â€“2136ã€‚PMLRï¼Œ2020å¹´ã€‚
- en: 'JGP^+ [23] Douglas Johnson, Rachel Goodman, JÂ Patrinely, Cosby Stone, Eli Zimmerman,
    Rebecca Donald, Sam Chang, Sean Berkowitz, Avni Finn, Eiman Jahangir, etÂ al. Assessing
    the accuracy and reliability of ai-generated medical responses: an evaluation
    of the chat-gpt model. ., 2023.'
  id: totrans-2195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JGP^+ [23] é“æ ¼æ‹‰æ–¯Â·çº¦ç¿°é€Šï¼Œç‘ç§‹Â·å¤å¾·æ›¼ï¼ŒJÂ·å¸•ç‰¹é‡Œå†…åˆ©ï¼Œç§‘æ–¯æ¯”Â·æ–¯é€šï¼ŒåŸƒåˆ©Â·é½é»˜æ›¼ï¼Œä¸½è´å¡Â·å”çº³å¾·ï¼Œè¨å§†Â·å¼ ï¼Œè‚–æ©Â·ä¼¯ç§‘ç»´èŒ¨ï¼Œé˜¿å¤«å°¼Â·èŠ¬ï¼Œè‰¾æ›¼Â·è´¾æ±‰å‰å°”ç­‰ã€‚ã€Šè¯„ä¼°AIç”Ÿæˆçš„åŒ»ç–—å›åº”çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼šå¯¹Chat-GPTæ¨¡å‹çš„è¯„ä¼°ã€‹ã€‚2023å¹´ã€‚
- en: JKL^+ [20] Haotian Jiang, Tarun Kathuria, YinÂ Tat Lee, Swati Padmanabhan, and
    Zhao Song. A faster interior point method for semidefinite programming. In 2020
    IEEE 61st annual symposium on foundations of computer science (FOCS), pages 910â€“918\.
    IEEE, 2020.
  id: totrans-2196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JKL^+ [20] ä¹”æµ©å¤©ï¼Œå¡”ä¼¦Â·å¡å›¾é‡Œäºšï¼Œå°¹Â·å¡”ç‰¹Â·æï¼Œæ–¯ç“¦è’‚Â·å¸•å¾·é©¬çº³ç­ï¼Œå’Œèµµæ¾ã€‚ã€Šä¸€ç§æ›´å¿«çš„åŠæ­£å®šè§„åˆ’å†…éƒ¨ç‚¹æ³•ã€‹ã€‚åœ¨2020å¹´IEEEç¬¬61å±Šè®¡ç®—æœºç§‘å­¦åŸºç¡€å¹´ä¼šï¼ˆFOCSï¼‰ï¼Œé¡µç 910â€“918ã€‚IEEEï¼Œ2020å¹´ã€‚
- en: JL [84] WilliamÂ B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings
    into a hilbert space. Contemporary mathematics, 26(189-206):1, 1984.
  id: totrans-2197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JL [84] å¨å»‰Â·BÂ·çº¦ç¿°é€Šå’Œçº¦æ‹‰å§†Â·æ—ç™»æ–¯ç‰¹åŠ³æ–¯ã€‚ã€Šæ˜ å°„åˆ°å¸Œå°”ä¼¯ç‰¹ç©ºé—´çš„åˆ©æ™®å¸ŒèŒ¨æ˜ å°„çš„æ‰©å±•ã€‹ã€‚ç°ä»£æ•°å­¦ï¼Œ26(189-206):1ï¼Œ1984å¹´ã€‚
- en: JLSW [20] Haotian Jiang, YinÂ Tat Lee, Zhao Song, and Sam Chiu-wai Wong. An improved
    cutting plane method for convex optimization, convex-concave games, and its applications.
    In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing,
    pages 944â€“953, 2020.
  id: totrans-2198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JLSW [20] ä¹”æµ©å¤©ï¼Œå°¹Â·å¡”ç‰¹Â·æï¼Œèµµæ¾ï¼Œå’Œè¨å§†Â·é‚±ä¼ŸÂ·é»„ã€‚ã€Šç”¨äºå‡¸ä¼˜åŒ–ã€å‡¸å‡¹æ¸¸æˆåŠå…¶åº”ç”¨çš„æ”¹è¿›åˆ‡å‰²å¹³é¢æ³•ã€‹ã€‚åœ¨ç¬¬52å±ŠACM SIGACTç†è®ºè®¡ç®—ç ”è®¨ä¼šè®ºæ–‡é›†ï¼Œé¡µç 944â€“953ï¼Œ2020å¹´ã€‚
- en: Joa [06] Thorsten Joachims. Training linear svms in linear time. In Proceedings
    of the 12th ACM SIGKDD international conference on Knowledge discovery and data
    mining, pages 217â€“226, 2006.
  id: totrans-2199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Joa [06] æ‰˜å°”æ–¯æ»•Â·ä¹”é˜¿å¸Œå§†æ–¯ã€‚ã€Šåœ¨çº¿æ€§æ—¶é—´å†…è®­ç»ƒçº¿æ€§æ”¯æŒå‘é‡æœºã€‹ã€‚åœ¨ç¬¬12å±ŠACM SIGKDDå›½é™…çŸ¥è¯†å‘ç°ä¸æ•°æ®æŒ–æ˜ä¼šè®®è®ºæ–‡é›†ï¼Œé¡µç 217â€“226ï¼Œ2006å¹´ã€‚
- en: JST [21] Ziwei Ji, Nathan Srebro, and Matus Telgarsky. Fast margin maximization
    via dual acceleration. In International Conference on Machine Learning, pages
    4860â€“4869\. PMLR, 2021.
  id: totrans-2200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JST [21] çºªå­ä¼Ÿï¼Œå†…æ£®Â·æ–¯é›·å¸ƒç½—ï¼Œå’Œé©¬å›¾æ–¯Â·ç‰¹å°”åŠ å°”æ–¯åŸºã€‚ã€Šé€šè¿‡åŒé‡åŠ é€Ÿå®ç°å¿«é€Ÿè¾¹é™…æœ€å¤§åŒ–ã€‹ã€‚åœ¨å›½é™…æœºå™¨å­¦ä¹ ä¼šè®®è®ºæ–‡é›†ï¼Œé¡µç 4860â€“4869ã€‚PMLRï¼Œ2021å¹´ã€‚
- en: JSWZ [21] Shunhua Jiang, Zhao Song, Omri Weinstein, and Hengjie Zhang. A faster
    algorithm for solving general lps. In Proceedings of the 53rd Annual ACM SIGACT
    Symposium on Theory of Computing, pages 823â€“832, 2021.
  id: totrans-2201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JSWZ [21] è’‹é¡ºåï¼Œèµµæ¾ï¼Œå¥¥å§†é‡ŒÂ·æ¸©æ–¯å¦ï¼Œå’Œæ’æ°Â·å¼ ã€‚ã€Šè§£å†³ä¸€èˆ¬LPSçš„æ›´å¿«ç®—æ³•ã€‹ã€‚åœ¨ç¬¬53å±ŠACM SIGACTç†è®ºè®¡ç®—ç ”è®¨ä¼šè®ºæ–‡é›†ï¼Œé¡µç 823â€“832ï¼Œ2021å¹´ã€‚
- en: JT [18] Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic
    regression. arXiv preprint arXiv:1803.07300, 2018.
  id: totrans-2202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JT [18] Ziwei Ji å’Œ Matus Telgarsky. é€»è¾‘å›å½’çš„é£é™©å’Œå‚æ•°æ”¶æ•›ã€‚arXiv é¢„å°æœ¬ arXiv:1803.07300ï¼Œ2018å¹´ã€‚
- en: '[95] Ziwei Ji and Matus Telgarsky. The implicit bias of gradient descent on
    nonseparable data. In Conference on Learning Theory, pages 1772â€“1798\. PMLR, 2019.'
  id: totrans-2203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Ziwei Ji å’Œ Matus Telgarsky. æ¢¯åº¦ä¸‹é™åœ¨ä¸å¯åˆ†æ•°æ®ä¸Šçš„éšå«åå·®ã€‚æ”¶å½•äºå­¦ä¹ ç†è®ºä¼šè®®ï¼Œç¬¬1772â€“1798é¡µï¼ŒPMLRï¼Œ2019å¹´ã€‚'
- en: '[96] Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient
    descent to achieve arbitrarily small test error with shallow relu networks. arXiv
    preprint arXiv:1909.12292, 2019.'
  id: totrans-2204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Ziwei Ji å’Œ Matus Telgarsky. å¤šå¯¹æ•°å®½åº¦è¶³ä»¥ä½¿æ¢¯åº¦ä¸‹é™åœ¨æµ…å±‚ relu ç½‘ç»œä¸­å®ç°ä»»æ„å°çš„æµ‹è¯•è¯¯å·®ã€‚arXiv é¢„å°æœ¬
    arXiv:1909.12292ï¼Œ2019å¹´ã€‚'
- en: JT [20] Ziwei Ji and Matus Telgarsky. Directional convergence and alignment
    in deep learning. Advances in Neural Information Processing Systems, 33:17176â€“17186,
    2020.
  id: totrans-2205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JT [20] Ziwei Ji å’Œ Matus Telgarsky. æ·±åº¦å­¦ä¹ ä¸­çš„æ–¹å‘æ€§æ”¶æ•›ä¸å¯¹é½ã€‚æ”¶å½•äºã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹ï¼Œ33:17176â€“17186ï¼Œ2020å¹´ã€‚
- en: JT [21] Ziwei Ji and Matus Telgarsky. Characterizing the implicit bias via a
    primal-dual analysis. In Algorithmic Learning Theory, pages 772â€“804\. PMLR, 2021.
  id: totrans-2206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JT [21] Ziwei Ji å’Œ Matus Telgarsky. é€šè¿‡åŸå§‹-å¯¹å¶åˆ†æåˆ»ç”»éšå«åå·®ã€‚æ”¶å½•äºã€Šç®—æ³•å­¦ä¹ ç†è®ºã€‹ï¼Œç¬¬772â€“804é¡µï¼ŒPMLRï¼Œ2021å¹´ã€‚
- en: 'KKL [20] Nikita Kitaev, Åukasz Kaiser, and Anselm Levskaya. Reformer: The efficient
    transformer. arXiv preprint arXiv:2001.04451, 2020.'
  id: totrans-2207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KKL [20] Nikita Kitaev, Åukasz Kaiser, å’Œ Anselm Levskaya. Reformerï¼šé«˜æ•ˆçš„ transformerã€‚arXiv
    é¢„å°æœ¬ arXiv:2001.04451ï¼Œ2020å¹´ã€‚
- en: KMH^+ [20] Jared Kaplan, Sam McCandlish, Tom Henighan, TomÂ B Brown, Benjamin
    Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling
    laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.
  id: totrans-2208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KMH^+ [20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin
    Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, å’Œ Dario Amodei. ç¥ç»è¯­è¨€æ¨¡å‹çš„è§„æ¨¡å®šå¾‹ã€‚arXiv
    é¢„å°æœ¬ arXiv:2001.08361ï¼Œ2020å¹´ã€‚
- en: KPOT [21] GaneshÂ Ramachandra Kini, Orestis Paraskevas, Samet Oymak, and Christos
    Thrampoulidis. Label-imbalanced and group-sensitive classification under overparameterization.
    Advances in Neural Information Processing Systems, 34:18970â€“18983, 2021.
  id: totrans-2209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KPOT [21] Ganesh Ramachandra Kini, Orestis Paraskevas, Samet Oymak, å’Œ Christos
    Thrampoulidis. åœ¨è¿‡å‚æ•°åŒ–ä¸‹çš„æ ‡ç­¾ä¸å¹³è¡¡å’Œç¾¤ä½“æ•æ„Ÿåˆ†ç±»ã€‚æ”¶å½•äºã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹ï¼Œ34:18970â€“18983ï¼Œ2021å¹´ã€‚
- en: LBL^+ [22] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara
    Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,
    etÂ al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110,
    2022.
  id: totrans-2210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LBL^+ [22] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara
    Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar
    ç­‰äººã€‚è¯­è¨€æ¨¡å‹çš„æ•´ä½“è¯„ä¼°ã€‚arXiv é¢„å°æœ¬ arXiv:2211.09110ï¼Œ2022å¹´ã€‚
- en: LBR^+ [23] MdÂ TahmidÂ Rahman Laskar, MÂ Saiful Bari, Mizanur Rahman, MdÂ AmranÂ Hossen
    Bhuiyan, Shafiq Joty, and JimmyÂ Xiangji Huang. A systematic study and comprehensive
    evaluation of chatgpt on benchmark datasets. arXiv preprint arXiv:2305.18486,
    2023.
  id: totrans-2211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LBR^+ [23] Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran
    Hossen Bhuiyan, Shafiq Joty, å’Œ Jimmy Xiangji Huang. å¯¹ ChatGPT åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„ç³»ç»Ÿç ”ç©¶å’Œç»¼åˆè¯„ä¼°ã€‚arXiv
    é¢„å°æœ¬ arXiv:2305.18486ï¼Œ2023å¹´ã€‚
- en: LDFU [13] Yichao Lu, Paramveer Dhillon, DeanÂ P Foster, and Lyle Ungar. Faster
    ridge regression via the subsampled randomized hadamard transform. Advances in
    neural information processing systems, 26, 2013.
  id: totrans-2212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LDFU [13] Yichao Lu, Paramveer Dhillon, Dean P Foster, å’Œ Lyle Ungar. é€šè¿‡å­æ ·æœ¬éšæœº
    Hadamard å˜æ¢åŠ é€Ÿå²­å›å½’ã€‚æ”¶å½•äºã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹ï¼Œ26ï¼Œ2013å¹´ã€‚
- en: LG [14] FranÃ§ois LeÂ Gall. Powers of tensors and fast matrix multiplication.
    In Proceedings of the 39th international symposium on symbolic and algebraic computation,
    pages 296â€“303, 2014.
  id: totrans-2213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LG [14] FranÃ§ois Le Gall. å¼ é‡çš„å¹‚ä¸å¿«é€ŸçŸ©é˜µä¹˜æ³•ã€‚æ”¶å½•äºç¬¬39å±Šå›½é™…ç¬¦å·ä¸ä»£æ•°è®¡ç®—ç ”è®¨ä¼šè®ºæ–‡é›†ï¼Œç¬¬296â€“303é¡µï¼Œ2014å¹´ã€‚
- en: LG [23] FranÃ§ois LeÂ Gall. Faster rectangular matrix multiplication by combination
    loss analysis. arXiv preprint arXiv:2307.06535, 2023.
  id: totrans-2214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LG [23] FranÃ§ois Le Gall. é€šè¿‡ç»„åˆæŸå¤±åˆ†æåŠ é€ŸçŸ©å½¢çŸ©é˜µä¹˜æ³•ã€‚arXiv é¢„å°æœ¬ arXiv:2307.06535ï¼Œ2023å¹´ã€‚
- en: LL [18] Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks
    via stochastic gradient descent on structured data. Advances in neural information
    processing systems, 31, 2018.
  id: totrans-2215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LL [18] Yuanzhi Li å’Œ Yingyu Liang. é€šè¿‡å¯¹ç»“æ„åŒ–æ•°æ®è¿›è¡Œéšæœºæ¢¯åº¦ä¸‹é™æ¥å­¦ä¹ è¿‡å‚æ•°åŒ–çš„ç¥ç»ç½‘ç»œã€‚æ”¶å½•äºã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹ï¼Œ31ï¼Œ2018å¹´ã€‚
- en: 'LL [21] XiangÂ Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous
    prompts for generation. arXiv preprint arXiv:2101.00190, 2021.'
  id: totrans-2216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LL [21] Xiang Lisa Li å’Œ Percy Liang. å‰ç¼€è°ƒä¼˜ï¼šä¼˜åŒ–ç”Ÿæˆçš„è¿ç»­æç¤ºã€‚arXiv é¢„å°æœ¬ arXiv:2101.00190ï¼Œ2021å¹´ã€‚
- en: LLGB [23] Xinzhe Li, Ming Liu, Shang Gao, and Wray Buntine. A survey on out-of-distribution
    evaluation of neural nlp models. arXiv preprint arXiv:2306.15261, 2023.
  id: totrans-2217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLGB [23] Xinzhe Li, Ming Liu, Shang Gao, å’Œ Wray Buntine. ç¥ç» NLP æ¨¡å‹çš„åˆ†å¸ƒå¤–è¯„ä¼°è°ƒæŸ¥ã€‚arXiv
    é¢„å°æœ¬ arXiv:2306.15261ï¼Œ2023å¹´ã€‚
- en: 'LLH^+ [23] Hong Liu, Zhiyuan Li, David Hall, Percy Liang, and Tengyu Ma. Sophia:
    A scalable stochastic second-order optimizer for language model pre-training.
    arXiv preprint arXiv:2305.14342, 2023.'
  id: totrans-2218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLH^+ [23] æ´ªÂ·åˆ˜ï¼Œå¿—è¿œÂ·æï¼Œå¤§å«Â·éœå°”ï¼Œä½©å°”è¥¿Â·æ¢ï¼Œå’Œæ»•å®‡Â·é©¬ã€‚Sophiaï¼šä¸€ç§å¯æ‰©å±•çš„éšæœºäºŒé˜¶ä¼˜åŒ–å™¨ï¼Œç”¨äºè¯­è¨€æ¨¡å‹é¢„è®­ç»ƒã€‚arXivé¢„å°æœ¬
    arXiv:2305.14342, 2023ã€‚
- en: 'LLR [23] Yuchen Li, Yuanzhi Li, and Andrej Risteski. How do transformers learn
    topic structure: Towards a mechanistic understanding. arXiv preprint arXiv:2303.04245,
    2023.'
  id: totrans-2219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLR [23] éƒè¾°Â·æï¼Œè¢æ™ºÂ·æï¼Œå’Œå®‰å¾·çƒˆÂ·é‡Œæ–¯ç‰¹æ–¯åŸºã€‚å˜å‹å™¨å¦‚ä½•å­¦ä¹ ä¸»é¢˜ç»“æ„ï¼šè¿ˆå‘æœºåˆ¶ç†è§£ã€‚arXivé¢„å°æœ¬ arXiv:2303.04245,
    2023ã€‚
- en: 'LNV^+ [23] VietÂ Dac Lai, NghiaÂ Trung Ngo, Amir PouranÂ Ben Veyseh, Hieu Man,
    Franck Dernoncourt, Trung Bui, and ThienÂ Huu Nguyen. Chatgpt beyond english: Towards
    a comprehensive evaluation of large language models in multilingual learning.
    arXiv preprint arXiv:2304.05613, 2023.'
  id: totrans-2220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LNV^+ [23] è¶Šå¾·Â·èµ–ï¼Œä¹‰ä¸­Â·å´ï¼Œé˜¿ç±³å°”Â·æ™®æœ—Â·æœ¬Â·ç»´èµ›ï¼Œå¸Œæ¬§Â·æ›¼ï¼Œå¼—æœ—å…‹Â·å¾·å†œåº“ç‰¹ï¼Œä¸­Â·å¸ƒï¼Œå’Œå¤©Â·å´Â·é˜®ã€‚Chatgptè¶…è¶Šè‹±è¯­ï¼šè¿ˆå‘å¯¹å¤šè¯­è¨€å­¦ä¹ çš„å¤§è¯­è¨€æ¨¡å‹çš„ç»¼åˆè¯„ä¼°ã€‚arXivé¢„å°æœ¬
    arXiv:2304.05613, 2023ã€‚
- en: LPM [15] Minh-Thang Luong, Hieu Pham, and ChristopherÂ D Manning. Effective approaches
    to attention-based neural machine translation. arXiv preprint arXiv:1508.04025,
    2015.
  id: totrans-2221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LPM [15] æ˜å¡˜Â·è‰¯ï¼Œå¸Œæ¬§Â·èŒƒï¼Œå’Œå…‹é‡Œæ–¯æ‰˜å¼—Â·DÂ·æ›¼å®ã€‚åŸºäºæ³¨æ„åŠ›çš„ç¥ç»æœºå™¨ç¿»è¯‘çš„æœ‰æ•ˆæ–¹æ³•ã€‚arXivé¢„å°æœ¬ arXiv:1508.04025,
    2015ã€‚
- en: 'LR [20] Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel â€œridgelessâ€
    regression can generalize. ., 2020.'
  id: totrans-2222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LR [20] æ»•å…ƒÂ·æ¢å’Œäºšå†å±±å¤§Â·æ‹‰èµ«æ—ã€‚ä»…æ’å€¼ï¼šæ ¸â€œæ— å²­â€å›å½’å¯ä»¥æ¨å¹¿ã€‚., 2020ã€‚
- en: LSS^+ [20] JasonÂ D Lee, Ruoqi Shen, Zhao Song, Mengdi Wang, etÂ al. Generalized
    leverage score sampling for neural networks. Advances in Neural Information Processing
    Systems, 33:10775â€“10787, 2020.
  id: totrans-2223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSS^+ [20] æ°æ£®Â·DÂ·æï¼Œè‹¥ç¦Â·æ²ˆï¼Œèµµå®‹ï¼Œè’™è¿ªÂ·ç‹ï¼Œç­‰ã€‚ç¥ç»ç½‘ç»œçš„å¹¿ä¹‰æ æ†å¾—åˆ†é‡‡æ ·ã€‚ã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹ï¼Œ33:10775â€“10787,
    2020ã€‚
- en: LSW [15] YinÂ Tat Lee, Aaron Sidford, and Sam Chiu-wai Wong. A faster cutting
    plane method and its implications for combinatorial and convex optimization. In
    2015 IEEE 56th Annual Symposium on Foundations of Computer Science, pages 1049â€“1065\.
    IEEE, 2015.
  id: totrans-2224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSW [15] é˜´è¾¾ç‰¹Â·æï¼Œäºšä¼¦Â·è¥¿å¾·ç¦å¾·ï¼Œå’Œå±±å§†Â·èµµä¼ŸÂ·é»„ã€‚ä¸€ç§æ›´å¿«çš„åˆ‡å‰²å¹³é¢æ–¹æ³•åŠå…¶å¯¹ç»„åˆä¼˜åŒ–å’Œå‡¸ä¼˜åŒ–çš„å½±å“ã€‚åœ¨2015å¹´IEEEç¬¬56å±Šè®¡ç®—æœºç§‘å­¦åŸºç¡€å¹´ä¼šï¼Œé¡µç 1049â€“1065\.
    IEEE, 2015ã€‚
- en: LSZ [19] YinÂ Tat Lee, Zhao Song, and Qiuyi Zhang. Solving empirical risk minimization
    in the current matrix multiplication time. In Conference on Learning Theory, pages
    2140â€“2157\. PMLR, 2019.
  id: totrans-2225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSZ [19] é˜´è¾¾ç‰¹Â·æï¼Œèµµå®‹ï¼Œå’Œç§‹æ€¡Â·å¼ ã€‚åœ¨å½“å‰çŸ©é˜µä¹˜æ³•æ—¶é—´ä¸­è§£å†³ç»éªŒé£é™©æœ€å°åŒ–ã€‚åœ¨å­¦ä¹ ç†è®ºä¼šè®®ï¼Œé¡µç 2140â€“2157\. PMLR, 2019ã€‚
- en: '[118] Zhihang Li, Zhao Song, and Tianyi Zhou. Solving regularized exp, cosh
    and sinh regression problems. arXiv preprint arXiv:2303.15725, 2023.'
  id: totrans-2226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] æ™ºèˆªÂ·æï¼Œèµµå®‹ï¼Œå’Œç”°å¥•Â·å‘¨ã€‚è§£å†³æ­£åˆ™åŒ–çš„expï¼Œcoshå’Œsinhå›å½’é—®é¢˜ã€‚arXivé¢„å°æœ¬ arXiv:2303.15725, 2023ã€‚'
- en: '[119] S.Â Cliff Liu, Zhao Song, Hengjie Zhang, Lichen Zhang, and Tianyi Zhou.
    Space-efficient interior point method, with applications to linear programming
    and maximum weight bipartite matching. In International Colloquium on Automata,
    Languages and Programming (ICALP), pages 88:1â€“88:14, 2023.'
  id: totrans-2227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] S. å…‹é‡Œå¤«Â·åˆ˜ï¼Œèµµå®‹ï¼Œæ’æ°Â·å¼ ï¼Œæè¾°Â·å¼ ï¼Œå’Œç”°å¥•Â·å‘¨ã€‚ç©ºé—´é«˜æ•ˆçš„å†…ç‚¹æ–¹æ³•åŠå…¶åœ¨çº¿æ€§è§„åˆ’å’Œæœ€å¤§æƒé‡äºŒåˆ†åŒ¹é…ä¸­çš„åº”ç”¨ã€‚åœ¨å›½é™…è‡ªåŠ¨æœºã€è¯­è¨€ä¸ç¼–ç¨‹ç ”è®¨ä¼šï¼ˆICALPï¼‰ï¼Œé¡µç 88:1â€“88:14,
    2023ã€‚'
- en: LWA [21] Zhiyuan Li, Tianhao Wang, and Sanjeev Arora. What happens after sgd
    reaches zero loss?â€“a mathematical framework. arXiv preprint arXiv:2110.06914,
    2021.
  id: totrans-2228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LWA [21] å¿—è¿œÂ·æï¼Œç”°æµ©Â·ç‹ï¼Œå’Œæ¡‘å‰å¤«Â·é˜¿ç½—æ‹‰ã€‚SGDè¾¾åˆ°é›¶æŸå¤±åä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿâ€“ä¸€ä¸ªæ•°å­¦æ¡†æ¶ã€‚arXivé¢„å°æœ¬ arXiv:2110.06914,
    2021ã€‚
- en: LWM [19] Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization
    effect of initial large learning rate in training neural networks. Advances in
    Neural Information Processing Systems, 32, 2019.
  id: totrans-2229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LWM [19] è¢æ™ºÂ·æï¼Œç§‘æ—Â·é­ï¼Œå’Œæ»•å®‡Â·é©¬ã€‚è§£é‡Šç¥ç»ç½‘ç»œè®­ç»ƒä¸­åˆå§‹å¤§å­¦ä¹ ç‡çš„æ­£åˆ™åŒ–æ•ˆåº”ã€‚ã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹ï¼Œ32ï¼Œ2019ã€‚
- en: LXWZ [23] Jiawei Liu, ChunqiuÂ Steven Xia, Yuyao Wang, and Lingming Zhang. Is
    your code generated by chatgpt really correct? rigorous evaluation of large language
    models for code generation. arXiv preprint arXiv:2305.01210, 2023.
  id: totrans-2230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LXWZ [23] åˆ˜å®¶ä¼Ÿï¼Œæ˜¥ç§‹Â·æ–¯è’‚æ–‡Â·å¤ï¼Œä½™ç‘¶Â·ç‹ï¼Œå’Œå‡Œé“­Â·å¼ ã€‚ä½ çš„ä»£ç æ˜¯ç”±chatgptç”Ÿæˆçš„ï¼ŒçœŸçš„æ­£ç¡®å—ï¼Ÿå¯¹å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆä»£ç çš„ä¸¥æ ¼è¯„ä¼°ã€‚arXivé¢„å°æœ¬
    arXiv:2305.01210, 2023ã€‚
- en: 'LYB^+ [22] Zonglin Li, Chong You, Srinadh Bhojanapalli, Daliang Li, AnkitÂ Singh
    Rawat, SashankÂ J Reddi, KeÂ Ye, Felix Chern, Felix Yu, Ruiqi Guo, etÂ al. Large
    models are parsimonious learners: Activation sparsity in trained transformers.
    arXiv preprint arXiv:2210.06313, 2022.'
  id: totrans-2231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LYB^+ [22] å®—æ—Â·æï¼Œå´‡Â·æ¸¸ï¼Œæ–¯é‡Œçº³å¾·Â·åšè´¾çº³å¸•åˆ©ï¼Œå¤§äº®Â·æï¼Œå®‰åŸºç‰¹Â·è¾›æ ¼Â·æ‹‰ç“¦ç‰¹ï¼Œè¨å°šå…‹Â·JÂ·é›·è¿ªï¼ŒæŸ¯Â·å¶ï¼Œè´¹åˆ©å…‹æ–¯Â·é™ˆï¼Œè´¹åˆ©å…‹æ–¯Â·äºï¼Œç‘å¥‡Â·éƒ­ï¼Œç­‰ã€‚å¤§æ¨¡å‹æ˜¯èŠ‚ä¿­çš„å­¦ä¹ è€…ï¼šè®­ç»ƒçš„å˜å‹å™¨ä¸­çš„æ¿€æ´»ç¨€ç–æ€§ã€‚arXivé¢„å°æœ¬
    arXiv:2210.06313, 2022ã€‚
- en: MBAB [22] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating
    and editing factual associations in gpt. Advances in Neural Information Processing
    Systems, 35:17359â€“17372, 2022.
  id: totrans-2232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MBAB [22] Kevin Mengã€David Bauã€Alex Andonian å’Œ Yonatan Belinkovã€‚å®šä½å’Œç¼–è¾‘ GPT ä¸­çš„äº‹å®å…³è”ã€‚ã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹ï¼Œ35:17359â€“17372ï¼Œ2022
    å¹´ã€‚
- en: MGN^+ [23] Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, JasonÂ D
    Lee, Danqi Chen, and Sanjeev Arora. Fine-tuning language models with just forward
    passes. arXiv preprint arXiv:2305.17333, 2023.
  id: totrans-2233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MGN^+ [23] Sadhika Malladiã€Tianyu Gaoã€Eshaan Nichaniã€Alex Damianã€Jason D Leeã€Danqi
    Chen å’Œ Sanjeev Aroraã€‚ä»…é€šè¿‡å‰å‘ä¼ é€’å¾®è°ƒè¯­è¨€æ¨¡å‹ã€‚arXiv é¢„å°æœ¬ arXiv:2305.17333ï¼Œ2023 å¹´ã€‚
- en: MM [13] Xiangrui Meng and MichaelÂ W Mahoney. Low-distortion subspace embeddings
    in input-sparsity time and applications to robust linear regression. In Proceedings
    of the forty-fifth annual ACM symposium on Theory of computing, pages 91â€“100,
    2013.
  id: totrans-2234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MM [13] Xiangrui Meng å’Œ Michael W Mahoneyã€‚åœ¨è¾“å…¥ç¨€ç–æ—¶é—´å†…ä½å¤±çœŸå­ç©ºé—´åµŒå…¥åŠå…¶åœ¨é²æ£’çº¿æ€§å›å½’ä¸­çš„åº”ç”¨ã€‚å‘è¡¨äºç¬¬
    45 å±Š ACM è®¡ç®—ç†è®ºå¹´ä¼šï¼Œç¬¬ 91â€“100 é¡µï¼Œ2013 å¹´ã€‚
- en: MOSW [22] Alexander Munteanu, Simon Omlor, Zhao Song, and David Woodruff. Bounding
    the width of neural networks via coupled initialization a worst case analysis.
    In International Conference on Machine Learning, pages 16083â€“16122\. PMLR, 2022.
  id: totrans-2235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MOSW [22] Alexander Munteanuã€Simon Omlorã€Zhao Song å’Œ David Woodruffã€‚é€šè¿‡è€¦åˆåˆå§‹åŒ–æ¥é™åˆ¶ç¥ç»ç½‘ç»œçš„å®½åº¦ï¼šæœ€åæƒ…å†µåˆ†æã€‚å‘è¡¨äºå›½é™…æœºå™¨å­¦ä¹ ä¼šè®®ï¼Œç¬¬
    16083â€“16122 é¡µã€‚PMLRï¼Œ2022 å¹´ã€‚
- en: MRS [20] Konstantin Makarychev, Aravind Reddy, and Liren Shan. Improved guarantees
    for k-means++ and k-means++ parallel. Advances in Neural Information Processing
    Systems, 33:16142â€“16152, 2020.
  id: totrans-2236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MRS [20] Konstantin Makarychevã€Aravind Reddy å’Œ Liren Shanã€‚æ”¹è¿›çš„ k-means++ å’Œ k-means++
    å¹¶è¡Œç®—æ³•çš„ä¿è¯ã€‚ã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹ï¼Œ33:16142â€“16152ï¼Œ2020 å¹´ã€‚
- en: MS [21] Linjian Ma and Edgar Solomonik. Fast and accurate randomized algorithms
    for low-rank tensor decompositions. Advances in Neural Information Processing
    Systems, 34:24299â€“24312, 2021.
  id: totrans-2237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MS [21] Linjian Ma å’Œ Edgar Solomonikã€‚ç”¨äºä½ç§©å¼ é‡åˆ†è§£çš„å¿«é€Ÿä¸”å‡†ç¡®çš„éšæœºç®—æ³•ã€‚ã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹ï¼Œ34:24299â€“24312ï¼Œ2021
    å¹´ã€‚
- en: 'MWG^+ [20] Edward Moroshko, BlakeÂ E Woodworth, Suriya Gunasekar, JasonÂ D Lee,
    Nati Srebro, and Daniel Soudry. Implicit bias in deep linear classification: Initialization
    scale vs training accuracy. Advances in neural information processing systems,
    33:22182â€“22193, 2020.'
  id: totrans-2238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MWG^+ [20] Edward Moroshkoã€Blake E Woodworthã€Suriya Gunasekarã€Jason D Leeã€Nati
    Srebro å’Œ Daniel Soudryã€‚æ·±åº¦çº¿æ€§åˆ†ç±»ä¸­çš„éšæ€§åå·®ï¼šåˆå§‹åŒ–è§„æ¨¡ä¸è®­ç»ƒå‡†ç¡®æ€§ã€‚ã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹ï¼Œ33:22182â€“22193ï¼Œ2020
    å¹´ã€‚
- en: 'NKL^+ [23] JohnÂ J Nay, David Karamardian, SarahÂ B Lawsky, Wenting Tao, Meghana
    Bhat, Raghav Jain, AaronÂ Travis Lee, JonathanÂ H Choi, and Jungo Kasai. Large language
    models as tax attorneys: A case study in legal capabilities emergence. arXiv preprint
    arXiv:2306.07075, 2023.'
  id: totrans-2239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NKL^+ [23] John J Nayã€David Karamardianã€Sarah B Lawskyã€Wenting Taoã€Meghana Bhatã€Raghav
    Jainã€Aaron Travis Leeã€Jonathan H Choi å’Œ Jungo Kasaiã€‚å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºç¨åŠ¡å¾‹å¸ˆï¼šæ³•å¾‹èƒ½åŠ›å‡ºç°çš„æ¡ˆä¾‹ç ”ç©¶ã€‚arXiv
    é¢„å°æœ¬ arXiv:2306.07075ï¼Œ2023 å¹´ã€‚
- en: NLG^+ [19] MorÂ Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro HenriqueÂ Pamplona
    Savarese, Nathan Srebro, and Daniel Soudry. Convergence of gradient descent on
    separable data. In The 22nd International Conference on Artificial Intelligence
    and Statistics, pages 3420â€“3428\. PMLR, 2019.
  id: totrans-2240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLG^+ [19] Mor Shpigel Nacsonã€Jason Leeã€Suriya Gunasekarã€Pedro Henrique Pamplona
    Savareseã€Nathan Srebro å’Œ Daniel Soudryã€‚æ¢¯åº¦ä¸‹é™åœ¨å¯åˆ†æ•°æ®ä¸Šçš„æ”¶æ•›æ€§ã€‚å‘è¡¨äºç¬¬ 22 å±Šå›½é™…äººå·¥æ™ºèƒ½ä¸ç»Ÿè®¡ä¼šè®®ï¼Œç¬¬ 3420â€“3428
    é¡µã€‚PMLRï¼Œ2019 å¹´ã€‚
- en: 'NN [13] Jelani Nelson and HuyÂ L NguyÃªn. Osnap: Faster numerical linear algebra
    algorithms via sparser subspace embeddings. In 2013 ieee 54th annual symposium
    on foundations of computer science, pages 117â€“126\. IEEE, 2013.'
  id: totrans-2241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NN [13] Jelani Nelson å’Œ Huy L NguyÃªnã€‚Osnapï¼šé€šè¿‡ç¨€ç–å­ç©ºé—´åµŒå…¥åŠ é€Ÿæ•°å€¼çº¿æ€§ä»£æ•°ç®—æ³•ã€‚å‘è¡¨äº 2013 IEEE
    ç¬¬ 54 å±Šè®¡ç®—æœºç§‘å­¦åŸºç¡€å¹´ä¼šï¼Œç¬¬ 117â€“126 é¡µã€‚IEEEï¼Œ2013 å¹´ã€‚
- en: Ope [23] OpenAI. Gpt-4 technical report, 2023.
  id: totrans-2242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ope [23] OpenAIã€‚GPT-4 æŠ€æœ¯æŠ¥å‘Šï¼Œ2023 å¹´ã€‚
- en: 'OS [20] Samet Oymak and Mahdi Soltanolkotabi. Toward moderate overparameterization:
    Global convergence guarantees for training shallow neural networks. IEEE Journal
    on Selected Areas in Information Theory, 1(1):84â€“105, 2020.'
  id: totrans-2243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OS [20] Samet Oymak å’Œ Mahdi Soltanolkotabiã€‚é€‚åº¦è¿‡å‚æ•°åŒ–çš„æ–¹å‘ï¼šæµ…å±‚ç¥ç»ç½‘ç»œè®­ç»ƒçš„å…¨å±€æ”¶æ•›ä¿è¯ã€‚ã€ŠIEEE ä¿¡æ¯ç†è®ºç²¾é€‰æœŸåˆŠã€‹ï¼Œ1(1):84â€“105ï¼Œ2020
    å¹´ã€‚
- en: Pag [13] Rasmus Pagh. Compressed matrix multiplication. ACM Transactions on
    Computation Theory (TOCT), 5(3):1â€“17, 2013.
  id: totrans-2244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pag [13] Rasmus Paghã€‚å‹ç¼©çŸ©é˜µä¹˜æ³•ã€‚ã€ŠACM è®¡ç®—ç†è®ºæœŸåˆŠã€‹ï¼ˆTOCTï¼‰ï¼Œ5(3):1â€“17ï¼Œ2013 å¹´ã€‚
- en: 'PD [23] Dongqi Pu and Vera Demberg. Chatgpt vs human-authored text: Insights
    into controllable text summarization and sentence style transfer. arXiv preprint
    arXiv:2306.07799, 2023.'
  id: totrans-2245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PD [23] Dongqi Pu å’Œ Vera Dembergã€‚ChatGPT ä¸äººå·¥æ’°å†™æ–‡æœ¬çš„æ¯”è¾ƒï¼šå¯¹å¯æ§æ–‡æœ¬æ‘˜è¦å’Œå¥å­é£æ ¼è½¬ç§»çš„è§è§£ã€‚arXiv
    é¢„å°æœ¬ arXiv:2306.07799ï¼Œ2023 å¹´ã€‚
- en: PMM^+ [23] Vishal Pallagani, Bharath Muppasani, Keerthiram Murugesan, Francesca
    Rossi, Biplav Srivastava, Lior Horesh, Francesco Fabiano, and Andrea Loreggia.
    Understanding the capabilities of large language models for automated planning.
    arXiv preprint arXiv:2305.16151, 2023.
  id: totrans-2246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PMM^+ [23] Vishal Pallagani, Bharath Muppasani, Keerthiram Murugesan, Francesca
    Rossi, Biplav Srivastava, Lior Horesh, Francesco Fabiano, å’Œ Andrea Loreggia. ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨è§„åˆ’ä¸­çš„èƒ½åŠ›ã€‚arXiv
    é¢„å°æœ¬ arXiv:2305.16151, 2023ã€‚
- en: PMXA [23] Abhishek Panigrahi, Sadhika Malladi, Mengzhou Xia, and Sanjeev Arora.
    Trainable transformer in transformer. arXiv preprint arXiv:2307.01189, 2023.
  id: totrans-2247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PMXA [23] Abhishek Panigrahi, Sadhika Malladi, Mengzhou Xia, å’Œ Sanjeev Arora.
    å¯è®­ç»ƒçš„ transformer in transformerã€‚arXiv é¢„å°æœ¬ arXiv:2307.01189, 2023ã€‚
- en: PSZA [23] Abhishek Panigrahi, Nikunj Saunshi, Haoyu Zhao, and Sanjeev Arora.
    Task-specific skill localization in fine-tuned language models. arXiv preprint
    arXiv:2302.06600, 2023.
  id: totrans-2248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PSZA [23] Abhishek Panigrahi, Nikunj Saunshi, Haoyu Zhao, å’Œ Sanjeev Arora. åœ¨å¾®è°ƒè¯­è¨€æ¨¡å‹ä¸­çš„ä»»åŠ¡ç‰¹å®šæŠ€èƒ½å®šä½ã€‚arXiv
    é¢„å°æœ¬ arXiv:2302.06600, 2023ã€‚
- en: 'QJS^+ [22] Lianke Qin, Rajesh Jayaram, Elaine Shi, Zhao Song, Danyang Zhuo,
    and Shumo Chu. Adore: Differentially oblivious relational database operators.
    In VLDB, 2022.'
  id: totrans-2249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'QJS^+ [22] Lianke Qin, Rajesh Jayaram, Elaine Shi, Zhao Song, Danyang Zhuo,
    å’Œ Shumo Chu. Adore: å·®åˆ†éšç§å…³ç³»æ•°æ®åº“æ“ä½œç¬¦ã€‚å‘è¡¨äº VLDB, 2022ã€‚'
- en: QQ [19] Qian Qian and Xiaoyuan Qian. The implicit bias of adagrad on separable
    data. Advances in Neural Information Processing Systems, 32, 2019.
  id: totrans-2250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: QQ [19] Qian Qian å’Œ Xiaoyuan Qian. Adagrad åœ¨å¯åˆ†æ•°æ®ä¸Šçš„éšå«åå·®ã€‚ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•, 32, 2019ã€‚
- en: QRS^+ [22] Lianke Qin, Aravind Reddy, Zhao Song, Zhaozhuo Xu, and Danyang Zhuo.
    Adaptive and dynamic multi-resolution hashing for pairwise summations. In BigData,
    2022.
  id: totrans-2251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: QRS^+ [22] Lianke Qin, Aravind Reddy, Zhao Song, Zhaozhuo Xu, å’Œ Danyang Zhuo.
    è‡ªé€‚åº”å’ŒåŠ¨æ€å¤šåˆ†è¾¨ç‡å“ˆå¸Œç”¨äºæˆå¯¹æ±‚å’Œã€‚å‘è¡¨äº BigData, 2022ã€‚
- en: QSW [23] Lianke Qin, Zhao Song, and Yitan Wang. Fast submodular function maximization.
    arXiv preprint arXiv:2305.08367, 2023.
  id: totrans-2252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: QSW [23] Lianke Qin, Zhao Song, å’Œ Yitan Wang. å¿«é€Ÿå­æ¨¡å‡½æ•°æœ€å¤§åŒ–ã€‚arXiv é¢„å°æœ¬ arXiv:2305.08367,
    2023ã€‚
- en: QSZ [23] Lianke Qin, Zhao Song, and Ruizhe Zhang. A general algorithm for solving
    rank-one matrix sensing. arXiv preprint arXiv:2303.12298, 2023.
  id: totrans-2253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: QSZ [23] Lianke Qin, Zhao Song, å’Œ Ruizhe Zhang. ä¸€ç§è§£å†³ç§©ä¸€çŸ©é˜µæ„ŸçŸ¥çš„é€šç”¨ç®—æ³•ã€‚arXiv é¢„å°æœ¬ arXiv:2303.12298,
    2023ã€‚
- en: QSZZ [23] Lianke Qin, Zhao Song, Lichen Zhang, and Danyang Zhuo. An online and
    unified algorithm for projection matrix vector multiplication with application
    to empirical risk minimization. In International Conference on Artificial Intelligence
    and Statistics (AISTATS), pages 101â€“156\. PMLR, 2023.
  id: totrans-2254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: QSZZ [23] Lianke Qin, Zhao Song, Lichen Zhang, å’Œ Danyang Zhuo. ä¸€ç§ç”¨äºæŠ•å½±çŸ©é˜µå‘é‡ä¹˜æ³•çš„åœ¨çº¿ç»Ÿä¸€ç®—æ³•åŠå…¶åœ¨ç»éªŒé£é™©æœ€å°åŒ–ä¸­çš„åº”ç”¨ã€‚å‘è¡¨äºå›½é™…äººå·¥æ™ºèƒ½ä¸ç»Ÿè®¡ä¼šè®®
    (AISTATS), ç¬¬ 101â€“156 é¡µ. PMLR, 2023ã€‚
- en: QZZ^+ [23] Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro
    Yasunaga, and Diyi Yang. Is chatgpt a general-purpose natural language processing
    task solver? arXiv preprint arXiv:2302.06476, 2023.
  id: totrans-2255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: QZZ^+ [23] Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro
    Yasunaga, å’Œ Diyi Yang. ChatGPT æ˜¯ä¸€ä¸ªé€šç”¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡æ±‚è§£å™¨å—ï¼ŸarXiv é¢„å°æœ¬ arXiv:2302.06476, 2023ã€‚
- en: Rat [20] Kovid Rathee. Meet google meena, 2020.
  id: totrans-2256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rat [20] Kovid Rathee. è®¤è¯† Google Meena, 2020ã€‚
- en: RNS^+ [18] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, etÂ al.
    Improving language understanding by generative pre-training. ., 2018.
  id: totrans-2257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNS^+ [18] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, ç­‰.
    é€šè¿‡ç”Ÿæˆé¢„è®­ç»ƒæé«˜è¯­è¨€ç†è§£èƒ½åŠ›ã€‚., 2018ã€‚
- en: RRS^+ [21] Aravind Reddy, RyanÂ A Rossi, Zhao Song, Anup Rao, Tung Mai, Nedim
    Lipka, Gang Wu, Eunyee Koh, and Nesreen Ahmed. Online map inference and learning
    for nonsymmetric determinantal point processes. arXiv preprint arXiv:2111.14674,
    2021.
  id: totrans-2258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RRS^+ [21] Aravind Reddy, Ryan A Rossi, Zhao Song, Anup Rao, Tung Mai, Nedim
    Lipka, Gang Wu, Eunyee Koh, å’Œ Nesreen Ahmed. éå¯¹ç§°è¡Œåˆ—å¼ç‚¹è¿‡ç¨‹çš„åœ¨çº¿æ˜ å°„æ¨æ–­å’Œå­¦ä¹ ã€‚arXiv é¢„å°æœ¬ arXiv:2111.14674,
    2021ã€‚
- en: 'RSM^+ [23] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, ChristopherÂ D
    Manning, and Chelsea Finn. Direct preference optimization: Your language model
    is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023.'
  id: totrans-2259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'RSM^+ [23] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher
    D Manning, å’Œ Chelsea Finn. ç›´æ¥åå¥½ä¼˜åŒ–: ä½ çš„è¯­è¨€æ¨¡å‹å®é™…ä¸Šæ˜¯ä¸€ä¸ªå¥–åŠ±æ¨¡å‹ã€‚arXiv é¢„å°æœ¬ arXiv:2305.18290,
    2023ã€‚'
- en: RSW [16] Ilya Razenshteyn, Zhao Song, and DavidÂ P Woodruff. Weighted low rank
    approximations with provable guarantees. In Proceedings of the forty-eighth annual
    ACM symposium on Theory of Computing, pages 250â€“263, 2016.
  id: totrans-2260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RSW [16] Ilya Razenshteyn, Zhao Song, å’Œ David P Woodruff. å…·æœ‰å¯è¯æ˜ä¿è¯çš„åŠ æƒä½ç§©è¿‘ä¼¼ã€‚å‘è¡¨äºç¬¬å››åå…«å±Šå¹´åº¦
    ACM è®¡ç®—ç†è®ºç ”è®¨ä¼š, ç¬¬ 250â€“263 é¡µ, 2016ã€‚
- en: RSZ [22] Aravind Reddy, Zhao Song, and Lichen Zhang. Dynamic tensor product
    regression. In NeurIPS, 2022.
  id: totrans-2261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RSZ [22] Aravind Reddy, Zhao Song, å’Œ Lichen Zhang. åŠ¨æ€å¼ é‡ç§¯å›å½’ã€‚å‘è¡¨äº NeurIPS, 2022ã€‚
- en: RWC^+ [19] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei,
    Ilya Sutskever, etÂ al. Language models are unsupervised multitask learners. OpenAI
    blog, 1(8):9, 2019.
  id: totrans-2262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RWC^+ [19] äºšå†å…‹Â·æ‹‰å¾·ç¦å¾·ã€æ°å¼—é‡ŒÂ·å´ã€é›·æ—ºÂ·æŸ¥å¾·ã€å¤§å«Â·é²å®‰ã€è¾¾é‡Œå¥¥Â·é˜¿è«ä»£ã€ä¼Šåˆ©äºšÂ·è‹èŒ¨å…‹ç»´å°”ç­‰ã€‚è¯­è¨€æ¨¡å‹æ˜¯æ— ç›‘ç£çš„å¤šä»»åŠ¡å­¦ä¹ è€…ã€‚OpenAI
    åšå®¢ï¼Œ1(8):9ï¼Œ2019å¹´ã€‚
- en: RYW^+ [19] Emily Reif, Ann Yuan, Martin Wattenberg, FernandaÂ B Viegas, Andy
    Coenen, Adam Pearce, and Been Kim. Visualizing and measuring the geometry of bert.
    Advances in Neural Information Processing Systems, 32, 2019.
  id: totrans-2263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RYW^+ [19] è‰¾ç±³è‰Â·ç‘å¤«ã€å®‰Â·è¢ã€é©¬ä¸Â·ç“¦æ»•ä¼¯æ ¼ã€è´¹å°”å—è¾¾Â·BÂ·ç»´æˆˆæ–¯ã€å®‰è¿ªÂ·ç§‘å—ã€äºšå½“Â·çš®å°”æ–¯å’Œå®¾Â·é‡‘ã€‚å¯è§†åŒ–å’Œæµ‹é‡ BERT çš„å‡ ä½•ç»“æ„ã€‚ã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹ï¼Œ32ï¼Œ2019å¹´ã€‚
- en: Sar [06] Tamas Sarlos. Improved approximation algorithms for large matrices
    via random projections. In 2006 47th annual IEEE symposium on foundations of computer
    science (FOCSâ€™06), pages 143â€“152\. IEEE, 2006.
  id: totrans-2264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sar [06] å¡”ç›æ–¯Â·è¨æ´›ä»€ã€‚é€šè¿‡éšæœºæŠ•å½±æ”¹è¿›å¤§çŸ©é˜µçš„è¿‘ä¼¼ç®—æ³•ã€‚å‘è¡¨äº2006å¹´ç¬¬47å±ŠIEEEè®¡ç®—æœºç§‘å­¦åŸºç¡€ç ”è®¨ä¼šï¼ˆFOCSâ€™06ï¼‰ï¼Œé¡µç  143â€“152ã€‚IEEEï¼Œ2006å¹´ã€‚
- en: SATA [22] Haoyuan Sun, Kwangjun Ahn, Christos Thrampoulidis, and Navid Azizan.
    Mirror descent maximizes generalized margin and can be implemented efficiently.
    Advances in Neural Information Processing Systems, 35:31089â€“31101, 2022.
  id: totrans-2265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SATA [22] å­™æµ©æºã€å®‰å…‰ä¿Šã€å…‹é‡Œæ–¯æ‰˜æ–¯Â·æ–¯æ‹‰å§†æ™®åˆ©è¿ªæ–¯å’Œçº³ç»´å¾·Â·é˜¿é½èµã€‚é•œåƒä¸‹é™æœ€å¤§åŒ–å¹¿ä¹‰è¾¹é™…ï¼Œå¹¶ä¸”å¯ä»¥é«˜æ•ˆå®ç°ã€‚ã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹ï¼Œ35:31089â€“31101ï¼Œ2022å¹´ã€‚
- en: SHN^+ [18] Daniel Soudry, Elad Hoffer, MorÂ Shpigel Nacson, Suriya Gunasekar,
    and Nathan Srebro. The implicit bias of gradient descent on separable data. The
    Journal of Machine Learning Research, 19:2822â€“2878, 2018.
  id: totrans-2266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SHN^+ [18] ä¸¹å°¼å°”Â·ç´¢å¾·é‡Œã€åŸƒæ‹‰å¾·Â·éœå¼—ã€æ‘©å°”Â·æ–½çš®æˆˆå°”Â·çº³å…‹æ£®ã€è‹åˆ©äºšÂ·å¤çº³å¡å¡å°”å’Œå†…æ£®Â·æ–¯é›·å¸ƒç½—ã€‚æ¢¯åº¦ä¸‹é™åœ¨å¯åˆ†æ•°æ®ä¸Šçš„éšå¼åå·®ã€‚ã€Šæœºå™¨å­¦ä¹ ç ”ç©¶æ‚å¿—ã€‹ï¼Œ19:2822â€“2878ï¼Œ2018å¹´ã€‚
- en: SHT [23] Clayton Sanford, Daniel Hsu, and Matus Telgarsky. Representational
    strengths and limitations of transformers. arXiv preprint arXiv:2306.02896, 2023.
  id: totrans-2267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SHT [23] å…‹è±é¡¿Â·æ¡‘ç¦å¾·ã€ä¸¹å°¼å°”Â·è®¸å’Œé©¬å›¾æ–¯Â·ç‰¹å°”åŠ æ–¯åŸºã€‚å˜æ¢å™¨çš„è¡¨å¾å¼ºåº¦å’Œå±€é™æ€§ã€‚arXiv é¢„å°æœ¬ arXiv:2306.02896ï¼Œ2023å¹´ã€‚
- en: 'SM^+ [23] Giriprasad Sridhara, Sourav Mazumdar, etÂ al. Chatgpt: A study on
    its utility for ubiquitous software engineering tasks. arXiv preprint arXiv:2305.16837,
    2023.'
  id: totrans-2268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SM^+ [23] å‰é‡Œæ™®æ‹‰è¨å¾·Â·æ–¯é‡Œè¾¾æ‹‰ã€è‹æ‹‰å¤«Â·é©¬ç¥–å§†è¾¾ç­‰ã€‚ChatGPTï¼šå…³äºå…¶åœ¨æ™®éè½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­çš„æ•ˆç”¨çš„ç ”ç©¶ã€‚arXiv é¢„å°æœ¬ arXiv:2305.16837ï¼Œ2023å¹´ã€‚
- en: Spa [23] Jared Spataro. Introducing microsoft 365 copilot â€“ your copilot for
    work, 2023.
  id: totrans-2269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spa [23] è´¾é‡Œå¾·Â·æ–¯å¸•å¡”ç½—ã€‚ä»‹ç»å¾®è½¯ 365 Copilotâ€”â€”ä½ çš„å·¥ä½œåŠ©æ‰‹ï¼Œ2023å¹´ã€‚
- en: SSZ [23] Ritwik Sinha, Zhao Song, and Tianyi Zhou. A mathematical abstraction
    for balancing the trade-off between creativity and reality in large language models.
    arXiv preprint arXiv:2306.02295, 2023.
  id: totrans-2270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SSZ [23] é‡Œç‰¹ç»´å…‹Â·è¾›å“ˆã€èµµæ¾å’Œç”°ä¸€å‘¨ã€‚å¤§è¯­è¨€æ¨¡å‹ä¸­å¹³è¡¡åˆ›é€ åŠ›ä¸ç°å®ä¹‹é—´æƒè¡¡çš„æ•°å­¦æŠ½è±¡ã€‚arXiv é¢„å°æœ¬ arXiv:2306.02295ï¼Œ2023å¹´ã€‚
- en: SWYZ [21] Zhao Song, David Woodruff, Zheng Yu, and Lichen Zhang. Fast sketching
    of polynomial kernels of polynomial degree. In International Conference on Machine
    Learning, pages 9812â€“9823\. PMLR, 2021.
  id: totrans-2271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SWYZ [21] èµµæ¾ã€å¤§å«Â·ä¼å¾·æ‹‰å¤«ã€éƒ‘å®‡å’Œææ™¨å¼ ã€‚å¤šé¡¹å¼æ ¸çš„å¤šé¡¹å¼åº¦çš„å¿«é€Ÿè‰å›¾ã€‚å‘è¡¨äºå›½é™…æœºå™¨å­¦ä¹ å¤§ä¼šï¼Œé¡µç  9812â€“9823ã€‚PMLRï¼Œ2021å¹´ã€‚
- en: SWZ [17] Zhao Song, DavidÂ P Woodruff, and Peilin Zhong. Low rank approximation
    with entrywise l1-norm error. In Proceedings of the 49th Annual ACM SIGACT Symposium
    on Theory of Computing, pages 688â€“701, 2017.
  id: totrans-2272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SWZ [17] èµµæ¾ã€å¤§å«Â·PÂ·ä¼å¾·æ‹‰å¤«å’Œé’Ÿä½©æ—ã€‚å¸¦æœ‰é€é¡¹ l1 èŒƒæ•°è¯¯å·®çš„ä½ç§©è¿‘ä¼¼ã€‚å‘è¡¨äºç¬¬49å±Šå¹´åº¦ ACM SIGACT ç†è®ºè®¡ç®—æœºç§‘å­¦ç ”è®¨ä¼šï¼Œé¡µç 
    688â€“701ï¼Œ2017å¹´ã€‚
- en: SWZ [19] Zhao Song, DavidÂ P Woodruff, and Peilin Zhong. Relative error tensor
    low rank approximation. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium
    on Discrete Algorithms, pages 2772â€“2789\. SIAM, 2019.
  id: totrans-2273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SWZ [19] èµµæ¾ã€å¤§å«Â·PÂ·ä¼å¾·æ‹‰å¤«å’Œé’Ÿä½©æ—ã€‚ç›¸å¯¹è¯¯å·®å¼ é‡ä½ç§©è¿‘ä¼¼ã€‚å‘è¡¨äºç¬¬ä¸‰åå±Šå¹´åº¦ ACM-SIAM ç¦»æ•£ç®—æ³•ç ”è®¨ä¼šï¼Œé¡µç  2772â€“2789ã€‚SIAMï¼Œ2019å¹´ã€‚
- en: SXZ [22] Zhao Song, Zhaozhuo Xu, and Lichen Zhang. Speeding up sparsification
    using inner product search data structures. arXiv preprint arXiv:2204.03209, 2022.
  id: totrans-2274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SXZ [22] èµµæ¾ã€èµµå“æ—­å’Œææ™¨å¼ ã€‚åˆ©ç”¨å†…ç§¯æœç´¢æ•°æ®ç»“æ„åŠ é€Ÿç¨€ç–åŒ–ã€‚arXiv é¢„å°æœ¬ arXiv:2204.03209ï¼Œ2022å¹´ã€‚
- en: SY [21] Zhao Song and Zheng Yu. Oblivious sketching-based central path method
    for solving linear programming problems. In 38th International Conference on Machine
    Learning (ICML), 2021.
  id: totrans-2275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SY [21] èµµæ¾å’Œéƒ‘å®‡ã€‚åŸºäºéšå¼è‰å›¾çš„ä¸­å¿ƒè·¯å¾„æ–¹æ³•è§£å†³çº¿æ€§è§„åˆ’é—®é¢˜ã€‚å‘è¡¨äºç¬¬38å±Šå›½é™…æœºå™¨å­¦ä¹ å¤§ä¼šï¼ˆICMLï¼‰ï¼Œ2021å¹´ã€‚
- en: SYYZ [23] Zhao Song, Mingquan Ye, Junze Yin, and Lichen Zhang. Efficient alternating
    minimization with applications to weighted low rank approximation. arXiv preprint
    arXiv:2306.04169, 2023.
  id: totrans-2276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SYYZ [23] èµµæ¾ã€æ˜å…¨å¶ã€å›æ³½å°¹å’Œææ™¨å¼ ã€‚å¸¦æœ‰åŠ æƒä½ç§©è¿‘ä¼¼åº”ç”¨çš„é«˜æ•ˆäº¤æ›¿æœ€å°åŒ–ã€‚arXiv é¢„å°æœ¬ arXiv:2306.04169ï¼Œ2023å¹´ã€‚
- en: SYZ [21] Zhao Song, Shuo Yang, and Ruizhe Zhang. Does preprocessing help training
    over-parameterized neural networks? Advances in Neural Information Processing
    Systems, 34:22890â€“22904, 2021.
  id: totrans-2277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SYZ [21] èµµæ¾ã€æ¨ç¡•å’Œå¼ ç‘å“²ã€‚é¢„å¤„ç†æ˜¯å¦æœ‰åŠ©äºè®­ç»ƒè¿‡å‚æ•°åŒ–ç¥ç»ç½‘ç»œï¼Ÿã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹ï¼Œ34:22890â€“22904ï¼Œ2021å¹´ã€‚
- en: '[170] Zhao Song, Mingquan Ye, and Lichen Zhang. Streaming semidefinite programs:
    $o(\sqrt{n})$ passes, small space and fast runtime. Manuscript, 2023.'
  id: totrans-2278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] èµµæ¾ã€å¶åæ³‰å’Œå¼ ç«‹è¾°ã€‚æµå¼åŠæ­£å®šç¨‹åºï¼š$o(\sqrt{n})$ æ¬¡é€šè¿‡ã€å°ç©ºé—´å’Œå¿«é€Ÿè¿è¡Œæ—¶é—´ã€‚æ‰‹ç¨¿ï¼Œ2023å¹´ã€‚'
- en: '[171] Zhao Song, Junze Yin, and Lichen Zhang. Solving attention kernel regression
    problem via pre-conditioner. arXiv preprint arXiv:2308.14304, 2023.'
  id: totrans-2279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] èµµæ¾ã€å°¹ä¿Šæ³½å’Œå¼ ç«‹è¾°ã€‚é€šè¿‡é¢„æ¡ä»¶å™¨è§£å†³æ³¨æ„åŠ›æ ¸å›å½’é—®é¢˜ã€‚arXiv é¢„å°æœ¬ arXiv:2308.14304ï¼Œ2023å¹´ã€‚'
- en: SZKS [21] Charlie Snell, Ruiqi Zhong, Dan Klein, and Jacob Steinhardt. Approximating
    how single head attention learns. arXiv preprint arXiv:2103.07601, 2021.
  id: totrans-2280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SZKS [21] æŸ¥ç†Â·æ–¯å†…å°”ã€é’Ÿç‘å¥‡ã€ä¸¹Â·å…‹è±å› å’Œé›…å„å¸ƒÂ·æ–¯å¦èµ«ç‰¹ã€‚è¿‘ä¼¼å•å¤´æ³¨æ„åŠ›å¦‚ä½•å­¦ä¹ ã€‚arXiv é¢„å°æœ¬ arXiv:2103.07601ï¼Œ2021å¹´ã€‚
- en: SZZ [21] Zhao Song, Lichen Zhang, and Ruizhe Zhang. Training multi-layer over-parametrized
    neural network in subquadratic time. arXiv preprint arXiv:2112.07628, 2021.
  id: totrans-2281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SZZ [21] èµµæ¾ã€å¼ ç«‹è¾°å’Œå¼ ç‘å“²ã€‚ä»¥äºšå¹³æ–¹æ—¶é—´è®­ç»ƒå¤šå±‚è¿‡å‚æ•°åŒ–ç¥ç»ç½‘ç»œã€‚arXiv é¢„å°æœ¬ arXiv:2112.07628ï¼Œ2021å¹´ã€‚
- en: 'TLI^+ [23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
    Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro,
    Faisal Azhar, etÂ al. Llama: Open and efficient foundation language models. arXiv
    preprint arXiv:2302.13971, 2023.'
  id: totrans-2282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TLI^+ [23] ä¼‘æˆˆÂ·å›¾å¤«é¾™ã€æåšÂ·æ‹‰å¤«é‡Œå°”ã€æˆˆè’‚åŸƒÂ·ä¼Šæ‰å¡å°”ã€æ³½ç»´å°”Â·é©¬å°”è’‚å†…ã€ç›ä¸½-å®‰Â·æ‹‰è‚–ã€è’‚è«æ³°Â·æ‹‰å…‹é²ç“¦ã€å·´è’‚æ–¯ç‰¹Â·ç½—å…¹è€¶ã€çº³æ›¼Â·æˆˆäºšå°”ã€åŸƒé‡Œå…‹Â·æ±‰å¸ƒç½—ã€è´¹è¨å°”Â·é˜¿æ‰å°”ç­‰ã€‚Llamaï¼šå¼€æ”¾å’Œé«˜æ•ˆçš„åŸºç¡€è¯­è¨€æ¨¡å‹ã€‚arXiv
    é¢„å°æœ¬ arXiv:2302.13971ï¼Œ2023å¹´ã€‚
- en: TLTO [23] DavoudÂ Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, and Samet
    Oymak. Transformers as support vector machines. arXiv preprint arXiv:2308.16898,
    2023.
  id: totrans-2283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TLTO [23] è¾¾ä¹Œå¾·Â·é˜¿å¡”ä¼ŠÂ·å¡”å°”æ‰çº³èµ«ã€æé¢–èªã€å…‹é‡Œæ–¯æ‰˜æ–¯Â·è¨å§†æ™®åˆ©è¿ªæ–¯å’Œè¨æ¢…ç‰¹Â·å¥¥ä¼Šé©¬å…‹ã€‚å°†å˜æ¢å™¨ä½œä¸ºæ”¯æŒå‘é‡æœºã€‚arXiv é¢„å°æœ¬ arXiv:2308.16898ï¼Œ2023å¹´ã€‚
- en: 'TMS^+ [23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    etÂ al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  id: totrans-2284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TMS^+ [23] ä¼‘æˆˆÂ·å›¾å¤«é¾™ã€è·¯æ˜“æ–¯Â·é©¬ä¸ã€å‡¯æ–‡Â·æ–¯é€šã€å½¼å¾—Â·é˜¿å°”ä¼¯ç‰¹ã€é˜¿å§†è´¾å¾·Â·é˜¿å°”é©¬å“ˆä¼Šé‡Œã€é›…æ–¯æ•Â·å·´å·´åŸƒã€å°¼å¤æ‹‰Â·å·´ä»€åˆ©ç§‘å¤«ã€è‹å§†äºšÂ·å·´ç‰¹æ‹‰ã€æ™®æ‹‰å‰ç“¦å°”Â·å·´å°”åŠ ç“¦ã€èˆ’å°”æÂ·åšè¨å°”ç­‰ã€‚Llama
    2ï¼šå¼€æ”¾åŸºç¡€å’Œå¾®è°ƒçš„èŠå¤©æ¨¡å‹ã€‚arXiv é¢„å°æœ¬ arXiv:2307.09288ï¼Œ2023å¹´ã€‚
- en: VCC^+ [17] Petar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero,
    Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903,
    2017.
  id: totrans-2285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VCC^+ [17] ä½©å¡”å°”Â·ç»´åˆ©å¥‡ç§‘ç»´å¥‡ã€å‰åˆ—å§†Â·åº“åº“é²ã€é˜¿å…°èÂ·å¡è¨è¯ºç“¦ã€é˜¿å¾·é‡Œå®‰å¨œÂ·ç½—æ¢…ç½—ã€çš®ç‰¹ç½—Â·åˆ©å¥¥å’Œçº¦ä¹¦äºšÂ·æœ¬å‰å¥¥ã€‚å›¾æ³¨æ„åŠ›ç½‘ç»œã€‚arXiv
    é¢„å°æœ¬ arXiv:1710.10903ï¼Œ2017å¹´ã€‚
- en: VKR [19] Tomas Vaskevicius, Varun Kanade, and Patrick Rebeschini. Implicit regularization
    for optimal sparse recovery. Advances in Neural Information Processing Systems,
    32, 2019.
  id: totrans-2286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VKR [19] æ‰˜é©¬æ–¯Â·ç“¦æ–¯å‡¯ç»´ä¿®æ–¯ã€ç“¦ä¼¦Â·å¡çº³å¾·å’Œå¸•ç‰¹é‡Œå…‹Â·é›·è´ä»€å°¼ã€‚ç”¨äºæœ€ä¼˜ç¨€ç–æ¢å¤çš„éšå¼æ­£åˆ™åŒ–ã€‚ã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹ï¼Œ32ï¼Œ2019å¹´ã€‚
- en: VSP^+ [17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
    Jones, AidanÂ N Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you
    need. Advances in neural information processing systems, 30, 2017.
  id: totrans-2287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VSP^+ [17] é˜¿å¸Œä»€Â·ç“¦æ–¯ç“¦å°¼ã€è¯ºå§†Â·æ²™æ³½å°”ã€å°¼åŸºÂ·å¸•å°”é©¬å°”ã€é›…å„å¸ƒÂ·ä¹Œæ–¯ç§‘é›·ç‰¹ã€åˆ©æ˜‚Â·ç¼æ–¯ã€è‰¾ä¸¹Â·NÂ·æˆˆéº¦æ–¯ã€å¢å¡æ–¯Â·å‡¯ç‘Ÿå’Œä¼Šåˆ©äºšÂ·æ³¢æ´›è‹é‡‘ã€‚æ³¨æ„åŠ›æ˜¯ä½ æ‰€éœ€è¦çš„ä¸€åˆ‡ã€‚ã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹ï¼Œ30ï¼Œ2017å¹´ã€‚
- en: WGL^+ [20] Blake Woodworth, Suriya Gunasekar, JasonÂ D Lee, Edward Moroshko,
    Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and rich
    regimes in overparametrized models. In Conference on Learning Theory, pages 3635â€“3673\.
    PMLR, 2020.
  id: totrans-2288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WGL^+ [20] å¸ƒé›·å…‹Â·ä¼å¾·æ²ƒæ–¯ã€è‹é‡ŒäºšÂ·å¤çº³å¡å¡ã€æ°æ£®Â·DÂ·æã€çˆ±å¾·åÂ·è«ç½—ä»€ç§‘ã€ä½©å¾·ç½—Â·è¨ç“¦é›·æ–¯ã€ä¼Šæ³°Â·æˆˆå…°ã€ä¸¹å°¼å°”Â·è‹å¾·é‡Œå’Œå†…æ£®Â·æ–¯é›·å¸ƒç½—ã€‚è¿‡å‚æ•°åŒ–æ¨¡å‹ä¸­çš„æ ¸å’Œä¸°å¯Œæœºåˆ¶ã€‚åœ¨å­¦ä¹ ç†è®ºä¼šè®®ä¸Šï¼Œç¬¬3635â€“3673é¡µã€‚PMLRï¼Œ2020å¹´ã€‚
- en: 'WHH^+ [23] Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong
    Wang, Linyi Yang, Haojun Huang, Wei Ye, Xiubo Geng, etÂ al. On the robustness of
    chatgpt: An adversarial and out-of-distribution perspective. arXiv preprint arXiv:2302.12095,
    2023.'
  id: totrans-2289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WHH^+ [23] æ±ªè¿›ä¸œã€èƒ¡å¸Œæ—­ã€ä¾¯æ–‡é‘«ã€é™ˆæµ©ã€éƒ‘æ¶¦å‡¯ã€ç‹äº¦ä¸œã€æ¨æ—æ¯…ã€é»„æµ©ä¿Šã€å¶å¨ã€è€¿è¥¿åšç­‰ã€‚å…³äº ChatGPT çš„é²æ£’æ€§ï¼šä¸€ç§å¯¹æŠ—æ€§å’Œåˆ†å¸ƒå¤–çš„è§†è§’ã€‚arXiv
    é¢„å°æœ¬ arXiv:2302.12095ï¼Œ2023å¹´ã€‚
- en: Wil [12] VirginiaÂ Vassilevska Williams. Multiplying matrices faster than coppersmith-winograd.
    In Proceedings of the forty-fourth annual ACM symposium on Theory of computing,
    pages 887â€“898, 2012.
  id: totrans-2290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wil [12] å¼—å‰å°¼äºšÂ·ç“¦è¥¿åˆ—å¤«æ–¯å¡Â·å¨å»‰å§†æ–¯ã€‚æ¯” Coppersmith-Winograd æ›´å¿«çš„çŸ©é˜µä¹˜æ³•ã€‚åœ¨ç¬¬å››åå››å±Šå¹´åº¦ ACM è®¡ç®—ç†è®ºç ”è®¨ä¼šè®ºæ–‡é›†ä¸­ï¼Œç¬¬887â€“898é¡µï¼Œ2012å¹´ã€‚
- en: WLJ^+ [23] Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming
    Shi, and Zhaopeng Tu. Document-level machine translation with large language models.
    arXiv preprint arXiv:2304.02210, 2023.
  id: totrans-2291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WLJ^+ [23] ç‹é¾™è¶Šã€å•æ™¨é˜³ã€å­£å¤©åšã€å¼ å¿—ç‘ã€ä½™ç”µå’ŒçŸ³æ ‘é“­ã€‚ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ–‡æ¡£çº§æœºå™¨ç¿»è¯‘ã€‚arXivé¢„å°æœ¬ arXiv:2304.02210ï¼Œ2023å¹´ã€‚
- en: 'WLL^+ [23] Tianwen Wei, Jian Luan, Wei Liu, Shuang Dong, and Bin Wang. Cmath:
    Can your language model pass chinese elementary school math test? arXiv preprint
    arXiv:2306.16636, 2023.'
  id: totrans-2292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WLL^+ [23] ç”°æ–‡ä¼Ÿã€æ®µå‰‘ã€åˆ˜ä¼Ÿã€è‘£çˆ½å’Œç‹æ–Œã€‚Cmathï¼šä½ çš„è¯­è¨€æ¨¡å‹èƒ½é€šè¿‡ä¸­æ–‡å°å­¦æ•°å­¦æµ‹è¯•å—ï¼ŸarXivé¢„å°æœ¬ arXiv:2306.16636ï¼Œ2023å¹´ã€‚
- en: WMCL [21] Bohan Wang, QiÂ Meng, Wei Chen, and Tie-Yan Liu. The implicit bias
    for adaptive optimization algorithms on homogeneous neural networks. In International
    Conference on Machine Learning, pages 10849â€“10858\. PMLR, 2021.
  id: totrans-2293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WMCL [21] ç‹åšæ¶µã€å­Ÿçªã€é™ˆä¼Ÿå’Œåˆ˜é“å²©ã€‚å‡è´¨ç¥ç»ç½‘ç»œä¸Šè‡ªé€‚åº”ä¼˜åŒ–ç®—æ³•çš„éšæ€§åå·®ã€‚åœ¨å›½é™…æœºå™¨å­¦ä¹ å¤§ä¼šä¸Šï¼Œé¡µ10849â€“10858ã€‚PMLRï¼Œ2021å¹´ã€‚
- en: WMZ^+ [21] Bohan Wang, QiÂ Meng, Huishuai Zhang, Ruoyu Sun, Wei Chen, and Zhi-Ming
    Ma. Momentum doesnâ€™t change the implicit bias. ., 2021.
  id: totrans-2294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WMZ^+ [21] ç‹åšæ¶µã€å­Ÿçªã€å¼ æƒ å¸…ã€å­™è‹¥æ„šã€é™ˆä¼Ÿå’Œé©¬å¿—é“­ã€‚åŠ¨é‡å¹¶ä¸ä¼šæ”¹å˜éšæ€§åå·®ã€‚ã€‚ï¼Œ2021å¹´ã€‚
- en: WQR^+ [23] Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin AkyÃ¼rek, Boyuan Chen, Bailin
    Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. Reasoning or reciting? exploring
    the capabilities and limitations of language models through counterfactual tasks.
    arXiv preprint arXiv:2307.02477, 2023.
  id: totrans-2295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WQR^+ [23] å´å…†å³°ã€é‚±ç³ç’ã€äºšå†å…‹è¥¿æ–¯Â·ç½—æ–¯ã€Ekin AkyÃ¼rekã€é™ˆåšè¿œã€ç‹ç™½æ—ã€é‡‘å¨œè£ã€é›…å„å¸ƒÂ·å®‰å¾·çƒˆäºšæ–¯ å’Œ é‡‘äº‘ã€‚æ¨ç†è¿˜æ˜¯èƒŒè¯µï¼Ÿé€šè¿‡åäº‹å®ä»»åŠ¡æ¢ç´¢è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›å’Œå±€é™æ€§ã€‚arXivé¢„å°æœ¬
    arXiv:2307.02477ï¼Œ2023å¹´ã€‚
- en: WWZ^+ [22] Xiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan Liu, and
    Juanzi Li. Finding skill neurons in pre-trained transformer-based language models.
    arXiv preprint arXiv:2211.07349, 2022.
  id: totrans-2296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WWZ^+ [22] ç‹æ™“æ™ºã€æ¸©å‡¯å²³ã€å¼ æ­£å²©ã€ä¾¯ç£Šã€åˆ˜å¿—è¿œå’Œæå¨Ÿå­ã€‚å¯»æ‰¾é¢„è®­ç»ƒå˜æ¢å™¨æ¨¡å‹ä¸­çš„æŠ€èƒ½ç¥ç»å…ƒã€‚arXivé¢„å°æœ¬ arXiv:2211.07349ï¼Œ2022å¹´ã€‚
- en: 'WXXZ [23] VirginiaÂ Vassilevska Williams, Yinzhan Xu, Zixuan Xu, and Renfei
    Zhou. New bounds for matrix multiplication: from alpha to omega, 2023.'
  id: totrans-2297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WXXZ [23] Virginia Vassilevska Williamsã€è®¸å¯…ç»ã€è®¸å­è½©å’Œå‘¨ä»é£ã€‚çŸ©é˜µä¹˜æ³•çš„æ–°ç•Œé™ï¼šä»alphaåˆ°omegaï¼Œ2023å¹´ã€‚
- en: WZ [16] DavidÂ P Woodruff and Peilin Zhong. Distributed low rank approximation
    of implicit functions of a matrix. In 2016 IEEE 32nd International Conference
    on Data Engineering (ICDE), pages 847â€“858\. IEEE, 2016.
  id: totrans-2298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WZ [16] David P Woodruff å’Œé’ŸåŸ¹æ—ã€‚çŸ©é˜µéšå‡½æ•°çš„åˆ†å¸ƒå¼ä½ç§©è¿‘ä¼¼ã€‚åœ¨2016 IEEEç¬¬32å±Šæ•°æ®å·¥ç¨‹å›½é™…ä¼šè®®ï¼ˆICDEï¼‰ä¸Šï¼Œé¡µ847â€“858ã€‚IEEEï¼Œ2016å¹´ã€‚
- en: 'WZD^+ [20] Ruosong Wang, Peilin Zhong, SimonÂ S Du, RussÂ R Salakhutdinov, and
    Lin Yang. Planning with general objective functions: Going beyond total rewards.
    Advances in Neural Information Processing Systems, 33:14486â€“14497, 2020.'
  id: totrans-2299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WZD^+ [20] ç‹è‹¥æ¾ã€é’ŸåŸ¹æ—ã€Simon S Duã€Russ R Salakhutdinov å’Œæ¨æ—ã€‚ä½¿ç”¨é€šç”¨ç›®æ ‡å‡½æ•°è¿›è¡Œè§„åˆ’ï¼šè¶…è¶Šæ€»å¥–åŠ±ã€‚ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ï¼Œ33:14486â€“14497ï¼Œ2020å¹´ã€‚
- en: 'XBK^+ [15] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville,
    Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural
    image caption generation with visual attention. In International conference on
    machine learning, pages 2048â€“2057\. PMLR, 2015.'
  id: totrans-2300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XBK^+ [15] è®¸å‡¯æ–‡ã€Jimmy Baã€Ryan Kirosã€Cho Kyunghyunã€Aaron Courvilleã€Ruslan Salakhudinovã€Rich
    Zemel å’Œ Yoshua Bengioã€‚å±•ç¤ºã€å…³æ³¨å¹¶è®²è¿°ï¼šå…·æœ‰è§†è§‰æ³¨æ„çš„ç¥ç»å›¾åƒæè¿°ç”Ÿæˆã€‚åœ¨å›½é™…æœºå™¨å­¦ä¹ å¤§ä¼šä¸Šï¼Œé¡µ2048â€“2057ã€‚PMLRï¼Œ2015å¹´ã€‚
- en: XLH^+ [23] Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, and Erik
    Cambria. Are large language models really good logical reasoners? a comprehensive
    evaluation from deductive, inductive and abductive views. arXiv preprint arXiv:2306.09841,
    2023.
  id: totrans-2301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XLH^+ [23] è®¸æ–¹ä¹‹ã€æ—å¯å¡ã€éŸ©å˜‰ä¼Ÿã€èµµå¤©å“²ã€åˆ˜å†› å’Œ Erik Cambriaã€‚å¤§å‹è¯­è¨€æ¨¡å‹çœŸçš„å¾ˆæ“…é•¿é€»è¾‘æ¨ç†å—ï¼Ÿä»æ¼”ç»ã€å½’çº³å’Œæº¯å› è§†è§’çš„å…¨é¢è¯„ä¼°ã€‚arXivé¢„å°æœ¬
    arXiv:2306.09841ï¼Œ2023å¹´ã€‚
- en: XQP^+ [22] Shuo Xie, Jiahao Qiu, Ankita Pasad, LiÂ Du, Qing Qu, and Hongyuan
    Mei. Hidden state variability of pretrained language models can guide computation
    reduction for transfer learning. arXiv preprint arXiv:2210.10041, 2022.
  id: totrans-2302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XQP^+ [22] è°¢ç¡•ã€é‚±å®¶æµ©ã€Ankita Pasadã€æœä¸½ã€æ›²é’å’Œæ¢…æ´ªæºã€‚é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„éšè—çŠ¶æ€å˜å¼‚æ€§å¯ä»¥æŒ‡å¯¼è¿ç§»å­¦ä¹ ä¸­çš„è®¡ç®—å‡å°‘ã€‚arXivé¢„å°æœ¬
    arXiv:2210.10041ï¼Œ2022å¹´ã€‚
- en: XSS [21] Zhaozhuo Xu, Zhao Song, and Anshumali Shrivastava. Breaking the linear
    iteration cost barrier for some well-known conditional gradient methods using
    maxip data-structures. Advances in Neural Information Processing Systems, 34:5576â€“5589,
    2021.
  id: totrans-2303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XSS [21] è®¸å…†å“ã€å®‹èµµå’Œå®‰èˆ’é©¬åˆ©Â·æ–½é‡Œç“¦æ–¯å¡”ç“¦ã€‚ä½¿ç”¨maxipæ•°æ®ç»“æ„æ‰“ç ´ä¸€äº›è‘—åæ¡ä»¶æ¢¯åº¦æ–¹æ³•çš„çº¿æ€§è¿­ä»£æˆæœ¬éšœç¢ã€‚ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ï¼Œ34:5576â€“5589ï¼Œ2021å¹´ã€‚
- en: XSS [23] Zhaozhuo Xu, Zhao Song, and Anshumali Shrivastava. A tale of two efficient
    value iteration algorithms for solving linear mdps with large action space. In
    AISTATS, 2023.
  id: totrans-2304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XSS [23] Zhaozhuo Xuã€Zhao Song å’Œ Anshumali Shrivastavaã€‚è§£å†³å…·æœ‰å¤§åŠ¨ä½œç©ºé—´çš„çº¿æ€§ MDPs çš„ä¸¤ä¸ªé«˜æ•ˆä»·å€¼è¿­ä»£ç®—æ³•çš„æ•…äº‹ã€‚AISTATS,
    2023ã€‚
- en: 'XZZ [18] Chang Xiao, Peilin Zhong, and Changxi Zheng. Bourgan: Generative networks
    with metric embeddings. Advances in neural information processing systems, 31,
    2018.'
  id: totrans-2305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XZZ [18] Chang Xiaoã€Peilin Zhong å’Œ Changxi Zhengã€‚Bourganï¼šå…·æœ‰åº¦é‡åµŒå…¥çš„ç”Ÿæˆç½‘ç»œã€‚ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ï¼Œ31,
    2018ã€‚
- en: YKM [20] Chulhee Yun, Shankar Krishnan, and Hossein Mobahi. A unifying view
    on implicit bias in training linear neural networks. arXiv preprint arXiv:2010.02501,
    2020.
  id: totrans-2306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YKM [20] Chulhee Yunã€Shankar Krishnan å’Œ Hossein Mobahiã€‚å…³äºè®­ç»ƒçº¿æ€§ç¥ç»ç½‘ç»œä¸­çš„éšæ€§åå·®çš„ç»Ÿä¸€è§†è§’ã€‚arXiv
    é¢„å°æœ¬ arXiv:2010.02501, 2020ã€‚
- en: 'ZAG^+ [23] Wenxuan Zhang, SharifahÂ Mahani Aljunied, Chang Gao, YewÂ Ken Chia,
    and Lidong Bing. M3exam: A multilingual, multimodal, multilevel benchmark for
    examining large language models. arXiv preprint arXiv:2306.05179, 2023.'
  id: totrans-2307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZAG^+ [23] Wenxuan Zhangã€Sharifah Mahani Aljuniedã€Chang Gaoã€Yew Ken Chia å’Œ Lidong
    Bingã€‚M3examï¼šä¸€ä¸ªå¤šè¯­è¨€ã€å¤šæ¨¡æ€ã€å¤šå±‚æ¬¡çš„åŸºå‡†ï¼Œç”¨äºæ£€éªŒå¤§è¯­è¨€æ¨¡å‹ã€‚arXiv é¢„å°æœ¬ arXiv:2306.05179, 2023ã€‚
- en: 'ZDL^+ [23] Wenxuan Zhang, Yue Deng, Bing Liu, SinnoÂ Jialin Pan, and Lidong
    Bing. Sentiment analysis in the era of large language models: A reality check.
    arXiv preprint arXiv:2305.15005, 2023.'
  id: totrans-2308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZDL^+ [23] Wenxuan Zhangã€Yue Dengã€Bing Liuã€Sinno Jialin Pan å’Œ Lidong Bingã€‚å¤§è¯­è¨€æ¨¡å‹æ—¶ä»£çš„æƒ…æ„Ÿåˆ†æï¼šç°å®æ£€éªŒã€‚arXiv
    é¢„å°æœ¬ arXiv:2305.15005, 2023ã€‚
- en: ZG [19] Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized
    deep neural networks. Advances in neural information processing systems, 32, 2019.
  id: totrans-2309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZG [19] Difan Zou å’Œ Quanquan Guã€‚è®­ç»ƒè¶…å‚æ•°åŒ–æ·±åº¦ç¥ç»ç½‘ç»œçš„æ”¹è¿›åˆ†æã€‚ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ï¼Œ32, 2019ã€‚
- en: 'Zha [22] Lichen Zhang. Speeding up optimizations via data structures: Faster
    search, sample and maintenance. Masterâ€™s thesis, Carnegie Mellon University, 2022.'
  id: totrans-2310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zha [22] Lichen Zhangã€‚é€šè¿‡æ•°æ®ç»“æ„åŠ é€Ÿä¼˜åŒ–ï¼šæ›´å¿«çš„æœç´¢ã€é‡‡æ ·å’Œç»´æŠ¤ã€‚ç¡•å£«è®ºæ–‡ï¼Œå¡å†…åŸºæ¢…éš†å¤§å­¦ï¼Œ2022ã€‚
- en: 'ZHDK [23] Amir Zandieh, Insu Han, Majid Daliri, and Amin Karbasi. Kdeformer:
    Accelerating transformers via kernel density estimation. arXiv preprint arXiv:2302.02451,
    2023.'
  id: totrans-2311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZHDK [23] Amir Zandiehã€Insu Hanã€Majid Daliri å’Œ Amin Karbasiã€‚Kdeformerï¼šé€šè¿‡æ ¸å¯†åº¦ä¼°è®¡åŠ é€Ÿå˜æ¢å™¨ã€‚arXiv
    é¢„å°æœ¬ arXiv:2302.02451, 2023ã€‚
- en: 'ZHL^+ [23] Eric Zelikman, Qian Huang, Percy Liang, Nick Haber, and NoahÂ D Goodman.
    Just one byte (per gradient): A note on low-bandwidth decentralized language model
    finetuning using shared randomness. arXiv preprint arXiv:2306.10015, 2023.'
  id: totrans-2312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZHL^+ [23] Eric Zelikmanã€Qian Huangã€Percy Liangã€Nick Haber å’Œ Noah D Goodmanã€‚æ¯ä¸ªæ¢¯åº¦åªæœ‰ä¸€ä¸ªå­—èŠ‚ï¼šå…³äºä½¿ç”¨å…±äº«éšæœºæ€§è¿›è¡Œä½å¸¦å®½åˆ†å¸ƒå¼è¯­è¨€æ¨¡å‹å¾®è°ƒçš„è¯´æ˜ã€‚arXiv
    é¢„å°æœ¬ arXiv:2306.10015, 2023ã€‚
- en: ZKV^+ [20] Jingzhao Zhang, SaiÂ Praneeth Karimireddy, Andreas Veit, Seungyeon
    Kim, Sashank Reddi, Sanjiv Kumar, and Suvrit Sra. Why are adaptive methods good
    for attention models? Advances in Neural Information Processing Systems, 33:15383â€“15393,
    2020.
  id: totrans-2313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZKV^+ [20] Jingzhao Zhangã€Sai Praneeth Karimireddyã€Andreas Veitã€Seungyeon Kimã€Sashank
    Reddiã€Sanjiv Kumar å’Œ Suvrit Sraã€‚ä¸ºä»€ä¹ˆè‡ªé€‚åº”æ–¹æ³•å¯¹æ³¨æ„åŠ›æ¨¡å‹æœ‰å¥½å¤„ï¼Ÿç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ï¼Œ33:15383â€“15393, 2020ã€‚
- en: 'ZPD^+ [20] YiÂ Zhang, Orestis Plevrakis, SimonÂ S Du, Xingguo Li, Zhao Song,
    and Sanjeev Arora. Over-parameterized adversarial training: An analysis overcoming
    the curse of dimensionality. Advances in Neural Information Processing Systems,
    33:679â€“688, 2020.'
  id: totrans-2314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZPD^+ [20] Yi Zhangã€Orestis Plevrakisã€Simon S Duã€Xingguo Liã€Zhao Song å’Œ Sanjeev
    Aroraã€‚è¶…å‚æ•°åŒ–å¯¹æŠ—è®­ç»ƒï¼šä¸€ç§å…‹æœç»´åº¦ç¾éš¾çš„åˆ†æã€‚ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ï¼Œ33:679â€“688, 2020ã€‚
- en: ZPD^+ [23] Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan Li, Ngai-Man
    Cheung, and Min Lin. On evaluating adversarial robustness of large vision-language
    models. arXiv preprint arXiv:2305.16934, 2023.
  id: totrans-2315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZPD^+ [23] Yunqing Zhaoã€Tianyu Pangã€Chao Duã€Xiao Yangã€Chongxuan Liã€Ngai-Man
    Cheung å’Œ Min Linã€‚è¯„ä¼°å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯¹æŠ—é²æ£’æ€§ã€‚arXiv é¢„å°æœ¬ arXiv:2305.16934, 2023ã€‚
- en: ZPGA [23] Haoyu Zhao, Abhishek Panigrahi, Rong Ge, and Sanjeev Arora. Do transformers
    parse while predicting the masked word? arXiv preprint arXiv:2303.08117, 2023.
  id: totrans-2316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZPGA [23] Haoyu Zhaoã€Abhishek Panigrahiã€Rong Ge å’Œ Sanjeev Aroraã€‚å˜æ¢å™¨åœ¨é¢„æµ‹æ©ç è¯æ—¶æ˜¯å¦è¿›è¡Œè§£æï¼ŸarXiv
    é¢„å°æœ¬ arXiv:2303.08117, 2023ã€‚
- en: 'ZRG^+ [22] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen,
    Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, XiÂ Victoria Lin, etÂ al. Opt:
    Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068,
    2022.'
  id: totrans-2317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZRG^+ [22] Susan Zhangã€Stephen Rollerã€Naman Goyalã€Mikel Artetxeã€Moya Chenã€Shuohui
    Chenã€Christopher Dewanã€Mona Diabã€Xian Liã€Xi Victoria Lin ç­‰ã€‚Optï¼šå¼€æ”¾çš„é¢„è®­ç»ƒå˜æ¢å™¨è¯­è¨€æ¨¡å‹ã€‚arXiv
    é¢„å°æœ¬ arXiv:2205.01068, 2022ã€‚
- en: ZWB^+ [21] Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, DeanÂ P Foster,
    and Sham Kakade. The benefits of implicit regularization from sgd in least squares
    problems. Advances in neural information processing systems, 34:5456â€“5468, 2021.
  id: totrans-2318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZWB^+ [21] Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, Dean P Foster
    å’Œ Sham Kakade. SGD åœ¨æœ€å°äºŒä¹˜é—®é¢˜ä¸­éšå¼æ­£åˆ™åŒ–çš„å¥½å¤„ã€‚ã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹ï¼Œ34:5456â€“5468ï¼Œ2021ã€‚
- en: ZZ [23] Ruizhe Zhang and Xinzhi Zhang. A hyperbolic extension of kadison-singer
    type results. In ICALP, 2023.
  id: totrans-2319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZZ [23] Ruizhe Zhang å’Œ Xinzhi Zhang. ä¸€ç§è¶…æ›²é¢æ‰©å±•çš„ Kadison-Singer ç±»å‹ç»“æœã€‚å‘è¡¨äº ICALPï¼Œ2023ã€‚
